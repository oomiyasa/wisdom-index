description,rationale,use_case,impact_area,transferability_score,actionability_rating,evidence_strength,type_(form),tag_(application),unique?,role,function,company,industry,country,date,source_(interview_#/_name),link,notes
docs: Enhance triggering_asset_event retrieval documentation in DAGs (#52666) **Closes:** #52666 This pull request enhances the Airflow documentation around fetching and using the `triggering_asset_events` context variable in DAGs. It provides clearer guidance and real‐world examples to improve authoring and scheduling workflows. ### What’s Changed * **Jinja Templating Examples** * Added a “Single Triggering Asset” snippet demonstrating how to access the first `AssetEvent` via `{{ triggering_as…,,,,,,Anecdotal,issue,,,,,,,,2025-07-02,github/Programmer-RD-AI,https://github.com/apache/airflow/pull/52674,repo: apache/airflow | keyword: best practice | state: open
"Congratulations on your first Pull Request and welcome to the Apache Airflow community! If you have any issues or are unsure about any anything please check our Contributors' Guide (https://github.com/apache/airflow/blob/main/contributing-docs/README.rst) Here are some useful points: - Pay attention to the quality of your code (ruff, mypy and type annotations). Our [pre-commits]( https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#prerequisites-for-pre-commit…",,,,,,Anecdotal,comment,,,,,,,,2025-07-02,github/boring-cyborg[bot],https://github.com/apache/airflow/pull/52674#issuecomment-3026044587,repo: apache/airflow | issue: docs: Enhance triggering_asset_event retrieval documentation in DAGs (#52666) | keyword: best practice
Looks good! Static checks are failing. Please run pre-commits to fix them. [Documentation](https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst),,,,,,Anecdotal,comment,,,,,,,,2025-07-02,github/vincbeck,https://github.com/apache/airflow/pull/52674#issuecomment-3028903342,repo: apache/airflow | issue: docs: Enhance triggering_asset_event retrieval documentation in DAGs (#52666) | keyword: best practice
Ensure AWS ORM initialization and improve session configuration handling ## Summary This PR introduces two related fixes to stabilize ORM initialization and session handling in AWS-related utilities. ## Changes 1. **Add `_ensure_db_session` in `eks_get_token`** * Introduced `_ensure_db_session()` to explicitly initialize the ORM when `engine` or `Session` are `None`. * Ensures that running `eks_get_token` as a standalone CLI properly initializes Airflow settings before creating the `EksHook`. *…,,,,,,Anecdotal,issue,,,,,,,,2025-08-17,github/viiccwen,https://github.com/apache/airflow/pull/54582,repo: apache/airflow | keyword: best practice | state: open
"The `eks_get_token` tool is used to generate an EKS access token, which is required for interacting with the Kubernetes API. This token is essential for tasks like using the `EksPodOperator` to interact with an EKS cluster. The tool is part of the process that enables Airflow to authenticate and authorize to interact with Kubernetes Pods in an EKS cluster. In this case, `eks_get_token` is a tool that depends on Airflow's ORM to correctly access AWS credentials stored in the metadata database. W…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/viiccwen,https://github.com/apache/airflow/pull/54582#issuecomment-3195763850,repo: apache/airflow | issue: Ensure AWS ORM initialization and improve session configuration handling | keyword: best practice
"Hi @potiuk, I just checked the usage of `eks_get_token`, and it appears to be a special case. The `EksHook.execute` method writes the kubeconfig file using `generate_config_file`: https://github.com/apache/airflow/blob/9b46d76ee4fffdbefa7fa14aba5b7d8a8ec15f3b/providers/amazon/src/airflow/providers/amazon/aws/operators/eks.py#L1067-L1075 The `generate_config_file` function leverages the `eks_get_token` module to generate the kubeconfig via `EksHook.fetch_access_token_for_cluster`: https://github…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/jason810496,https://github.com/apache/airflow/pull/54582#issuecomment-3197427713,repo: apache/airflow | issue: Ensure AWS ORM initialization and improve session configuration handling | keyword: best practice
"> In this case, eks_get_token is a tool that depends on Airflow's ORM to correctly access AWS credentials stored in the metadata database. While Airflow 3 tries to reduce ORM dependency, some actions still require it for specific use cases like reading connection settings. task.sdk already supports retrieving credentials stored in airflow configuration and this is THE way to retrieve it. I assume this command is run internally in the worker, not maually by the user at some point in time, and in…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/potiuk,https://github.com/apache/airflow/pull/54582#issuecomment-3197567000,repo: apache/airflow | issue: Ensure AWS ORM initialization and improve session configuration handling | keyword: best practice
"Simply: unconditional ORM configuration in provider's code run in the worker is just plain wrong. It would only be acceptable if it is run by a human running this command manually after logging in to one of the containers that have database access - scheduler, triggerer, dag file processor. When your code is run in the worker on Airflow 3, you have **no** access to database credentials and ORM initialization will fail.",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/potiuk,https://github.com/apache/airflow/pull/54582#issuecomment-3197574477,repo: apache/airflow | issue: Ensure AWS ORM initialization and improve session configuration handling | keyword: best practice
"I understand your point, and I agree that in Airflow 3 we should not have access to the database. If I am not mistaken about the purpose of the `eks_get_token` module, it is designed to be called directly with the Python executable, rather than being initialized through TaskSDK. Since it is executed by Python directly, and not within executor, triggerer, or similar processes, it operates outside the typical Airflow runtime. https://github.com/apache/airflow/blob/9b46d76ee4fffdbefa7fa14aba5b7d8a…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/jason810496,https://github.com/apache/airflow/pull/54582#issuecomment-3197757508,repo: apache/airflow | issue: Ensure AWS ORM initialization and improve session configuration handling | keyword: best practice
"> If I am not mistaken about the purpose of the `eks_get_token` module, it is designed to be called directly with the Python executable, rather than being initialized through TaskSDK. Since it is executed by Python directly, and not within executor, triggerer, or similar processes, it operates outside the typical Airflow runtime. Well. from what I see this command is executed on the pod within EKS cluster - and it would mean that EKS cluster will have to have DB credentials in order to initiali…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/potiuk,https://github.com/apache/airflow/pull/54582#issuecomment-3198125901,repo: apache/airflow | issue: Ensure AWS ORM initialization and improve session configuration handling | keyword: best practice
"Improving Airflow UI with Accessibility and Best Practices ### Apache Airflow version main (development) ### If ""Other Airflow 2 version"" selected, which one? _No response_ ### What happened? In the [Airflow UI](https://github.com/apache/airflow/tree/main/airflow-core/src/airflow/ui) lacks some important accessibility features such as a proper lang attribute on the <html> tag and semantic navigation landmarks with appropriate ARIA labels. Additionally, iframe sandbox permissions are not explici…",,,,,,Anecdotal,issue,,,,,,,,2025-08-17,github/NithinU2802,https://github.com/apache/airflow/issues/54587,repo: apache/airflow | keyword: best practice | state: open
"I think there are two concerns mixed here: 1) languge 2) accessibility 2) security My suggestion is, if you are going to try to address those issues, is to split it into three, separate PRs. Also on top of the - undoubtedly - AI-generated generic advices, those PRs shoudl contain a little bit more of application of the advices to our specific cases: For 1) and 2), it would be great if you apply in your PRs using of the language translation framework we already have - as you might not realise in…",,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/potiuk,https://github.com/apache/airflow/issues/54587#issuecomment-3194375659,repo: apache/airflow | issue: Improving Airflow UI with Accessibility and Best Practices | keyword: best practice
"Hi @potiuk, Thank you assigning this to me. Thank you for your concerns since I'm new to contributing I had used AI to explain it much better. I'll check up your advice for those changes and looking forward to raise seperate PRs accordingly✌️.",,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/NithinU2802,https://github.com/apache/airflow/issues/54587#issuecomment-3194404670,repo: apache/airflow | issue: Improving Airflow UI with Accessibility and Best Practices | keyword: best practice
"> Hi [@potiuk](https://github.com/potiuk), Thank you assigning this to me. Thank you for your concerns since I'm new to contributing I had used AI to explain it much better. I'll check up your advice for those changes and looking forward to raise seperate PRs accordingly✌️. Sure. And there is nothing wrong with using AI to enhance your skills. We are using it more and more and even had blog post about it https://news.apache.org/foundation/entry/ai-and-open-source-expanding-apache-airflows-globa…",,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/potiuk,https://github.com/apache/airflow/issues/54587#issuecomment-3194410253,repo: apache/airflow | issue: Improving Airflow UI with Accessibility and Best Practices | keyword: best practice
"ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() ### Apache Airflow version 3.0.2 ### If ""Other Airflow 2 version"" selected, which one? _No response_ ### What happened? The exception occurred when the DAG ran with `dag.test()` attempted to retrieve a variable from the API server. Some similar issues have been opened (#48554, #51062, #51316). The PRs provided as a solution (#50300, #50419) were included in 3.0.2 but did not fix the problem. ``` Exception has occurred: ImportErr…",,,,,,Anecdotal,issue,,,,,,,,2025-06-16,github/opeida,https://github.com/apache/airflow/issues/51816,repo: apache/airflow | keyword: best practice | state: open
Yeah it is in an issue. The problem is the dag is already parsed before the `dag.test` code can be called which would assign `task_runner.SUPERVISOR_COMMS = InProcessSupervisorComms`,,,,,,Anecdotal,comment,,,,,,,,2025-06-17,github/kaxil,https://github.com/apache/airflow/issues/51816#issuecomment-2980436635,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: best practice
"Assuming we can detect the case when this is being imported from doing `python mydag.py` (which I'm 95% sure we can, so that is the 'easy' part) how should we actually obtain a value/variable? I.e. ignoring all practicialities or other issues, what _should_ this do, given the increased security model in Airflow 3 of not allowing unfettered DB access?",,,,,,Anecdotal,comment,,,,,,,,2025-06-17,github/ashb,https://github.com/apache/airflow/issues/51816#issuecomment-2980472886,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: best practice
"> Assuming we can detect the case when this is being imported from doing `python mydag.py` (which I'm 95% sure we can, so that is the 'easy' part) how should we actually obtain a value/variable? > > I.e. ignoring all practicialities or other issues, what _should_ this do, given the increased security model in Airflow 3 of not allowing unfettered DB access? Since we allow that in the actual dag parsing models (as it goes via DAG processor -> Supervisor comms -> Variable), we should do the same f…",,,,,,Anecdotal,comment,,,,,,,,2025-06-17,github/kaxil,https://github.com/apache/airflow/issues/51816#issuecomment-2980495661,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: best practice
"@opeida The ideal way would be to not access Variables or Connections at the top of your file (i.e outside Task Context). In your case, you can rewrite the dag as follows: ```py import logging from airflow import DAG from airflow.providers.standard.operators.empty import EmptyOperator from airflow.providers.standard.operators.python import PythonOperator def my_function(my_var: str) -> None: logging.getLogger(__name__).info(my_var) with DAG(""test_dag"") as dag: start = EmptyOperator(task_id=""sta…",,,,,,Anecdotal,comment,,,,,,,,2025-06-17,github/kaxil,https://github.com/apache/airflow/issues/51816#issuecomment-2980512138,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: best practice
"So: detecting when we have top-level dag code, at least in the `python mydagfile.py` case: ```python def __getattr__(name): if name == ""SUPERVISOR_COMMS"" and ""__main__"" in sys.modules: import traceback frames = [ frame for (frame, lnum) in traceback.walk_stack(None) if not frame.f_code.co_filename.startswith(""<frozen importlib."") ] if sys.modules[""__main__""].__file__ == frames[-1].f_code.co_filename: raise RuntimeError(""Top level API access here"") raise AttributeError(f""module {__name__!r} has …",,,,,,Anecdotal,comment,,,,,,,,2025-06-17,github/ashb,https://github.com/apache/airflow/issues/51816#issuecomment-2980679579,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: best practice
"> [@opeida](https://github.com/opeida) The ideal way would be to not access Variables or Connections at the top of your file (i.e outside Task Context) @kaxil are there workarounds to eliminate the use of top-level variables for dynamic DAG generation? We pull data from numerous accounts of a third-party provider. The accounts variable can change dynamically and stores sensitive information including API keys. In all other cases, we utilize Jinja templates within Task Context following Airflow'…",,,,,,Anecdotal,comment,,,,,,,,2025-06-17,github/opeida,https://github.com/apache/airflow/issues/51816#issuecomment-2980811694,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: best practice
"You can just do following in your case: ```py provider_name = ""<provider_name>"" def my_function(my_var: str) -> None: logging.getLogger(__name__).info(my_var) with DAG(""test_dag"") as dag: start = EmptyOperator(task_id=""start"") py_func = PythonOperator( task_id=""py_func"", python_callable=my_function, op_kwargs={ ""my_var"": ""{{ var.value."" + provider_name + ""_accounts }}"" } ) end = EmptyOperator(task_id=""end"") start >> py_func >> end ```",,,,,,Anecdotal,comment,,,,,,,,2025-06-17,github/kaxil,https://github.com/apache/airflow/issues/51816#issuecomment-2980857201,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: best practice
"> So: detecting when we have top-level dag code, at least in the `python mydagfile.py` case: > > def __getattr__(name): > if name == ""SUPERVISOR_COMMS"" and ""__main__"" in sys.modules: > import traceback > > frames = [ > frame > for (frame, lnum) in traceback.walk_stack(None) > if not frame.f_code.co_filename.startswith(""<frozen importlib."") > ] > if sys.modules[""__main__""].__file__ == frames[-1].f_code.co_filename: > raise RuntimeError(""Top level API access here"") > > raise AttributeError(f""modu…",,,,,,Anecdotal,comment,,,,,,,,2025-06-17,github/kaxil,https://github.com/apache/airflow/issues/51816#issuecomment-2980867342,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: best practice
"> py_func = PythonOperator( > task_id=""py_func"", > python_callable=my_function, > op_kwargs={ > ""my_var"": ""{{ var.value."" + provider_name + ""_accounts }}"" > } > ) @kaxil the provided DAG reproduces the issue only and does not reflect our actual case. We need the ability to trigger a certain account, gather metrics to StatsD for each account, and utilize the additional flexibility that dynamic DAG generation offers. Thank you for your reply. I look forward to testing the fix if one is provided. …",,,,,,Anecdotal,comment,,,,,,,,2025-06-17,github/opeida,https://github.com/apache/airflow/issues/51816#issuecomment-2981493262,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: best practice
"> UPD: import `Variable` from `airflow.models` instead of `airflow.sdk` fixes the problem but doesn't seem like a best practice solution. Yeah, that still works as it's falling back to the direct DB access based approach",,,,,,Anecdotal,comment,,,,,,,,2025-06-18,github/ashb,https://github.com/apache/airflow/issues/51816#issuecomment-2984241505,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: best practice
"Import from `airflow.models` works for me to load ENV defined variable. Is `airflow.sdk` not well oriented for non-runner environment yet? I have seen various troubles already like `dag.folder` is wrong when running thru CLI, `sys.path` not being properly enhanced...",,,,,,Anecdotal,comment,,,,,,,,2025-06-26,github/simi,https://github.com/apache/airflow/issues/51816#issuecomment-3007736559,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: best practice
"We use variables for dynamic DAG scheduling, so we can access and modify schedules via web-UI by ourselves, without having to change deployment configs (OS environment variables) through DevOps guys. Example: ```python with DAG( ""DAG_1"", schedule=Variable.get(""DAG_1_SCHEDULE"", None) ) ``` So I feel like I have to ask: will the final fixed version allow for us to do the same?",,,,,,Anecdotal,comment,,,,,,,,2025-07-04,github/DartVeDroid,https://github.com/apache/airflow/issues/51816#issuecomment-3036921222,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: best practice
"> We use variables for dynamic DAG scheduling, so we can access and modify schedules via web-UI by ourselves, without having to change deployment configs (OS environment variables) through DevOps guys. Example: > > with DAG( > ""DAG_1"", > schedule=Variable.get(""DAG_1_SCHEDULE"", None) > ) > So I feel like I have to ask: will the final fixed version allow for us to do the same? Yes, although timeline TBD",,,,,,Anecdotal,comment,,,,,,,,2025-07-04,github/kaxil,https://github.com/apache/airflow/issues/51816#issuecomment-3036961653,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: best practice
"Add support to `--build-constraint(s)` flag for our constraint preparation Currently in airflow CI process we prepare constraints automatically and both our CI process and user installation are designed around using it to ""stabilize"" both CI (PR stability) and Reproducible installation of airflow. This prevents Airflow from being susceptible to breaking installation and tests when new version of a dependency (including transitive dependencies) is released. We can detect such breakages and remed…",,,,,,Anecdotal,issue,,,,,,,,2025-08-12,github/potiuk,https://github.com/apache/airflow/issues/54394,repo: apache/airflow | keyword: best practice | state: open
"FYI, it's not clear what is best practices for build constraints, it might be the case you can pin them all, alternatively you might just be able to put upper bounds or ranges on them. And there is likely a difference between dev workflows (which can be somewhat scripted) and user workflows which need to ""just work"". As this is an area not well historically served by Python package installer tools there is a lot less experience out there.",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/notatallshaw,https://github.com/apache/airflow/issues/54394#issuecomment-3179663929,repo: apache/airflow | issue: Add support to `--build-constraint(s)` flag for our constraint preparation | keyword: best practice
"> FYI, it's not clear what is best practices for build constraints, it might be the case you can pin them all, alternatively you might just be able to put upper bounds or ranges on them. Yep I know. It's just our choice and we have good reasons for Airlfow to do so.",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/potiuk,https://github.com/apache/airflow/issues/54394#issuecomment-3180159412,repo: apache/airflow | issue: Add support to `--build-constraint(s)` flag for our constraint preparation | keyword: best practice
"Support a global template_searchpath config option in Airflow ### Description **Apache Airflow version** 3 **What happened?** While upgrading to Airflow 3, we restructured our DAGs to follow the recommended best practices — specifically, moving shared SQL templates out of the `dags/` directory and into the `include/` folder. However, once we made this change, our DAGs could no longer find the SQL files unless we explicitly added a `template_searchpath` argument inside every single DAG definitio…",,,,,,Anecdotal,issue,,,,,,,,2025-07-31,github/gunjisairevanth,https://github.com/apache/airflow/issues/53972,repo: apache/airflow | keyword: best practice | state: open
"Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.",,,,,,Anecdotal,comment,,,,,,,,2025-07-31,github/boring-cyborg[bot],https://github.com/apache/airflow/issues/53972#issuecomment-3139507037,repo: apache/airflow | issue: Support a global template_searchpath config option in Airflow | keyword: best practice
"I'd like to pick up this issue if it's not already being handled and looks like a usefull feature. Please let me know if there are any constraints, expectations, or directions I should follow before I start working on a solution.",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/Brunda10,https://github.com/apache/airflow/issues/53972#issuecomment-3142650806,repo: apache/airflow | issue: Support a global template_searchpath config option in Airflow | keyword: best practice
"Hi @amoghrajesh , As you suggested, I tried using the environment variable AIRFLOW__CORE__TEMPLATE_SEARCHPATH in the Breeze terminal, and used it in the DAGs with template_searchpath able to access it.In development mode, we can set the default path while starting Airflow. Is it okay to set it this way , or is there any other preferred approach for production as well?",,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/Brunda10,https://github.com/apache/airflow/issues/53972#issuecomment-3150301097,repo: apache/airflow | issue: Support a global template_searchpath config option in Airflow | keyword: best practice
"Hi @potiuk amd @amoghrajesh , In development (using Breeze), we can access the value if we export AIRFLOW__CORE__TEMPLATE_SEARCHPATH=""<path>"" inside the Breeze terminal. That works fine. Also, i have added support for setting default path when Breeze starts — via CLI option like breeze start-airflow --template-searchpath ""<path>"" to make it easier instead of exporting every time. For **Production**, I tried passing the variable as an environment variable in the docker run command, and it worked…",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/Brunda10,https://github.com/apache/airflow/issues/53972#issuecomment-3159742446,repo: apache/airflow | issue: Support a global template_searchpath config option in Airflow | keyword: best practice
"> But I’m not sure if we’re supposed to set a default path somewhere in deployment, or if it always has to be passed manually. Would be great to know what the recommended way is for setting it up . I have no idea as well - I don't even know where this behaviour you are observing is coming from, because it is not documented in config_templates/config.yml and resulting documentation - so it woudl require to go through codebase and check/find out how it is derived. But yes, I think it would be a g…",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/potiuk,https://github.com/apache/airflow/issues/53972#issuecomment-3159816002,repo: apache/airflow | issue: Support a global template_searchpath config option in Airflow | keyword: best practice
"Thanks @potiuk, makes sense! I’ve tested this solution by adding the line below to `files/airflow-breeze-config/init.sh`: ```bash export AIRFLOW__CORE__TEMPLATE_SEARCHPATH=/files/my_templates",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/Brunda10,https://github.com/apache/airflow/issues/53972#issuecomment-3159983238,repo: apache/airflow | issue: Support a global template_searchpath config option in Airflow | keyword: best practice
@potiuk I've made the change you suggested: Introduced a new template_searchpath parameter in config_templates/config.yml to make the configuration explicit. Investigated the codebase (especially around DAG rendering logic) to trace where the Jinja template environment is created and used.,,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/Brunda10,https://github.com/apache/airflow/issues/53972#issuecomment-3166677774,repo: apache/airflow | issue: Support a global template_searchpath config option in Airflow | keyword: best practice
"Upgrade FAB to FAB 5 Switch to using flask-sqlalchemy db session management, and include Auth Manager Provider Test Isolation and Reliability - Refactor test fixtures to always use Flask app contexts for DB and app operations. - Add a global pytest fixture to clear SQLAlchemy metadata before each test, reducing test flakiness. - Standardize session access and cleanup patterns across all tests. - Refactor user/role creation and deletion to ensure proper isolation. - Update test logic to use new …",,,,,,Anecdotal,issue,,,,,,,,2025-05-22,github/potiuk,https://github.com/apache/airflow/pull/50960,repo: apache/airflow | keyword: best practice | state: open
"Configurable/airules Description Fixes #53748 This PR enhances the React plugin bootstrap process by introducing a Y/n prompt that lets users choose whether to include the ai_agent_rules folder when generating a new plugin. This streamlines plugin creation by automating conditional file inclusion, reducing manual setup, and supporting future AI-driven plugin development. Key improvements: -User-friendly prompt during project bootstrap -Conditional copying of ai_agent_rules based on user input -…",,,,,,Anecdotal,issue,,,,,,,,2025-08-02,github/urvisrikm9,https://github.com/apache/airflow/pull/54060,repo: apache/airflow | keyword: best practice | state: open
"Congratulations on your first Pull Request and welcome to the Apache Airflow community! If you have any issues or are unsure about any anything please check our Contributors' Guide (https://github.com/apache/airflow/blob/main/contributing-docs/README.rst) Here are some useful points: - Pay attention to the quality of your code (ruff, mypy and type annotations). Our [pre-commits]( https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#prerequisites-for-pre-commit…",,,,,,Anecdotal,comment,,,,,,,,2025-08-02,github/boring-cyborg[bot],https://github.com/apache/airflow/pull/54060#issuecomment-3146714711,repo: apache/airflow | issue: Configurable/airules | keyword: best practice
"All failed workflows in this PR are not related to my changes, which only affect the React plugin template generator and AI rule configuration. The failures are in Helm charts, AMD, or unrelated infrastructure jobs. Please advise if any action is needed from my side.",,,,,,Anecdotal,comment,,,,,,,,2025-08-03,github/urvisrikm9,https://github.com/apache/airflow/pull/54060#issuecomment-3148281511,repo: apache/airflow | issue: Configurable/airules | keyword: best practice
Hi @potiuk @gopidesupavan Just a gentle reminder — this PR is ready for review whenever you get a chance. Let me know if I should improve anything. Thanks!,,,,,,Anecdotal,comment,,,,,,,,2025-08-03,github/urvisrikm9,https://github.com/apache/airflow/pull/54060#issuecomment-3148521705,repo: apache/airflow | issue: Configurable/airules | keyword: best practice
"RuntimeError in trigger creating hooks, Airflow 3.0.3 ### Apache Airflow version 3.0.3 ### If ""Other Airflow 2 version"" selected, which one? _No response_ ### What happened? The error `RuntimeError: You cannot use AsyncToSync in the same thread as an async event loop - just await the async function directly.` is occurred when `BigQueryHook` inits in `async def run(self)`. The issue began immediately after upgrading from 3.0.2 to 3.0.3. ```Python class BigQueryTaskStatusTrigger(BaseTrigger): asy…",,,,,,Anecdotal,issue,,,,,,,,2025-07-17,github/opeida,https://github.com/apache/airflow/issues/53447,repo: apache/airflow | keyword: best practice | state: open
hm the problem is self.get_connection call is being made as sync call . one solution to is to change self.get_connection to async may be updating the GoogleBaseAsyncHook with sync_to_async(self.get_connection) like this.,,,,,,Anecdotal,comment,,,,,,,,2025-07-17,github/gopidesupavan,https://github.com/apache/airflow/issues/53447#issuecomment-3083747404,"repo: apache/airflow | issue: RuntimeError in trigger creating hooks, Airflow 3.0.3 | keyword: best practice"
"this thinks me i think we have to re look all the providers how its fetching connection/ or any task sdk call, now with recent change these calls must be async call. otherwise we end above errors ? @ashb",,,,,,Anecdotal,comment,,,,,,,,2025-07-17,github/gopidesupavan,https://github.com/apache/airflow/issues/53447#issuecomment-3083752016,"repo: apache/airflow | issue: RuntimeError in trigger creating hooks, Airflow 3.0.3 | keyword: best practice"
"This would have previously blocked the event loop too, so it may have ""worked"" before but it was never the right way of doing it. The error isn't great, but I think all we can do is improve the error message -- the trigger code is wrong. Also note: this is not the part of the offical Airflow code but a users custom trigger. Yes it sucks that it is now breaking. Overall are options are: Improve the error message so the user can fix their code, or switch to something like Greenlet so we can more-…",,,,,,Anecdotal,comment,,,,,,,,2025-07-17,github/ashb,https://github.com/apache/airflow/issues/53447#issuecomment-3084298443,"repo: apache/airflow | issue: RuntimeError in trigger creating hooks, Airflow 3.0.3 | keyword: best practice"
"@opeida In this case, the fix is to change your trigger code from this: ```python hook = BigQueryHook(gcp_conn_id=self.gcp_conn_id, use_legacy_sql=False) ``` to this ```python hook = await sync_to_async(BigQueryHook)(gcp_conn_id=self.gcp_conn_id, use_legacy_sql=False) ```",,,,,,Anecdotal,comment,,,,,,,,2025-07-17,github/ashb,https://github.com/apache/airflow/issues/53447#issuecomment-3084302166,"repo: apache/airflow | issue: RuntimeError in trigger creating hooks, Airflow 3.0.3 | keyword: best practice"
"Well, this is exactly the problem I described in #50185. In fact, this issue is that issue - in both cases we are dealing with custom triggers (see [the last paragraph of the original issue author's last comment](https://github.com/apache/airflow/issues/50185#issuecomment-2891834967)), but that one did not have enough feedback. So now we have [the effect I described in the comment on the linked PR](https://github.com/apache/airflow/pull/51699#issuecomment-2971117179) - the fix may be one of the…",,,,,,Anecdotal,comment,,,,,,,,2025-07-26,github/x42005e1f,https://github.com/apache/airflow/issues/53447#issuecomment-3120798640,"repo: apache/airflow | issue: RuntimeError in trigger creating hooks, Airflow 3.0.3 | keyword: best practice"
"Add ObjectStoragePath.from_conn method. ### Problem Currently, when working with `ObjectStoragePath`, you need to redundantly specify both the `conn_id` and the full URI including the protocol (scheme), even though the protocol can be inferred from the connection itself. For example, in two different environments: ```bash # Local development AIRFLOW_CONN_STORAGE='{ ""conn_type"": ""objectstore"", ""extra"": { ""provider"": ""file"", ""base_path"": ""/opt/airflow/storage"" } }' # Production AIRFLOW_CONN_STORA…",,,,,,Anecdotal,issue,,,,,,,,2025-06-21,github/simi,https://github.com/apache/airflow/pull/52002,repo: apache/airflow | keyword: best practice | state: open
"Is the `objectstore` conn type a thing? It’s somewhat awkward for me that we must have a separate conn type to use with OSP, while many existing type (e.g. ftp) can do the same.",,,,,,Anecdotal,comment,,,,,,,,2025-06-23,github/uranusjr,https://github.com/apache/airflow/pull/52002#issuecomment-2994805053,repo: apache/airflow | issue: Add ObjectStoragePath.from_conn method. | keyword: best practice
"@uranusjr looks like you are right, there are connection types for s3 and local file system storage, perhaps, a good suggestion here should be to check if the connection type is in an array that includes all object storage connection types or all file system compatible storage types",,,,,,Anecdotal,comment,,,,,,,,2025-06-23,github/Nataneljpwd,https://github.com/apache/airflow/pull/52002#issuecomment-2994998982,repo: apache/airflow | issue: Add ObjectStoragePath.from_conn method. | keyword: best practice
"@Nataneljpwd @uranusjr thanks for the review. I did another take on this. Now it is about to create like ""root"" `ObjectStoragePath` you use later in your dags with Path API. It is similar what's described in OSP docs today, just no need to specify fsspec URI and thus backing connection could be different per environment if passing needed extra. It can be GCP with GCS config, fs with file or AWS with S3 config and DAG will use it with no code change. WDYT? ```python storage = ObjectStoragePath.f…",,,,,,Anecdotal,comment,,,,,,,,2025-07-02,github/simi,https://github.com/apache/airflow/pull/52002#issuecomment-3029396890,repo: apache/airflow | issue: Add ObjectStoragePath.from_conn method. | keyword: best practice
"@Nataneljpwd I'm not sure it is related. But I can rebase to restart tests. ``` E sqlalchemy.exc.OperationalError: (MySQLdb.OperationalError) (2013, 'Lost connection to server during query') E [SQL: UPDATE dag_bundle SET last_refreshed=%s WHERE dag_bundle.name = %s] E [parameters: (datetime.datetime(2025, 7, 4, 13, 0, 38, 455528), 'testing')] E (Background on this error at: https://sqlalche.me/e/14/e3q8) ```",,,,,,Anecdotal,comment,,,,,,,,2025-07-20,github/simi,https://github.com/apache/airflow/pull/52002#issuecomment-3094435095,repo: apache/airflow | issue: Add ObjectStoragePath.from_conn method. | keyword: best practice
"@potiuk thanks for the review. I do agree on almost everything written. Currently I do host this as ""utility"" shared method (shared at the end of comment). > I don't think the pattern you explain is ""generic"" that ""everyone"" is using . It's very specific for your case, where you want to use different provider locally and for production. This is somethign your company chose to do, but this is not something that is very generic pattern. I'm trying to disconnect Airflow environment from DAGs. When…",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/simi,https://github.com/apache/airflow/pull/52002#issuecomment-3095546058,repo: apache/airflow | issue: Add ObjectStoragePath.from_conn method. | keyword: best practice
Is it worth to explore what we can get out of https://github.com/apache/airflow/blob/f896807026c7f75f686309293e72a1140a7a9cf9/providers/google/src/airflow/providers/google/get_provider_info.py#L783 and similar entries?,,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/simi,https://github.com/apache/airflow/pull/52002#issuecomment-3095559241,repo: apache/airflow | issue: Add ObjectStoragePath.from_conn method. | keyword: best practice
"> I understand current approach is super brittle and provides almost no guarantees. Indeed Connection is too wide abstraction and doesn't currently couple with ObjectStoragePath. I have seen similar brittle coupling in other Airflow parts and considered that would be maybe ok in here also. I would say ""loose"" coupling rather than ""brittle"". ""Brittle"" suggest instability. ""Loose"" means that the coupling is deliberately loose. As everything in IT choices are often trade-offs. And ""strong coupling…",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/potiuk,https://github.com/apache/airflow/pull/52002#issuecomment-3095638532,repo: apache/airflow | issue: Add ObjectStoragePath.from_conn method. | keyword: best practice
"@potiuk looking at 3 ""random"" providers, https://github.com/apache/airflow/blob/f896807026c7f75f686309293e72a1140a7a9cf9/providers/microsoft/azure/src/airflow/providers/microsoft/azure/fs/adls.py https://github.com/apache/airflow/blob/f896807026c7f75f686309293e72a1140a7a9cf9/providers/google/src/airflow/providers/google/cloud/fs/gcs.py https://github.com/apache/airflow/blob/f896807026c7f75f686309293e72a1140a7a9cf9/providers/amazon/src/airflow/providers/amazon/aws/fs/s3.py They do provide `files…",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/simi,https://github.com/apache/airflow/pull/52002#issuecomment-3095684217,repo: apache/airflow | issue: Add ObjectStoragePath.from_conn method. | keyword: best practice
"> They do provide `filesystems` (acquirable from `get_provider_info`) exposing `get_fs` method and also `schemes` list. Is that something recommended to rely on? This is a built in feature of ObjectStorage implementation - it chooses the right implementation of Object storage depending on the URL /schema used. https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/objectstorage.html Not sure what do you mean by ""rely on"", but it's part of the implementation and the way how pluggabl…",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/potiuk,https://github.com/apache/airflow/pull/52002#issuecomment-3095734675,repo: apache/airflow | issue: Add ObjectStoragePath.from_conn method. | keyword: best practice
"@potiuk I was looking at this again. It is somehow possible to get provider out of connection, lookup for filesystem, import and initialize. But it will initialize `FileSystem` not `ObjectStorage` anyway and I understand this is not intended to be used this way. ```python from airflow.providers_manager import ProvidersManager pm = ProvidersManager() def get_filesystems_for_conn_type(conn_type: str) -> list[str] | None: for provider_name, provider_info in pm.providers.items(): for conn_type_info…",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/simi,https://github.com/apache/airflow/pull/52002#issuecomment-3095917546,repo: apache/airflow | issue: Add ObjectStoragePath.from_conn method. | keyword: best practice
"> Looking at connection code, it seems to be originally intended mostly for DB connections (looking at all the comments in the class) and today the meaning changed, but not the class internals. Would it make sense to make it more generic and able to store more structured data for example ""filesystem info"" for ObjectStorage out of it? No. not really. It was supposed to be generic connection, and actually mostly it was started wiht ""http"" type of connections. I do not think we need to update it r…",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/potiuk,https://github.com/apache/airflow/pull/52002#issuecomment-3096199766,repo: apache/airflow | issue: Add ObjectStoragePath.from_conn method. | keyword: best practice
"@potiuk I was looking mainly at those comments. https://github.com/apache/airflow/blob/0f0ea3b5c444465777a7c5eb5188b301faed1aee/airflow-core/src/airflow/models/connection.py#L99-L103 https://github.com/apache/airflow/blob/0f0ea3b5c444465777a7c5eb5188b301faed1aee/airflow-core/src/airflow/models/connection.py#L254 https://github.com/apache/airflow/blob/0f0ea3b5c444465777a7c5eb5188b301faed1aee/airflow-core/src/airflow/models/connection.py#L469-L471 Looking at connection unification, for some provi…",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/simi,https://github.com/apache/airflow/pull/52002#issuecomment-3096247635,repo: apache/airflow | issue: Add ObjectStoragePath.from_conn method. | keyword: best practice
"> @potiuk I'm not 100% happy with any current solution and documenting my helper method is not good idea IMHO. It should be decided good pattern and merged or left on side, since it will be questionable why this helper is not part of the codebase if documented. And as mentioned it introduces some requirements on extra columns, which is not standard. Quite often we do not WANT to decide. When you build product, implementing single opinionated way means that you limit your users from doing things…",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/potiuk,https://github.com/apache/airflow/pull/52002#issuecomment-3096296262,repo: apache/airflow | issue: Add ObjectStoragePath.from_conn method. | keyword: best practice
"Just to make it clear, I'm not pushing for this pattern to be considered best practice. I'm looking for best practice to implement on my side. But I wasn't able to find any so I tried to share this one. I agree @potiuk it will be the best to try to collect community experience (if there's any since Object Storage was experimental until 3.0 release) and find the best practice all together. I'll send a message linking to this discussion into mail list. As usual, thanks for your patience and under…",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/simi,https://github.com/apache/airflow/pull/52002#issuecomment-3096315699,repo: apache/airflow | issue: Add ObjectStoragePath.from_conn method. | keyword: best practice
"[DRAFT] [Storyline example] Turbine Report - Shareholder Reporting Dynamic Dag closes: #52481 <!-- Please keep an empty line above the dashes. --> --- **^ Add meaningful description above** Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#pull-request-guidelines)** for more information. In case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Impr…",,,,,,Anecdotal,issue,,,,,,,,2025-07-14,github/shubhamraj-git,https://github.com/apache/airflow/pull/53352,repo: apache/airflow | keyword: best practice | state: open
"New example along storyline: Dynamic Dag generation ### Description As described in https://cwiki.apache.org/confluence/display/AIRFLOW/Examples+Refurbish A new Dag is implemented along the described storyline for ""Dynamic dag generation"" This includes: - New Dag is added in Taskflow as well as classic implementation - Dag is according to checklist - Previous examples that are content-wise replaced are deleted to incrementally clean-up - Airflow documentation is adjusted to point to new example…",,,,,,Anecdotal,issue,,,,,,,,2025-06-29,github/jscheffl,https://github.com/apache/airflow/issues/52478,repo: apache/airflow | keyword: best practice | state: open
Simepl tutorial Dags are polished ### Description As described in https://cwiki.apache.org/confluence/display/AIRFLOW/Examples+Refurbish The simple tutorial Dags are planned to be kept - but they also should be re-worked according to new quality standard so they are matching to best practices. ### Use case/motivation _No response_ ### Related issues _No response_ ### Are you willing to submit a PR? - [ ] Yes I am willing to submit a PR! ### Code of Conduct - [x] I agree to follow this project's…,,,,,,Anecdotal,issue,,,,,,,,2025-06-29,github/jscheffl,https://github.com/apache/airflow/issues/52487,repo: apache/airflow | keyword: best practice | state: open
"New example along storyline: Turbine monitoring ### Description As described in https://cwiki.apache.org/confluence/display/AIRFLOW/Examples+Refurbish A new Dag is implemented along the described storyline for ""Turbine monitoring"" This includes: - New Dag is added in Taskflow as well as classic implementation - Dag is according to checklist - Previous examples that are content-wise replaced are deleted to incrementally clean-up - Airflow documentation is adjusted to point to new examples are re…",,,,,,Anecdotal,issue,,,,,,,,2025-06-29,github/jscheffl,https://github.com/apache/airflow/issues/52480,repo: apache/airflow | keyword: best practice | state: open
"Example storyline overview in Airflow core docs ### Description As described in https://cwiki.apache.org/confluence/display/AIRFLOW/Examples+Refurbish New examples are made including tutorials. This should be weaved into the general Airflow docs in the howto section thus people can see, find and learn from the examples ### Use case/motivation Airflow Examples have been grown in number and focus over the past years. They purpose multiple things: Serve as tutorials to learn Airflow DAG implementa…",,,,,,Anecdotal,issue,,,,,,,,2025-06-29,github/jscheffl,https://github.com/apache/airflow/issues/52484,repo: apache/airflow | keyword: best practice | state: open
"New example along storyline: Measurement correction ### Description As described in https://cwiki.apache.org/confluence/display/AIRFLOW/Examples+Refurbish A new Dag is implemented along the described storyline for ""Measurement correction"" This includes: - New Dag is added in Taskflow as well as classic implementation - Dag is according to checklist - Previous examples that are content-wise replaced are deleted to incrementally clean-up - Airflow documentation is adjusted to point to new example…",,,,,,Anecdotal,issue,,,,,,,,2025-06-29,github/jscheffl,https://github.com/apache/airflow/issues/52482,repo: apache/airflow | keyword: best practice | state: open
Split testing and example dags in loading ### Description As described in https://cwiki.apache.org/confluence/display/AIRFLOW/Examples+Refurbish After #52469 and #52474 are implemented - another increment is to split example and testing dags which are today somehow interlinked. The following rule should apply: - pure example dags are stored per provider in <provider-root>/src/airflow/providers/<provider-id>/example_dags/ - Testing dags should be moved to <provider-root>/src/airflow/providers/<p…,,,,,,Anecdotal,issue,,,,,,,,2025-06-29,github/jscheffl,https://github.com/apache/airflow/issues/52475,repo: apache/airflow | keyword: best practice | state: open
"New example along storyline: Turbine report ### Description As described in https://cwiki.apache.org/confluence/display/AIRFLOW/Examples+Refurbish A new Dag is implemented along the described storyline for ""Turbine report"" This includes: - New Dag is added in Taskflow as well as classic implementation - Dag is according to checklist - Previous examples that are content-wise replaced are deleted to incrementally clean-up - Airflow documentation is adjusted to point to new examples are replaced -…",,,,,,Anecdotal,issue,,,,,,,,2025-06-29,github/jscheffl,https://github.com/apache/airflow/issues/52481,repo: apache/airflow | keyword: best practice | state: open
"New example along storyline (bonus): New example for Human in the Loop operators ### Description As described in https://cwiki.apache.org/confluence/display/AIRFLOW/Examples+Refurbish If HITL implementation is completed, add a Human in the Loop example to the storyline. Storyline is to be extended for this - creativity for a use case is needed. ### Use case/motivation Airflow Examples have been grown in number and focus over the past years. They purpose multiple things: Serve as tutorials to le…",,,,,,Anecdotal,issue,,,,,,,,2025-06-29,github/jscheffl,https://github.com/apache/airflow/issues/52485,repo: apache/airflow | keyword: best practice | state: open
"New example along storyline: City reporting ### Description As described in https://cwiki.apache.org/confluence/display/AIRFLOW/Examples+Refurbish A new Dag is implemented along the described storyline for ""City reporting"" This includes: - New Dag is added in Taskflow as well as classic implementation - Dag is according to checklist - Previous examples that are content-wise replaced are deleted to incrementally clean-up - Airflow documentation is adjusted to point to new examples are replaced -…",,,,,,Anecdotal,issue,,,,,,,,2025-06-29,github/jscheffl,https://github.com/apache/airflow/issues/52483,repo: apache/airflow | keyword: best practice | state: open
"Fix type annotation for self.dialect in _UniqueConstraintErrorHandler <!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/lice…",,,,,,Anecdotal,issue,,,,,,,,2025-08-15,github/NithinU2802,https://github.com/apache/airflow/pull/54558,repo: apache/airflow | keyword: best practice | state: closed
"Congratulations on your first Pull Request and welcome to the Apache Airflow community! If you have any issues or are unsure about any anything please check our Contributors' Guide (https://github.com/apache/airflow/blob/main/contributing-docs/README.rst) Here are some useful points: - Pay attention to the quality of your code (ruff, mypy and type annotations). Our [pre-commits]( https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#prerequisites-for-pre-commit…",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/boring-cyborg[bot],https://github.com/apache/airflow/pull/54558#issuecomment-3192408994,repo: apache/airflow | issue: Fix type annotation for self.dialect in _UniqueConstraintErrorHandler | keyword: best practice
"Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.",,,,,,Anecdotal,comment,,,,,,,,2025-08-16,github/boring-cyborg[bot],https://github.com/apache/airflow/pull/54558#issuecomment-3193688060,repo: apache/airflow | issue: Fix type annotation for self.dialect in _UniqueConstraintErrorHandler | keyword: best practice
[Proposal]Add support for provenance attestations in Airflow's release workflow (PEP 740) ### Description ### Summary I would like to work on integrating [PEP 740](https://peps.python.org/pep-0740/) compliance into Apache Airflow by generating provenance attestations (`.intoto.jsonl`) during PyPI release workflows. This aligns with PyPI's recent support for [digital attestations](https://blog.pypi.org/posts/2024-11-14-pypi-now-supports-digital-attestations/). ### Motivation Software supply chai…,,,,,,Anecdotal,issue,,,,,,,,2025-04-30,github/kyungjunleeme,https://github.com/apache/airflow/issues/50005,repo: apache/airflow | keyword: best practice | state: closed
"<img width=""1279"" alt=""Image"" src=""https://github.com/user-attachments/assets/86d78902-6057-4454-8d47-c020307ddf97"" /> - airflow does not observe pep 740 yet.",,,,,,Anecdotal,comment,,,,,,,,2025-04-30,github/kyungjunleeme,https://github.com/apache/airflow/issues/50005#issuecomment-2841016462,repo: apache/airflow | issue: [Proposal]Add support for provenance attestations in Airflow's release workflow (PEP 740) | keyword: best practice
"We are working on Trusted Publishing workflow for the whole ASF and it will be integrated with upcoming ""Apache Trusted Releases"" platform that is being developed in the foundation. The Trusted Publishing and Attestation PEP 740 will be implemented as part of that - to provide not only GitHub Attestations but also build attestations on the ASF-controlled hardware, this is an ASF -wide effort that we are driving internally in the ASF.",,,,,,Anecdotal,comment,,,,,,,,2025-04-30,github/potiuk,https://github.com/apache/airflow/issues/50005#issuecomment-2841050660,repo: apache/airflow | issue: [Proposal]Add support for provenance attestations in Airflow's release workflow (PEP 740) | keyword: best practice
@potiuk hello :) We’re currently discussing the ASF-wide Trusted Publishing workflow and the upcoming Apache Trusted Releases platform. Am I correct that the [apache/tooling-trusted-release](https://github.com/apache/tooling-trusted-release) repository is the codebase for this effort (covering Trusted Publishing and PEP 740 attestation support)? Thanks for clarifying!,,,,,,Anecdotal,comment,,,,,,,,2025-08-16,github/kyungjunleeme,https://github.com/apache/airflow/issues/50005#issuecomment-3193285045,repo: apache/airflow | issue: [Proposal]Add support for provenance attestations in Airflow's release workflow (PEP 740) | keyword: best practice
"Add CI support for SQLAlchemy 2.0 closes: #48953, #52663 related: #28723, #50960 (prerequisite that's included in this PR) - Add the `check_upgrade_sqlalchemy` container entrypoint for testing SQLA v2 on CI - Fix some existing shellcheck violations reported by shellcheck-py. - Robustify multiple unit tests and amend some business logic. <!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for addit…",,,,,,Anecdotal,issue,,,,,,,,2025-06-25,github/Dev-iL,https://github.com/apache/airflow/pull/52233,repo: apache/airflow | keyword: best practice | state: closed
The tests are failing as expected on SQLA v2 incompatibilities. Now what? Should I suggest in #28723 that developers rebase to my branch?,,,,,,Anecdotal,comment,,,,,,,,2025-06-25,github/Dev-iL,https://github.com/apache/airflow/pull/52233#issuecomment-3004793506,repo: apache/airflow | issue: Add CI support for SQLAlchemy 2.0 | keyword: best practice
Yes. And make it a part of your PR and we can continuously rebase the PRs on top of yours. This is going to be a little involved and require rebasing here and there.,,,,,,Anecdotal,comment,,,,,,,,2025-06-25,github/potiuk,https://github.com/apache/airflow/pull/52233#issuecomment-3005017431,repo: apache/airflow | issue: Add CI support for SQLAlchemy 2.0 | keyword: best practice
"Should we perhaps decide on a single ""main"" branch (e.g. your one) and target the various SQLA2 PRs into it? Just to save the effort of rebasing across multiple forks...",,,,,,Anecdotal,comment,,,,,,,,2025-06-25,github/Dev-iL,https://github.com/apache/airflow/pull/52233#issuecomment-3005580385,repo: apache/airflow | issue: Add CI support for SQLAlchemy 2.0 | keyword: best practice
"@potiuk Good news - [one of the recent builds](https://github.com/apache/airflow/actions/runs/15965464504) was all-green (with the exception of some breeze outputs). Now I'm thinking about splitting this PR into two for better focus: 1. The updates to the tests and business logic that allow dual support of SQLA 1.4 and 2.0. 2. Enabling the SQLA2 tests in the CI. Then, the 1st part can be merged (thus enabling ""experimental"" support for SQLA2), and the 2nd can wait for FAB5. Fixing the deprecati…",,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/Dev-iL,https://github.com/apache/airflow/pull/52233#issuecomment-3018147444,repo: apache/airflow | issue: Add CI support for SQLAlchemy 2.0 | keyword: best practice
Is it possible (and a good idea) to split the breeze changes from the github ones? The result will be having support for running SQLA2 tests on breeze but without affecting the CI. It should make it easier to work on fixing deprecations using ```bash breeze testing core-tests --force-sa-warnings --upgrade-sqlalchemy ```,,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/Dev-iL,https://github.com/apache/airflow/pull/52233#issuecomment-3018674290,repo: apache/airflow | issue: Add CI support for SQLAlchemy 2.0 | keyword: best practice
"@anton-bedrock I fixed the conflicts as best I could. If it didn't work, we'll have to wait for @potiuk to update his FAB5 branch.",,,,,,Anecdotal,comment,,,,,,,,2025-07-15,github/Dev-iL,https://github.com/apache/airflow/pull/52233#issuecomment-3072614216,repo: apache/airflow | issue: Add CI support for SQLAlchemy 2.0 | keyword: best practice
"I rebased my + @vincbeck https://github.com/apache/airflow/pull/50960 to latest main (maybe I will need to fix something to make it green - let's see. So feel free to rebase yours @Dev-iL -> and yeah @uranusjr -> possibly reviewing the FAB-5 PR and adding suggestions there is better, as I continue rebasing it waiting for FAB5 release.",,,,,,Anecdotal,comment,,,,,,,,2025-07-19,github/potiuk,https://github.com/apache/airflow/pull/52233#issuecomment-3092483663,repo: apache/airflow | issue: Add CI support for SQLAlchemy 2.0 | keyword: best practice
I think we should (in the upgrade sqlalchemy path in the entrypoint_ci.sh): * run `uv sync --exclude --no-install-package apache-airlfow-providers-fab --all-packages` - this should automatically bring all packages to good versions that are resolved without fab. * add --ignores to pytest args to remove appbuilder tests * if needed add importorskip() for flask_appbuilder That should get the tests green.,,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/potiuk,https://github.com/apache/airflow/pull/52233#issuecomment-3096826725,repo: apache/airflow | issue: Add CI support for SQLAlchemy 2.0 | keyword: best practice
"> @potiuk I don't know how to fix the latest failures. Could you take it from here? Static check seems easy. The other ones, look like we need to add some conditional code to `importorskip` tests when flask_appbuilder is not installed (you can look up other importorskip cases), or maybe do some try/catch on import errors in a few cases - generally speaking i see most of it is in databricks that still has a dependency on flask_appbuilder . There are two issues to solve that dependency: * https:/…",,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/potiuk,https://github.com/apache/airflow/pull/52233#issuecomment-3103727016,repo: apache/airflow | issue: Add CI support for SQLAlchemy 2.0 | keyword: best practice
"BTW. It is likely that those tests already have the code to skip them - but it might be wrong trigger to skip them . For example it could be that those tests (have not looked at them) are already skipped when flask is not installed or some specific versions of dependencies - so likely those conditions shoudl be changed to ""is flask_appbuilder installed"" or smth like that.",,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/potiuk,https://github.com/apache/airflow/pull/52233#issuecomment-3103742591,repo: apache/airflow | issue: Add CI support for SQLAlchemy 2.0 | keyword: best practice
LGTM. The MyPy fix has been fixed separated (leaving some TODOs: in type checking https://github.com/apache/airflow/pull/54321,,,,,,Anecdotal,comment,,,,,,,,2025-08-10,github/potiuk,https://github.com/apache/airflow/pull/52233#issuecomment-3172781030,repo: apache/airflow | issue: Add CI support for SQLAlchemy 2.0 | keyword: best practice
"Add UI for human in the loop operators <!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless require…",,,,,,Anecdotal,issue,,,,,,,,2025-07-08,github/guan404ming,https://github.com/apache/airflow/pull/53035,repo: apache/airflow | keyword: best practice | state: closed
"Hey @guan404ming , the backend API has been merged. Could you please rebase from the main branch and see whether it works for the frontend? the endpoints are detailed in https://github.com/apache/airflow/pull/52868#issue-3202463223 and here're the example dags and the script to call rest API https://github.com/apache/airflow/pull/52868#issuecomment-3037857107",,,,,,Anecdotal,comment,,,,,,,,2025-07-11,github/Lee-W,https://github.com/apache/airflow/pull/53035#issuecomment-3060393485,repo: apache/airflow | issue: Add UI for human in the loop operators | keyword: best practice
"Oh, preview looks cool - very promising. Would need to take a look tomorrow as already quite late here. From the preview video I see some UX things that might need to be adjusted. But would propose to have a ""feeling"" on my own. And this probably needs a bit more time.",,,,,,Anecdotal,comment,,,,,,,,2025-07-20,github/jscheffl,https://github.com/apache/airflow/pull/53035#issuecomment-3094813971,repo: apache/airflow | issue: Add UI for human in the loop operators | keyword: best practice
"Hi @guan404ming - so now was able to dive into the PR - and as I am a bit ""picky"" on UI journey, sorry for the amount of comments. I assume we did not ""design"" all UI elments in detail and when I provide feedback this means it is ""just my personal opinion"" and there might be others valid as well. So feedback from UX side: - I see that HITL task instances are listed as a new tab in the right panels of the Dag view - but only on Dag global view! This means that I can see there the tasks for all D…",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/jscheffl,https://github.com/apache/airflow/pull/53035#issuecomment-3098222299,repo: apache/airflow | issue: Add UI for human in the loop operators | keyword: best practice
"Big 👍 for getting core HITL UI in! This will unblock 3.1. Jens did the heavy lifting with the review, and I see a lot of his points are already addressed!! Some additional feedback: **User-facing label** “HITL” and even “Human Tasks” describe the implementation, not the user’s job. Most approvers just need to know that something is blocking a run and needs their attention. I suggest renaming all UI surfaces to “Action Required” (top nav, tabs). Per-row status chips can then show the specific ne…",,,,,,Anecdotal,comment,,,,,,,,2025-07-23,github/cmarteepants,https://github.com/apache/airflow/pull/53035#issuecomment-3109481902,repo: apache/airflow | issue: Add UI for human in the loop operators | keyword: best practice
"Thanks both for the feedback! I'm currently working through Jens' review. Some functionality might not be fully implemented yet, but it's in progress and I'll let everyone know once it's ready. - The naming suggestion to switch from “HITL” to “Action Required” makes a lot of sense and is much clearer to end users. I’ll proceed with the renaming and will reach out if any i18n mapping help is needed. - The updated UX definitely feels more intuitive. For the options > 4 case, I’ll implement it as …",,,,,,Anecdotal,comment,,,,,,,,2025-07-23,github/guan404ming,https://github.com/apache/airflow/pull/53035#issuecomment-3109519512,repo: apache/airflow | issue: Add UI for human in the loop operators | keyword: best practice
">Regarding logs, I think that it is probably out of scope for the UI. Creating a separate issue to track that sounds like a good approach. Valid :) Created another issue, and flagging it internally to see what can be done. Thanks again!",,,,,,Anecdotal,comment,,,,,,,,2025-07-23,github/cmarteepants,https://github.com/apache/airflow/pull/53035#issuecomment-3110129933,repo: apache/airflow | issue: Add UI for human in the loop operators | keyword: best practice
"https://github.com/user-attachments/assets/9c4792c3-505b-4e85-8de4-1ebb266422a9 ## 07/26 Update ### Naming & User-Facing Terminology 1. Rename **“HITL” / “Human Tasks”** to **“Action Required”** to better reflect the user’s task rather than the technical implementation. 2. Add a **Status** column based on the TaskInstance operator (e.g., Approval Required, Input Required). --- ### UI & UX Improvements 1. Add **Action Required** tab and extend the **“hide if empty”** logic on Dag, Dag Run, Task,…",,,,,,Anecdotal,comment,,,,,,,,2025-07-25,github/guan404ming,https://github.com/apache/airflow/pull/53035#issuecomment-3120375169,repo: apache/airflow | issue: Add UI for human in the loop operators | keyword: best practice
"I just played with it, and it looks good to me. However, we might need @jscheffl and @pierrejeambrun's help to review the frontend code. Thanks!",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/Lee-W,https://github.com/apache/airflow/pull/53035#issuecomment-3127947052,repo: apache/airflow | issue: Add UI for human in the loop operators | keyword: best practice
"<img width=""214"" height=""73"" alt=""image"" src=""https://github.com/user-attachments/assets/63e7cce8-2721-4104-8b7a-70c17679833c"" /> Hey @guan404ming , I just played with it a bit. It seems we're not correctly highlight defaults. ```python valid_input_and_options = ApprovalOperator( task_id=""valid_input_and_options"", subject=""Are the following input and options valid?"", body="""""" Input: {{ task_instance.xcom_pull(task_ids='wait_for_input', key='return_value')[""params_input""][""information""] }} Optio…",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/Lee-W,https://github.com/apache/airflow/pull/53035#issuecomment-3131134510,repo: apache/airflow | issue: Add UI for human in the loop operators | keyword: best practice
"To check if we should display or not the HITL tab, we query the TI HITL details, this cause multiple 404 not found errors for tasks that do not have HITL. We should probably handle this nicely in the UI. Like this is not an error, catch it and return empty HITL, and keep going with that. <img width=""1920"" height=""753"" alt=""Screenshot 2025-07-29 at 13 34 47"" src=""https://github.com/user-attachments/assets/9c97d67d-89c9-4875-be37-8decb4570a39"" />",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/pierrejeambrun,https://github.com/apache/airflow/pull/53035#issuecomment-3132057741,repo: apache/airflow | issue: Add UI for human in the loop operators | keyword: best practice
"> To check if we should display or not the HITL tab, we query the TI HITL details, this cause multiple 404 not found errors for tasks that do not have HITL. > > We should probably handle this nicely in the UI. Like this is not an error, catch it and return empty HITL, and keep going with that. <img alt=""Screenshot 2025-07-29 at 13 34 47"" width=""1920"" height=""753"" src=""https://private-user-images.githubusercontent.com/14861206/471971070-9c97d67d-89c9-4875-be37-8decb4570a39.png?jwt=eyJhbGciOiJIUz…",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/guan404ming,https://github.com/apache/airflow/pull/53035#issuecomment-3133614944,repo: apache/airflow | issue: Add UI for human in the loop operators | keyword: best practice
"> Hey @guan404ming , I just played with it a bit. It seems we're not correctly highlight defaults. I've also updated the highlight logic in this pr, please help check and test, thanks! <img width=""901"" height=""290"" alt=""image"" src=""https://github.com/user-attachments/assets/765087e5-45c6-4d8f-96b8-7dc8a6f576c9"" />",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/guan404ming,https://github.com/apache/airflow/pull/53035#issuecomment-3133850265,repo: apache/airflow | issue: Add UI for human in the loop operators | keyword: best practice
"> > To check if we should display or not the HITL tab, we query the TI HITL details, this cause multiple 404 not found errors for tasks that do not have HITL. > > We should probably handle this nicely in the UI. Like this is not an error, catch it and return empty HITL, and keep going with that. <img alt=""Screenshot 2025-07-29 at 13 34 47"" width=""1920"" height=""753"" src=""https://private-user-images.githubusercontent.com/14861206/471971070-9c97d67d-89c9-4875-be37-8decb4570a39.png?jwt=eyJhbGciOiJI…",,,,,,Anecdotal,comment,,,,,,,,2025-07-30,github/Lee-W,https://github.com/apache/airflow/pull/53035#issuecomment-3135490248,repo: apache/airflow | issue: Add UI for human in the loop operators | keyword: best practice
"**Update**: markdown render is enabled now for action body <img width=""843"" height=""181"" alt=""Screenshot 2025-07-31 at 10 56 02 PM"" src=""https://github.com/user-attachments/assets/c645a890-681b-4495-b6cc-4b3ff0ef30f0"" />",,,,,,Anecdotal,comment,,,,,,,,2025-07-31,github/guan404ming,https://github.com/apache/airflow/pull/53035#issuecomment-3140395796,repo: apache/airflow | issue: Add UI for human in the loop operators | keyword: best practice
"> **Update**: markdown render is enabled now for action body <img alt=""Screenshot 2025-07-31 at 10 56 02 PM"" width=""843"" height=""181"" src=""https://private-user-images.githubusercontent.com/105915352/473063396-c645a890-681b-4495-b6cc-4b3ff0ef30f0.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTM5OTUyNDIsIm5iZiI6MTc1Mzk5NDk0MiwicGF0aCI6Ii8xMDU5MTUzNTIvNDczMDYzMzk2LWM2NDVhODkwLTY4MWItNDQ5NS1iNmNjLTRi…",,,,,,Anecdotal,comment,,,,,,,,2025-07-31,github/jscheffl,https://github.com/apache/airflow/pull/53035#issuecomment-3141297037,repo: apache/airflow | issue: Add UI for human in the loop operators | keyword: best practice
> Very very cool! Can you add some example Markdown to the hitl example Dag? Otherwise once the conflict is resolved I'd propose to get this merged! Thanks for the review. I've resolved the conflict. I'll be working with @Jasperora to wrap up the UI-related docs. We'll add examples and handle the rest in the follow up PR.,,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/guan404ming,https://github.com/apache/airflow/pull/53035#issuecomment-3143914044,repo: apache/airflow | issue: Add UI for human in the loop operators | keyword: best practice
"Listeners not firing on task success ### Apache Airflow version 3.0.2 ### If ""Other Airflow 2 version"" selected, which one? _No response_ ### What happened? I am attempting to implement a listener on task success ( or dag success - either one doesnt work). Here is a sample of my code plugin code ``` from airflow.plugins_manager import AirflowPlugin from listeners_code import on_task_instance_running, on_dag_run_running,on_task_instance_success class MyListenerPlugin(AirflowPlugin): name = ""my_l…",,,,,,Anecdotal,issue,,,,,,,,2025-07-11,github/plovegro,https://github.com/apache/airflow/issues/53162,repo: apache/airflow | keyword: best practice | state: closed
"Hello\! Thanks for the detailed report. Based on the code you provided, the most likely reason your listener isn't firing is that the `requests` library was not imported into your plugin file. This would cause an `ImportError` when the listener tries to execute, preventing the webhook call and any subsequent code from running. ----- ### The Problem: Potential Causes 1. **Missing `requests` Import:** The listener code uses `requests.get()` but doesn't include `import requests` at the top of the …",,,,,,Anecdotal,comment,,,,,,,,2025-07-15,github/tanujdargan,https://github.com/apache/airflow/issues/53162#issuecomment-3072270388,repo: apache/airflow | issue: Listeners not firing on task success | keyword: best practice
"Thanks so much for your response. To be clear - i did have import requests at the top of my code, i just didnt import that in my description sorry! I have copied your code exactly and still i am not receiving a notification.There are no errors in the schedule logs, or on the task itself. Are you running Airflow 3.0.2?",,,,,,Anecdotal,comment,,,,,,,,2025-07-15,github/plovegro,https://github.com/apache/airflow/issues/53162#issuecomment-3072506457,repo: apache/airflow | issue: Listeners not firing on task success | keyword: best practice
My apologies I was testing on the dev branch but I seem to be able to replicate the issue on the current dev version as well. I'll get back to you on this shortly,,,,,,Anecdotal,comment,,,,,,,,2025-07-16,github/tanujdargan,https://github.com/apache/airflow/issues/53162#issuecomment-3081751858,repo: apache/airflow | issue: Listeners not firing on task success | keyword: best practice
Hey @plovegro I was able to fix this issue. The code for the dag remains the same however the code for the plugin and airflow's config file need to be changed a little. I was able to get the webhook to trigger successfully by changing the plugin code to the following: ```python import logging import requests from airflow.plugins_manager import AirflowPlugin from airflow.listeners import hookimpl # It's best practice to get a logger instance instead of using print() log = logging.getLogger(__nam…,,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/tanujdargan,https://github.com/apache/airflow/issues/53162#issuecomment-3096005490,repo: apache/airflow | issue: Listeners not firing on task success | keyword: best practice
It should have worked: https://github.com/apache/airflow/blob/3.0.3/task-sdk/src/airflow/sdk/execution_time/task_runner.py#L1250-L1257,,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/kaxil,https://github.com/apache/airflow/issues/53162#issuecomment-3125692007,repo: apache/airflow | issue: Listeners not firing on task success | keyword: best practice
> Hey [@plovegro](https://github.com/plovegro) I was able to fix this issue. The code for the dag remains the same however the code for the plugin and airflow's config file need to be changed a little. > > I was able to get the webhook to trigger successfully by changing the plugin code to the following: > > import logging > import requests > from airflow.plugins_manager import AirflowPlugin > from airflow.listeners import hookimpl > > # It's best practice to get a logger instance instead of us…,,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/tanujdargan,https://github.com/apache/airflow/issues/53162#issuecomment-3126147817,repo: apache/airflow | issue: Listeners not firing on task success | keyword: best practice
"@tanujdargan It was https://github.com/apache/airflow/issues/53162#issuecomment-3072270388 -- you pretty much just pasted the output of an LLM into an comment. It made a whole load of guesses and assumptions about the error that no human would make. By even making me type this you have wasted more time. Learn when LLM is appropriate to use, or else stop trying to help and wasting everyone's time.",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/ashb,https://github.com/apache/airflow/issues/53162#issuecomment-3126684503,repo: apache/airflow | issue: Listeners not firing on task success | keyword: best practice
"Listeners are working in 3.0.3 https://github.com/apache/airflow/blob/85d80cdecd25ce3d6d57e36a71825b8af32218fa/airflow-core/src/airflow/example_dags/plugins/event_listener.py#L77-L78 <img width=""1114"" height=""301"" alt=""Image"" src=""https://github.com/user-attachments/assets/467be3fe-46c7-422c-8dcd-785f36000674"" />",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/ashb,https://github.com/apache/airflow/issues/53162#issuecomment-3126691852,repo: apache/airflow | issue: Listeners not firing on task success | keyword: best practice
Sorry @ashb Original poster here - i am running 3.0.2 - was something fixed in 3.0.3 for yours to work? or is it my implementation?,,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/plovegro,https://github.com/apache/airflow/issues/53162#issuecomment-3126915395,repo: apache/airflow | issue: Listeners not firing on task success | keyword: best practice
"@plovegro Not _knowingly_ fixes between 3.0.2 and 3.0.3, but I'm unable to reproduce it with the example listener plugin in the repo. Does `airflow plugins` list your plugin and the listeners? If you put `print(""hello"")` at the first line of it, do you see that message?",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/ashb,https://github.com/apache/airflow/issues/53162#issuecomment-3127408253,repo: apache/airflow | issue: Listeners not firing on task success | keyword: best practice
"@ashb I can confirm that tasks listeners are working ( ie. task succeed) but i have been unable to see any of the DAG listeners fire - like success, running or failure ( on_dag_run_running etc). Can you confirm you can see these DAG related listeners working? I have just reverted back to the example given in the docs ( same as what youve used above)",,,,,,Anecdotal,comment,,,,,,,,2025-07-31,github/plovegro,https://github.com/apache/airflow/issues/53162#issuecomment-3138323629,repo: apache/airflow | issue: Listeners not firing on task success | keyword: best practice
"Best Practices - Unit test for custom operator ### What do you see as an issue? I copied the code from https://airflow.apache.org/docs/apache-airflow/3.0.2/best-practices.html#testing-a-dag section ""Unit test for custom operator"" and found out that the DAG class instance was missing the method `create_dagrun` ```text dag = <DAG: my_custom_operator_dag> def test_my_custom_operator_execute_no_trigger(dag): > dagrun = dag.create_dagrun( ^^^^^^^^^^^^^^^^^ run_id=TEST_RUN_ID, logical_date=DATA_INTER…",,,,,,Anecdotal,issue,,,,,,,,2025-07-04,github/AchimGaedkeLynker,https://github.com/apache/airflow/issues/52862,repo: apache/airflow | keyword: best practice | state: closed
"PTAL: https://github.com/lynker-analytics/airflow-conda-operator/blob/main/airflow_conda_operator/tests/test_operators.py#L8 (Please forgive the bad code quality, love to figure out how to use the airflow pytest plugin buried in the airflow repo). I use airflow 3.0.2 with standard providers. The version in the repo works, but importing the DAG as in line 7 (according to the docs, see above) results in the following error/s (obviously the same twice): ```text airflow-conda-operator$ pytest airfl…",,,,,,Anecdotal,comment,,,,,,,,2025-07-08,github/AchimGaedkeLynker,https://github.com/apache/airflow/issues/52862#issuecomment-3050396172,repo: apache/airflow | issue: Best Practices - Unit test for custom operator | keyword: best practice
"Ok, I did my first round of fixing: Maybe this is now ""as intended""? https://github.com/lynker-analytics/airflow-conda-operator/blob/35f811483dabd73f1820d694bdaca8805295ee55/airflow_conda_operator/tests/test_operators.py#L17 It works (TM) if you run the tests in an environment with Airflow set up correctly. Below is also a first cut of a PR # #53625 for the documentation.",,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/AchimGaedkeLynker,https://github.com/apache/airflow/issues/52862#issuecomment-3101720035,repo: apache/airflow | issue: Best Practices - Unit test for custom operator | keyword: best practice
@GujaLomsadze please take a look at the MR. I have removed the pytest dependency / fixture as the test function is now short and concise. Please feel free to re-add (if you are a fixtures advocate).,,,,,,Anecdotal,comment,,,,,,,,2025-07-23,github/AchimGaedkeLynker,https://github.com/apache/airflow/issues/52862#issuecomment-3105634489,repo: apache/airflow | issue: Best Practices - Unit test for custom operator | keyword: best practice
"Following the thread. I think this is resolved. Thanks, all! Feel free to reopen or create a new one if we still have something we want to improve",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/Lee-W,https://github.com/apache/airflow/issues/52862#issuecomment-3126641830,repo: apache/airflow | issue: Best Practices - Unit test for custom operator | keyword: best practice
"FlinkKubernetesSensor._log_driver() namespace for pod retrieval is hardcoded to ""default"" ### Apache Airflow Provider(s) apache-flink ### Versions of Apache Airflow Providers apache-airflow-providers-apache-flink==1.6.0 ### Apache Airflow version 2.10.5 ### Operating System Debian GNU/Linux 12 (bookworm) ### Deployment Official Apache Airflow Helm Chart ### Deployment details Deployed on local minikube instance Executor: KubernetesExecutor Flink Operator Repo: https://downloads.apache.org/flink…",,,,,,Anecdotal,issue,,,,,,,,2025-03-25,github/kmacdonald76,https://github.com/apache/airflow/issues/48369,repo: apache/airflow | keyword: best practice | state: closed
"Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.",,,,,,Anecdotal,comment,,,,,,,,2025-03-25,github/boring-cyborg[bot],https://github.com/apache/airflow/issues/48369#issuecomment-2752661632,"repo: apache/airflow | issue: FlinkKubernetesSensor._log_driver() namespace for pod retrieval is hardcoded to ""default"" | keyword: best practice"
"Inconsistent DAG Import Errors Display in Airflow UI ### Apache Airflow version 2.10.4 ### If ""Other Airflow 2 version"" selected, which one? _No response_ ### What happened? When refreshing the Airflow UI, the displayed count of DAG import errors is inconsistent. The following issues are observed: 1. The number of DAG import errors changes with each refresh. For example, errors may fluctuate between 13, 14, and 17. 2. Some old import errors disappear after a refresh but reappear during subseque…",,,,,,Anecdotal,issue,,,,,,,,2024-12-30,github/swapnilpatil0905,https://github.com/apache/airflow/issues/45276,repo: apache/airflow | keyword: best practice | state: closed
"Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.",,,,,,Anecdotal,comment,,,,,,,,2024-12-30,github/boring-cyborg[bot],https://github.com/apache/airflow/issues/45276#issuecomment-2565025086,repo: apache/airflow | issue: Inconsistent DAG Import Errors Display in Airflow UI | keyword: best practice
"DAGs are constantly being parsed, and if some are due to import timeouts during parsing, then those DAGs could pass at times and fail at other times. I think that's what you are experiencing, and it's not a bug",,,,,,Anecdotal,comment,,,,,,,,2024-12-30,github/ephraimbuddy,https://github.com/apache/airflow/issues/45276#issuecomment-2565117784,repo: apache/airflow | issue: Inconsistent DAG Import Errors Display in Airflow UI | keyword: best practice
"@tirkarthi Few are in dags folder whereas few are zipped. Note: We are running on multipod webserver mode. <img width=""1453"" alt=""Screenshot 2024-12-30 at 1 03 47 PM"" src=""https://github.com/user-attachments/assets/975c3285-25ac-42d7-8638-6c510a94d3ca"" /> <img width=""1463"" alt=""Screenshot 2024-12-30 at 1 05 26 PM"" src=""https://github.com/user-attachments/assets/2a1eacb6-7290-44ce-aa53-08bd052fd4a4"" /> <img width=""1458"" alt=""Screenshot 2024-12-30 at 1 04 39 PM"" src=""https://github.com/user-attac…",,,,,,Anecdotal,comment,,,,,,,,2024-12-30,github/swapnilpatil0905,https://github.com/apache/airflow/issues/45276#issuecomment-2565128774,repo: apache/airflow | issue: Inconsistent DAG Import Errors Display in Airflow UI | keyword: best practice
"@ephraimbuddy How can one find whether this was due to an import timeout? The actually shown import error indicate real issues, not timeouts.",,,,,,Anecdotal,comment,,,,,,,,2025-07-20,github/rwitzel,https://github.com/apache/airflow/issues/45276#issuecomment-3094397846,repo: apache/airflow | issue: Inconsistent DAG Import Errors Display in Airflow UI | keyword: best practice
"> @ephraimbuddy How can one find whether this was due to an import timeout? The actually shown import error indicate real issues, not timeouts. You need to look at the errors. It does not have to be timeouts, it can be **ANYTHING** that changes between one parsing and second parsing. This basically means that parsing of some of your Dags is simply **unstable** and sometimes works, sometimes does not. You need to look at those import errors (also they should be logged by dag file processor I thi…",,,,,,Anecdotal,comment,,,,,,,,2025-07-20,github/potiuk,https://github.com/apache/airflow/issues/45276#issuecomment-3094469726,repo: apache/airflow | issue: Inconsistent DAG Import Errors Display in Airflow UI | keyword: best practice
"KeyError: 'Variable GIT_TOKEN does not exist' even if defined ### Apache Airflow version Other Airflow 2 version (please specify below) ### If ""Other Airflow 2 version"" selected, which one? 2.9.2 ### What happened? I have this dag: ``` from datetime import timedelta, datetime import os from airflow.decorators import dag, task from airflow.models import Variable default_args = { ""owner"": ""pippo"", ""depends_on_past"": False, ""retries"": 4, ""retry_delay"": timedelta(microseconds=666), ""email"": [""pippo…",,,,,,Anecdotal,issue,,,,,,,,2025-07-03,github/santurini,https://github.com/apache/airflow/issues/52793,repo: apache/airflow | keyword: best practice | state: closed
"In addition, I can see that for some dags this error is only temporary, in fact I have some scheduled dags that had no problem with the run and now I'm having the import error. How is this possible, every time I check I have a different number of import errors",,,,,,Anecdotal,comment,,,,,,,,2025-07-04,github/santurini,https://github.com/apache/airflow/issues/52793#issuecomment-3034932043,repo: apache/airflow | issue: KeyError: 'Variable GIT_TOKEN does not exist' even if defined | keyword: best practice
"@kgw7401 I had it for a while preventing my dags from running and then it auto fixed, but i'm scared it will happen again",,,,,,Anecdotal,comment,,,,,,,,2025-07-16,github/santurini,https://github.com/apache/airflow/issues/52793#issuecomment-3077622765,repo: apache/airflow | issue: KeyError: 'Variable GIT_TOKEN does not exist' even if defined | keyword: best practice
"Hmm... okay. for now, there doesn’t seem to be any issue in the code related to this, so I suggest we close this issue. If the problem occurs again, we can raise a new issue at that time.",,,,,,Anecdotal,comment,,,,,,,,2025-07-16,github/kgw7401,https://github.com/apache/airflow/issues/52793#issuecomment-3078206218,repo: apache/airflow | issue: KeyError: 'Variable GIT_TOKEN does not exist' even if defined | keyword: best practice
"Agree. Not enough actionable data. And yes you need to follow best practices when you define your Dags - and make sure your environment is sound and replicable for all your instances of all your deployments. Even if you have an issue - it's part of your deployment configuration, not airflow Issue.",,,,,,Anecdotal,comment,,,,,,,,2025-07-17,github/potiuk,https://github.com/apache/airflow/issues/52793#issuecomment-3084302190,repo: apache/airflow | issue: KeyError: 'Variable GIT_TOKEN does not exist' even if defined | keyword: best practice
Add proper spec parameters to ``MagicMock`` usage in dag processing tests Fixed MagicMock instances without spec parameters in dag processing tests. Added proper spec parameters using `FilteringBoundLogger` for logger objects and BinaryIO for `logger_filehandle` objects to improve test effectiveness. This follows testing best practices by ensuring mocks properly simulate the expected interface of the objects they replace. <!-- Licensed to the Apache Software Foundation (ASF) under one or more c…,,,,,,Anecdotal,issue,,,,,,,,2025-07-11,github/kaxil,https://github.com/apache/airflow/pull/53205,repo: apache/airflow | keyword: best practice | state: closed
"Nil pointer error in configmap.yaml when .Values.config.api.base_url is not set ### Official Helm Chart version 1.17.0 (latest released) ### Apache Airflow version 3.0.2 ### Kubernetes Version 1.33.0 ### Helm Chart configuration I am using the following yaml file [values.yaml](https://github.com/apache/airflow/blob/helm-chart/1.17.0/chart/values.yaml) ### Docker Image customizations _No response_ ### What happened When deploying the Airflow Helm chart, I encounter the following error during tem…",,,,,,Anecdotal,issue,,,,,,,,2025-07-01,github/rg2609,https://github.com/apache/airflow/issues/52645,repo: apache/airflow | keyword: best practice | state: closed
"Provide an alternative OpenTelemetry implementation for traces that follows standard otel practices <!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License…",,,,,,Anecdotal,issue,,,,,,,,2024-11-12,github/xBis7,https://github.com/apache/airflow/pull/43941,repo: apache/airflow | keyword: best practice | state: closed
"Congratulations on your first Pull Request and welcome to the Apache Airflow community! If you have any issues or are unsure about any anything please check our Contributors' Guide (https://github.com/apache/airflow/blob/main/contributing-docs/README.rst) Here are some useful points: - Pay attention to the quality of your code (ruff, mypy and type annotations). Our [pre-commits]( https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#prerequisites-for-pre-commit…",,,,,,Anecdotal,comment,,,,,,,,2024-11-12,github/boring-cyborg[bot],https://github.com/apache/airflow/pull/43941#issuecomment-2471243631,repo: apache/airflow | issue: Provide an alternative OpenTelemetry implementation for traces that follows standard otel practices | keyword: best practice
"I'm confirming, but otel traces might have been experimental still, and if that's the case we are free to change them for a good reason, and your description certainly sounds like one!",,,,,,Anecdotal,comment,,,,,,,,2024-11-12,github/ashb,https://github.com/apache/airflow/pull/43941#issuecomment-2471372742,repo: apache/airflow | issue: Provide an alternative OpenTelemetry implementation for traces that follows standard otel practices | keyword: best practice
"If I may, can you add some indication to the title that this is related to OTel Traces specifically? We also have OTel Metrics implemented and it would be nice to minimize confusion.",,,,,,Anecdotal,comment,,,,,,,,2024-11-12,github/ferruzzi,https://github.com/apache/airflow/pull/43941#issuecomment-2471507662,repo: apache/airflow | issue: Provide an alternative OpenTelemetry implementation for traces that follows standard otel practices | keyword: best practice
"@ferruzzi I adjusted the title. The only change in this patch that is related to metrics, it's https://github.com/apache/airflow/pull/43941/files#diff-1cca954ec0be1aaf2c212e718c004cb0902a96ac60043bf0c97a782dee52cc32R85-R86 If you think that it's out of scope, then I can remove it.",,,,,,Anecdotal,comment,,,,,,,,2024-11-13,github/xBis7,https://github.com/apache/airflow/pull/43941#issuecomment-2472870498,repo: apache/airflow | issue: Provide an alternative OpenTelemetry implementation for traces that follows standard otel practices | keyword: best practice
"My initial approach wasn't considering scheduler HA. I've updated the patch accordingly. There have been two main challenges * Opentelemetry spans are designed so that only the process that starts them, can end them * The span objects can't be shared or stored to a db * The airflow philosophy for scheduler HA is that the only shared state between multiple schedulers is the db * It is very common that one scheduler starts a dag (also starts the span) and another scheduler finishes the dag (shoul…",,,,,,Anecdotal,comment,,,,,,,,2024-12-02,github/xBis7,https://github.com/apache/airflow/pull/43941#issuecomment-2512595217,repo: apache/airflow | issue: Provide an alternative OpenTelemetry implementation for traces that follows standard otel practices | keyword: best practice
"Hi @ashb, * I've addressed all your comments, * I created a new migration file, * I added some unit tests for the scheduler changes * I fixed 2 bugs that I came across while making the changes. Please take a look when you get a chance and let me know how it looks. I'll push some commits for moving the dagrun/scheduler span changes to new methods, to make the code more readable. > I think this will work, though the main question I have is: if we can re-create the span for an unhealthy scheduler,…",,,,,,Anecdotal,comment,,,,,,,,2024-12-18,github/xBis7,https://github.com/apache/airflow/pull/43941#issuecomment-2551301114,repo: apache/airflow | issue: Provide an alternative OpenTelemetry implementation for traces that follows standard otel practices | keyword: best practice
"I'm aware this is waiting on my for a review, I will look at this when I can, but I've got some critical bits of AIP-72 (Task Execution Interface) to land first.",,,,,,Anecdotal,comment,,,,,,,,2025-01-15,github/ashb,https://github.com/apache/airflow/pull/43941#issuecomment-2593336281,repo: apache/airflow | issue: Provide an alternative OpenTelemetry implementation for traces that follows standard otel practices | keyword: best practice
"> I'm aware this is waiting on my for a review, I will look at this when I can, but I've got some critical bits of AIP-72 (Task Execution Interface) to land first. hey @ashb ! just checking in—do you have a rough idea of when you might be able to take a look? we’re really excited about this change and appreciate your time!",,,,,,Anecdotal,comment,,,,,,,,2025-03-03,github/mladjan-gadzic,https://github.com/apache/airflow/pull/43941#issuecomment-2694744900,repo: apache/airflow | issue: Provide an alternative OpenTelemetry implementation for traces that follows standard otel practices | keyword: best practice
"I got most of the way though this, I'll finish reviewing tomorrow. Overall looks like a sound plan, very well documented and commented.",,,,,,Anecdotal,comment,,,,,,,,2025-03-04,github/ferruzzi,https://github.com/apache/airflow/pull/43941#issuecomment-2695965297,repo: apache/airflow | issue: Provide an alternative OpenTelemetry implementation for traces that follows standard otel practices | keyword: best practice
"Alright, I apologize for all the individual comments, I wasn't able to carve out a big block of time to get it all in one go, so I made my way through when I could. Overall it looks great and I love how detailed your tests are.",,,,,,Anecdotal,comment,,,,,,,,2025-03-05,github/ferruzzi,https://github.com/apache/airflow/pull/43941#issuecomment-2699774229,repo: apache/airflow | issue: Provide an alternative OpenTelemetry implementation for traces that follows standard otel practices | keyword: best practice
"@ferruzzi Thanks for the review! I've merged the branch with main. I used to pass the carrier as a parameter to the task command so that it can be accessed from within the dag but I can see that the `task_run` method has been removed. Once I figure out a new approach for this and fix any failures that come from the update, I'll start addressing your comments.",,,,,,Anecdotal,comment,,,,,,,,2025-03-09,github/xBis7,https://github.com/apache/airflow/pull/43941#issuecomment-2708836656,repo: apache/airflow | issue: Provide an alternative OpenTelemetry implementation for traces that follows standard otel practices | keyword: best practice
"Oh amazing, glad to see this was updated a bit. I plan to re-review and hopefully merge next week, particularly in light of the Task Execution api changes",,,,,,Anecdotal,comment,,,,,,,,2025-03-22,github/ashb,https://github.com/apache/airflow/pull/43941#issuecomment-2745247228,repo: apache/airflow | issue: Provide an alternative OpenTelemetry implementation for traces that follows standard otel practices | keyword: best practice
"@ashb @ferruzzi Thank you for all the help! Although the PR is in a good state and the CI is green, I'm going to have to revisit the changes on the `base_executor.py`. Due to the task api changes, I'm not convinced that creating spans under a task work as expected. Also, I used to set the `start_time` for task spans to the `queued_dttm`which isn't available anymore at the time of initializing a workload `TaskInstance`. My new integration tests don't run on the CI but when I run them locally, ta…",,,,,,Anecdotal,comment,,,,,,,,2025-03-22,github/xBis7,https://github.com/apache/airflow/pull/43941#issuecomment-2745399420,repo: apache/airflow | issue: Provide an alternative OpenTelemetry implementation for traces that follows standard otel practices | keyword: best practice
"Hello @ferruzzi @ashb, the PR is ready for review. My tests are all passing. I've run them at least 10 times in a row. From what I understand, the new task sdk has removed direct db access for tasks. I've gone through most of the new code but I'm not entirely familiar with the full extend of it. There might be some db related changes in this patch that aren't needed anymore. If you see something that is redundant, please point it out. BTW, I plan to create a follow up PR for cleanup but I would…",,,,,,Anecdotal,comment,,,,,,,,2025-04-02,github/xBis7,https://github.com/apache/airflow/pull/43941#issuecomment-2773064308,repo: apache/airflow | issue: Provide an alternative OpenTelemetry implementation for traces that follows standard otel practices | keyword: best practice
"Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.",,,,,,Anecdotal,comment,,,,,,,,2025-04-03,github/boring-cyborg[bot],https://github.com/apache/airflow/pull/43941#issuecomment-2775659328,repo: apache/airflow | issue: Provide an alternative OpenTelemetry implementation for traces that follows standard otel practices | keyword: best practice
"Remove `pytest.mark.db_test` from providers where possible ### Body After #51930 where @amoghrajesh introduces a way how tests can define connections without DB and follow up in #52017 where I made an attempt to remove some of the `pytest.mark.db_test` we should now remove those marks from providers, where it is easy. Some providers still use DB for other things, but likely there are many providers that only used db to create a Connection and we can turn those tests into non-db tests. - [x] air…",,,,,,Anecdotal,issue,,,,,,,,2025-06-22,github/potiuk,https://github.com/apache/airflow/issues/52020,repo: apache/airflow | keyword: best practice | state: closed
Also -> https://github.com/apache/airflow/pull/52021 added all the providers that have currently no db_tests to the pre-commit.,,,,,,Anecdotal,comment,,,,,,,,2025-06-22,github/potiuk,https://github.com/apache/airflow/issues/52020#issuecomment-2994213433,repo: apache/airflow | issue: Remove `pytest.mark.db_test` from providers where possible | keyword: best practice
"> I will have a look at Weaviate For Weaviate, three test cases are decorated with `@pytest.mark.db_test`. * `tests/unit/weaviate/operators/test_weaviate.py::TestWeaviateIngestOperator::test_partial_batch_hook_params` * `tests/unit/weaviate/operators/test_weaviate.py::TestWeaviateDocumentIngestOperator::test_partial_hook_params` * `tests/unit/weaviate/operators/test_weaviate.py::TestWeaviateIngestOperator::test_templates` After removing the marks, all three tests reported FAILED/ERROR as shown …",,,,,,Anecdotal,comment,,,,,,,,2025-06-22,github/sjyangkevin,https://github.com/apache/airflow/issues/52020#issuecomment-2994378535,repo: apache/airflow | issue: Remove `pytest.mark.db_test` from providers where possible | keyword: best practice
"Snowflake, there are 9 test cases are decorated by `@pytest.mark.db_test`. No change at the moment. * 8 test cases using `dag_maker` * 1 test case create an instance of `SQLExecuteQueryOperator` (with mocked `get_db_hook`), and execute the operators. Test Cases: * `tests/unit/snowflake/decorators/test_snowpark.py::TestSnowparkDecorator::test_snowpark_decorator_no_param` * `tests/unit/snowflake/decorators/test_snowpark.py::TestSnowparkDecorator::test_snowpark_decorator_with_param` * `tests/unit/…",,,,,,Anecdotal,comment,,,,,,,,2025-06-22,github/sjyangkevin,https://github.com/apache/airflow/issues/52020#issuecomment-2994490605,repo: apache/airflow | issue: Remove `pytest.mark.db_test` from providers where possible | keyword: best practice
"Alibaba, there are 9 test cases that access DB, due to the creation of task instance. No change at the moment. * `tests/unit/alibaba/cloud/log/test_oss_task_handler.py::TestOSSTaskHandler::test_hook` * `tests/unit/alibaba/cloud/log/test_oss_task_handler.py::TestOSSTaskHandler::test_oss_log_exists` * `tests/unit/alibaba/cloud/log/test_oss_task_handler.py::TestOSSTaskHandler::test_oss_read` * `tests/unit/alibaba/cloud/log/test_oss_task_handler.py::TestOSSTaskHandler::test_oss_write_into_remote_ex…",,,,,,Anecdotal,comment,,,,,,,,2025-06-23,github/sjyangkevin,https://github.com/apache/airflow/issues/52020#issuecomment-2994654796,repo: apache/airflow | issue: Remove `pytest.mark.db_test` from providers where possible | keyword: best practice
There is only one mark and after removing `pytestmark = pytest.mark.db_test` in `apprise` 3 tests failed: ``` FAILED tests/unit/apprise/notifications/test_apprise.py::TestAppriseNotifier::test_notifier - airflow.exceptions.AirflowInternalRuntimeError: Your test accessed the DB but `_AIRFLOW_SKIP_DB_TESTS` is set. Either make sure your test does not use database or mark the test with `@pytest.mark.db_test` See https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#b…,,,,,,Anecdotal,comment,,,,,,,,2025-06-23,github/Prab-27,https://github.com/apache/airflow/issues/52020#issuecomment-2994755171,repo: apache/airflow | issue: Remove `pytest.mark.db_test` from providers where possible | keyword: best practice
"For celery, there are 5 tests decorated with `@pytest.mark.db_test`. No change is required at the moment, and they are DB tests. Apart from that, a small PR: https://github.com/apache/airflow/pull/52067",,,,,,Anecdotal,comment,,,,,,,,2025-06-23,github/sunank200,https://github.com/apache/airflow/issues/52020#issuecomment-2995224860,repo: apache/airflow | issue: Remove `pytest.mark.db_test` from providers where possible | keyword: best practice
"> I attempted in ssh and failed miserably applying the new fixture... But SMTP was partly cleaned now... Oh, what issue did you run into?",,,,,,Anecdotal,comment,,,,,,,,2025-06-23,github/amoghrajesh,https://github.com/apache/airflow/issues/52020#issuecomment-2995646938,repo: apache/airflow | issue: Remove `pytest.mark.db_test` from providers where possible | keyword: best practice
"> > I attempted in ssh and failed miserably applying the new fixture... But SMTP was partly cleaned now... > > Oh, what issue did you run into? Try it :) you will see - I think the problem was that sshHook tries to do ""too much""",,,,,,Anecdotal,comment,,,,,,,,2025-06-23,github/potiuk,https://github.com/apache/airflow/issues/52020#issuecomment-2995949527,repo: apache/airflow | issue: Remove `pytest.mark.db_test` from providers where possible | keyword: best practice
"I have requested removal for provider/asana. for asana, there is nothing to remove for the hooks but only to add mark pytest.mark.db_test for operators. please review my first PR is correct. I appreciate.",,,,,,Anecdotal,comment,,,,,,,,2025-06-23,github/yonghyeokrhee,https://github.com/apache/airflow/issues/52020#issuecomment-2996624232,repo: apache/airflow | issue: Remove `pytest.mark.db_test` from providers where possible | keyword: best practice
"There is no change to * apache `druid`, `hdfs`, `kylin`, `pig`, and `pinot`. * atlassian * common `io` Still db tests left.",,,,,,Anecdotal,comment,,,,,,,,2025-06-23,github/kevinhongzl,https://github.com/apache/airflow/issues/52020#issuecomment-2996863650,repo: apache/airflow | issue: Remove `pytest.mark.db_test` from providers where possible | keyword: best practice
There is only one mark and after removing it in `microsoft/mssql` 1 test failed: ``` FAILED tests/unit/microsoft/mssql/hooks/test_mssql.py::TestMsSqlHook::test_sqlalchemy_scheme_is_from_hook - airflow.exceptions.AirflowInternalRuntimeError: Your test accessed the DB but `_AIRFLOW_SKIP_DB_TESTS` is set. Either make sure your test does not use database or mark the test with `@pytest.mark.db_test` ```,,,,,,Anecdotal,comment,,,,,,,,2025-06-23,github/guan404ming,https://github.com/apache/airflow/issues/52020#issuecomment-2997011410,repo: apache/airflow | issue: Remove `pytest.mark.db_test` from providers where possible | keyword: best practice
Regarding the Slack provider: * I confirmed that there is **no** `pytestmark = pytest.mark.db_test` in the Slack provider. * There is one test using `@pytest.mark.db_test` in https://github.com/apache/airflow/blob/main/providers/slack/tests/unit/slack/transfers/test_sql_to_slack_webhook.py but the database dependency cannot be removed there so no changes were made to the Slack provider.,,,,,,Anecdotal,comment,,,,,,,,2025-06-23,github/choo121600,https://github.com/apache/airflow/issues/52020#issuecomment-2997769178,repo: apache/airflow | issue: Remove `pytest.mark.db_test` from providers where possible | keyword: best practice
"> > > I attempted in ssh and failed miserably applying the new fixture... But SMTP was partly cleaned now... > > > > > > Oh, what issue did you run into? > > Try it :) you will see - I think the problem was that sshHook tries to do ""too much"" @amoghrajesh problem is: ``` tests/unit/ssh/hooks/test_ssh.py:479: in test_ssh_connection hook = SSHHook(ssh_conn_id=""ssh_default"") src/airflow/providers/ssh/hooks/ssh.py:152: in __init__ conn = self.get_connection(self.ssh_conn_id) ../../airflow-core/src/…",,,,,,Anecdotal,comment,,,,,,,,2025-06-23,github/jscheffl,https://github.com/apache/airflow/issues/52020#issuecomment-2998042231,repo: apache/airflow | issue: Remove `pytest.mark.db_test` from providers where possible | keyword: best practice
"Fix: Unclosed aiohttp ClientSession and TCPConnector in DatabricksRunNowOperator (deferrable=True) Closes: #51910 ### What this PR does Fixes unclosed `aiohttp.ClientSession` and `TCPConnector` warnings when using `DatabricksRunNowOperator` with `deferrable=True` in Airflow 3.0.2 and Databricks Provider 7.4.0. ### Background As described in #51910, the following errors appear during deferrable task execution: ``` Unclosed client session client_session: <aiohttp.client.ClientSession object at 0x…",,,,,,Anecdotal,issue,,,,,,,,2025-06-23,github/SalikramPaudel,https://github.com/apache/airflow/pull/52119,repo: apache/airflow | keyword: best practice | state: closed
"Congratulations on your first Pull Request and welcome to the Apache Airflow community! If you have any issues or are unsure about any anything please check our Contributors' Guide (https://github.com/apache/airflow/blob/main/contributing-docs/README.rst) Here are some useful points: - Pay attention to the quality of your code (ruff, mypy and type annotations). Our [pre-commits]( https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#prerequisites-for-pre-commit…",,,,,,Anecdotal,comment,,,,,,,,2025-06-23,github/boring-cyborg[bot],https://github.com/apache/airflow/pull/52119#issuecomment-2998207867,repo: apache/airflow | issue: Fix: Unclosed aiohttp ClientSession and TCPConnector in DatabricksRunNowOperator (deferrable=True) | keyword: best practice
"@potiuk , I've submitted this PR as part of the effort to resolve issue #51910 Whenever you have a moment, please take a look and share your feedback. I'd appreciate your review. Thanks in advance for your time.",,,,,,Anecdotal,comment,,,,,,,,2025-06-26,github/SalikramPaudel,https://github.com/apache/airflow/pull/52119#issuecomment-3009332829,repo: apache/airflow | issue: Fix: Unclosed aiohttp ClientSession and TCPConnector in DatabricksRunNowOperator (deferrable=True) | keyword: best practice
"cc: @pankajkoti => since you are working on this operator's Airflow 3 compatibility, maybe you can take a look as well ?",,,,,,Anecdotal,comment,,,,,,,,2025-06-26,github/potiuk,https://github.com/apache/airflow/pull/52119#issuecomment-3010268929,repo: apache/airflow | issue: Fix: Unclosed aiohttp ClientSession and TCPConnector in DatabricksRunNowOperator (deferrable=True) | keyword: best practice
"Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.",,,,,,Anecdotal,comment,,,,,,,,2025-06-27,github/boring-cyborg[bot],https://github.com/apache/airflow/pull/52119#issuecomment-3012016585,repo: apache/airflow | issue: Fix: Unclosed aiohttp ClientSession and TCPConnector in DatabricksRunNowOperator (deferrable=True) | keyword: best practice
"Unclosed aiohttp ClientSession and TCPConnector in DatabricksRunNowOperator with deferrable=True (Airflow 3.0.2, Databricks Provider 7.4.0) ### Apache Airflow Provider(s) databricks ### Versions of Apache Airflow Providers 7.4.0 ### Apache Airflow version 3.0.2 ### Operating System Ubuntu 24.04.2 LTS ### Deployment Virtualenv installation ### Deployment details - **Deployment Type**: Virtualenv installation - **Operating System**: Ubuntu 24.04.2 LTS - **Python Version**: 3.12.3 - **Airflow Vers…",,,,,,Anecdotal,issue,,,,,,,,2025-06-19,github/albertwangnz,https://github.com/apache/airflow/issues/51910,repo: apache/airflow | keyword: best practice | state: closed
"Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.",,,,,,Anecdotal,comment,,,,,,,,2025-06-19,github/boring-cyborg[bot],https://github.com/apache/airflow/issues/51910#issuecomment-2986473144,"repo: apache/airflow | issue: Unclosed aiohttp ClientSession and TCPConnector in DatabricksRunNowOperator with deferrable=True (Airflow 3.0.2, Databricks Provider 7.4.0) | keyword: best practice"
"Hi team, I was able to reproduce this issue using the latest stable versions: apache-airflow==3.0.2 apache-airflow-providers-databricks==7.4.0 Authentication: Azure AAD with ClientSecretCredential Operator: DatabricksRunNowOperator(deferrable=True) When the task is deferred, I still see the following warning right after token refresh succeeds: ``` [2025-06-20, 22:53:02] INFO - ClientSecretCredential.get_token succeeded: source=""azure.identity.aio._internal.get_token_mixin"" [2025-06-20, 22:53:02…",,,,,,Anecdotal,comment,,,,,,,,2025-06-21,github/SalikramPaudel,https://github.com/apache/airflow/issues/51910#issuecomment-2993308864,"repo: apache/airflow | issue: Unclosed aiohttp ClientSession and TCPConnector in DatabricksRunNowOperator with deferrable=True (Airflow 3.0.2, Databricks Provider 7.4.0) | keyword: best practice"
"Hi @potiuk, I tried but failed to fix the issue. @SalikramPaudel would like to help. Can I or you assign the issue to him? Cheers. Regards, Albert",,,,,,Anecdotal,comment,,,,,,,,2025-06-22,github/albertwangnz,https://github.com/apache/airflow/issues/51910#issuecomment-2994431180,"repo: apache/airflow | issue: Unclosed aiohttp ClientSession and TCPConnector in DatabricksRunNowOperator with deferrable=True (Airflow 3.0.2, Databricks Provider 7.4.0) | keyword: best practice"
"Enhance Snowflake SQL API Hook with Retry Logic for Query Status Polling ### Apache Airflow Provider(s) snowflake ### Versions of Apache Airflow Providers We’ve identified an issue with the SnowflakeSqlApiHook used by the SnowflakeSqlApiOperator in the Apache Airflow Snowflake provider. In certain cases, after submitting a query to Snowflake’s SQL API, the initial request succeeds, but polling the status endpoint fails due to a RemoteDisconnected error, caused by the remote Snowflake endpoint f…",,,,,,Anecdotal,issue,,,,,,,,2025-05-12,github/jonathanleek,https://github.com/apache/airflow/issues/50514,repo: apache/airflow | keyword: best practice | state: closed
"Chart: Always deploy jwt secret While using a pre-install hook is nice since the secret doesn't change, the downside is on upgrade the jwt secret will never be added. As this is a new secret for 3.0, that is more problematic than having a changing secret. Plus, it's best practice to set this explicitly anyway. Follow up of #49923.",,,,,,Anecdotal,issue,,,,,,,,2025-06-16,github/jedcunningham,https://github.com/apache/airflow/pull/51799,repo: apache/airflow | keyword: best practice | state: closed
Impove dynamic DAG generation docs ### Description The main purpose of this MR is to enhance documentation by describing a way for `Abstraction of DAG objects generation` following the discussion from [here](https://github.com/apache/airflow/issues/50020). I strongly believe this part is essential in docs because it shows an easy way for creating an abstraction for DAG templates and reusing it both in single DAG generation and in dynamic way. related: #50020 <!-- Licensed to the Apache Software…,,,,,,Anecdotal,issue,,,,,,,,2025-05-10,github/sanederchik,https://github.com/apache/airflow/pull/50441,repo: apache/airflow | keyword: best practice | state: closed
"Congratulations on your first Pull Request and welcome to the Apache Airflow community! If you have any issues or are unsure about any anything please check our Contributors' Guide (https://github.com/apache/airflow/blob/main/contributing-docs/README.rst) Here are some useful points: - Pay attention to the quality of your code (ruff, mypy and type annotations). Our [pre-commits]( https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#prerequisites-for-pre-commit…",,,,,,Anecdotal,comment,,,,,,,,2025-05-10,github/boring-cyborg[bot],https://github.com/apache/airflow/pull/50441#issuecomment-2869042016,repo: apache/airflow | issue: Impove dynamic DAG generation docs | keyword: best practice
"i18n: Replaced hard-coded ""no {modelName} found"" message in DataTable ## Why introduce `noItemsFound` as an improved default fallback For pages that currently lack a customized noRowsMessage, DataTable will now display a fully translated message. For example, instead of an erroneous ""No 任務實例 found"" (a mix of English and Chinese), the UI will correctly render a message like ""找不到任務實例"". ## Why is the `noRowsMessage` prop still crucial Relying solely on this generic template with modelName interpol…",,,,,,Anecdotal,issue,,,,,,,,2025-05-29,github/RoyLee1224,https://github.com/apache/airflow/pull/51209,repo: apache/airflow | keyword: best practice | state: closed
Feature/implement i18n for Dashboard and SideBar ## Related Issue #9864 cc: @bbovenzi ## Summary This is the initial phase of internationalization (i18n) for the **Dashboard** and **Navigation (Nav)** components. ## Changes Made - Integrated `react-i18next` to enable translation of static text. - Enabled automatic language detection and persistence using `i18next-browser-languagedetector`: - Enabled caching of the user's selected language directly into `localStorage`. - Added translation json f…,,,,,,Anecdotal,issue,,,,,,,,2025-05-14,github/RoyLee1224,https://github.com/apache/airflow/pull/50626,repo: apache/airflow | keyword: best practice | state: closed
update: enabled translation for states ![CleanShot 2025-05-15 at 18 12 17@2x](https://github.com/user-attachments/assets/cb0c5607-405d-400a-9817-ecbebde126bb),,,,,,Anecdotal,comment,,,,,,,,2025-05-15,github/RoyLee1224,https://github.com/apache/airflow/pull/50626#issuecomment-2883294799,repo: apache/airflow | issue: Feature/implement i18n for Dashboard and SideBar | keyword: best practice
"Hi @pierrejeambrun , Thanks for your guidance and the clear path forward! I completely agree that separating the ""Terminology Translations for Chinese"" into a distinct PR is the best approach. I also agree with your point that it's better to fully apply the i18n infrastructure before merging other translations. However, in my opinion, temporarily retaining the ""Traditional Chinese"" option in the language switcher might be beneficial. If `zh_TW` is selected, any text that hasn't correctly wrappe…",,,,,,Anecdotal,comment,,,,,,,,2025-05-15,github/RoyLee1224,https://github.com/apache/airflow/pull/50626#issuecomment-2884268284,repo: apache/airflow | issue: Feature/implement i18n for Dashboard and SideBar | keyword: best practice
"Actually, I think I'm leaning towards Pierre's side now. I would like to encourage discussion on how to translate Airflow topics separate from the system setup. We can fully test the switching languages in that second PR too. Filling in every spot that's missing will still take time and we can add that linting rule in later too #protm",,,,,,Anecdotal,comment,,,,,,,,2025-05-15,github/bbovenzi,https://github.com/apache/airflow/pull/50626#issuecomment-2884859399,repo: apache/airflow | issue: Feature/implement i18n for Dashboard and SideBar | keyword: best practice
@josix I know you have been working on Python translation for a long time and understand Airflow well. Would you be interested in helping on this and the following ones 🙂,,,,,,Anecdotal,comment,,,,,,,,2025-05-16,github/Lee-W,https://github.com/apache/airflow/pull/50626#issuecomment-2885658355,repo: apache/airflow | issue: Feature/implement i18n for Dashboard and SideBar | keyword: best practice
Update: Fully implemented i18n for Nav Bar and dashboard. - https://github.com/user-attachments/assets/f1a832a5-8c73-419e-bcdc-198f2003c9d1,,,,,,Anecdotal,comment,,,,,,,,2025-05-17,github/RoyLee1224,https://github.com/apache/airflow/pull/50626#issuecomment-2887873504,repo: apache/airflow | issue: Feature/implement i18n for Dashboard and SideBar | keyword: best practice
Very cool! Looking forward to contribute German translation! Will be a challenge in the future that whenever the UI is extended that all languages are considered later to translate DIFFs... me might need a small tool to check for missing texts to that some native-speaker volunteers can check prior a release if some leftovers require translation. I opt-in for German then... so maybe we need to add some CODEOWNERS here.... once this is merged.,,,,,,Anecdotal,comment,,,,,,,,2025-05-17,github/jscheffl,https://github.com/apache/airflow/pull/50626#issuecomment-2888548097,repo: apache/airflow | issue: Feature/implement i18n for Dashboard and SideBar | keyword: best practice
> Very cool! Looking forward to contribute German translation! Will be a challenge in the future that whenever the UI is extended that all languages are considered later to translate DIFFs... me might need a small tool to check for missing texts to that some native-speaker volunteers can check prior a release if some leftovers require translation. I opt-in for German then... so maybe we need to add some CODEOWNERS here.... once this is merged. I can help with Mandarin 🙂,,,,,,Anecdotal,comment,,,,,,,,2025-05-18,github/Lee-W,https://github.com/apache/airflow/pull/50626#issuecomment-2888784480,repo: apache/airflow | issue: Feature/implement i18n for Dashboard and SideBar | keyword: best practice
> Very cool! Looking forward to contribute German translation! Will be a challenge in the future that whenever the UI is extended that all languages are considered later to translate DIFFs... me might need a small tool to check for missing texts to that some native-speaker volunteers can check prior a release if some leftovers require translation. I opt-in for German then... so maybe we need to add some CODEOWNERS here.... once this is merged. After this PR we should add two linting rules: - Ch…,,,,,,Anecdotal,comment,,,,,,,,2025-05-19,github/bbovenzi,https://github.com/apache/airflow/pull/50626#issuecomment-2891357813,repo: apache/airflow | issue: Feature/implement i18n for Dashboard and SideBar | keyword: best practice
"> After this PR we should add two linting rules: > > * Check to see if a language is missing a translation string and throw a warning Mhm, if we have 15 languages and you add some small piece to a screen this would be a blocker if you need to take care for translations prior merge. That would slow-down a lot of UI changes. I assume at least this needs to be satisfied for PRs that are back-ported (in future to 3.1) or before a release is made to close all translation gaps. Ah, but you say ""Warni…",,,,,,Anecdotal,comment,,,,,,,,2025-05-19,github/jscheffl,https://github.com/apache/airflow/pull/50626#issuecomment-2892160924,repo: apache/airflow | issue: Feature/implement i18n for Dashboard and SideBar | keyword: best practice
"> > After this PR we should add two linting rules: > > ``` > > * Check to see if a language is missing a translation string and throw a warning > > ``` > > Mhm, if we have 15 languages and you add some small piece to a screen this would be a blocker if you need to take care for translations prior merge. That would slow-down a lot of UI changes. I assume at least this needs to be satisfied for PRs that are back-ported (in future to 3.1) or before a release is made to close all translation gaps. …",,,,,,Anecdotal,comment,,,,,,,,2025-05-20,github/bbovenzi,https://github.com/apache/airflow/pull/50626#issuecomment-2895367833,repo: apache/airflow | issue: Feature/implement i18n for Dashboard and SideBar | keyword: best practice
"AIP-84 | Add Auth for Dags related: #42360 ## Why Since #47062 was reverted in #47402, this PR will also address the `full test` issue here.",,,,,,Anecdotal,issue,,,,,,,,2025-03-06,github/jason810496,https://github.com/apache/airflow/pull/47433,repo: apache/airflow | keyword: lesson learned | state: closed
As expected full tests break and need fixing. Also while testing the UI I realized that the server is not happy with this change (can't load dags internal error 500) and we will need to wait for @vincbeck fix. `FabAuthManager` is crashing on `get_permitted_dag_ids`. This is a bug in the auth manager implementation and not detected by our tests at the moment because unit tests use the `SimpleAuthManager` where the dev setup use `FabAuthManager` by default.,,,,,,Anecdotal,comment,,,,,,,,2025-03-06,github/pierrejeambrun,https://github.com/apache/airflow/pull/47433#issuecomment-2703412456,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"> As expected full tests break and need fixing. Also while testing the UI I realized that the server is not happy with this change (can't load dags internal error 500) and we will need to wait for @vincbeck fix. `FabAuthManager` is crashing on `get_permitted_dag_ids`. This is a bug in the auth manager implementation and not detected by our tests at the moment because unit tests use the `SimpleAuthManager` where the dev setup use `FabAuthManager` by default. Just to double-check, do you mean tha…",,,,,,Anecdotal,comment,,,,,,,,2025-03-06,github/jason810496,https://github.com/apache/airflow/pull/47433#issuecomment-2703749450,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"There are two problems: - this PR is breaking the k8s KubeExec integration tests, this need to be fixed - this PR uses `get_permitted_dag_ids` which breaks the front-end at the moment, Vincent will fix this I believe. Other auth PRs do not use `get_permitted_dag_ids`, this is only introduced here, so other PRs don't need to be reverted.",,,,,,Anecdotal,comment,,,,,,,,2025-03-06,github/pierrejeambrun,https://github.com/apache/airflow/pull/47433#issuecomment-2703847317,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"> There are two problems: > > * this PR is breaking the k8s KubeExec integration tests, this need to be fixed > * this PR uses `get_permitted_dag_ids` which breaks the front-end at the moment, Vincent will fix this I believe. Other auth PRs do not use `get_permitted_dag_ids`, this is only introduced here, so other PRs don't need to be reverted. Thanks for confirmation, I will start fixing k8s integration tests.",,,,,,Anecdotal,comment,,,,,,,,2025-03-06,github/jason810496,https://github.com/apache/airflow/pull/47433#issuecomment-2703880409,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"Finally fixed the Kubernetes tests locally! Some notes: 1. The API server still uses FAB Auth Manager, so when retrieving a JWT, the same session must be used to handle the CSRF token session issue. Otherwise, it results in: ``` <html lang=en> <title>400 Bad Request</title> <h1>Bad Request</h1> <p>The CSRF session token is missing.</p> ``` 2. The current `redirect_url` from FAB is still `http://localhost:8080/?token=...`, but `KUBERNETES_HOST_PORT` is not `localhost:8080`. ~This can be fixed in…",,,,,,Anecdotal,comment,,,,,,,,2025-03-07,github/jason810496,https://github.com/apache/airflow/pull/47433#issuecomment-2707168758,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"I ran the Kubernetes tests locally yesterday, and they all passed. However, after rebasing today, they consistently fail. It seems that triggering a `dagRun` results in a 404 error: https://github.com/apache/airflow/blob/2ea7aed755006cfff3dedd30b5654e8e2534e197/kubernetes_tests/test_base.py#L265-L270 ``` self = <requests.adapters.HTTPAdapter object at 0x119d835b0>, request = <PreparedRequest [POST]>, stream = False, timeout = Timeout(connect=None, read=None, total=None), verify = True, cert = N…",,,,,,Anecdotal,comment,,,,,,,,2025-03-08,github/jason810496,https://github.com/apache/airflow/pull/47433#issuecomment-2708289346,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
No error with [Kubernetes tests / K8S System:KubernetesExecutor-3.9-v1.29.12-true](https://github.com/apache/airflow/actions/runs/13738401436/job/38425329184?pr=47433#logs) but [Kubernetes tests / K8S System:KubernetesExecutor-3.9-v1.29.12-false](https://github.com/apache/airflow/actions/runs/13738401436/job/38425329252?pr=47433#logs) always fail due to timeout. ``` =========================== short test summary info ============================ FAILED kubernetes_tests/test_kubernetes_executor.…,,,,,,Anecdotal,comment,,,,,,,,2025-03-09,github/jason810496,https://github.com/apache/airflow/pull/47433#issuecomment-2708685695,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"The Kubernetes test has finally been fixed! 🎉 However, the CI failed due to the flakiness of CeleryExecutor and LocalExecutor.",,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/jason810496,https://github.com/apache/airflow/pull/47433#issuecomment-2709694469,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"> It seems the reserialize command does not work well in the CI 🤔 I'll retrigger and see how it works If the current retry still fail. I will try removing `dag_reserialize` in tests, as it doesn’t seem to occur in the CI environment and only happened once in my local Kind cluster. I believe it should be safe to remove.",,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/jason810496,https://github.com/apache/airflow/pull/47433#issuecomment-2709771385,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"> > It seems the reserialize command does not work well in the CI 🤔 I'll retrigger and see how it works > > If the current retry still fail. I will try removing `dag_reserialize` in tests, as it doesn’t seem to occur in the CI environment and only happened once in my local Kind cluster. I believe it should be safe to remove. Sounds like a good idea. I do have some concerns on this 🤔 let's see how it goes this time",,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/Lee-W,https://github.com/apache/airflow/pull/47433#issuecomment-2709778074,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"Retired a few times. Still fails. @jason810496 do you remember why it passed this morning 🤔 (but it came with conflict, we still need to reran then 🥲)",,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/Lee-W,https://github.com/apache/airflow/pull/47433#issuecomment-2710213578,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"> Retired a few times. Still fails. @jason810496 do you remember why it passed this morning 🤔 (but it came with conflict, we still need to reran then 🥲) At that time: - `dag_reserialize` was called before the test. - However, with `dag_reserialize` enabled: - The KubernetesExecutor seemed more stable in tests. - The CeleryExecutor and LocalExecutor tended to time out during tests. Regardless, I will first fix `docker_tests/test_docker_compose_quick_start.py`. When I push the fix, I’ll also run …",,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/jason810496,https://github.com/apache/airflow/pull/47433#issuecomment-2710315601,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"> Adding dag_reserialize back. Why is this needed? Because the dag is not yet serialized? If so, should we add wait in the test instead?",,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/Lee-W,https://github.com/apache/airflow/pull/47433#issuecomment-2710346460,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"> > Adding dag_reserialize back. > > Why is this needed? Because the dag is not yet serialized? Yes, explicitly running `airflow dags reserialize` can resolve the issue described in https://github.com/apache/airflow/pull/47433#issuecomment-2708289346 (where triggering a new `dagRun` results in a 404, indicating that `DagModel.is_active` is `False`). > If so, should we add a wait in the test instead? I believe explicitly running `reserialize` is more precise than waiting for a fixed duration, wh…",,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/jason810496,https://github.com/apache/airflow/pull/47433#issuecomment-2710365025,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"We already have each executor successfully ran in k8s test, but we are not lucky enough to have them **all** green in same try. Do we still need to keep retry the test until we get all green in one try ? Most of them are failed with timeout.",,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/jason810496,https://github.com/apache/airflow/pull/47433#issuecomment-2710763390,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"> We already have each executor successfully ran in k8s test, but we are not lucky enough to have them **all** green in same try. > > Do we still need to keep retry the test until we get all green in one try ? Most of them are failed with timeout. TBH, I'm good with it. But I might not be the right person to judge. Would love to know how @ephraimbuddy or @jedcunningham think 🙂",,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/Lee-W,https://github.com/apache/airflow/pull/47433#issuecomment-2710809942,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
We can't merge anything that has a high chance of breaking the CI because it's hard to get all tests green at the same time (cause they are too flaky). We need something more stable. (or this will be hard to manage for other PRs that need to run these tests),,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/pierrejeambrun,https://github.com/apache/airflow/pull/47433#issuecomment-2710892505,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"> We can't merge anything that has a high chance of breaking the CI because it's hard to get all tests green at the same time (cause they are too flaky). > > We need something more stable. > > (or this will be hard to manage for other PRs that need to run these tests) Yep, what I meant was maybe we can extend the timeout time. but would like to confirm whether it was set that way for a reason",,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/Lee-W,https://github.com/apache/airflow/pull/47433#issuecomment-2710916494,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"> Yep, what I meant was maybe we can extend the timeout time. but would like to confirm whether it was set that way for a reason Sure. will increase it to 500 to ensure that the test failure is solely due to the timeout threshold. I will push if this try still fail.",,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/jason810496,https://github.com/apache/airflow/pull/47433#issuecomment-2710923367,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"Base on the following traceback, I think we need to add retry mechanism for `_get_jwt_token` part, or maybe having HTTP Adapter for 401, 403 status code. https://github.com/apache/airflow/actions/runs/13768914292/job/38504602702",,,,,,Anecdotal,comment,,,,,,,,2025-03-11,github/jason810496,https://github.com/apache/airflow/pull/47433#issuecomment-2712346686,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
New CI fail:[Prepare breeze & CI image:3.9](https://github.com/apache/airflow/actions/runs/13779187121/job/38534589200?pr=47433) probably related to rebase ? cc @Lee-W,,,,,,Anecdotal,comment,,,,,,,,2025-03-11,github/jason810496,https://github.com/apache/airflow/pull/47433#issuecomment-2712434185,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"@rawwar also ran the k8s test locally and found that it only fails when running all tests but passes when running a specific test. I'm looking into the reason now. --- Update: Not found the root cause, but add the following mechanism - Add `JWTRefreshAdapter` to handle 401, 403 status code. - Only rollout restart `airflow-api-server` deployment if needed.",,,,,,Anecdotal,comment,,,,,,,,2025-03-11,github/jason810496,https://github.com/apache/airflow/pull/47433#issuecomment-2712806654,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"Only 1 k8s test fail flaky 🎉 , I will rebase main and run again. ``` kubernetes_tests/test_kubernetes_pod_operator.py::TestKubernetesPodOperator::test_kubernetes_pod_operator_active_deadline_seconds[10] - requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')) ```",,,,,,Anecdotal,comment,,,,,,,,2025-03-11,github/jason810496,https://github.com/apache/airflow/pull/47433#issuecomment-2713262884,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"Limit celery by excluding 5.3.2 and 5.3.3 There is a new database field introduced by Celery in 5.3.2 and repeated in 5.3.3 wihch is not included in automated migrations, so users upgrading celery might have failing celery installation. The issue is already reported https://github.com/celery/celery/issues/8470 and acknowledged, so it is lilely to be fixed in 5.3.4 - so excluding 5.3.2 and 5.3.3 is the best approach. <!-- Licensed to the Apache Software Foundation (ASF) under one or more contrib…",,,,,,Anecdotal,issue,,,,,,,,2023-09-02,github/potiuk,https://github.com/apache/airflow/pull/34031,repo: apache/airflow | keyword: lesson learned | state: closed
"Also - question - I understand that this is purely celery thing ? When celery uses the DB, they manage their own migrations? I looked in our migrations and I have not found any celery migrations we run :)?",,,,,,Anecdotal,comment,,,,,,,,2023-09-02,github/potiuk,https://github.com/apache/airflow/pull/34031#issuecomment-1703760166,repo: apache/airflow | issue: Limit celery by excluding 5.3.2 and 5.3.3 | keyword: lesson learned
"> Also - question - I understand that this is purely celery thing ? When celery uses the DB, they manage their own migrations? I looked in our migrations and I have not found any celery migrations we run :)? In airflow, we run a query that somehow calls the new field and fails. It was reported in our internal testing of main yesterday. ``` scheduler Traceback (most recent call last): scheduler File ""/usr/local/lib/python3.11/site-packages/airflow/cli/commands/scheduler_command.py"", line 47, in …",,,,,,Anecdotal,comment,,,,,,,,2023-09-02,github/ephraimbuddy,https://github.com/apache/airflow/pull/34031#issuecomment-1703761391,repo: apache/airflow | issue: Limit celery by excluding 5.3.2 and 5.3.3 | keyword: lesson learned
"> Also - question - I understand that this is purely celery thing ? When celery uses the DB, they manage their own migrations? I looked in our migrations and I have not found any celery migrations we run :)? Yes, when user configures Celery to use SQLAlchemy as results backend, Celery will take care of table migration and schema. I think the bug is in their classes which try to use a field not created because of problem in the migration script. However Celery supports other backends (Redis, Rab…",,,,,,Anecdotal,comment,,,,,,,,2023-09-02,github/hussein-awala,https://github.com/apache/airflow/pull/34031#issuecomment-1703763497,repo: apache/airflow | issue: Limit celery by excluding 5.3.2 and 5.3.3 | keyword: lesson learned
"Yeah. It's all the ""celery"" internal code. And yeah - not everyone is affected. So I think the approach where we exclude only those two versions is a sound approach. I also asked if we can expect that 5.3.4 will have a fix - but I guess they have no choice but to fix it.",,,,,,Anecdotal,comment,,,,,,,,2023-09-02,github/potiuk,https://github.com/apache/airflow/pull/34031#issuecomment-1703765187,repo: apache/airflow | issue: Limit celery by excluding 5.3.2 and 5.3.3 | keyword: lesson learned
"Yep. Looks like the exclusion works as expected: <img width=""205"" alt=""Screenshot 2023-09-02 at 10 37 29"" src=""https://github.com/apache/airflow/assets/595491/c9bd38c7-47c6-4ac5-b9ca-e397304b943e"">",,,,,,Anecdotal,comment,,,,,,,,2023-09-02,github/potiuk,https://github.com/apache/airflow/pull/34031#issuecomment-1703765667,repo: apache/airflow | issue: Limit celery by excluding 5.3.2 and 5.3.3 | keyword: lesson learned
"> However Celery supports other backends (Redis, RabbitMQ, custom result backend, ...), meaning users using Airflow with Celery with a different backend than SQLAlchemy will not be impacted by this bug. BTW. This is why our `canary` builds did not detect it - we use Redis as the only backend we test for integration. But maybe this is a good idea to split them to ""celery-redis"", ""celery-sqlalchemy"", ""celery-rabbitmq"". Then we would detect it a bit earlier.",,,,,,Anecdotal,comment,,,,,,,,2023-09-02,github/potiuk,https://github.com/apache/airflow/pull/34031#issuecomment-1703766138,repo: apache/airflow | issue: Limit celery by excluding 5.3.2 and 5.3.3 | keyword: lesson learned
Celery v5.3.2 & v5.3.3 were yanked and the breaking changes reverted in `main`. A new stable patch version will be released soon. Keep this in mind so you won't exclude a valid version with important bug fixes @potiuk,,,,,,Anecdotal,comment,,,,,,,,2023-09-02,github/Nusnus,https://github.com/apache/airflow/pull/34031#issuecomment-1703833380,repo: apache/airflow | issue: Limit celery by excluding 5.3.2 and 5.3.3 | keyword: lesson learned
"> Celery v5.3.2 & v5.3.3 were yanked and the breaking changes reverted in `main`. A new stable patch version will be released soon. > > Keep this in mind so you won't exclude a valid version with important bug fixes @potiuk @Nusnus This new stable patch you're talking about will be released in v5.3.4, right?",,,,,,Anecdotal,comment,,,,,,,,2023-09-02,github/hussein-awala,https://github.com/apache/airflow/pull/34031#issuecomment-1703834705,repo: apache/airflow | issue: Limit celery by excluding 5.3.2 and 5.3.3 | keyword: lesson learned
"> > Celery v5.3.2 & v5.3.3 were yanked and the breaking changes reverted in `main`. A new stable patch version will be released soon. > > Keep this in mind so you won't exclude a valid version with important bug fixes @potiuk > > @Nusnus This new stable patch you're talking about will be released in v5.3.4, right? We've completely removed the 5.3.2+ releases. We'll have a single **5.3.2** new release. Not 5.3.4. That's why I commented on this issue to let you know guys that 5.3.2 does not exist…",,,,,,Anecdotal,comment,,,,,,,,2023-09-02,github/Nusnus,https://github.com/apache/airflow/pull/34031#issuecomment-1703847566,repo: apache/airflow | issue: Limit celery by excluding 5.3.2 and 5.3.3 | keyword: lesson learned
"> > > Celery v5.3.2 & v5.3.3 were yanked and the breaking changes reverted in `main`. A new stable patch version will be released soon. > > > Keep this in mind so you won't exclude a valid version with important bug fixes @potiuk > > > > > > @Nusnus This new stable patch you're talking about will be released in v5.3.4, right? > > We've completely removed the 5.3.2+ releases. We'll have a single **5.3.2** new release. Not 5.3.4. That's why I commented on this issue to let you know guys that 5.3.…",,,,,,Anecdotal,comment,,,,,,,,2023-09-02,github/hussein-awala,https://github.com/apache/airflow/pull/34031#issuecomment-1703850133,repo: apache/airflow | issue: Limit celery by excluding 5.3.2 and 5.3.3 | keyword: lesson learned
I think you really can't event have another 5.3.2 release. PyPi will not let you upload another 5.3.2 even if you delete the previous one,,,,,,Anecdotal,comment,,,,,,,,2023-09-02,github/potiuk,https://github.com/apache/airflow/pull/34031#issuecomment-1703851208,repo: apache/airflow | issue: Limit celery by excluding 5.3.2 and 5.3.3 | keyword: lesson learned
"@hussein-awala @potiuk With every failure, we learn new lessons. As the most recent Owner at Celery, I am still discovering what's under the hood and I've been investigating the release flow for some time. > I really don't know how you manage the releases in Celery, I will check if this procedure is described somewhere. Unfortunately, the release flow is very lacking as I have discovered these past few days. As far as I am aware, there isn't an official updated procedure, which is why this late…",,,,,,Anecdotal,comment,,,,,,,,2023-09-02,github/Nusnus,https://github.com/apache/airflow/pull/34031#issuecomment-1703860020,repo: apache/airflow | issue: Limit celery by excluding 5.3.2 and 5.3.3 | keyword: lesson learned
"> @Nusnus look here [pypa/packaging-problems#74](https://github.com/pypa/packaging-problems/issues/74) Seems like the “solutions"" there are even messier. v5.3.4 then. Lesson learned. Thank you!",,,,,,Anecdotal,comment,,,,,,,,2023-09-02,github/Nusnus,https://github.com/apache/airflow/pull/34031#issuecomment-1703860461,repo: apache/airflow | issue: Limit celery by excluding 5.3.2 and 5.3.3 | keyword: lesson learned
Support for Python 3.11 for Google Provider (upgrading all dependencies) I know it is eaarly (Python 3.11 has just been released yesterday) but we are hoping in Apache Airflow to a much faster cycle of adding new Python releases - especially that Pyhon 3.11 introduces huge performance improvements (25% is the average number claimed) due to a very focused effort to increase single-threaded Python performance (Specialized interpreter being the core of it but also many other improvements) without …,,,,,,Anecdotal,issue,,,,,,,,2022-10-26,github/potiuk,https://github.com/apache/airflow/issues/27292,repo: apache/airflow | keyword: lesson learned | state: closed
Upgrading dependencies for google provider package can be tested with Airflow System Tests and CI that is in under construction atm. FYI @bhirsz,,,,,,Anecdotal,comment,,,,,,,,2022-11-01,github/kosteev,https://github.com/apache/airflow/issues/27292#issuecomment-1298435393,repo: apache/airflow | issue: Support for Python 3.11 for Google Provider (upgrading all dependencies) | keyword: lesson learned
It seems that [python-bigquery-sqlalchemy](https://github.com/googleapis/python-bigquery-sqlalchemy) already supports Python 3.11,,,,,,Anecdotal,comment,,,,,,,,2023-01-17,github/rafalbiegacz,https://github.com/apache/airflow/issues/27292#issuecomment-1386013307,repo: apache/airflow | issue: Support for Python 3.11 for Google Provider (upgrading all dependencies) | keyword: lesson learned
https://github.com/googleapis/python-translate/releases/tag/v3.10.0 oogle-cloud-translat :heavy_check_mark: https://github.com/googleapis/python-texttospeech/releases/tag/v2.14.0 google-cloud-texttospeech :heavy_check_mark: https://github.com/googleapis/python-speech/releases/tag/v2.17.0 google-cloud-speech :heavy_check_mark: https://github.com/googleapis/python-spanner/releases/tag/v3.27.0 google-cloud-spanner :heavy_check_mark: https://github.com/googleapis/python-secret-manager/releases/tag/…,,,,,,Anecdotal,comment,,,,,,,,2023-03-02,github/raphaelauv,https://github.com/apache/airflow/issues/27292#issuecomment-1452686038,repo: apache/airflow | issue: Support for Python 3.11 for Google Provider (upgrading all dependencies) | keyword: lesson learned
I din't found a working constraint of `google-cloud-aiplatform` for `apache-airflow-providers-google==8.10.0` with `python 3.11`,,,,,,Anecdotal,comment,,,,,,,,2023-03-02,github/raphaelauv,https://github.com/apache/airflow/issues/27292#issuecomment-1452712966,repo: apache/airflow | issue: Support for Python 3.11 for Google Provider (upgrading all dependencies) | keyword: lesson learned
As of `2023-03-04` `google-cloud-aiplatform` is not marked as supporting python 3.11 [source](https://github.com/googleapis/python-aiplatform/blob/c515510a2c474772d2ae0fc1e715f6c919d5aebb/setup.py#L161-L172). I've opened https://github.com/googleapis/python-aiplatform/issues/2006 as it appears to have been unrequested. @potiuk Question for you that I've wondered about after chasing down a few of these updates. Has there been any thought given to breaking apart the google-provider into extras? i…,,,,,,Anecdotal,comment,,,,,,,,2023-03-04,github/r-richmond,https://github.com/apache/airflow/issues/27292#issuecomment-1454832422,repo: apache/airflow | issue: Support for Python 3.11 for Google Provider (upgrading all dependencies) | keyword: lesson learned
"Not only thoughts. There is an issue for actually spliting the provider : https://github.com/apache/airflow/issues/15933 - but this one is complex because of common parts so maintaining such split provider would be difficult to maintain (we learned a lot about it when we added `common.sql`. However- when it comes to extras, it could be a better solution indeed, I have not thought about it, but it might actually make it much easier for users and would let us pick and choose which extras in googl…",,,,,,Anecdotal,comment,,,,,,,,2023-03-04,github/potiuk,https://github.com/apache/airflow/issues/27292#issuecomment-1454842888,repo: apache/airflow | issue: Support for Python 3.11 for Google Provider (upgrading all dependencies) | keyword: lesson learned
the issue with spliting the provider is mostly that no one from Google picked it. Once someone picks it and start working on it we will be able to overcome the tech difficulties. We don't know yet how the provider will be split but we do know it must be done.,,,,,,Anecdotal,comment,,,,,,,,2023-03-04,github/eladkal,https://github.com/apache/airflow/issues/27292#issuecomment-1454850204,repo: apache/airflow | issue: Support for Python 3.11 for Google Provider (upgrading all dependencies) | keyword: lesson learned
"> the issue with spliting the provider is mostly that no one from Google picked it. Once someone picks it and start working on it we will be able to overcome the tech difficulties. We don't know yet how the provider will be split but we do know it must be done. I am not so sure. Actually - using extras might be way simpler approach that is going to solve most of the pains with getting all the libraries in I **think**, without introducing the **huge** hassle of extracting common code and using i…",,,,,,Anecdotal,comment,,,,,,,,2023-03-04,github/potiuk,https://github.com/apache/airflow/issues/27292#issuecomment-1454858434,repo: apache/airflow | issue: Support for Python 3.11 for Google Provider (upgrading all dependencies) | keyword: lesson learned
I don't fully agree and I don't think it's the same case. `common.sql` added a whole new functionality of a new generic operator. The issues were mostly around the new functionality and not around the new provider by itself. The lesson I learned there is add the new functionality first and give it some time before converting all other providers to use it. Back to the Google case. We are not adding anything new. This is more about re-organizing the existed code. To me it seems that the main reas…,,,,,,Anecdotal,comment,,,,,,,,2023-03-04,github/eladkal,https://github.com/apache/airflow/issues/27292#issuecomment-1454873522,repo: apache/airflow | issue: Support for Python 3.11 for Google Provider (upgrading all dependencies) | keyword: lesson learned
> Back to the Google case. We are not adding anything new. This is more about re-organizing the existed code. To me it seems that the main reason it's not split is the the [common folder](https://github.com/apache/airflow/tree/main/airflow/providers/google/common) which is being used by almost all google space and it will be hard to break it to individual providers. However this folder is not changing that much. Check the commit history and when it does change most of the commits are about styl…,,,,,,Anecdotal,comment,,,,,,,,2023-03-04,github/potiuk,https://github.com/apache/airflow/issues/27292#issuecomment-1454879163,repo: apache/airflow | issue: Support for Python 3.11 for Google Provider (upgrading all dependencies) | keyword: lesson learned
I'm experiencing some issues upgrading the Google-ads python package. Version 18 is deprecated since the beginning of this week and higher versions on protobuf > 4.5.x . This is all well and good but I'm piggybacking this issue ticket as I eventually get stuck with apache-airflow-providers-google depending on google-cloud-secret-manager < 2.x . Which depend on protobuf 3 which causes my predicament. Is the google-cloud-secret-manager dependency needed or could it be easily upgraded to the newer…,,,,,,Anecdotal,comment,,,,,,,,2023-03-15,github/felicienveldema,https://github.com/apache/airflow/issues/27292#issuecomment-1470055802,repo: apache/airflow | issue: Support for Python 3.11 for Google Provider (upgrading all dependencies) | keyword: lesson learned
> Is the google-cloud-secret-manager dependency needed or could it be easily upgraded to the newer 2.x versions? Im sure it is still needed. I'd recommend trying to upgrade that package first in a separate pr. FWIW I've had several of these situations where I want package a upgraded but have to do package b & c first.,,,,,,Anecdotal,comment,,,,,,,,2023-03-15,github/r-richmond,https://github.com/apache/airflow/issues/27292#issuecomment-1470141616,repo: apache/airflow | issue: Support for Python 3.11 for Google Provider (upgrading all dependencies) | keyword: lesson learned
@potiuk Given https://github.com/apache/airflow/pull/30067#issuecomment-1549960088 I was curious if there has been any additional conversations around extras vs provider breakout. (I have a small preference towards extras since it seems easier / faster to implement given the conversations above).,,,,,,Anecdotal,comment,,,,,,,,2023-05-16,github/r-richmond,https://github.com/apache/airflow/issues/27292#issuecomment-1549962820,repo: apache/airflow | issue: Support for Python 3.11 for Google Provider (upgrading all dependencies) | keyword: lesson learned
> @potiuk Given [#30067 (comment)](https://github.com/apache/airflow/pull/30067#issuecomment-1549960088) I was curious if there has been any additional conversations around extras vs provider breakout. (I have a small preference towards extras since it seems easier / faster to implement given the conversations above). No - no discussions. And I think they are not needed. I **personally** think once we get it updated now and keep on updating to the new versions (which should happen pretty much a…,,,,,,Anecdotal,comment,,,,,,,,2023-05-16,github/potiuk,https://github.com/apache/airflow/issues/27292#issuecomment-1550157300,repo: apache/airflow | issue: Support for Python 3.11 for Google Provider (upgrading all dependencies) | keyword: lesson learned
"Having all that in place, I do not see really the need to split Google at all - maybe the extras will save a bit of space when installing the provider, but there will be very little need to split it IMHO.",,,,,,Anecdotal,comment,,,,,,,,2023-05-16,github/potiuk,https://github.com/apache/airflow/issues/27292#issuecomment-1550160793,repo: apache/airflow | issue: Support for Python 3.11 for Google Provider (upgrading all dependencies) | keyword: lesson learned
"Just also to explain why - there was a bit of a story few days back about Aamazon going back to smarter monolyth from microservices https://thenewstack.io/return-of-the-monolith-amazon-dumps-microservices-for-video-monitoring/ and this goes hand-in-hand with my observations (and the reason why we still have monorepo for airflow and providers). Splitting up into pieces looks cool but in a number of cases it is not a ""golden bullet"" while it adds isolation and decouples stuff, when there are hidd…",,,,,,Anecdotal,comment,,,,,,,,2023-05-16,github/potiuk,https://github.com/apache/airflow/issues/27292#issuecomment-1550173096,repo: apache/airflow | issue: Support for Python 3.11 for Google Provider (upgrading all dependencies) | keyword: lesson learned
>IMHO we should not discuss splitting vs extras but extras vs. doing nothing at most. Makes 💯 sense to me >maybe the extras will save a bit of space when installing the provider Yes my main interest stems from the desire to save space & more importantly ignore google libraries I don't use. Particularly the ones that lag behind python version and other dependency updates.,,,,,,Anecdotal,comment,,,,,,,,2023-05-16,github/r-richmond,https://github.com/apache/airflow/issues/27292#issuecomment-1550246076,repo: apache/airflow | issue: Support for Python 3.11 for Google Provider (upgrading all dependencies) | keyword: lesson learned
"Dag processor manager queue split (fixes SLAs) # The ""I'm determined to fix SLAs"" PR OK, so #25147 made a start in this direction. Summing up the, er, summary from _that_ MR, the problem was that SLA callbacks could keep occuring, and prevent the dag processor manager from ever processing more than 2 or 3 dags in the queue before the SLA callbacks re-upped and went to the front of the queue. Under the new behaviour, the SLA callbacks went to the _back_ of the queue. This guaranteed that the que…",,,,,,Anecdotal,issue,,,,,,,,2022-08-02,github/argibbs,https://github.com/apache/airflow/pull/25489,repo: apache/airflow | keyword: lesson learned | state: closed
"Thanks for so thorough description and your willingness to improve SLA (I really, really appreciate it as this is one of the things that we had on our backburner for quite a while). However, I think @argibbs this is the kind of change and desciption is such big and potential of breaking things is such big that it requires a devlist discussion IMHO to drag attention of people who should be dragged rather than dicussing it only in PR. I know you wrote (""probably not make it in 2.3.4""), but I also…",,,,,,Anecdotal,comment,,,,,,,,2022-08-04,github/potiuk,https://github.com/apache/airflow/pull/25489#issuecomment-1205190941,repo: apache/airflow | issue: Dag processor manager queue split (fixes SLAs) | keyword: lesson learned
But I do feel that direction you took is sound. We need some prioritisation of callbacks. It just need deeper look and discussions that fits more than a PR opened maybe 2 weeks before substantial release which is already undergoing quite substantial testing.,,,,,,Anecdotal,comment,,,,,,,,2022-08-04,github/potiuk,https://github.com/apache/airflow/pull/25489#issuecomment-1205197801,repo: apache/airflow | issue: Dag processor manager queue split (fixes SLAs) | keyword: lesson learned
"Sounds good to me. I'm running with this locally, so I don't really mind about timelines. I'm just trying to give back (and also push upstream so I don't have to keep patching each time there's a release :smile: ) Will do as you suggested re: the digest etc.",,,,,,Anecdotal,comment,,,,,,,,2022-08-04,github/argibbs,https://github.com/apache/airflow/pull/25489#issuecomment-1205215917,repo: apache/airflow | issue: Dag processor manager queue split (fixes SLAs) | keyword: lesson learned
"> Sounds good to me. I'm running with this locally, so I don't really mind about timelines. I'm just trying to give back (and also push upstream so I don't have to keep patching each time there's a release smile ) Cool! This is the RIGHT approach! Much appreciated!",,,,,,Anecdotal,comment,,,,,,,,2022-08-04,github/potiuk,https://github.com/apache/airflow/pull/25489#issuecomment-1205217323,repo: apache/airflow | issue: Dag processor manager queue split (fixes SLAs) | keyword: lesson learned
"@argibbs these changes look great! Very well described as well. > am acutely aware / wary of the fact that this separation-of-the-queues-plus-config-param is purely solving a problem that is (for me at least) purely a theoretically-possible issue. I've never seen it, but I don't think it's premature optimisation to tackle it in this change. We have had the case where because of a crucial singel point of failure, hardly any DAG was able to continue (as we use a lot of ExternalTaskSensors). This …",,,,,,Anecdotal,comment,,,,,,,,2022-08-09,github/Jorricks,https://github.com/apache/airflow/pull/25489#issuecomment-1209121259,repo: apache/airflow | issue: Dag processor manager queue split (fixes SLAs) | keyword: lesson learned
"@Jorricks thank you for the kind words. I agree, SLAs could/should be made more efficient. I found this change easier/simpler to make & test, possibly because it's a problem I've tackled many times in my ""real"" job. Also, as you note, I think this change defends against a range of possible problems, rather than just SLAs. I may try to improve SLA firing in the future, but I will be upfront about my motivations; I'm not looking to be ""The SLA guy"", as I simply don't have the spare time. If I end…",,,,,,Anecdotal,comment,,,,,,,,2022-08-10,github/argibbs,https://github.com/apache/airflow/pull/25489#issuecomment-1211303956,repo: apache/airflow | issue: Dag processor manager queue split (fixes SLAs) | keyword: lesson learned
I think it is a bit risky for 2.4 - so this one might stay open for a while until we branch off for 2.4 (@argibbs - it's not forgotten I can assure you).,,,,,,Anecdotal,comment,,,,,,,,2022-08-27,github/potiuk,https://github.com/apache/airflow/pull/25489#issuecomment-1229061586,repo: apache/airflow | issue: Dag processor manager queue split (fixes SLAs) | keyword: lesson learned
"Some rebase is needed after the __future__annotation change, but I think we are pretty close to the ""focused"" 2.4.0 release effort. I think this is a good time to rebase/fix and maybe re-raise a devlist discussion about it.",,,,,,Anecdotal,comment,,,,,,,,2022-09-18,github/potiuk,https://github.com/apache/airflow/pull/25489#issuecomment-1250386654,repo: apache/airflow | issue: Dag processor manager queue split (fixes SLAs) | keyword: lesson learned
"Hiya, yes, I hadn't forgotten either. :smile: Have just found some time to refresh this, and am sorting the rebase now. Will re-raise the dev-list email once that's done (and I'll spell your name right this time too!)",,,,,,Anecdotal,comment,,,,,,,,2022-09-26,github/argibbs,https://github.com/apache/airflow/pull/25489#issuecomment-1258284677,repo: apache/airflow | issue: Dag processor manager queue split (fixes SLAs) | keyword: lesson learned
"I have no idea why, but after rebasing, github seems convinced there's no changed in the branch, and has closed and won't let me reopen it ... I've created a new PR (from the same branch - see #27317 - it's the exact same change)",,,,,,Anecdotal,comment,,,,,,,,2022-10-27,github/argibbs,https://github.com/apache/airflow/pull/25489#issuecomment-1293332170,repo: apache/airflow | issue: Dag processor manager queue split (fixes SLAs) | keyword: lesson learned
"Document Airflow with multiple webservers/schedulers From the gitter channel @r39132: > r39132 19:19 > Hi Folks! So, we are running dual webserver/schedulers in prod.. and they are running pretty well. > So, we had a fire today.. we didn't know that someone setup airflow to run a webserver & scheduler combo on 2 boxes in prod > We upgraded airflow on one machine and not the other and then deployed dags to both. The patch included a new API that the DAGs depended on. > Well, one machine worked f…",,,,,,Anecdotal,issue,,,,,,,,2016-02-26,github/criccomini,https://github.com/apache/airflow/issues/1087,repo: apache/airflow | keyword: lesson learned | state: closed
"Airflow 3.0.2: Airflow Graph UI not rendering task state colors. ### Apache Airflow version 3.0.2 ### If ""Other Airflow 2 version"" selected, which one? _No response_ ### What happened? I have been using Airflow 3.0.2 for my own data-engineering-pipeline code. After running the DAG, I am noticing that the graph is not rendering the task states colors even after refreshing the page. Note: I am using a dynamically generated DAG using loops. <img width=""1512"" alt=""Image"" src=""https://github.com/use…",,,,,,Anecdotal,issue,,,,,,,,2025-06-20,github/arunangshu01,https://github.com/apache/airflow/issues/51954,repo: apache/airflow | keyword: workaround | state: open
"Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.",,,,,,Anecdotal,comment,,,,,,,,2025-06-20,github/boring-cyborg[bot],https://github.com/apache/airflow/issues/51954#issuecomment-2990446561,repo: apache/airflow | issue: Airflow 3.0.2: Airflow Graph UI not rendering task state colors. | keyword: workaround
"``` from airflow.sdk import DAG from airflow.providers.standard.operators.python import PythonOperator from airflow.sdk import Variable import pendulum from datetime import timedelta import time import ast default_args = { 'owner': 'airflow', 'retries': 5, 'retry_delay': timedelta(minutes=1), } def set_list_variable(**kwargs): Variable.set('sample_list', str(list(range(1, 1500)))) def get_list(): val = Variable.get('sample_list', default='[1]') return ast.literal_eval(val) if isinstance(val, st…",,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/arunangshu01,https://github.com/apache/airflow/issues/51954#issuecomment-3018531112,repo: apache/airflow | issue: Airflow 3.0.2: Airflow Graph UI not rendering task state colors. | keyword: workaround
"@bbovenzi my usecase is also similar i have a dynamic dag, where one of the initial task sets the configuration and later on one of the configuration i have written a for loop. Since the configuration is a airflow variable it has a dummy value at the start, so when the dag is parsed its parsed with dummy value and V1 version is tagged. After triggering the dag the configuration gets updated and the dynamic tasks are generated in the for loop so the version V2 gets tagged and the DAG either fail…",,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/manoj-ia,https://github.com/apache/airflow/issues/51954#issuecomment-3018564849,repo: apache/airflow | issue: Airflow 3.0.2: Airflow Graph UI not rendering task state colors. | keyword: workaround
"@arunangshu01 I verified this in 3.0.3, and state color does show you need to select the specific Dag run. https://github.com/user-attachments/assets/a4bf919a-4839-4302-93f7-8d1f07893bfc",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/vatsrahul1001,https://github.com/apache/airflow/issues/51954#issuecomment-3130978234,repo: apache/airflow | issue: Airflow 3.0.2: Airflow Graph UI not rendering task state colors. | keyword: workaround
@vatsrahul1001 - Our use-case is a bit different. We are still seeing the same issue while re-parsing the dag. Maybe the upcoming upgrade would fix the issue.,,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/arunangshu01,https://github.com/apache/airflow/issues/51954#issuecomment-3149824956,repo: apache/airflow | issue: Airflow 3.0.2: Airflow Graph UI not rendering task state colors. | keyword: workaround
This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.,,,,,,Anecdotal,comment,,,,,,,,2025-08-19,github/github-actions[bot],https://github.com/apache/airflow/issues/51954#issuecomment-3198792766,repo: apache/airflow | issue: Airflow 3.0.2: Airflow Graph UI not rendering task state colors. | keyword: workaround
"remove: set_transport_variable import Hi i am making some function on dataproc and i found unnessary import in there. I don't know why this import line added but whenever i run the system code, import error is always raised due to that line. So i suggest removing that line for local testing. <img width=""824"" alt=""Screenshot 2025-06-29 at 4 19 44 PM"" src=""https://github.com/user-attachments/assets/e8981f2b-d1b8-4a62-955e-e2224cc76305"" /> **^ Add meaningful description above** Read the **[Pull Re…",,,,,,Anecdotal,issue,,,,,,,,2025-06-29,github/kgw7401,https://github.com/apache/airflow/pull/52441,repo: apache/airflow | keyword: workaround | state: open
"This may need to be updated after switch to new provider layout, what we should import is [this](https://github.com/apache/airflow/blob/main/providers/openlineage/tests/system/openlineage/conftest.py#L28) auto-use fixture. Many google operators use OpenLineage so we're also testing OL events in google system tests, without this fixture and setting OL transport, the system tests where we check OL would fail.",,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/kacpermuda,https://github.com/apache/airflow/pull/52441#issuecomment-3018255583,repo: apache/airflow | issue: remove: set_transport_variable import | keyword: workaround
"Intresting issue. I have a problem in breeze development environment <img width=""533"" alt=""image"" src=""https://github.com/user-attachments/assets/e0fa9fd4-87c6-4050-88a1-2d83753af798"" /> I think that we shoud trace history!. It can be very helpful to understand system-test #22311 New design system #45681 ```python from providers.tests.system.openlineage.conftest import set_transport_variable # noqa: F401 ``` #46608 ```python from system.openlineage.conftest import set_transport_variable # noqa:…",,,,,,Anecdotal,comment,,,,,,,,2025-07-03,github/kyungjunleeme,https://github.com/apache/airflow/pull/52441#issuecomment-3031547218,repo: apache/airflow | issue: remove: set_transport_variable import | keyword: workaround
"Commenting out will not help, as we're also importing from system.openlineage within google system tests e.g. `from system.openlineage.operator import OpenLineageTestOperator`. Some tests may pass, but some will still fail. One option would be to copy needed modules from OL system tests to Google system tests, but maybe there is en easier way. @potiuk Is here a way to have a cross-provider pytest fixture or some cross-provider code used for system tests ?",,,,,,Anecdotal,comment,,,,,,,,2025-07-04,github/kacpermuda,https://github.com/apache/airflow/pull/52441#issuecomment-3036070566,repo: apache/airflow | issue: remove: set_transport_variable import | keyword: workaround
"@kgw7401 @VladaZakharova I think that would the way to go, copying OL variable transport and operator to common test utils and then importing from there in google system tests. I can look into it maybe next week, so if you want to do it faster, I'll be happy to review the PR. Not sure how `import from openlineage.X` will behave in those common utils, so definitely something to check. Also @mobuchowski, do we want to move the OL VariableTransport and check operator into common utils entirely or …",,,,,,Anecdotal,comment,,,,,,,,2025-07-04,github/kacpermuda,https://github.com/apache/airflow/pull/52441#issuecomment-3036263040,repo: apache/airflow | issue: remove: set_transport_variable import | keyword: workaround
"I'm learning a lot through this process — thank you. However, one thing I’m still a bit concerned about is the fact that we’re getting import errors in local development environments. I think it would be a good idea to also review and ensure that these new from system... imports work reliably in local setups as well, not just in CI or Breeze. It feels important to ensure a smooth experience for all contributors who are developing and testing locally. ![image](https://github.com/user-attachments…",,,,,,Anecdotal,comment,,,,,,,,2025-07-04,github/kyungjunleeme,https://github.com/apache/airflow/pull/52441#issuecomment-3036273242,repo: apache/airflow | issue: remove: set_transport_variable import | keyword: workaround
This pull request has been automatically marked as stale because it has not had recent activity. It will be closed in 5 days if no further activity occurs. Thank you for your contributions.,,,,,,Anecdotal,comment,,,,,,,,2025-08-19,github/github-actions[bot],https://github.com/apache/airflow/pull/52441#issuecomment-3198792717,repo: apache/airflow | issue: remove: set_transport_variable import | keyword: workaround
"Add support for automatic clearing of failed child DAG tasks in TriggerDagRunOperator ### Description I propose an enhancement to TriggerDagRunOperator that would allow automatic clearing of failed task instances in a triggered DAG run if that DAG run previously failed. ### Use case/motivation When using TriggerDagRunOperator in synchronous mode (i.e., wait_for_completion=True), if the triggered child DAG fails (due to a failed task), the parent task also fails. However, when the operator is cl…",,,,,,Anecdotal,issue,,,,,,,,2025-07-16,github/Pad71,https://github.com/apache/airflow/issues/53402,repo: apache/airflow | keyword: workaround | state: open
"> However, when the operator is cleared in the UI (e.g., to retry), the associated failed child DAG run remains unchanged. This is not something that should be controlled by parameter in operator. You should clear with recursive. `Recursive - All the tasks in the child dags and parent dags` https://airflow.apache.org/docs/apache-airflow/3.0.3/core-concepts/dag-run.html#re-run-tasks @bbovenzi @pierrejeambrun I see that this option is not available in Airflow 3 but it does show up in the docs? I …",,,,,,Anecdotal,comment,,,,,,,,2025-07-16,github/eladkal,https://github.com/apache/airflow/issues/53402#issuecomment-3077098394,repo: apache/airflow | issue: Add support for automatic clearing of failed child DAG tasks in TriggerDagRunOperator | keyword: workaround
"> > However, when the operator is cleared in the UI (e.g., to retry), the associated failed child DAG run remains unchanged. > > This is not something that should be controlled by parameter in operator. You should clear with recursive. > > `Recursive - All the tasks in the child dags and parent dags` https://airflow.apache.org/docs/apache-airflow/3.0.3/core-concepts/dag-run.html#re-run-tasks > > [@bbovenzi](https://github.com/bbovenzi) [@pierrejeambrun](https://github.com/pierrejeambrun) I see …",,,,,,Anecdotal,comment,,,,,,,,2025-07-16,github/Pad71,https://github.com/apache/airflow/issues/53402#issuecomment-3077184579,repo: apache/airflow | issue: Add support for automatic clearing of failed child DAG tasks in TriggerDagRunOperator | keyword: workaround
The clear action may change depends on use case. It needs to be controlled from the UI not from operator code. What am I missing?,,,,,,Anecdotal,comment,,,,,,,,2025-07-16,github/eladkal,https://github.com/apache/airflow/issues/53402#issuecomment-3077245381,repo: apache/airflow | issue: Add support for automatic clearing of failed child DAG tasks in TriggerDagRunOperator | keyword: workaround
"> The clear action may change depends on use case. It needs to be controlled from the UI not from operator code. What am I missing? @eladkal The main point of this request is that currently, clearing a TriggerDagRunOperator task does not allow you to automatically clear failed tasks within the corresponding child DAG run. This means that if one or more tasks in the child DAG fail, you must first manually fix or clear those failed tasks, then wait for the entire child DAG run to complete success…",,,,,,Anecdotal,comment,,,,,,,,2025-07-16,github/Pad71,https://github.com/apache/airflow/issues/53402#issuecomment-3077708182,repo: apache/airflow | issue: Add support for automatic clearing of failed child DAG tasks in TriggerDagRunOperator | keyword: workaround
"I think Elad is right, and the closest we had in AF2 was `recursive` option in clear. I also agree that this shouldn't be controlled by the operator but by the UI. Recursive controlled `include_subdags` and `include_parentdag`. (is_subdag, parent_dag, etc.) All that was removed with airflow 3 removal of subdags. Also I don't think this would handle 'external triggers', only subdags (here i'm not sure), so this request might be slightly different and still make sense for AF3. (we want to clear e…",,,,,,Anecdotal,comment,,,,,,,,2025-07-16,github/pierrejeambrun,https://github.com/apache/airflow/issues/53402#issuecomment-3078101759,repo: apache/airflow | issue: Add support for automatic clearing of failed child DAG tasks in TriggerDagRunOperator | keyword: workaround
"> I think Elad is right, and the closest we had in AF2 was `recursive` option in clear. I also agree that this shouldn't be controlled by the operator but by the UI. > > Recursive controlled `include_subdags` and `include_parentdag`. (is_subdag, parent_dag, etc.) All that was removed with airflow 3 removal of subdags. > > Also I don't think this would handle 'external triggers', only subdags (here i'm not sure), so this request might be slightly different and still make sense for AF3. (we want …",,,,,,Anecdotal,comment,,,,,,,,2025-07-16,github/Pad71,https://github.com/apache/airflow/issues/53402#issuecomment-3078172621,repo: apache/airflow | issue: Add support for automatic clearing of failed child DAG tasks in TriggerDagRunOperator | keyword: workaround
"I think, there are several use cases for TriggerDagRunOperator task Clear action: ### Use Case 1: Do not clear child DAG run – raise exception **Description**: The triggered DAG run is not cleared. **Behavior**: If the DAG run is still in a failed or running state, the TriggerDagRunOperator task will raise an exception during the next attempt. **Current status**: ✅ This is the current behavior when using reset_dag_run = False. **Purpose**: Maintains strict separation and manual control between …",,,,,,Anecdotal,comment,,,,,,,,2025-07-16,github/Pad71,https://github.com/apache/airflow/issues/53402#issuecomment-3078218909,repo: apache/airflow | issue: Add support for automatic clearing of failed child DAG tasks in TriggerDagRunOperator | keyword: workaround
"> [@Pad71](https://github.com/Pad71) Are you willing to work on that and open a PR for this ? @pierrejeambrun I'm just a user, I'm not that familiar with Python to be able to do this :(",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/Pad71,https://github.com/apache/airflow/issues/53402#issuecomment-3095533264,repo: apache/airflow | issue: Add support for automatic clearing of failed child DAG tasks in TriggerDagRunOperator | keyword: workaround
"@Pad71 No worries, thanks for reporting that :) Also it's never to late to become one of the 3400+ contributors of apache airflow, it's the best way to be sure the feature you want gets implemented, and it's a great way to give back to the community for the free software you are using. (just saying 🙂)",,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/pierrejeambrun,https://github.com/apache/airflow/issues/53402#issuecomment-3101782338,repo: apache/airflow | issue: Add support for automatic clearing of failed child DAG tasks in TriggerDagRunOperator | keyword: workaround
"> [@Pad71](https://github.com/Pad71) No worries, thanks for reporting that :) > > Also it's never to late to become one of the 3400+ contributors of apache airflow, it's the best way to be sure the feature you want gets implemented, and it's a great way to give back to the community for the free software you are using. (just saying 🙂) @pierrejeambrun Yes, I'm considering it. I think I would actually enjoy it :) But first, I need to find out how the process actually works. I have no experience w…",,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/Pad71,https://github.com/apache/airflow/issues/53402#issuecomment-3104490488,repo: apache/airflow | issue: Add support for automatic clearing of failed child DAG tasks in TriggerDagRunOperator | keyword: workaround
"Cool 🎉 You can take a look at the contribution guides located there: https://github.com/apache/airflow/tree/main/contributing-docs This covers everything (local env setup, PRs, internal processes etc...)",,,,,,Anecdotal,comment,,,,,,,,2025-07-23,github/pierrejeambrun,https://github.com/apache/airflow/issues/53402#issuecomment-3108879901,repo: apache/airflow | issue: Add support for automatic clearing of failed child DAG tasks in TriggerDagRunOperator | keyword: workaround
"> [@Pad71](https://github.com/Pad71) [@pierrejeambrun](https://github.com/pierrejeambrun) I'm interested on this, shall I analyse and work on this? It would be great,, thanks :)",,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/Pad71,https://github.com/apache/airflow/issues/53402#issuecomment-3182954101,repo: apache/airflow | issue: Add support for automatic clearing of failed child DAG tasks in TriggerDagRunOperator | keyword: workaround
"Replace API server’s direct Connection access workaround in BaseHook <!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licen…",,,,,,Anecdotal,issue,,,,,,,,2025-08-04,github/amoghrajesh,https://github.com/apache/airflow/pull/54083,repo: apache/airflow | keyword: workaround | state: open
"Add an ability to prevent DAG future runs after a failure DAG ### Description Add a DAG argument which when true it will prevent the start of future runs of a DAG which had a failure in the last instance. Don't pause (disactivate) the DAG, because developers can miss it without knowing it was deactivated automatically. depend_on_past do something similar but only work on the task level and not on the DAG level. Using depend_on_past in the DAG arguments just prevent a task to run if the previous…",,,,,,Anecdotal,issue,,,,,,,,2025-08-13,github/LiranBenShushan,https://github.com/apache/airflow/issues/54456,repo: apache/airflow | keyword: workaround | state: open
"Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.",,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/boring-cyborg[bot],https://github.com/apache/airflow/issues/54456#issuecomment-3183441325,repo: apache/airflow | issue: Add an ability to prevent DAG future runs after a failure DAG | keyword: workaround
"I like this idea, would it be some sort of a `pause_on_dag_failure` parameter that gets passed into the DAG during definition? Something like this? ```python ... with DAG( dag_id=""my_dag"", start_date=datetime(2025, 1, 1), schedule=""@daily"", # Pause if the last run is a failure pause_on_dag_failure=True ) ... ```",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/jroachgolf84,https://github.com/apache/airflow/issues/54456#issuecomment-3186529530,repo: apache/airflow | issue: Add an ability to prevent DAG future runs after a failure DAG | keyword: workaround
"Yes, @jroachgolf84 , exactly like this. The name you gave (pause_on_dag_failure) is accurate. On the other hand, it may be useful to include in that other statuses that are not success (like skipped), so in that case I would prefer pause_dag_after_non_success or pause_on_dag_non_success .",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/LiranBenShushan,https://github.com/apache/airflow/issues/54456#issuecomment-3187265463,repo: apache/airflow | issue: Add an ability to prevent DAG future runs after a failure DAG | keyword: workaround
"Well, seems like something already exists for this! You can use the `max_consecutive_failed_dag_runs` parameter when defining a DAG to pause it after a certain number of failures. Check out this link: https://www.astronomer.io/docs/learn/airflow-dag-parameters/",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/jroachgolf84,https://github.com/apache/airflow/issues/54456#issuecomment-3188487749,repo: apache/airflow | issue: Add an ability to prevent DAG future runs after a failure DAG | keyword: workaround
"@jroachgolf84 It's a great feature, but it disactivate the DAG after the failure and in our case we can easily miss this DAG with all the other disactivated DAGs so we prefer another sort of pause (even fail/skip without execution). In the meantime, I implemented a workaround for that - an operator that checks for failed task in the last run of the DAG using the Airflow's DB (session) and than fail it so the depend_on_past will work from the begining.",,,,,,Anecdotal,comment,,,,,,,,2025-08-16,github/LiranBenShushan,https://github.com/apache/airflow/issues/54456#issuecomment-3193923329,repo: apache/airflow | issue: Add an ability to prevent DAG future runs after a failure DAG | keyword: workaround
"[webui] DAG Backfills broken on Firefox ### Apache Airflow version 3.0.4 ### If ""Other Airflow 2 version"" selected, which one? _No response_ ### What happened? the DAG backfill ui doesn't work on firefox browser(desktop). on firefox(desktop) we only have date picker. no timer picker ### What you think should happen instead? make the ui work. if not, there should be kind of notification for supported browser or workaround for the unsupported bowsers ### How to reproduce after pick a date (from) …",,,,,,Anecdotal,issue,,,,,,,,2025-08-13,github/obarisk,https://github.com/apache/airflow/issues/54429,repo: apache/airflow | keyword: workaround | state: open
"Operators (and Hooks) of the google-provider should allow custom api_endpoints ### Description Google's client library allows to set the api_endpoint. Unfortunately the Hooks and Operators in the google-provider do not forward this option. It would be great, if they would. As a workaround we override the Hook class to set this option, and the Operator class to use our custom Hook, e.g.: ```Python class CustomApiPubSubHook(PubSubHook): def get_conn(self) -> PublisherClient: if not self._client: …",,,,,,Anecdotal,issue,,,,,,,,2025-07-21,github/christopherfrieler,https://github.com/apache/airflow/issues/53596,repo: apache/airflow | keyword: workaround | state: open
"Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/boring-cyborg[bot],https://github.com/apache/airflow/issues/53596#issuecomment-3097182745,repo: apache/airflow | issue: Operators (and Hooks) of the google-provider should allow custom api_endpoints | keyword: workaround
"add option to set DAG level Dataset outlets ### Description add a new argument `outlets` to the DAG constructor with the same effect as the BaseOperator's outlets argument when the full dag run is successfull ### Use case/motivation use case: I'm currently maintaining several DAGs with ""Dataset Scheduling"", where each DAG handles a specific dataset and declares that it is ready for downstream DAGs by an EmptyOperator with an `outlets` value this means I need to fan-in all the leafs of each DAG:…",,,,,,Anecdotal,issue,,,,,,,,2024-04-18,github/nimrodV81,https://github.com/apache/airflow/issues/39105,repo: apache/airflow | keyword: workaround | state: open
"Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.",,,,,,Anecdotal,comment,,,,,,,,2024-04-18,github/boring-cyborg[bot],https://github.com/apache/airflow/issues/39105#issuecomment-2063283155,repo: apache/airflow | issue: add option to set DAG level Dataset outlets | keyword: workaround
"Initially I thought this was a perfect use case for `EmptyOperator`, but that's just one more thing to maintain in potentially many DAGs. I've assigned you to this issue :)",,,,,,Anecdotal,comment,,,,,,,,2024-04-18,github/RNHTTR,https://github.com/apache/airflow/issues/39105#issuecomment-2065314858,repo: apache/airflow | issue: add option to set DAG level Dataset outlets | keyword: workaround
"@RNHTTR Thanks for the feedback I want to add this feature myself, but I'm afraid I need some guidance I've forked the repo and started to dive into the code Can you point me to a good place to place on_success code for a `DagRun` (similar to `airflow.models.taskinstance._run_finished_callback` for `TaskInstance`)",,,,,,Anecdotal,comment,,,,,,,,2024-04-24,github/nimrodV81,https://github.com/apache/airflow/issues/39105#issuecomment-2074396575,repo: apache/airflow | issue: add option to set DAG level Dataset outlets | keyword: workaround
"@RNHTTR after some observation, the design I came up with is to create a callback and append it to `DAG.on_success_callback` during init is this an acceptable solution?",,,,,,Anecdotal,comment,,,,,,,,2024-06-17,github/nimrodV81,https://github.com/apache/airflow/issues/39105#issuecomment-2173566009,repo: apache/airflow | issue: add option to set DAG level Dataset outlets | keyword: workaround
"> @RNHTTR after some observation, the design I came up with is to create a callback and append it to `DAG.on_success_callback` during init is this an acceptable solution? Interesting! Does this make an API call to trigger a Dataset and thus avoids any changes to the Airflow source code? Or does this take a new parameter in the DAG's initialization?",,,,,,Anecdotal,comment,,,,,,,,2024-06-17,github/RNHTTR,https://github.com/apache/airflow/issues/39105#issuecomment-2173618621,repo: apache/airflow | issue: add option to set DAG level Dataset outlets | keyword: workaround
"@RNHTTR to trigger the API call I would need to make some small code changes the design is to add and `outlets` argument to DAG initialization (to match the usage of `BaseOperator`) then in `DAG.__init__`, if the user provided Datasets in `outlets`, create a callback that calls `dataset_manager.register_dataset_change` and append it to `DAG.on_success_callback` how does that sound?",,,,,,Anecdotal,comment,,,,,,,,2024-06-19,github/nimrodV81,https://github.com/apache/airflow/issues/39105#issuecomment-2178033631,repo: apache/airflow | issue: add option to set DAG level Dataset outlets | keyword: workaround
I think it sounds good. What about having `on_success_outlets` and `on_failure_outlets`? What do you think @uranusjr,,,,,,Anecdotal,comment,,,,,,,,2024-06-19,github/RNHTTR,https://github.com/apache/airflow/issues/39105#issuecomment-2178975662,repo: apache/airflow | issue: add option to set DAG level Dataset outlets | keyword: workaround
Hi @nimrodV81 did you get around to implementing this? Feature makes a lot of sense and I'm currently going to implement the same workaround.,,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/ralichkov,https://github.com/apache/airflow/issues/39105#issuecomment-3191003613,repo: apache/airflow | issue: add option to set DAG level Dataset outlets | keyword: workaround
"ProvidersManager: ""AttributeError: 'NoneType' object has no attribute 'connection_type'"" @ FSHook ### Apache Airflow version 3.0.0 ### If ""Other Airflow 2 version"" selected, which one? _No response_ ### What happened? Running a DagBag unit test raises the following: ```none ERROR airflow.models.dagbag.DagBag:dagbag.py:394 Failed to import: .... from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator File "".../.venv300/lib64/python3.12/site-packages/airflow/providers/commo…",,,,,,Anecdotal,issue,,,,,,,,2025-04-07,github/Dev-iL,https://github.com/apache/airflow/issues/48859,repo: apache/airflow | keyword: workaround | state: open
"Upon trying to add an MCVE, it seems that this failure only appears when treating warning as errors in pytest, i.e. ```toml [tool.pytest.ini_options] filterwarnings = [ ""error"", ] ``` Which causes a various `DeprecationWarning`s to be raised: - FAB: `_request_ctx_stack`, `_app_ctx_stack` - marshmallow_sqlalchemy: `The '__version__' attribute is deprecated and will be removed in in a future version. Use feature detection or 'importlib.metadata.version(""marshmallow"")' instead. - openlineage: `Dep…",,,,,,Anecdotal,comment,,,,,,,,2025-04-07,github/Dev-iL,https://github.com/apache/airflow/issues/48859#issuecomment-2782457789,"repo: apache/airflow | issue: ProvidersManager: ""AttributeError: 'NoneType' object has no attribute 'connection_type'"" @ FSHook | keyword: workaround"
"If anyone's looking for a workaround, these are the warning filters I'm currently using with 3.0.1/py3.12: ```toml # pyproject.toml [tool.pytest.ini_options] filterwarnings = [ ""error"", ""ignore::RuntimeWarning"", ""ignore:.*MapContainer uses PyType_Spec with a metaclass that has custom tp_new.*3.14.:DeprecationWarning"", ""ignore:Timer and timing metrics publish in seconds were deprecated.*:airflow.exceptions.RemovedInAirflow4Warning:airflow"", ""ignore::DeprecationWarning:flask_appbuilder"", ""ignore:…",,,,,,Anecdotal,comment,,,,,,,,2025-05-21,github/Dev-iL,https://github.com/apache/airflow/issues/48859#issuecomment-2897459025,"repo: apache/airflow | issue: ProvidersManager: ""AttributeError: 'NoneType' object has no attribute 'connection_type'"" @ FSHook | keyword: workaround"
I had the same issue when I tried to upgrade the AWS provider package to the latest `9.12.0`. Upgrading to `apache-airflow-providers-amazon==9.4.0` instead fixed the issue.,,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/OlafenwaMoses,https://github.com/apache/airflow/issues/48859#issuecomment-3189757491,"repo: apache/airflow | issue: ProvidersManager: ""AttributeError: 'NoneType' object has no attribute 'connection_type'"" @ FSHook | keyword: workaround"
"Task Runner Dails to Update rendered_task_instance_fields ### Apache Airflow version Other Airflow 2 version (please specify below) ### If ""Other Airflow 2 version"" selected, which one? 2.10.2 ### What happened? Airflow tasks fail with the following error. It always failed on one specific dag with specific task_id. The error repeats even after every retry. It is fixed after I manually clean the rendered_task_instance_fields table using a query like `DELETE FROM rendered_task_instance_fields WHE…",,,,,,Anecdotal,issue,,,,,,,,2024-12-23,github/hbc-acai,https://github.com/apache/airflow/issues/45186,repo: apache/airflow | keyword: workaround | state: open
> [@hbc-acai](https://github.com/hbc-acai) were you ever able to figure out a workaround? I used the DELETE to temporarily work around the issue. Then we upgraded to 2.10.4 and the issue never happened again,,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/hbc-acai,https://github.com/apache/airflow/issues/45186#issuecomment-3185778577,repo: apache/airflow | issue: Task Runner Dails to Update rendered_task_instance_fields | keyword: workaround
"Remove/Replace DSSKey references from Airflow ### Body As seen from: https://github.com/apache/airflow/pull/54078, which is a short term fix to workaround DSSKey 1. Remove DSSKey references entirely in the codebase 2. Bump minimum to paramiko 4.0+ 3. Provide migration guide for users with DSA keys 4. Bump types-paramiko version that was limited in https://github.com/apache/airflow/pull/54173 Migration that could be required: Users would need to: Generate other keys to be used in their connectio…",,,,,,Anecdotal,issue,,,,,,,,2025-08-04,github/amoghrajesh,https://github.com/apache/airflow/issues/54079,repo: apache/airflow | keyword: workaround | state: open
Go for it @joshuabvarghese. Feel free to run through your approach on this if you aren't sure. cc @Lee-W since you expressed interest too.,,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/amoghrajesh,https://github.com/apache/airflow/issues/54079#issuecomment-3153625947,repo: apache/airflow | issue: Remove/Replace DSSKey references from Airflow | keyword: workaround
"ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() ### Apache Airflow version 3.0.2 ### If ""Other Airflow 2 version"" selected, which one? _No response_ ### What happened? The exception occurred when the DAG ran with `dag.test()` attempted to retrieve a variable from the API server. Some similar issues have been opened (#48554, #51062, #51316). The PRs provided as a solution (#50300, #50419) were included in 3.0.2 but did not fix the problem. ``` Exception has occurred: ImportErr…",,,,,,Anecdotal,issue,,,,,,,,2025-06-16,github/opeida,https://github.com/apache/airflow/issues/51816,repo: apache/airflow | keyword: workaround | state: open
Yeah it is in an issue. The problem is the dag is already parsed before the `dag.test` code can be called which would assign `task_runner.SUPERVISOR_COMMS = InProcessSupervisorComms`,,,,,,Anecdotal,comment,,,,,,,,2025-06-17,github/kaxil,https://github.com/apache/airflow/issues/51816#issuecomment-2980436635,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: workaround
"Assuming we can detect the case when this is being imported from doing `python mydag.py` (which I'm 95% sure we can, so that is the 'easy' part) how should we actually obtain a value/variable? I.e. ignoring all practicialities or other issues, what _should_ this do, given the increased security model in Airflow 3 of not allowing unfettered DB access?",,,,,,Anecdotal,comment,,,,,,,,2025-06-17,github/ashb,https://github.com/apache/airflow/issues/51816#issuecomment-2980472886,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: workaround
"> Assuming we can detect the case when this is being imported from doing `python mydag.py` (which I'm 95% sure we can, so that is the 'easy' part) how should we actually obtain a value/variable? > > I.e. ignoring all practicialities or other issues, what _should_ this do, given the increased security model in Airflow 3 of not allowing unfettered DB access? Since we allow that in the actual dag parsing models (as it goes via DAG processor -> Supervisor comms -> Variable), we should do the same f…",,,,,,Anecdotal,comment,,,,,,,,2025-06-17,github/kaxil,https://github.com/apache/airflow/issues/51816#issuecomment-2980495661,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: workaround
"@opeida The ideal way would be to not access Variables or Connections at the top of your file (i.e outside Task Context). In your case, you can rewrite the dag as follows: ```py import logging from airflow import DAG from airflow.providers.standard.operators.empty import EmptyOperator from airflow.providers.standard.operators.python import PythonOperator def my_function(my_var: str) -> None: logging.getLogger(__name__).info(my_var) with DAG(""test_dag"") as dag: start = EmptyOperator(task_id=""sta…",,,,,,Anecdotal,comment,,,,,,,,2025-06-17,github/kaxil,https://github.com/apache/airflow/issues/51816#issuecomment-2980512138,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: workaround
"So: detecting when we have top-level dag code, at least in the `python mydagfile.py` case: ```python def __getattr__(name): if name == ""SUPERVISOR_COMMS"" and ""__main__"" in sys.modules: import traceback frames = [ frame for (frame, lnum) in traceback.walk_stack(None) if not frame.f_code.co_filename.startswith(""<frozen importlib."") ] if sys.modules[""__main__""].__file__ == frames[-1].f_code.co_filename: raise RuntimeError(""Top level API access here"") raise AttributeError(f""module {__name__!r} has …",,,,,,Anecdotal,comment,,,,,,,,2025-06-17,github/ashb,https://github.com/apache/airflow/issues/51816#issuecomment-2980679579,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: workaround
"> [@opeida](https://github.com/opeida) The ideal way would be to not access Variables or Connections at the top of your file (i.e outside Task Context) @kaxil are there workarounds to eliminate the use of top-level variables for dynamic DAG generation? We pull data from numerous accounts of a third-party provider. The accounts variable can change dynamically and stores sensitive information including API keys. In all other cases, we utilize Jinja templates within Task Context following Airflow'…",,,,,,Anecdotal,comment,,,,,,,,2025-06-17,github/opeida,https://github.com/apache/airflow/issues/51816#issuecomment-2980811694,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: workaround
"You can just do following in your case: ```py provider_name = ""<provider_name>"" def my_function(my_var: str) -> None: logging.getLogger(__name__).info(my_var) with DAG(""test_dag"") as dag: start = EmptyOperator(task_id=""start"") py_func = PythonOperator( task_id=""py_func"", python_callable=my_function, op_kwargs={ ""my_var"": ""{{ var.value."" + provider_name + ""_accounts }}"" } ) end = EmptyOperator(task_id=""end"") start >> py_func >> end ```",,,,,,Anecdotal,comment,,,,,,,,2025-06-17,github/kaxil,https://github.com/apache/airflow/issues/51816#issuecomment-2980857201,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: workaround
"> So: detecting when we have top-level dag code, at least in the `python mydagfile.py` case: > > def __getattr__(name): > if name == ""SUPERVISOR_COMMS"" and ""__main__"" in sys.modules: > import traceback > > frames = [ > frame > for (frame, lnum) in traceback.walk_stack(None) > if not frame.f_code.co_filename.startswith(""<frozen importlib."") > ] > if sys.modules[""__main__""].__file__ == frames[-1].f_code.co_filename: > raise RuntimeError(""Top level API access here"") > > raise AttributeError(f""modu…",,,,,,Anecdotal,comment,,,,,,,,2025-06-17,github/kaxil,https://github.com/apache/airflow/issues/51816#issuecomment-2980867342,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: workaround
"> py_func = PythonOperator( > task_id=""py_func"", > python_callable=my_function, > op_kwargs={ > ""my_var"": ""{{ var.value."" + provider_name + ""_accounts }}"" > } > ) @kaxil the provided DAG reproduces the issue only and does not reflect our actual case. We need the ability to trigger a certain account, gather metrics to StatsD for each account, and utilize the additional flexibility that dynamic DAG generation offers. Thank you for your reply. I look forward to testing the fix if one is provided. …",,,,,,Anecdotal,comment,,,,,,,,2025-06-17,github/opeida,https://github.com/apache/airflow/issues/51816#issuecomment-2981493262,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: workaround
"> UPD: import `Variable` from `airflow.models` instead of `airflow.sdk` fixes the problem but doesn't seem like a best practice solution. Yeah, that still works as it's falling back to the direct DB access based approach",,,,,,Anecdotal,comment,,,,,,,,2025-06-18,github/ashb,https://github.com/apache/airflow/issues/51816#issuecomment-2984241505,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: workaround
"Import from `airflow.models` works for me to load ENV defined variable. Is `airflow.sdk` not well oriented for non-runner environment yet? I have seen various troubles already like `dag.folder` is wrong when running thru CLI, `sys.path` not being properly enhanced...",,,,,,Anecdotal,comment,,,,,,,,2025-06-26,github/simi,https://github.com/apache/airflow/issues/51816#issuecomment-3007736559,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: workaround
"We use variables for dynamic DAG scheduling, so we can access and modify schedules via web-UI by ourselves, without having to change deployment configs (OS environment variables) through DevOps guys. Example: ```python with DAG( ""DAG_1"", schedule=Variable.get(""DAG_1_SCHEDULE"", None) ) ``` So I feel like I have to ask: will the final fixed version allow for us to do the same?",,,,,,Anecdotal,comment,,,,,,,,2025-07-04,github/DartVeDroid,https://github.com/apache/airflow/issues/51816#issuecomment-3036921222,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: workaround
"> We use variables for dynamic DAG scheduling, so we can access and modify schedules via web-UI by ourselves, without having to change deployment configs (OS environment variables) through DevOps guys. Example: > > with DAG( > ""DAG_1"", > schedule=Variable.get(""DAG_1_SCHEDULE"", None) > ) > So I feel like I have to ask: will the final fixed version allow for us to do the same? Yes, although timeline TBD",,,,,,Anecdotal,comment,,,,,,,,2025-07-04,github/kaxil,https://github.com/apache/airflow/issues/51816#issuecomment-3036961653,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: workaround
"Add Confirmation Pop-up to Prevent Multiple Reruns of Failed Tasks ### Description Currently, in the Airflow UI, users can re-run failed tasks multiple times when multiple users act on the same task simultaneously. This behavior may lead to duplicate runs, unnecessary load on the system, and confusion in monitoring task progress. We'd like to add a confirmation pop-up before re-running failed tasks. The pop-up should: - Prompt users to confirm before initiating the rerun. - Optionally re-check …",,,,,,Anecdotal,issue,,,,,,,,2025-08-12,github/jisundr,https://github.com/apache/airflow/issues/54379,repo: apache/airflow | keyword: workaround | state: open
"Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/boring-cyborg[bot],https://github.com/apache/airflow/issues/54379#issuecomment-3177398129,repo: apache/airflow | issue: Add Confirmation Pop-up to Prevent Multiple Reruns of Failed Tasks | keyword: workaround
"GitDagBundle: DAG processor fails to load versioned DAGs due to missing materialized folders ### Apache Airflow Provider(s) git ### Versions of Apache Airflow Providers _No response_ ### Apache Airflow version 3.0.2 ### Operating System Ubuntu 20.04.6 LTS ### Deployment Virtualenv installation ### Deployment details _No response_ ### What happened When using GitDagBundle with `supports_versioning=True`, Airflow DAG Processor fails to execute DAG callbacks because it cannot find the materialized…",,,,,,Anecdotal,issue,,,,,,,,2025-06-22,github/liyude-tw,https://github.com/apache/airflow/issues/52040,repo: apache/airflow | keyword: workaround | state: open
"Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.",,,,,,Anecdotal,comment,,,,,,,,2025-06-22,github/boring-cyborg[bot],https://github.com/apache/airflow/issues/52040#issuecomment-2994425884,repo: apache/airflow | issue: GitDagBundle: DAG processor fails to load versioned DAGs due to missing materialized folders | keyword: workaround
"Add `localstack` Breeze integration ## Why As Ash suggested setting up AWS CloudWatch to test system behavior using LocalStack in https://github.com/apache/airflow/pull/49470#discussion_r2239313403, we should provide an easy way to run LocalStack with Breeze. ## What Add `localstack` to the Breeze `--integration` flag to enable seamless setup of the Docker network for both Breeze and LocalStack. Then contributor can setup the following env in `files/airflow-breeze-config/environment_variables.e…",,,,,,Anecdotal,issue,,,,,,,,2025-08-02,github/jason810496,https://github.com/apache/airflow/pull/54050,repo: apache/airflow | keyword: workaround | state: open
"Very nice! good addition to Breeze! Few things: * breeze tests * we neeed at least one integration test for localstack - and I think it's generally a good thing to have even if it is quite simple one - because it will also test if the integration is still working * possibly - if the integration is targetted for testing AWS integrations, it would be great if we had some specific documents on how to test things locally with the localstack - I think it could be a good way to add somewhere in contr…",,,,,,Anecdotal,comment,,,,,,,,2025-08-02,github/potiuk,https://github.com/apache/airflow/pull/54050#issuecomment-3146619553,repo: apache/airflow | issue: Add `localstack` Breeze integration | keyword: workaround
"Nice. One nit here -- with the `AWS_ENDPOINT_URL=http://localstack:4566` env var and the other envs set we shouldn't need to set the connection as you have it. I think we should do one or the other (AWS env vars, or Airflow connection), but not both.",,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/ashb,https://github.com/apache/airflow/pull/54050#issuecomment-3150011034,repo: apache/airflow | issue: Add `localstack` Breeze integration | keyword: workaround
"Nice! Could be a nice solution to test AWS resources indeed. Potentially, in the future, we could also use that to, say, run AWS system tests in Airflow CI",,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/vincbeck,https://github.com/apache/airflow/pull/54050#issuecomment-3150731230,repo: apache/airflow | issue: Add `localstack` Breeze integration | keyword: workaround
"Thanks all for the review, I will catch up after #54054. Since I'm preparing presentation for [COSCUP](https://coscup.org/2025/en/about) and have limited bandwidth, I will need some additional time to get this PR ready.",,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/jason810496,https://github.com/apache/airflow/pull/54050#issuecomment-3151429214,repo: apache/airflow | issue: Add `localstack` Breeze integration | keyword: workaround
"> Nice! Could be a nice solution to test AWS resources indeed. Potentially, in the future, we could also use that to, say, run AWS system tests in Airflow CI Crossed my mind too :)",,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/potiuk,https://github.com/apache/airflow/pull/54050#issuecomment-3155165904,repo: apache/airflow | issue: Add `localstack` Breeze integration | keyword: workaround
"I'm really excited to hear that LocalStack is being added to Breeze! That said, I've seen many setups in the industry that combine Breeze with MinIO. I was wondering — was there a specific reason for choosing LocalStack over MinIO in this case? Was LocalStack chosen because other AWS services needed to be tested as well, beyond just S3?",,,,,,Anecdotal,comment,,,,,,,,2025-08-07,github/kyungjunleeme,https://github.com/apache/airflow/pull/54050#issuecomment-3162249011,repo: apache/airflow | issue: Add `localstack` Breeze integration | keyword: workaround
"@kyungjunleeme Minio is just S3 compatible storage right? Localstack has lots of other AWS services, including CloudWatch Logs etc.",,,,,,Anecdotal,comment,,,,,,,,2025-08-07,github/ashb,https://github.com/apache/airflow/pull/54050#issuecomment-3163272709,repo: apache/airflow | issue: Add `localstack` Breeze integration | keyword: workaround
"> @kyungjunleeme Minio is just S3 compatible storage right? > Localstack has lots of other AWS services, including CloudWatch Logs etc. Yes, Minio is just S3 compatible storage and Localstack is more than Minio. In current case I use Localstack to test the CloudWatchTaskHandler behavior. > Potentially, in the future, we could also use that to, say, run AWS system tests in Airflow CI LGTM for using Localstack as system test for AWS, as long as the Localstack is _fully compatible_ ( as least will…",,,,,,Anecdotal,comment,,,,,,,,2025-08-07,github/jason810496,https://github.com/apache/airflow/pull/54050#issuecomment-3163298419,repo: apache/airflow | issue: Add `localstack` Breeze integration | keyword: workaround
"> LGTM for using Localstack as system test for AWS, as long as the Localstack is _fully compatible_ ( as least will not show inconsistent behavior or introduce flakiness in the system test ). Yep. I think it won't be a ""full"" replacement - we'd likely have to skip some tests or add some workarounds, but for me that would be a very cool way to make System Tests for AWS run during the PRs. We were discussing ways how we can do it for Google - and apparently there is a https://cloud.google.com/sdk…",,,,,,Anecdotal,comment,,,,,,,,2025-08-07,github/potiuk,https://github.com/apache/airflow/pull/54050#issuecomment-3164414206,repo: apache/airflow | issue: Add `localstack` Breeze integration | keyword: workaround
"Deferrable HttpSensor does not move to Triggerer when using `response_check` ### Discussed in https://github.com/apache/airflow/discussions/39597 <div type='discussions-op-text'> <sup>Originally posted by **bejota** May 13, 2024</sup> Hello, I'm using HttpSensor to poll SLURM's REST API for job status. I'm using `deferrable` mode with `response_check`. The task only goes to the triggerer if I remove the `response_check`. Airflow Docker Image `apache/airflow:latest-python3.9` Airflow Version `2.…",,,,,,Anecdotal,issue,,,,,,,,2024-06-13,github/josh-fell,https://github.com/apache/airflow/issues/40209,repo: apache/airflow | keyword: workaround | state: open
Probably what the solution _should_ be. May need to confirm with `HttpOperator` too: https://github.com/apache/airflow/discussions/39597#discussioncomment-9758322,,,,,,Anecdotal,comment,,,,,,,,2024-06-18,github/josh-fell,https://github.com/apache/airflow/issues/40209#issuecomment-2176299198,repo: apache/airflow | issue: Deferrable HttpSensor does not move to Triggerer when using `response_check` | keyword: workaround
"I think for the check_response to be taken into account, you need to pass extra_options={""check_response"": False}, otherwise the check_response parameter won't be taken into account. I've created a [PR](https://github.com/apache/airflow/pull/45451) for this to allow to configure this also within extra of your http connection. So maybe try this, if this doesn't work the a PR will be needed: ``` check_slurm_job = HttpSensor( task_id=""check_slurm_job"", http_conn_id=""slurm_conn_id"", method=""get"", e…",,,,,,Anecdotal,comment,,,,,,,,2025-03-11,github/dabla,https://github.com/apache/airflow/issues/40209#issuecomment-2713042329,repo: apache/airflow | issue: Deferrable HttpSensor does not move to Triggerer when using `response_check` | keyword: workaround
"Hey @dabla – I looked into your PR that was merged related to this issue. Curious to understand: what was the motivation for moving the response_check logic to the connection level? In my case, I’m generating DAGs dynamically via Jinja templates based on user input. Each connection acts as a root, and I dynamically attach endpoints and response_check functions per task. So, binding response_check at the connection level isn’t ideal for this setup, and I'm still running into the same issue descr…",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/k4rtikp4til,https://github.com/apache/airflow/issues/40209#issuecomment-3158739953,repo: apache/airflow | issue: Deferrable HttpSensor does not move to Triggerer when using `response_check` | keyword: workaround
"> Hey [@dabla](https://github.com/dabla) – I looked into your PR that was merged related to this issue. Curious to understand: what was the motivation for moving the response_check logic to the connection level? > > In my case, I’m generating DAGs dynamically via Jinja templates based on user input. Each connection acts as a root, and I dynamically attach endpoints and response_check functions per task. So, binding response_check at the connection level isn’t ideal for this setup, and I'm still…",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/dabla,https://github.com/apache/airflow/issues/40209#issuecomment-3160909935,repo: apache/airflow | issue: Deferrable HttpSensor does not move to Triggerer when using `response_check` | keyword: workaround
"Upgrade FAB to FAB 5 Switch to using flask-sqlalchemy db session management, and include Auth Manager Provider Test Isolation and Reliability - Refactor test fixtures to always use Flask app contexts for DB and app operations. - Add a global pytest fixture to clear SQLAlchemy metadata before each test, reducing test flakiness. - Standardize session access and cleanup patterns across all tests. - Refactor user/role creation and deletion to ensure proper isolation. - Update test logic to use new …",,,,,,Anecdotal,issue,,,,,,,,2025-05-22,github/potiuk,https://github.com/apache/airflow/pull/50960,repo: apache/airflow | keyword: workaround | state: open
"Datetime params missing microseconds in Trigger DAG menu ### Apache Airflow version 2.9.3 ### If ""Other Airflow 2 version"" selected, which one? _No response_ ### What happened? Greetings I've stumbled upon a problem and inconsistency with datetime-typed DAG params Let's take a DAG with datetime param that has a default value matching end of some day - 23:59:59.999999 ```python ""metrics_end_time"": Param( default = pendulum.now(tz = ""UTC"").first_of(""quarter"").end_of(""day"").subtract(days = 1).isof…",,,,,,Anecdotal,issue,,,,,,,,2024-07-22,github/Nick-Nal,https://github.com/apache/airflow/issues/40932,repo: apache/airflow | keyword: workaround | state: open
"Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.",,,,,,Anecdotal,comment,,,,,,,,2024-07-22,github/boring-cyborg[bot],https://github.com/apache/airflow/issues/40932#issuecomment-2242811146,repo: apache/airflow | issue: Datetime params missing microseconds in Trigger DAG menu | keyword: workaround
I’d prefer implementing #9237 and adding an inclusive/exclusive toggle to the time fields. That’s more intuitive than adding `.99999` to the time IMO.,,,,,,Anecdotal,comment,,,,,,,,2024-08-01,github/uranusjr,https://github.com/apache/airflow/issues/40932#issuecomment-2262258251,repo: apache/airflow | issue: Datetime params missing microseconds in Trigger DAG menu | keyword: workaround
This issue has been automatically marked as stale because it has been open for 365 days without any activity. There has been several Airflow releases since last activity on this issue. Kindly asking to recheck the report against latest Airflow version and let us know if the issue is reproducible. The issue will be closed in next 30 days if no further activity occurs from the issue author.,,,,,,Anecdotal,comment,,,,,,,,2025-08-02,github/github-actions[bot],https://github.com/apache/airflow/issues/40932#issuecomment-3146279009,repo: apache/airflow | issue: Datetime params missing microseconds in Trigger DAG menu | keyword: workaround
"dags folder not present in PYTHONPATH ### Apache Airflow version 3.0.3 ### If ""Other Airflow 2 version"" selected, which one? _No response_ ### What happened? The DAGs folder is expected in PYTHONPATH, and we should be able to import modules in the folder, as specified in https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/modules_management.html However after Airflow 3.0.2 the folder is no longer exist in PYTHONPATH and we cannot import modules in dag files. ### …",,,,,,Anecdotal,issue,,,,,,,,2025-07-22,github/widewing,https://github.com/apache/airflow/issues/53617,repo: apache/airflow | keyword: workaround | state: open
"I believe this is a documentation issue. Since we hav dag bundles, which are really only evaluated by dag file processor, there is no reason why the old `dag folder` should be added to PYTHONPATH for all kinds of `airflow` commands. It **might** matter though for some commands (like dag test) that are supposed to be run in the environment where the dags are present (note that for example scheduler and api_server are not even supposed to have dag folder present). But even that is not supposed to…",,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/potiuk,https://github.com/apache/airflow/issues/53617#issuecomment-3102587499,repo: apache/airflow | issue: dags folder not present in PYTHONPATH | keyword: workaround
"Also if that will be confirmed - are you willing to help and update our docs @widewing ? I think - since you are apparently user of our documentation and unlike many users - read it, you'd be probably one of the best people to update the docs if it is clarified what the behaviour is and in the way that will be easy to understand by others like you who read it. And it's easy - just click ""suggest a change on this page"" and you will get PR where you will be able to update the docs. Can we count o…",,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/potiuk,https://github.com/apache/airflow/issues/53617#issuecomment-3102596450,repo: apache/airflow | issue: dags folder not present in PYTHONPATH | keyword: workaround
Hi @potiuk I'm in the process of migrating Airflow 2 to Airflow 3 and I'm still learning the changes and new concepts. This issue was found indeed when it failed to test dags. I don't think I'm the best person to update the doc since I am not familiar with the new system yet and don't have much knowledge of expected behaviors.,,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/widewing,https://github.com/apache/airflow/issues/53617#issuecomment-3103923233,repo: apache/airflow | issue: dags folder not present in PYTHONPATH | keyword: workaround
"You explained it well. I'll just add that there is no single ""folder"" to add any more - you can have many dag bundles at the same time! And some may not even be a meaningful folder for users (e.g. with git, the repo is manged by airflow itself). That said, some cli commands are known to not add the bundle root to sys.path. @simi has a wip change to address it, or at least provide the ability to address it.",,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/jedcunningham,https://github.com/apache/airflow/issues/53617#issuecomment-3104098024,repo: apache/airflow | issue: dags folder not present in PYTHONPATH | keyword: workaround
"mentioned change is at https://github.com/simi/airflow/commit/e9166cb05b8904a5bb7ff441beb167849821d5aa, I'll open PR with some tests added soon 🙏",,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/simi,https://github.com/apache/airflow/issues/53617#issuecomment-3104108480,repo: apache/airflow | issue: dags folder not present in PYTHONPATH | keyword: workaround
"> That said, some cli commands are known to not add the bundle root to sys.path. @simi has a wip change to address it, or at least provide the ability to address it. Cool. Just nice thing to mention that likely (as mentioned above) some CLI commands probably should not even try (api-server, scheduler) :). I think eventually - when we continue splitting our codebase, maybe some day we extract the dagbag (parsing) part to a separate shared library that will be used only by specialized distributio…",,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/potiuk,https://github.com/apache/airflow/issues/53617#issuecomment-3104320674,repo: apache/airflow | issue: dags folder not present in PYTHONPATH | keyword: workaround
"✅ Workarounds to Resolve the Issue We’ve identified a few workarounds for the missing PYTHONPATH entry for the dags/ folder in Airflow 3.x: Option 1 (✅ Recommended) Move your custom module to a separate directory: Preferably place it inside the plugins/ folder (which is already included in sys.path), OR Create a new directory (e.g., libs/) and manually add it to the PYTHONPATH (e.g., via environment variable or modifying sys.path). Option 2 (Migration Support) If you're migrating from Airflow 2…",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/suhail-zemoso,https://github.com/apache/airflow/issues/53617#issuecomment-3126368940,repo: apache/airflow | issue: dags folder not present in PYTHONPATH | keyword: workaround
@suhail-zemoso And how to modify `sys.path`? This is IMHO regression since until 3.0.1 dags folder was in `sys.path` and by no announcement it was removed. It should be added back. Plugins seems to have different purpose. https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/plugins.html#what-for,,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/simi,https://github.com/apache/airflow/issues/53617#issuecomment-3126386757,repo: apache/airflow | issue: dags folder not present in PYTHONPATH | keyword: workaround
> [@suhail-zemoso](https://github.com/suhail-zemoso) And how to modify `sys.path`? > > This is IMHO regression since until 3.0.1 dags folder was in `sys.path` and by no announcement it was removed. It should be added back. > > Plugins seems to have different purpose. https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/plugins.html#what-for **Option 1**: Modify sys.path in code You can dynamically add the path at runtime within your DAG Python file import sys impo…,,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/suhail-zemoso,https://github.com/apache/airflow/issues/53617#issuecomment-3126681310,repo: apache/airflow | issue: dags folder not present in PYTHONPATH | keyword: workaround
This came about due to https://github.com/apache/airflow/pull/50385 So the solution is DAG bundles? Does that make most of this page obsolete? https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/modules_management.html,,,,,,Anecdotal,comment,,,,,,,,2025-07-31,github/ralichkov,https://github.com/apache/airflow/issues/53617#issuecomment-3139707186,repo: apache/airflow | issue: dags folder not present in PYTHONPATH | keyword: workaround
"> This is IMHO regression since until 3.0.1 dags folder was in sys.path and by no announcement it was removed. It should be added back. There is no singular ""dags folder"" any longer, there is one or more bundles, which may or may not include the AF2 style dags folder. The bundle root is added on the fly during execution and parsing of dags, but some CLI commands were missed and have not been moved over to this new world. Until 3.0.2, we were incorrectly adding the AF2 style dags folder onto sys…",,,,,,Anecdotal,comment,,,,,,,,2025-07-31,github/jedcunningham,https://github.com/apache/airflow/issues/53617#issuecomment-3140899790,repo: apache/airflow | issue: dags folder not present in PYTHONPATH | keyword: workaround
"DAG Import Errors message is dangling in web-interface ### Apache Airflow version Other Airflow 2 version (please specify below) ### If ""Other Airflow 2 version"" selected, which one? 2.10.2 ### What happened? <img width=""420"" alt=""dag_report_error"" src=""https://github.com/user-attachments/assets/fc99412d-7ca8-4308-88b1-cd568594846e"" /> We have this message in the Airflow's web-interface no matter what. This DAG can exist or not - the error still shows. We have found many different topics across…",,,,,,Anecdotal,issue,,,,,,,,2024-12-27,github/anikin-devops,https://github.com/apache/airflow/issues/45227,repo: apache/airflow | keyword: workaround | state: open
"Yeah. I think you'd need to fix the dag first and then delete it, which is annoying - as a workaround you can delete the import_error entry related to the DAG - marked it as a good first issue for someone to tackle (if they can reproduce it of course)",,,,,,Anecdotal,comment,,,,,,,,2025-01-16,github/potiuk,https://github.com/apache/airflow/issues/45227#issuecomment-2596981750,repo: apache/airflow | issue: DAG Import Errors message is dangling in web-interface | keyword: workaround
"@potiuk Thanks! A workaround helped. In the future we will try to work like that ""you'd need to fix the dag first and then delete it"". But I guess it would be helpful to validate entries in this table somehow? At least, if the DAG is no longer exist, then Airflow should delete the related entry.",,,,,,Anecdotal,comment,,,,,,,,2025-01-17,github/anikin-devops,https://github.com/apache/airflow/issues/45227#issuecomment-2597406008,repo: apache/airflow | issue: DAG Import Errors message is dangling in web-interface | keyword: workaround
"> But I guess it would be helpful to validate entries in this table somehow? At least, if the DAG is no longer exist, then Airflow should delete the related entry. Sure. If you wish to contribute it - feel free.",,,,,,Anecdotal,comment,,,,,,,,2025-01-17,github/potiuk,https://github.com/apache/airflow/issues/45227#issuecomment-2598491054,repo: apache/airflow | issue: DAG Import Errors message is dangling in web-interface | keyword: workaround
"Hi @eanikindfi & @potiuk , I need clarification on the correct codebase version. The [ Artifact Hub](https://artifacthub.io/packages/helm/apache-airflow/airflow) lists the version as 2.9.3, but the description mentions 2.10.2. Should I refer to the 2.10.2 codebase for this?",,,,,,Anecdotal,comment,,,,,,,,2025-02-19,github/ishtiaqSamdani007,https://github.com/apache/airflow/issues/45227#issuecomment-2667662943,repo: apache/airflow | issue: DAG Import Errors message is dangling in web-interface | keyword: workaround
"Not sure about the question. ArtifactHub mentiond ""default"" version of airflow but you can change it in the chart. So I am not sure what you are asking for. All the changes in airflow are developed in main anyway, with few exception where we cannot make a change any more and backport it, we might contribute to latest stable branch for core airflow. And we are currently in the stage where we develop Airflow 3 and the rules are slighlty different - see https://github.com/apache/airflow/blob/main/…",,,,,,Anecdotal,comment,,,,,,,,2025-02-20,github/potiuk,https://github.com/apache/airflow/issues/45227#issuecomment-2672484231,repo: apache/airflow | issue: DAG Import Errors message is dangling in web-interface | keyword: workaround
I am working with airflow version 2.9.3 and facing the similar issue with the dangling error message of import error. If i exec to the pod and run: airflow dags list-import-errors i see the correct state but on web the error message is not going away. This means that the meta DB is having the right state but some how the error message is stuck with the web and restarting the pod is not resolving it. Rollout deployment is also not fixing the error. Deleting the helm release and redeploying not w…,,,,,,Anecdotal,comment,,,,,,,,2025-04-15,github/akashniranjan64,https://github.com/apache/airflow/issues/45227#issuecomment-2807593573,repo: apache/airflow | issue: DAG Import Errors message is dangling in web-interface | keyword: workaround
> I am working with airflow version 2.9.3 and facing the similar issue with the dangling error message of import error. > > If i exec to the pod and run: airflow dags list-import-errors i see the correct state but on web the error message is not going away. This means that the meta DB is having the right state but some how the error message is stuck with the web and restarting the pod is not resolving it. Rollout deployment is also not fixing the error. Deleting the helm release and redeploying…,,,,,,Anecdotal,comment,,,,,,,,2025-04-18,github/anikin-devops,https://github.com/apache/airflow/issues/45227#issuecomment-2814497998,repo: apache/airflow | issue: DAG Import Errors message is dangling in web-interface | keyword: workaround
"Hi @vatsrahul1001 . I'm trying to catch this issue but unsuccessfully. Please unassign me from this issue, maybe someone else can catch the error and fix it. Sorry for that.",,,,,,Anecdotal,comment,,,,,,,,2025-05-14,github/valentinDruzhinin,https://github.com/apache/airflow/issues/45227#issuecomment-2878402829,repo: apache/airflow | issue: DAG Import Errors message is dangling in web-interface | keyword: workaround
"Issue summary: 1. Create a DAG with a parsing error inside files/dags/{file_name}. 2. Wait for the bundle sync (~3–4 mins), then open the Airflow UI/reload the web interface. 3. The UI highlights a parsing error using data from the `import_errors` table. 4. Now delete the faulty DAG file without fixing the error. 5. The error still shows up in the UI even though the file no longer exists. Why is this happening ? The issue lies in the `_refresh_dag_bundles` method of `DagFileProcessorManager`. T…",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/anasatzemoso,https://github.com/apache/airflow/issues/45227#issuecomment-3127265417,repo: apache/airflow | issue: DAG Import Errors message is dangling in web-interface | keyword: workaround
"You’ll need to change the `found_files` structure a bit since the refresh time check is before the loop. But you also can’t simply calculate all bundle files eagerly since it’d require refreshing all the bundles first, and that could be costly. Honestly I don’t feel this is too worthwhile to fix. This is just a temporary glitch. Maybe somewhere down the line we can allow each dag bundle to set its refresh interval instead—and a bundle that’s cheap to refresh, such as a local directory, can simp…",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/uranusjr,https://github.com/apache/airflow/issues/45227#issuecomment-3130300477,repo: apache/airflow | issue: DAG Import Errors message is dangling in web-interface | keyword: workaround
"> Honestly I don’t feel this is too worthwhile to fix. This is just a temporary glitch. Maybe somewhere down the line we can allow each dag bundle to set its refresh interval instead—and a bundle that’s cheap to refresh, such as a local directory, can simply set it very low, even 0. That's the plan of AIP-66 which still needs to be completed https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=294816356 But maybe a good (and simple) solution will be to have a possibility to ""clear"" …",,,,,,Anecdotal,comment,,,,,,,,2025-07-30,github/potiuk,https://github.com/apache/airflow/issues/45227#issuecomment-3136348513,repo: apache/airflow | issue: DAG Import Errors message is dangling in web-interface | keyword: workaround
"@potiuk Just to clarify - the entries for deleted DAGs do get cleared when the bundle is refreshed. The issue happens only when the bundle refresh is skipped (due to to the if condition shared above), so it's more of a temporary inconsistency — as @uranusjr pointed out. Since AIP-66 already aims to give finer control over refresh intervals (which would help fix this more systematically), maybe we can consider not addressing this right now unless it becomes a problem in actual usage. Your sugges…",,,,,,Anecdotal,comment,,,,,,,,2025-07-31,github/anasatzemoso,https://github.com/apache/airflow/issues/45227#issuecomment-3138928968,repo: apache/airflow | issue: DAG Import Errors message is dangling in web-interface | keyword: workaround
[v3-0-test] Upgrade to prek 0.0.29 (#54621) This upgrades prek to **just released** 0.0.29 and removes the workaround implemented in #54613 to add build-essential to the image that is used to build distributions - because prek 0.0.29 comes with all the necessary binary wheels/platforms that remove the need to build in in our Linux ARM image (issue https://github.com/j178/prek/issues/451 has been fixed). (cherry picked from commit a4fa2182004034ca27d4d43b973fdd4de7f99d5a) <!-- Licensed to the Ap…,,,,,,Anecdotal,issue,,,,,,,,2025-08-18,github/potiuk,https://github.com/apache/airflow/pull/54624,repo: apache/airflow | keyword: workaround | state: closed
Upgrade to prek 0.0.29 This upgrades prek to **just released** 0.0.29 and removes the workaround implemented in #54613 to add build-essential to the image that is used to build distributions - because prek 0.0.29 comes with all the necessary binary wheels/platforms that remove the need to build in in our Linux ARM image (issue https://github.com/j178/prek/issues/451 has been fixed). <!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the N…,,,,,,Anecdotal,issue,,,,,,,,2025-08-18,github/potiuk,https://github.com/apache/airflow/pull/54621,repo: apache/airflow | keyword: workaround | state: closed
"### Backport failed to create: v3-0-test. View the failure log <a href='https://github.com/apache/airflow/actions/runs/17045396012'> Run details </a> <table> <tr> <th>Status</th> <th>Branch</th> <th>Result</th> </tr> <tr> <td>❌</td> <td>v3-0-test</td> <td><a href=""https://github.com/apache/airflow/commit/a4fa2182004034ca27d4d43b973fdd4de7f99d5a""><img src='https://img.shields.io/badge/Commit-a4fa218-red' alt='Commit Link'></a></td> </tr> </table> You can attempt to backport this manually by runn…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/github-actions[bot],https://github.com/apache/airflow/pull/54621#issuecomment-3197455944,repo: apache/airflow | issue: Upgrade to prek 0.0.29 | keyword: workaround
"Add full support for AWS SSM Run Command in Airflow ### Summary Adds full support for AWS Systems Manager (SSM) Run Command in Apache Airflow, including: - SsmRunCommandOperator — sends commands to resources via SSM - SsmRunCommandCompletedSensor — waits for command completion using list_command_invocations. - Checks the status of all target resources for the given command invocation. - It waits until all resources have completed the command successfully. - If any resource reports a failure sta…",,,,,,Anecdotal,issue,,,,,,,,2025-07-03,github/Shlomit-B,https://github.com/apache/airflow/pull/52769,repo: apache/airflow | keyword: workaround | state: closed
"Congratulations on your first Pull Request and welcome to the Apache Airflow community! If you have any issues or are unsure about any anything please check our Contributors' Guide (https://github.com/apache/airflow/blob/main/contributing-docs/README.rst) Here are some useful points: - Pay attention to the quality of your code (ruff, mypy and type annotations). Our [pre-commits]( https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#prerequisites-for-pre-commit…",,,,,,Anecdotal,comment,,,,,,,,2025-07-03,github/boring-cyborg[bot],https://github.com/apache/airflow/pull/52769#issuecomment-3031853932,repo: apache/airflow | issue: Add full support for AWS SSM Run Command in Airflow | keyword: workaround
"> Hey @vincbeck, just a gentle reminder on this one — let me know if there's anything else you'd like me to adjust. Sorry I missed that! LGTM :)",,,,,,Anecdotal,comment,,,,,,,,2025-07-25,github/vincbeck,https://github.com/apache/airflow/pull/52769#issuecomment-3118712139,repo: apache/airflow | issue: Add full support for AWS SSM Run Command in Airflow | keyword: workaround
"Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.",,,,,,Anecdotal,comment,,,,,,,,2025-07-25,github/boring-cyborg[bot],https://github.com/apache/airflow/pull/52769#issuecomment-3118717014,repo: apache/airflow | issue: Add full support for AWS SSM Run Command in Airflow | keyword: workaround
"Remove end_from_trigger attribute in MwaaTaskSensor trigger Currently there's a bug that prevents MwaaTaskSensor from working in deferrable mode. It refers to an attribute `end_from_trigger`, the intention is for the trigger to complete without spinning up a call to `execute_complete`. However, `end_from_trigger` is a new attribute not yet implemented for this trigger, so I removed it. Without this change, the `example_mwaa.py` system integration test fails when `deferrable=True` on the `MwaaTa…",,,,,,Anecdotal,issue,,,,,,,,2025-08-15,github/seanghaeli,https://github.com/apache/airflow/pull/54531,repo: apache/airflow | keyword: workaround | state: closed
"> However, end_from_trigger is a new attribute not yet implemented for this trigger, so I removed it. Can you further explain? If this is not avaliable in all airflow versions than we should workaround for the unsupported versions.",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/eladkal,https://github.com/apache/airflow/pull/54531#issuecomment-3190546845,repo: apache/airflow | issue: Remove end_from_trigger attribute in MwaaTaskSensor trigger | keyword: workaround
"> > However, end_from_trigger is a new attribute not yet implemented for this trigger, so I removed it. > > Can you further explain? If this is not avaliable in all airflow versions than we should workaround for the unsupported versions. I just traced the code a bit. This argument does not exist in `MwaaDagRunCompletedTrigger`, `AwsBaseWaiterTrigger`, and `BaseTrigger`. It's a wrong example in example Dag so I think we're good to just remove it",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/Lee-W,https://github.com/apache/airflow/pull/54531#issuecomment-3190802864,repo: apache/airflow | issue: Remove end_from_trigger attribute in MwaaTaskSensor trigger | keyword: workaround
"> is a new attribute not yet implemented for this trigger To echo Elad's concern, does that mean the attribute is not in a released version? If it's an unreleased thing I'm fine with just pulling it from here. If the public has seen it, we're stuck with workarounds and deprecation.",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/ferruzzi,https://github.com/apache/airflow/pull/54531#issuecomment-3192265244,repo: apache/airflow | issue: Remove end_from_trigger attribute in MwaaTaskSensor trigger | keyword: workaround
"So based on [these docs](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/deferring.html#exiting-deferred-task-from-triggers) I incorrectly assumed the `end_from_trigger` parameter would be supported by the `BaseTrigger`. Because of this, `MwaaTaskSensor` doesn't work in deferrable mode. In this PR I remove the usage of `end_from_trigger` and the sensor now works. There would not be any versioning concerns because this is the standard way a Trigger is used.",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/seanghaeli,https://github.com/apache/airflow/pull/54531#issuecomment-3192321899,repo: apache/airflow | issue: Remove end_from_trigger attribute in MwaaTaskSensor trigger | keyword: workaround
feat: persist Dag search query across navigation Enhance Dag list search functionality to persist search queries when navigating between Dag details and back to the list view. ## Problem Search query is lost when navigating to Dag details and returning via browser back button https://github.com/user-attachments/assets/98789116-1a03-4a75-b4f1-b229d5f9293f ## Solution The search query now persists until the browser session ends. https://github.com/user-attachments/assets/c354eb48-a868-40c0-a592-9…,,,,,,Anecdotal,issue,,,,,,,,2025-08-02,github/choo121600,https://github.com/apache/airflow/pull/54059,repo: apache/airflow | keyword: workaround | state: closed
"> Looks good for me! > > Can you replace this (in another PR) for other search boxes across the UI? Oh Right, it seems other places with search boxes might have the same issue. I’ll check for them and make a separate PR. Thanks 😀",,,,,,Anecdotal,comment,,,,,,,,2025-08-03,github/choo121600,https://github.com/apache/airflow/pull/54059#issuecomment-3148641059,repo: apache/airflow | issue: feat: persist Dag search query across navigation | keyword: workaround
"I checked, and other search boxes store the query in the URL, so they’re fine. The issue only occurs in pages with navigation, such as the Dag list and Asset list. I’ve updated the search functionality in the Asset list and submitted a PR #54074 👍",,,,,,Anecdotal,comment,,,,,,,,2025-08-03,github/choo121600,https://github.com/apache/airflow/pull/54059#issuecomment-3148653804,repo: apache/airflow | issue: feat: persist Dag search query across navigation | keyword: workaround
"1. There’s a difference in how search works between components like Variables/Pools and DagList/AssetList. The former use modals, so search state is preserved via the URL query alone. But for DagList and AssetList, navigating to a detail page causes the search state to be lost. A common workaround is to include return paths or search state in the URL, but this results in long and cluttered URLs like `/assets/1?returnTo=%2Fassets....` I felt that detail page URLs should remain clean, reflecting …",,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/choo121600,https://github.com/apache/airflow/pull/54059#issuecomment-3153341134,repo: apache/airflow | issue: feat: persist Dag search query across navigation | keyword: workaround
"Thanks for your message, I would be in favor of following the same pattern as we do for other filters which is use the URL. The details page url will not be changed, we will push the search query param in the URL, before navigating to the details page, then hitting 'back' again will land us on the previous url that contains the `search` query parameter, properly restoring the search. It is already working for other filters, for instance (state filter) and it should be the same for search.",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/pierrejeambrun,https://github.com/apache/airflow/pull/54059#issuecomment-3160568065,repo: apache/airflow | issue: feat: persist Dag search query across navigation | keyword: workaround
"Retry exponential backoff max float overflow ### Apache Airflow version Other Airflow 2 version (please specify below) ### If ""Other Airflow 2 version"" selected, which one? 2.10.3 ### What happened? Hello, I encountered with a bug. My DAG configs were: retries=1000, retry_delay=5 min (300 seconds), max_retry_delay=1h (3600 seconds). My DAG failed ~1000 times and after that Scheduler broke down. After that retries exceeded 1000 and stopped on 1017 retry attempt. I did my research on this problem…",,,,,,Anecdotal,issue,,,,,,,,2025-03-19,github/alealandreev,https://github.com/apache/airflow/issues/47971,repo: apache/airflow | keyword: workaround | state: closed
"> It's a very, very, very niche case. Yes, but it is definitely a bug, which should be fixed. I did new pull request: https://github.com/apache/airflow/pull/48051",,,,,,Anecdotal,comment,,,,,,,,2025-03-21,github/alealandreev,https://github.com/apache/airflow/issues/47971#issuecomment-2743087624,repo: apache/airflow | issue: Retry exponential backoff max float overflow | keyword: workaround
"Yes it is rare case, but It leads scheduler crash. From the provided configuration this failure will happen in ~41 days. If max_retry_delay will be 15 minutes it will be in 10+ days... Scheduler will be failed and you can't understand why without touching logs.",,,,,,Anecdotal,comment,,,,,,,,2025-03-21,github/kenmy,https://github.com/apache/airflow/issues/47971#issuecomment-2743542863,repo: apache/airflow | issue: Retry exponential backoff max float overflow | keyword: workaround
"> Yes it is rare case, but It leads scheduler crash. From the provided configuration this failure will happen in ~41 days. If max_retry_delay will be 15 minutes it will be in 10+ days... Scheduler will be failed and you can't understand why without touching logs. Which does not make it realistic case to be honest. That's why it's super niche. You probably can get hundreds of unrealistic cases like that. And I am not saying it does not need to be fixed, it might, but looking at the PR, the code …",,,,,,Anecdotal,comment,,,,,,,,2025-03-22,github/potiuk,https://github.com/apache/airflow/issues/47971#issuecomment-2745390622,repo: apache/airflow | issue: Retry exponential backoff max float overflow | keyword: workaround
"Dear all, Please see [commit](https://github.com/alealandreev/airflow/commit/9ea065b7df74158646e1913f83b39738269543cd) Pull request updated with simplified logic: https://github.com/apache/airflow/pull/48057",,,,,,Anecdotal,comment,,,,,,,,2025-03-24,github/alealandreev,https://github.com/apache/airflow/issues/47971#issuecomment-2747881219,repo: apache/airflow | issue: Retry exponential backoff max float overflow | keyword: workaround
"Dear all, Please see [commit](https://github.com/apache/airflow/pull/48378) patch in /main, not only in 2.10.3 If it is not correct by form please do correct fix by yourself, if possible. Thank you in advance!",,,,,,Anecdotal,comment,,,,,,,,2025-03-26,github/alealandreev,https://github.com/apache/airflow/issues/47971#issuecomment-2753487887,repo: apache/airflow | issue: Retry exponential backoff max float overflow | keyword: workaround
"Dear @potiuk , What will be done with this bug? I provided different possible solutions for it, including simple patch to limit try_number to 500 in function, which calculates next retry delay. It will be nice to fix this bug in upcoming releases.",,,,,,Anecdotal,comment,,,,,,,,2025-03-28,github/alealandreev,https://github.com/apache/airflow/issues/47971#issuecomment-2760638969,repo: apache/airflow | issue: Retry exponential backoff max float overflow | keyword: workaround
"I think it is better to catch the overflow and use the maximum delay from the task or the environment. However, I added the missing test cases to show the patch from alealandreev works and made a PR against their branch. I submitted a PR against main that just catches the overflow and falls back to the maximum delay. No strong opinion about which is better.",,,,,,Anecdotal,comment,,,,,,,,2025-03-30,github/perry2of5,https://github.com/apache/airflow/issues/47971#issuecomment-2764298014,repo: apache/airflow | issue: Retry exponential backoff max float overflow | keyword: workaround
Is there a problem with my approach in PR #48557? https://github.com/apache/airflow/pull/48557 If this isn't worth fixing then I can close the PR. To me it seems worth fixing even though it is a niche case. I can update the PR if someone wants me to.....,,,,,,Anecdotal,comment,,,,,,,,2025-05-24,github/perry2of5,https://github.com/apache/airflow/issues/47971#issuecomment-2906103070,repo: apache/airflow | issue: Retry exponential backoff max float overflow | keyword: workaround
I just had this issue in prod! is there a workaround to make it work? thanks god it just happened today in the morning! no one task was bing scheduled anymore! this needs to be fixed ASAP!,,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/netogerbi,https://github.com/apache/airflow/issues/47971#issuecomment-3019428328,repo: apache/airflow | issue: Retry exponential backoff max float overflow | keyword: workaround
"I addressed the comments on my PR which should solve the issue. You can back port it to whatever version you are using, @netogerbi.",,,,,,Anecdotal,comment,,,,,,,,2025-07-01,github/perry2of5,https://github.com/apache/airflow/issues/47971#issuecomment-3025807213,repo: apache/airflow | issue: Retry exponential backoff max float overflow | keyword: workaround
"Simplify installing airflow from GitHub repo in breeze Breeze had the possibility of installing airflow from a branch of any Github repo - by providing VCS url, but it's been broken since the split of distributions. This PR adds capability of using `owner/repo:branch` as `--use-airflow-version` and the installation will be done using this GitHub repo. Note! Such installation will NOT (currently) compile the assets so you will not be able to run api_server easily after such installation We might…",,,,,,Anecdotal,issue,,,,,,,,2025-08-03,github/potiuk,https://github.com/apache/airflow/pull/54070,repo: apache/airflow | keyword: workaround | state: closed
"### Backport failed to create: v3-0-test. View the failure log <a href='https://github.com/apache/airflow/actions/runs/16709068843'> Run details </a> <table> <tr> <th>Status</th> <th>Branch</th> <th>Result</th> </tr> <tr> <td>❌</td> <td>v3-0-test</td> <td><a href=""https://github.com/apache/airflow/commit/40ccc55536b9c081104e54e24ccf3a7de85151e5""><img src='https://img.shields.io/badge/Commit-40ccc55-red' alt='Commit Link'></a></td> </tr> </table> You can attempt to backport this manually by runn…",,,,,,Anecdotal,comment,,,,,,,,2025-08-03,github/github-actions[bot],https://github.com/apache/airflow/pull/54070#issuecomment-3148677269,repo: apache/airflow | issue: Simplify installing airflow from GitHub repo in breeze | keyword: workaround
"BTW. @amoghrajesh @jason810496 -> I thought about next steps: 1) add asset compilation after installing airflow (that might be tricky as currently we have no node in the image - so it might requires some tricks. Likely it will require downlading the sources, unpacking them and installing airflow with `hatch build -c -t custom` after installing node. 2) adding compat test in CI for `v3-0-test` - this way at ANY point in time we would know that all ""main"" providers are tested with ""3.0.X in progr…",,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/potiuk,https://github.com/apache/airflow/pull/54070#issuecomment-3155292576,repo: apache/airflow | issue: Simplify installing airflow from GitHub repo in breeze | keyword: workaround
"There frontend not built before error when specifing with `--use-airflow-version apache/airflow:main`( even with `start-airflow --dev-mode` ). ```bash breeze start-airflow --dev-mode --python 3.10 --backend postgres --integration localstack --mount-sources providers-and-tests --use-airflow-version apache/airflow:main ``` The Airflow start successfully in Breeze, but the frontend is not built before. So it will result in 500 internal error when accessing the UI due to missing the dist of fronten…",,,,,,Anecdotal,comment,,,,,,,,2025-08-10,github/jason810496,https://github.com/apache/airflow/pull/54070#issuecomment-3172716863,repo: apache/airflow | issue: Simplify installing airflow from GitHub repo in breeze | keyword: workaround
> I will check where should I add the breeze compile-ui-assets or how to compile the frontend before start-airflow when I have time tomorrow. Yes- that is a missing thing. The problem with it is that we currently run asset compilation outside of breeze - but the installation with `--use-airflow-version` already happens fully in breeze. And we have no `node/yarn/pnpm` installed in CI image.... The problem is that when we install from GitHub URL - we do not have local sources (which would be a di…,,,,,,Anecdotal,comment,,,,,,,,2025-08-10,github/potiuk,https://github.com/apache/airflow/pull/54070#issuecomment-3172892790,repo: apache/airflow | issue: Simplify installing airflow from GitHub repo in breeze | keyword: workaround
"> adding compat test in CI for v3-0-test - this way at ANY point in time we would know that all ""main"" providers are tested with ""3.0.X in progress"" at the tip of our branch. This does make a lot of sense yes.",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/amoghrajesh,https://github.com/apache/airflow/pull/54070#issuecomment-3177776594,repo: apache/airflow | issue: Simplify installing airflow from GitHub repo in breeze | keyword: workaround
Converted those two tasks into two issues: * https://github.com/apache/airflow/issues/54453 * https://github.com/apache/airflow/issues/54454,,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/potiuk,https://github.com/apache/airflow/pull/54070#issuecomment-3183413782,repo: apache/airflow | issue: Simplify installing airflow from GitHub repo in breeze | keyword: workaround
"Add cert used for apiserver to client (fix for self-signed) <!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICEN…",,,,,,Anecdotal,issue,,,,,,,,2025-07-20,github/danieldean,https://github.com/apache/airflow/pull/53574,repo: apache/airflow | keyword: workaround | state: closed
"Congratulations on your first Pull Request and welcome to the Apache Airflow community! If you have any issues or are unsure about any anything please check our Contributors' Guide (https://github.com/apache/airflow/blob/main/contributing-docs/README.rst) Here are some useful points: - Pay attention to the quality of your code (ruff, mypy and type annotations). Our [pre-commits]( https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#prerequisites-for-pre-commit…",,,,,,Anecdotal,comment,,,,,,,,2025-07-20,github/boring-cyborg[bot],https://github.com/apache/airflow/pull/53574#issuecomment-3094726209,repo: apache/airflow | issue: Add cert used for apiserver to client (fix for self-signed) | keyword: workaround
"Although there are no config changes with this the self-signed certificate does need subject alt names set: ``` openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -sha256 -days 3650 -nodes \ -subj ""CN=<common name>"" \ -addext ""subjectAltName=DNS:localhost,DNS:airflow-apiserver"" ``` For the certificate to be accepted (this is based on the quick start docker-compose.yaml). As well as the cert being passed to curl using `curl --cacert ` for the apiserver health check plus changing th…",,,,,,Anecdotal,comment,,,,,,,,2025-07-20,github/danieldean,https://github.com/apache/airflow/pull/53574#issuecomment-3094747264,repo: apache/airflow | issue: Add cert used for apiserver to client (fix for self-signed) | keyword: workaround
"Also, it would be great to add a unit test covering it. @Subham-KRLX -> you said you can collaborate, so maybe adding a unit test as PR to the PR might be a good idea :)",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/potiuk,https://github.com/apache/airflow/pull/53574#issuecomment-3095343862,repo: apache/airflow | issue: Add cert used for apiserver to client (fix for self-signed) | keyword: workaround
Hi @danieldean I have added a unit test for the new SSL certificate config logic in the API client and confirmed that it passes locally I have opened a PR with this test .,,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/Subham-KRLX,https://github.com/apache/airflow/pull/53574#issuecomment-3096753525,repo: apache/airflow | issue: Add cert used for apiserver to client (fix for self-signed) | keyword: workaround
> Hi @danieldean I have added a unit test for the new SSL certificate config logic in the API client and confirmed that it passes locally I have opened a PR with this test . This appears to be based on a different logic contained within the client to that in this PR? I think you need to use my branch as a base and make a test based on that if possible.,,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/danieldean,https://github.com/apache/airflow/pull/53574#issuecomment-3101391213,repo: apache/airflow | issue: Add cert used for apiserver to client (fix for self-signed) | keyword: workaround
I have rebased my branch on top of yours and updated the base of my PR accordingly. The unit test now aligns with your changes. Please let me know if you need any adjustments or additional coverage.,,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/Subham-KRLX,https://github.com/apache/airflow/pull/53574#issuecomment-3101536451,repo: apache/airflow | issue: Add cert used for apiserver to client (fix for self-signed) | keyword: workaround
> I have rebased my branch on top of yours and updated the base of my PR accordingly. The unit test now aligns with your changes. Please let me know if you need any adjustments or additional coverage. I am unsure what has happened but there appears to be something wrong with this. You have different code in the `client.py` and other changes pulling in many approvers. I have built an image from my branch so I can get on with things. It is here if it is of any use to anyone: https://hub.docker.co…,,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/danieldean,https://github.com/apache/airflow/pull/53574#issuecomment-3103898349,repo: apache/airflow | issue: Add cert used for apiserver to client (fix for self-signed) | keyword: workaround
"@danieldean ,Could you please clarify which specific changes or tests you want me to update so everything aligns? I will make the adjustments accordingly.",,,,,,Anecdotal,comment,,,,,,,,2025-07-23,github/Subham-KRLX,https://github.com/apache/airflow/pull/53574#issuecomment-3106098185,repo: apache/airflow | issue: Add cert used for apiserver to client (fix for self-signed) | keyword: workaround
I see the MyPy checks are failing due to type errors involving direct assignment to ssl.SSLContext methods and some type mismatches in task-sdk/tests/task_sdk/__init__.py. If you can share how you had prefer the SSL method mocking or type annotations to be adjusted . Just let me know if you want me to try specific fixes or take a look at detailed errors to help move this forward.,,,,,,Anecdotal,comment,,,,,,,,2025-07-25,github/Subham-KRLX,https://github.com/apache/airflow/pull/53574#issuecomment-3119673319,repo: apache/airflow | issue: Add cert used for apiserver to client (fix for self-signed) | keyword: workaround
I have done some reading on testing and revised my methods. I believe static checks and tests all now pass. Just a short how to on this and all is hopefully done.,,,,,,Anecdotal,comment,,,,,,,,2025-07-25,github/danieldean,https://github.com/apache/airflow/pull/53574#issuecomment-3120433495,repo: apache/airflow | issue: Add cert used for apiserver to client (fix for self-signed) | keyword: workaround
"Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.",,,,,,Anecdotal,comment,,,,,,,,2025-07-26,github/boring-cyborg[bot],https://github.com/apache/airflow/pull/53574#issuecomment-3121500549,repo: apache/airflow | issue: Add cert used for apiserver to client (fix for self-signed) | keyword: workaround
"Migrate `HiveToDynamoDBOperator` and `SqlToS3Operator` to use `get_df` <!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/lic…",,,,,,Anecdotal,issue,,,,,,,,2025-05-02,github/guan404ming,https://github.com/apache/airflow/pull/50126,repo: apache/airflow | keyword: workaround | state: closed
Failed since the `hive` also need the overload like something I added in `common-sql`. I would open PR to solve it later.,,,,,,Anecdotal,comment,,,,,,,,2025-05-05,github/guan404ming,https://github.com/apache/airflow/pull/50126#issuecomment-2850971490,repo: apache/airflow | issue: Migrate `HiveToDynamoDBOperator` and `SqlToS3Operator` to use `get_df` | keyword: workaround
"Hi, I’ve tried to figure out why `mypy` passes locally but fails in CI: the CI pipeline seems not import (or install ?) the dependency (common-sql/sql.pyi) mypy needs for its checks. If you compare the local and CI error logs, you’ll see that with our new type overload the error disappears locally, but CI still reports it. I'm not sure that should we update the CI configuration to force import that file, or just add a `# type: ignore` here or is there any better way to solve this in Slack/trans…",,,,,,Anecdotal,comment,,,,,,,,2025-05-06,github/guan404ming,https://github.com/apache/airflow/pull/50126#issuecomment-2853587307,repo: apache/airflow | issue: Migrate `HiveToDynamoDBOperator` and `SqlToS3Operator` to use `get_df` | keyword: workaround
"When I keep searching the answer I find the type def did not really following the newest python typing spec, thus I update them in #50229. I think maybe it could solve the error here since it didn't rely on the `pyi` file but to directly write the overload in the `.py` which I think it must be imported here.",,,,,,Anecdotal,comment,,,,,,,,2025-05-06,github/guan404ming,https://github.com/apache/airflow/pull/50126#issuecomment-2853770479,repo: apache/airflow | issue: Migrate `HiveToDynamoDBOperator` and `SqlToS3Operator` to use `get_df` | keyword: workaround
"### Backport successfully created: v3-0-test <table> <tr> <th>Status</th> <th>Branch</th> <th>Result</th> </tr> <tr> <td>✅</td> <td>v3-0-test</td> <td><a href=""https://github.com/apache/airflow/pull/54409""><img src=""https://img.shields.io/badge/PR-54409-blue"" alt=""PR Link""></a></td> </tr> </table>",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/github-actions[bot],https://github.com/apache/airflow/pull/50126#issuecomment-3179068838,repo: apache/airflow | issue: Migrate `HiveToDynamoDBOperator` and `SqlToS3Operator` to use `get_df` | keyword: workaround
"Allow failure callbacks for stuck in queued TIs that fail <!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE…",,,,,,Anecdotal,issue,,,,,,,,2025-07-16,github/karenbraganz,https://github.com/apache/airflow/pull/53435,repo: apache/airflow | keyword: workaround | state: closed
"I was testing this PR out by repeatedly getting tasks stuck in queued. Initially, the task instances would get stuck in queued, fail, and the callbacks would run. After a few rounds of this, the below exception was raised: <img width=""1798"" height=""730"" alt=""image"" src=""https://github.com/user-attachments/assets/217ef825-55b8-45a8-aa51-966a2bea3c10"" /> According to the code, the session is not committed until after `_maybe_requeue_stuck_ti` completes running. Any idea what would cause the TaskI…",,,,,,Anecdotal,comment,,,,,,,,2025-07-27,github/karenbraganz,https://github.com/apache/airflow/pull/53435#issuecomment-3124501876,repo: apache/airflow | issue: Allow failure callbacks for stuck in queued TIs that fail | keyword: workaround
"@karenbraganz it could be due to the expundge happening here https://github.com/apache/airflow/blob/main/airflow-core%2Fsrc%2Fairflow%2Fjobs%2Fscheduler_job_runner.py#L1286 This detaches all the instance objects from the session, I couldn't follow the flow as I did not have time but I think it might be related, or at least it is a good place to start investigating from.",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/Nataneljpwd,https://github.com/apache/airflow/pull/53435#issuecomment-3125862692,repo: apache/airflow | issue: Allow failure callbacks for stuck in queued TIs that fail | keyword: workaround
@Nataneljpwd The session being expunged in the link you provided is not the same session being used in `_maybe_requeue_stuck_ti`. The function `_maybe_requeue_stuck_ti` uses the session that is [passed to it when it is called in _handle_tasks_stuck_in_queued](https://github.com/apache/airflow/blob/f1a76a104a4fb42d87bb6e5fb6d3f071f991d9eb/airflow-core/src/airflow/jobs/scheduler_job_runner.py#L1976). This session is [created in _handle_tasks_stuck_in_queued](https://github.com/apache/airflow/blob…,,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/karenbraganz,https://github.com/apache/airflow/pull/53435#issuecomment-3127508820,repo: apache/airflow | issue: Allow failure callbacks for stuck in queued TIs that fail | keyword: workaround
"@karenbraganz, I think that the issue might be that if there is more than 1 stuck task, it commits before it changed or requed all tasks, and so the other tasks are lost, and we can see the commit [here](https://github.com/apache/airflow/blob/f1a76a104a4fb42d87bb6e5fb6d3f071f991d9eb/airflow-core/src/airflow/jobs/scheduler_job_runner.py#L1978C1-L1979C1), I would try moving the commit out of the loop and see if it changes anything, nevertheless, I would do it either way as we batch requests to th…",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/Nataneljpwd,https://github.com/apache/airflow/pull/53435#issuecomment-3128061249,repo: apache/airflow | issue: Allow failure callbacks for stuck in queued TIs that fail | keyword: workaround
"@karenbraganz A similar ""detached instance error"" was just fixed by Kaxil, but maybe we need another fix like #53838 (in a separate PR please)",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/ashb,https://github.com/apache/airflow/pull/53435#issuecomment-3131673430,repo: apache/airflow | issue: Allow failure callbacks for stuck in queued TIs that fail | keyword: workaround
"I moved `session.commit()` outside the loop and re-ran the repeated tests overnight. The first 45 task instances got stuck and failed as expected, but the 46th task instance that got stuck experienced the `DetachedInstanceError` again. I will take a look at Kaxil's fix next.",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/karenbraganz,https://github.com/apache/airflow/pull/53435#issuecomment-3132554054,repo: apache/airflow | issue: Allow failure callbacks for stuck in queued TIs that fail | keyword: workaround
"@ashb looks like eager loading the attributes is the solution. Still need to implement and test this out to confirm that it works. Is there a reason why you want this in a separate PR? I would be eager loading attributes that are only used in this PR, so I thought it would be more appropriate to add that code in this PR itself. For example, `ti.dag_model.relative_fileloc` is one of the attributes I would be eager loading, and this attribute is directly used in the code I am adding in this PR.",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/karenbraganz,https://github.com/apache/airflow/pull/53435#issuecomment-3133937981,repo: apache/airflow | issue: Allow failure callbacks for stuck in queued TIs that fail | keyword: workaround
"@karenbraganz Ah, if this PR is where you first access them, then yes making them eager load here is probably the right thing to do. However one thing we need to consider is if eagerloading them is going to have performance impact on a rarely used case (I don't have the context on this path or PR to say either way), and if it might better to load it from the DB in some other way only when needed.",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/ashb,https://github.com/apache/airflow/pull/53435#issuecomment-3134001662,repo: apache/airflow | issue: Allow failure callbacks for stuck in queued TIs that fail | keyword: workaround
Confirmed that eager loading resolves the issue. Working through the correct way to implement this since the attributes will only be used if the task is failed and has a failure callback.,,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/karenbraganz,https://github.com/apache/airflow/pull/53435#issuecomment-3141762540,repo: apache/airflow | issue: Allow failure callbacks for stuck in queued TIs that fail | keyword: workaround
"I was able to find a workaround for the `DetachedInstanceError` that does not involve eager loading. Right before the `TaskCallbackRequest` is created (where the error occurs), I added a condition to check whether the ti is detached. If it is, it will be merged into the session. I found that this prevents the issue from occurring without having to eager load any attributes. I have implemented this in my latest commit. Please let me know if there are any objections to this workaround.",,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/karenbraganz,https://github.com/apache/airflow/pull/53435#issuecomment-3152130914,repo: apache/airflow | issue: Allow failure callbacks for stuck in queued TIs that fail | keyword: workaround
"### Backport failed to create: v3-0-test. View the failure log <a href='https://github.com/apache/airflow/actions/runs/16905519789'> Run details </a> <table> <tr> <th>Status</th> <th>Branch</th> <th>Result</th> </tr> <tr> <td>❌</td> <td>v3-0-test</td> <td><a href=""https://github.com/apache/airflow/commit/6da77b1fdfc0b51762b47638489e752384911758""><img src='https://img.shields.io/badge/Commit-6da77b1-red' alt='Commit Link'></a></td> </tr> </table> You can attempt to backport this manually by runn…",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/github-actions[bot],https://github.com/apache/airflow/pull/53435#issuecomment-3178659580,repo: apache/airflow | issue: Allow failure callbacks for stuck in queued TIs that fail | keyword: workaround
Pymongo 4.11 breaks Mongo provider ### Apache Airflow Provider(s) mongo ### Versions of Apache Airflow Providers Failed CI run: https://github.com/apache/airflow/actions/runs/13018061271/job/36312691670 Errors when using pymongo 4.11: FAILED providers/mongo/tests/provider_tests/mongo/hooks/test_mongo.py::TestMongoHook::test_replace_many - TypeError: BulkOperationBuilder.add_replace() got an unexpected keyword argument 'sort' FAILED providers/mongo/tests/provider_tests/mongo/hooks/test_mongo.py:…,,,,,,Anecdotal,issue,,,,,,,,2025-01-28,github/jscheffl,https://github.com/apache/airflow/issues/46215,repo: apache/airflow | keyword: workaround | state: closed
Status of testing Apache Airflow 3.0.4rc2 and TaskSDK 1.0.4rc1 We are kindly requesting that contributors to [Apache Airflow RC 3.0.4rc1](https://pypi.org/project/apache-airflow/3.0.4rc1/) help test the RC. Please let us know by commenting if the issue is addressed in the latest RC. - [ ] [use of debugpy to run airflow components via breeze (#51763)](https://github.com/apache/airflow/pull/51763): @rawwar - [x] [[Backport v3-0-test] Add back dag parsing pre-import optimization (#50371) (#52698)]…,,,,,,Anecdotal,issue,,,,,,,,2025-08-05,github/ashb,https://github.com/apache/airflow/issues/54124,repo: apache/airflow | keyword: workaround | state: closed
Tested and checked all my changes - they look good. I found one issue with #54020 -> there was another cherry-pick missing - #54129 -> which caused generated diagrams to miss icons (see here: https://airflow.staged.apache.org/docs/apache-airflow/stable/core-concepts/overview.html) . I am cherry-picking the missing change to v3-0-test (https://github.com/apache/airflow/pull/54129). No need to fix anything - we will just have to add the commit (alongside #54100) when building documentation.,,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/potiuk,https://github.com/apache/airflow/issues/54124#issuecomment-3154941094,repo: apache/airflow | issue: Status of testing Apache Airflow 3.0.4rc2 and TaskSDK 1.0.4rc1 | keyword: workaround
CI / Dev tool fixes: - https://github.com/apache/airflow/pull/52961 - https://github.com/apache/airflow/pull/52925 - https://github.com/apache/airflow/pull/53451 - https://github.com/apache/airflow/pull/53659 - https://github.com/apache/airflow/pull/53778 - https://github.com/apache/airflow/pull/53996 - https://github.com/apache/airflow/pull/54078 Actual fixes: - https://github.com/apache/airflow/pull/53093 - https://github.com/apache/airflow/pull/53518 - https://github.com/apache/airflow/pull/…,,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/amoghrajesh,https://github.com/apache/airflow/issues/54124#issuecomment-3155650985,repo: apache/airflow | issue: Status of testing Apache Airflow 3.0.4rc2 and TaskSDK 1.0.4rc1 | keyword: workaround
"Tested a few dags with airflow==3.0.4rc1 and task-sdk==1.0.4rc1. Works fine. One issue I encountered was that after running `uv add apache-airflow==3.0.4rc1 --prerelease=allow`, it does not use *task-sdk==1.0.4rc1*. Additionally, *airflow 3.0.4rc1* does not work with *task-sdk 1.0.3*. Not sure whether we need to pin any deps",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/Lee-W,https://github.com/apache/airflow/issues/54124#issuecomment-3157165079,repo: apache/airflow | issue: Status of testing Apache Airflow 3.0.4rc2 and TaskSDK 1.0.4rc1 | keyword: workaround
Tested with 3.0.4rc1: - My change [#50371](https://github.com/apache/airflow/pull/50371) works well. - The backport [#52698](https://github.com/apache/airflow/pull/52698) does not affect the RC functionality. Looks good from my side.,,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/Lzzz666,https://github.com/apache/airflow/issues/54124#issuecomment-3158757594,repo: apache/airflow | issue: Status of testing Apache Airflow 3.0.4rc2 and TaskSDK 1.0.4rc1 | keyword: workaround
"> Tested a few dags with airflow==3.0.4rc1 and task-sdk==1.0.4rc1. Works fine. > > One issue I encountered was that after running `uv add apache-airflow==3.0.4rc1 --prerelease=allow`, it does not use _task-sdk==1.0.4rc1_. Additionally, _airflow 3.0.4rc1_ does not work with _task-sdk 1.0.3_. Not sure whether we need to pin any deps Fixed in apache-airflow rc2",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/ashb,https://github.com/apache/airflow/issues/54124#issuecomment-3160633097,repo: apache/airflow | issue: Status of testing Apache Airflow 3.0.4rc2 and TaskSDK 1.0.4rc1 | keyword: workaround
"Verified my changes LGTM, ran few examples working fine. I find one issue that is related to standard provider example dag while running `PythonVirtualenvOperator` . getting below error <img width=""1638"" height=""482"" alt=""Image"" src=""https://github.com/user-attachments/assets/2ba8e69e-7193-4f44-b405-c894db986cce"" /> This was introduced by this https://github.com/apache/airflow/pull/53390/files#diff-80d85a10fe6b4cea5c06a9959a893c000cc90fd04ff1b0c0344ef937b0251111R892 Going to fix it.",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/gopidesupavan,https://github.com/apache/airflow/issues/54124#issuecomment-3161779344,repo: apache/airflow | issue: Status of testing Apache Airflow 3.0.4rc2 and TaskSDK 1.0.4rc1 | keyword: workaround
"> Verified my changes LGTM, ran few examples working fine. > > I find one issue that is related to standard provider example dag while running `PythonVirtualenvOperator` . getting below error > > <img alt=""Image"" width=""1638"" height=""482"" src=""https://private-user-images.githubusercontent.com/31437079/475246483-2ba8e69e-7193-4f44-b405-c894db986cce.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQ1…",,,,,,Anecdotal,comment,,,,,,,,2025-08-07,github/gopidesupavan,https://github.com/apache/airflow/issues/54124#issuecomment-3163256193,repo: apache/airflow | issue: Status of testing Apache Airflow 3.0.4rc2 and TaskSDK 1.0.4rc1 | keyword: workaround
Tested the latest compatibility fix and my other changes as reported in https://github.com/apache/airflow/issues/54124#issuecomment-3155650985. ``` uv pip install --prerelease allow apache-airflow==3.0.4rc2 - apache-airflow==3.0.4 (from file:///opt/airflow) + apache-airflow==3.0.4rc2 - apache-airflow-core==3.0.4 (from file:///opt/airflow/airflow-core) + apache-airflow-core==3.0.4rc2 - apache-airflow-task-sdk==1.0.4 (from file:///opt/airflow/task-sdk) + apache-airflow-task-sdk==1.0.4rc1 ``` It r…,,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/amoghrajesh,https://github.com/apache/airflow/issues/54124#issuecomment-3166801900,repo: apache/airflow | issue: Status of testing Apache Airflow 3.0.4rc2 and TaskSDK 1.0.4rc1 | keyword: workaround
"Fix for Edit and Delete request issue 53681 closes: https://github.com/apache/airflow/issues/53681 <!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License …",,,,,,Anecdotal,issue,,,,,,,,2025-07-28,github/mandeepzemo,https://github.com/apache/airflow/pull/53815,repo: apache/airflow | keyword: workaround | state: closed
"Congratulations on your first Pull Request and welcome to the Apache Airflow community! If you have any issues or are unsure about any anything please check our Contributors' Guide (https://github.com/apache/airflow/blob/main/contributing-docs/README.rst) Here are some useful points: - Pay attention to the quality of your code (ruff, mypy and type annotations). Our [pre-commits]( https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#prerequisites-for-pre-commit…",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/boring-cyborg[bot],https://github.com/apache/airflow/pull/53815#issuecomment-3127172383,repo: apache/airflow | issue: Fix for Edit and Delete request issue 53681 | keyword: workaround
"Did we also consider to restrict pool names to use a ""/"" character? Like we did with run_ids? I assume there might be a bit more places like variables where the same problem applies? closes: #53681",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/jscheffl,https://github.com/apache/airflow/pull/53815#issuecomment-3128832560,repo: apache/airflow | issue: Fix for Edit and Delete request issue 53681 | keyword: workaround
"We could consider banning slashes altogether, but it’d cause too much breakage we need a long deprecation period, and a fix like this would be required in the meantime anyway. As a side, this will not only affect slashes, but also some percent usages that may be unintentionally identified as escape sequences. This should have a significant newsfragment calling out potential incompatibilities.",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/uranusjr,https://github.com/apache/airflow/pull/53815#issuecomment-3130268567,repo: apache/airflow | issue: Fix for Edit and Delete request issue 53681 | keyword: workaround
"@uranusjr ; I am not directly working on task, just overviewing with @mandeepzemo , please excuse if I do oversight. could we take both the approaches we could restrict `create request` from using \ or other chars; using a valid regular expression for identifier https://stackoverflow.com/questions/14953861/representing-identifiers-using-regular-expression and for those entities already created we can avoid the error using encodeUriComponent? should we check how to put deprecation notice on this.",,,,,,Anecdotal,comment,,,,,,,,2025-07-30,github/akshayvijayjain,https://github.com/apache/airflow/pull/53815#issuecomment-3136119164,repo: apache/airflow | issue: Fix for Edit and Delete request issue 53681 | keyword: workaround
"@jscheffl As taken refer from variables and updated the pool routes code for PATCH, DELETE and GET Methods https://github.com/apache/airflow/pull/53815",,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/mandeepzemo,https://github.com/apache/airflow/pull/53815#issuecomment-3149124447,repo: apache/airflow | issue: Fix for Edit and Delete request issue 53681 | keyword: workaround
"@pierrejeambrun @jason810496 Thanks for the update, Updated the test cases for testing Pool Name with '/'. And Also sharing the local test result below. Please also let me know, if any other changes need to update. <img width=""951"" height=""354"" alt=""image"" src=""https://github.com/user-attachments/assets/3718e6b8-e316-41ba-90ee-d785c929ff7d"" />",,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/mandeepzemo,https://github.com/apache/airflow/pull/53815#issuecomment-3153667019,repo: apache/airflow | issue: Fix for Edit and Delete request issue 53681 | keyword: workaround
"Static check need fixing, we can also take the opportunity to rename `POOL3_NAME = ""test/3""` to `POOL3_NAME = ""pool/3""`. (pool1 and pool2 are named `pool1` and `pool2`)",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/pierrejeambrun,https://github.com/apache/airflow/pull/53815#issuecomment-3160669190,repo: apache/airflow | issue: Fix for Edit and Delete request issue 53681 | keyword: workaround
"@pierrejeambrun The pool list ordering test was passing locally but failing in CI/CD due to locale-dependent sorting of POOL3_NAME = ""pool/3"". To make the result consistent across environments, I updated the assertion to compare sorted lists in Python: `assert sorted([pool[""name""] for pool in body[""pools""]]) == sorted(expected_ids)` This avoids collation issues while still verifying the correct pool names are returned.",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/mandeepzemo,https://github.com/apache/airflow/pull/53815#issuecomment-3166616485,repo: apache/airflow | issue: Fix for Edit and Delete request issue 53681 | keyword: workaround
"> failing in CI/CD due to locale-dependent sorting of POOL3_NAME = ""pool/3"" Which test ordering was failing specifically?",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/pierrejeambrun,https://github.com/apache/airflow/pull/53815#issuecomment-3167293246,repo: apache/airflow | issue: Fix for Edit and Delete request issue 53681 | keyword: workaround
"@pierrejeambrun In test_pool.py (lines 186–195), the test checks if the /pools endpoint returns the pool list in the expected order when using the order_by=name query param. **Example** `assert [pool[""name""] for pool in body[""pools""]] == expected_ids` **Expected Behavior (Local):** _Given: POOL1_NAME = ""pool1"", POOL2_NAME = ""pool2"", POOL3_NAME = ""pool/3""_ **Database sorting by name should return:** `[""pool/3"", ""pool2"", ""pool1""]` (/ sorts before alphanumeric characters in ASCII order). **Issue (…",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/mandeepzemo,https://github.com/apache/airflow/pull/53815#issuecomment-3167424848,repo: apache/airflow | issue: Fix for Edit and Delete request issue 53681 | keyword: workaround
"Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/boring-cyborg[bot],https://github.com/apache/airflow/pull/53815#issuecomment-3167795226,repo: apache/airflow | issue: Fix for Edit and Delete request issue 53681 | keyword: workaround
"### Backport successfully created: v3-0-test <table> <tr> <th>Status</th> <th>Branch</th> <th>Result</th> </tr> <tr> <td>✅</td> <td>v3-0-test</td> <td><a href=""https://github.com/apache/airflow/pull/54268""><img src=""https://img.shields.io/badge/PR-54268-blue"" alt=""PR Link""></a></td> </tr> </table>",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/github-actions[bot],https://github.com/apache/airflow/pull/53815#issuecomment-3167799244,repo: apache/airflow | issue: Fix for Edit and Delete request issue 53681 | keyword: workaround
[Airbyte] Airflow Airbyte provider 4.0 not allowing disabled auth and does not work with auth ### Apache Airflow Provider(s) airbyte ### Versions of Apache Airflow Providers apache-airflow-providers-airbyte | 4.0.0 ### Apache Airflow version 2.8.1 ### Operating System AWS MWAA ### Deployment Amazon (AWS) MWAA ### Deployment details Nothing really relevant. Standard AWS MWAA installation. ### What happened There are two issues with the version 4.0.0 of the airbyte provider: 1 - In the airbyte pr…,,,,,,Anecdotal,issue,,,,,,,,2024-09-26,github/baugarcia,https://github.com/apache/airflow/issues/42520,repo: apache/airflow | keyword: workaround | state: closed
"Hello, I have exactly the same issue (OSS instalation and version 4.0.0 of the airbyte provider). I get this error `airflow.exceptions.AirflowException: Unexpected status code 404 from token endpoint`",,,,,,Anecdotal,comment,,,,,,,,2024-10-30,github/querbesd,https://github.com/apache/airflow/issues/42520#issuecomment-2447746814,repo: apache/airflow | issue: [Airbyte] Airflow Airbyte provider 4.0 not allowing disabled auth and does not work with auth | keyword: workaround
"If you use the ""Token URL"" `api/v1/applications/token` then I think that will deal with the 404 at least, but instead I get a 401. I've created an issue https://github.com/airbytehq/airbyte-api-python-sdk/issues/112 to track, since the core problem seems to be from the airbyte-api SDK, rather than from the provider. Alternatively, the airbyte-api SDK _does_ seem to work with username/password auth, so possibly the change could still be made here in the provider, with the option to auth using us…",,,,,,Anecdotal,comment,,,,,,,,2024-11-20,github/amardatar,https://github.com/apache/airflow/issues/42520#issuecomment-2489072269,repo: apache/airflow | issue: [Airbyte] Airflow Airbyte provider 4.0 not allowing disabled auth and does not work with auth | keyword: workaround
"For anyone looking for a workaround - this is a little involved, but I've got it working by making the following changes: The connection in Airflow has the details: Host: `<your airbyte domain>/api/public/v1` Token URL: `v1/applications/token` Client ID: `<your client ID>` Client Secret: `<your client secret>` And the clientcredentials.py file needs to be updated [here](https://github.com/airbytehq/airbyte-api-python-sdk/blob/048c847914113b75a246ce122fe1cce4f48fee64/src/airbyte_api/_hooks/clien…",,,,,,Anecdotal,comment,,,,,,,,2024-11-20,github/amardatar,https://github.com/apache/airflow/issues/42520#issuecomment-2489306026,repo: apache/airflow | issue: [Airbyte] Airflow Airbyte provider 4.0 not allowing disabled auth and does not work with auth | keyword: workaround
"For those who are encountering this issue, a quickfix I did was to use the `apache-airflow-providers-airbyte` package version 3.0.0 instead of version >=4.0.0. With this version authentication is not necessary.",,,,,,Anecdotal,comment,,,,,,,,2025-06-23,github/NahidOulmi,https://github.com/apache/airflow/issues/42520#issuecomment-2996404050,repo: apache/airflow | issue: [Airbyte] Airflow Airbyte provider 4.0 not allowing disabled auth and does not work with auth | keyword: workaround
"You may also add this as well at the top of your DAGs ```python import airbyte_api._hooks.clientcredentials as _cc _cc.ClientCredentialsHook.get_credentials = lambda self, source: None ``` So you can keep an up to date version of the Airflow operators",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/CrisNavasM,https://github.com/apache/airflow/issues/42520#issuecomment-3167103299,repo: apache/airflow | issue: [Airbyte] Airflow Airbyte provider 4.0 not allowing disabled auth and does not work with auth | keyword: workaround
"UI - Home screen does not filter based on logged in user's role's access to dags ### Apache Airflow version 3.0.3 ### If ""Other Airflow 2 version"" selected, which one? _No response_ ### What happened? Case: Custom airflow role with access to only some dags. The ""Home"" screen shows the Stats and History for all dags running on the airflow instance which includes the dags that the logged in user does even not have access to. For example: here i see 6 Dag runs but this airflow user role just has a…",,,,,,Anecdotal,issue,,,,,,,,2025-07-30,github/devgonvarun,https://github.com/apache/airflow/issues/53938,repo: apache/airflow | keyword: workaround | state: closed
✅ Workaround to Resolve the Issue I implemented the following solution to restrict the /dag_stats endpoint to only return data for DAGs that a user has access to: ✅ Attach the Filter to Endpoint Dependencies I added the permission filter as a dependency to the FastAPI route using readable_dags_filter: ReadableDagsFilterDep. This ensures that only permitted DAGs are considered during the request lifecycle. ✅ Apply DAG Filter to All Queries I updated the queries inside the endpoint to filter by p…,,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/suhail-zemoso,https://github.com/apache/airflow/issues/53938#issuecomment-3150219985,repo: apache/airflow | issue: UI - Home screen does not filter based on logged in user's role's access to dags | keyword: workaround
"Close German translation gaps for full UI translation 2025-07-29 Yet another small round of translation. Review requested from @TJaniF and @m1racoli before merging. <img width=""932"" height=""561"" alt=""image"" src=""https://github.com/user-attachments/assets/3af4fab6-c702-459c-8a36-63637b6f4d9e"" />",,,,,,Anecdotal,issue,,,,,,,,2025-07-29,github/jscheffl,https://github.com/apache/airflow/pull/53897,repo: apache/airflow | keyword: workaround | state: closed
"> LGTM! > > Also `runOnLatestVersion` 👀 I did not know this was coming, this is great! no more backfill workaround :) So your translation contributions are a positive outcome that you get an early sneak preview about what is coming next release :-D",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/jscheffl,https://github.com/apache/airflow/pull/53897#issuecomment-3133802047,repo: apache/airflow | issue: Close German translation gaps for full UI translation 2025-07-29 | keyword: workaround
"Google Cloud + CNCF Kubernetes OnFinishAction equality test ### Apache Airflow Provider(s) cncf-kubernetes, google ### Versions of Apache Airflow Providers I guess you can take the latest ones, should be reproducible there, but definitely those included in official Docker image for 2.10.5. ### Apache Airflow version 2.10.5 ### Operating System Linux ### Deployment Other Docker-based deployment ### Deployment details Using official Docker image in k8s (issue is related to usage of GKEStartPodOpe…",,,,,,Anecdotal,issue,,,,,,,,2025-03-11,github/miloszszymczak,https://github.com/apache/airflow/issues/47601,repo: apache/airflow | keyword: workaround | state: closed
"Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.",,,,,,Anecdotal,comment,,,,,,,,2025-03-11,github/boring-cyborg[bot],https://github.com/apache/airflow/issues/47601#issuecomment-2713214089,repo: apache/airflow | issue: Google Cloud + CNCF Kubernetes OnFinishAction equality test | keyword: workaround
"### Apache Airflow Version - `2.11.0` ### Apache Airflow Providers Versions - `apache-airflow-providers-cncf-kubernetes==10.5.0` - `apache-airflow-providers-google==15.1.0` --- ### Issue Description I encountered the same issue when using the `GKEStartPodOperator` with `on_finish_action=""delete_pod""`. Despite this configuration, the pod does **not** get deleted at the end of a task run. The task logs contain messages like: ``` ""Skipping deleting pod ..."" ``` --- ### Diagnostics To isolate the c…",,,,,,Anecdotal,comment,,,,,,,,2025-06-07,github/neko1437,https://github.com/apache/airflow/issues/47601#issuecomment-2952231722,repo: apache/airflow | issue: Google Cloud + CNCF Kubernetes OnFinishAction equality test | keyword: workaround
"👋 We also faced this error and found a workaround. Our setup: * `Airflow Version 2.10.5` * `apache-airflow-providers-cncf-kubernetes==10.1.0` * `apache-airflow-providers-google==12.0.0` The bug was introduced in the google provider 12.0.0, specifically in [this commit](https://github.com/apache/airflow/pull/44568/files#diff-f5760a834348cda4a0d190366066c1e16dc0ab9eb8a8e08a92948bbdb3465bfbR620). `on_finish_action` was added to the list of `template_fields` of the `GKEStartPodOperator`, as shown i…",,,,,,Anecdotal,comment,,,,,,,,2025-06-19,github/agomez-etsy,https://github.com/apache/airflow/issues/47601#issuecomment-2988850159,repo: apache/airflow | issue: Google Cloud + CNCF Kubernetes OnFinishAction equality test | keyword: workaround
"> ### 1. Remove `on_finish_action` from `template_fields` > Simple, yet effective :). Seems it's already been done in https://github.com/apache/airflow/pull/49637 and released in `apache-airflow-providers-google==16.0.0`.",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/olegkachur-e,https://github.com/apache/airflow/issues/47601#issuecomment-3126735366,repo: apache/airflow | issue: Google Cloud + CNCF Kubernetes OnFinishAction equality test | keyword: workaround
"Resolve OOM When Reading Large Logs in Webserver related issue: #45079 related PR: #45129 related discussion on slack: https://apache-airflow.slack.com/archives/CCZRF2U5A/p1736767159693839 ## Why In short, this PR aims to eliminate OOM issues by: - Replacing full log sorting with a **K-Way Merge** - Making the entire log reading path **streamable** (using `yield` generators instead of returning a list of strings) More detailed reasoning is already described in the linked issue. Due to too many …",,,,,,Anecdotal,issue,,,,,,,,2025-04-20,github/jason810496,https://github.com/apache/airflow/pull/49470,repo: apache/airflow | keyword: workaround | state: closed
"There is still an issue with the `compat 3.0.0` tests for `ElasticsearchTaskHandler` and `OpensearchTaskHandler`, as they directly implement the `FileTaskHandler._read` method. This refactor introduces a new **streaming** return type for `_read`, which causes a breaking change for both `ElasticsearchTaskHandler` and `OpensearchTaskHandler` in `3.0.0`. If we backport this PR to `v3-0-test`, it **might** fix the `compat 3.0.0` tests — but we need to merge this first before cherry-picking. I'm not…",,,,,,Anecdotal,comment,,,,,,,,2025-04-27,github/jason810496,https://github.com/apache/airflow/pull/49470#issuecomment-2833180610,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
"> If we backport this PR to `v3-0-test`, it **might** fix the `compat 3.0.0` tests — but we need to merge this first before cherry-picking. > I'm not entirely sure how to best handle this chicken-and-egg situation. Any suggestions? I am afraid we will have to make it backwards-compatible in some way. I don't think we are going to yank 3.0.0, so theorethically (and many people do that) someone could install airflow 3.0.0 and then use newer provider version. We could - of course- potentially add …",,,,,,Anecdotal,comment,,,,,,,,2025-04-27,github/potiuk,https://github.com/apache/airflow/pull/49470#issuecomment-2833426668,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
"> I am afraid we will have to make it backwards-compatible in some way. I don't think we are going to yank 3.0.0, so theorethically (and many people do that) someone could install airflow 3.0.0 and then use newer provider version. We could - of course- potentially add exclusion `apache-airflow != 3.0.0` in those providers, but that would not be very nice message and we would have to - indeed - split it, backport only the the core change to 3.0.0, release 3.0.1, and only after that merge the cha…",,,,,,Anecdotal,comment,,,,,,,,2025-04-27,github/jason810496,https://github.com/apache/airflow/pull/49470#issuecomment-2833442597,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
"The PR to resolve OOM when reading large logs in the webserver, targeting the 3.0+ branch, is ready for review. I’d really appreciate your feedback.",,,,,,Anecdotal,comment,,,,,,,,2025-05-05,github/jason810496,https://github.com/apache/airflow/pull/49470#issuecomment-2850928365,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
"Hi @uranusjr I’ve resolved the compatibility issue related to the `log_pos` key in the log metadata by introducing a `LogStreamCounter`. This allows us to accurately determine the total line count while still supporting streaming log responses. Additionally, I’ve added tests for the `get_log` endpoint to cover different `Accept` headers. Thanks for the review!",,,,,,Anecdotal,comment,,,,,,,,2025-05-26,github/jason810496,https://github.com/apache/airflow/pull/49470#issuecomment-2908301181,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
"A detailed benchmark covering various scenarios after this refactor is also included. cc @potiuk, @ashb ## Setup The benchmark is conducted using Breeze with only the `api-server` process running. - **Log Sizes**: - **Small Logs (≤10MB)**: - 800 lines, 0.1MB - 4,000 lines, 0.5MB - 8,000 lines, 1MB - 40,000 lines, 5MB - 80,000 lines, 10MB - **Large Logs (≥50MB)**: - 400,000 lines, 50MB - 800,000 lines, 100MB - 4,000,000 lines, 500MB - Each log line is a simple `print(uuid4())`, producing output …",,,,,,Anecdotal,comment,,,,,,,,2025-05-26,github/jason810496,https://github.com/apache/airflow/pull/49470#issuecomment-2908306229,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
"> Nice, thanks for the benchmark, the memory improvement seems promising. > > Do you know why it looks like the total response time is much longer after refactoring? Is that inherent to the stream solution? Also I'm not sure we care that much because the user will be able to see logs after the first chunck retrieval, wich might still be faster. Yes, that's correct — the longer total response time is due to the use of [StreamingResponse](https://fastapi.tiangolo.com/advanced/custom-response/#str…",,,,,,Anecdotal,comment,,,,,,,,2025-06-02,github/jason810496,https://github.com/apache/airflow/pull/49470#issuecomment-2930184615,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
"Hi @eladkal, @ashb, Would you mind taking another look at this PR when you have time? I’d really appreciate your time, thanks in advance!",,,,,,Anecdotal,comment,,,,,,,,2025-06-23,github/jason810496,https://github.com/apache/airflow/pull/49470#issuecomment-2996396585,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
A user has a problem that seems related (#52772) - and maybe we can work together with the user and help them to patch their airflow with this fix to confirm the isssue is resolved (but it would need to be rebased with conflict resolution - and maybe indeed that can get some reviews them and get prioritised for next patchlevel of Airflow ?,,,,,,Anecdotal,comment,,,,,,,,2025-07-03,github/potiuk,https://github.com/apache/airflow/pull/49470#issuecomment-3033072477,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
"> A user has a problem that seems related (#52772) Yes, this PR will resolve the issue and the corresponding fix on frontend side is already merged ( #50333 ) There are at least 2 user DM me for this issue and there 3 related issues on GitHub ( now there are 4! ). Here is what the user DM me last month: <img width=""849"" alt=""Screenshot 2025-07-04 at 12 29 07 PM"" src=""https://github.com/user-attachments/assets/6970b9e4-3374-45a7-b643-325a488f40cc"" /> > and maybe we can work together with the use…",,,,,,Anecdotal,comment,,,,,,,,2025-07-04,github/jason810496,https://github.com/apache/airflow/pull/49470#issuecomment-3034512454,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
@ashb WDYT? I would be for merging this one (if you are OK with the code) and then re-applying it to the structlog changes - it seems important enough to be cherry-picked to 3.0.4 - we stil have months till 3.1 will be out. Happy to help with cherry-picking.,,,,,,Anecdotal,comment,,,,,,,,2025-07-04,github/potiuk,https://github.com/apache/airflow/pull/49470#issuecomment-3034872800,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
I would love to merge it - but there is a bit of a silence for @ashb side on it. @ashb -> what's your take ? I don't thing keeping @jason810496 for a long time with this one is a good idea - unless you have some strong reasons why this one should not be merged (as apparently you had other ideas that are possibly manifested in https://github.com/apache/airflow/pull/52651 ). Any commnents? I would be inclined to merge it if we do not hear back unless @Lee-W and @uranusjr have something to say her…,,,,,,Anecdotal,comment,,,,,,,,2025-07-08,github/potiuk,https://github.com/apache/airflow/pull/49470#issuecomment-3048826915,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
"> Yeah don't block on my PR, that is a slow background task for me for 3.1 Yeah. Thought so : @Lee-W @uranusjr -> I will do one more pass but if you have any more comments - feel free (we can schedule it for 3.0.4 if it will be easily cherry-pickable,",,,,,,Anecdotal,comment,,,,,,,,2025-07-08,github/potiuk,https://github.com/apache/airflow/pull/49470#issuecomment-3049003709,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
"> > Yeah don't block on my PR, that is a slow background task for me for 3.1 > > Yeah. Thought so : @Lee-W @uranusjr -> I will do one more pass but if you have any more comments - feel free (we can schedule it for 3.0.4 if it will be easily cherry-pickable, I don't have major concerns if there is a second pair of eyes. My comments mostly focused on minor improvements and typos. Not sure 3.0.4 is a good idea (looks more like a feature) but I'm looking forward to merging this one as well.",,,,,,Anecdotal,comment,,,,,,,,2025-07-08,github/Lee-W,https://github.com/apache/airflow/pull/49470#issuecomment-3049014215,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
"I only have one comment - we already check if new providers will work with last released airflow versions - our compatibility checks are testing it. This is very cool. But since that change also touches core, I am not 100% sure if the old providers will work with the ""upcoming"" version of airlfow - we do not have such ""forward"" looking changes, but I can imagine that someone will get Airflow 3.1 and will want to downgrade (say) google provider to the version released now - it should also work. …",,,,,,Anecdotal,comment,,,,,,,,2025-07-08,github/potiuk,https://github.com/apache/airflow/pull/49470#issuecomment-3049441340,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
"> Is it possible to make such test(s) - just to be sure @jason810496 ? Sure, I will test this with real remote logging setup and update here.",,,,,,Anecdotal,comment,,,,,,,,2025-07-08,github/jason810496,https://github.com/apache/airflow/pull/49470#issuecomment-3049461367,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
"> Sure, I will test this with real remote logging setup and update here. If you have any problems- let me know .. It could be that this recipe needs a bit of fixes and adjustments after recent changes ;)",,,,,,Anecdotal,comment,,,,,,,,2025-07-08,github/potiuk,https://github.com/apache/airflow/pull/49470#issuecomment-3049702462,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
"I just test with the following setup with `breeze k8s` and it works smooth! And I re-build the k8s image before running the manual test for sure. I will keep on testing setup with remote logging as well tomorrow. **Without remote logging** 1. **KubernetesExecutor** ( with no log persistent setup ): Works well, can only show logs when TI is in running state ( as expected ) 2. **CeleryExecutor**: Works well, can show logs no matter TI is in running or success, which means get get `serve_logs` wel…",,,,,,Anecdotal,comment,,,,,,,,2025-07-08,github/jason810496,https://github.com/apache/airflow/pull/49470#issuecomment-3049840906,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
"Hi @potiuk, I test with scenarios including - `Kuberntes` or `Celery` Executor - Without remote logging or with Google Cloud Storage as remote logging so there will be 4 permutations, and all of them work well !",,,,,,Anecdotal,comment,,,,,,,,2025-07-09,github/jason810496,https://github.com/apache/airflow/pull/49470#issuecomment-3051093451,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
"Hi @potiuk, Just wanted to check, do you think this is good to merge? Or is there any additional testing I should perform? Thanks!",,,,,,Anecdotal,comment,,,,,,,,2025-07-10,github/jason810496,https://github.com/apache/airflow/pull/49470#issuecomment-3056934961,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
"> Hi @potiuk, I test with scenarios including > Kuberntes or Celery Executor > Without remote logging or with Google Cloud Storage as remote logging > so there will be 4 permutations, and all of them work well ! Fantastic! Thanks! All good!",,,,,,Anecdotal,comment,,,,,,,,2025-07-10,github/potiuk,https://github.com/apache/airflow/pull/49470#issuecomment-3057111508,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
"### Backport failed to create: v3-0-test. View the failure log <a href='https://github.com/apache/airflow/actions/runs/16194256714'> Run details </a> <table> <tr> <th>Status</th> <th>Branch</th> <th>Result</th> </tr> <tr> <td>❌</td> <td>v3-0-test</td> <td><a href=""https://github.com/apache/airflow/commit/ee54fe93768e70da5bd2f0bb7398707181bee26a""><img src='https://img.shields.io/badge/Commit-ee54fe9-red' alt='Commit Link'></a></td> </tr> </table> You can attempt to backport this manually by runn…",,,,,,Anecdotal,comment,,,,,,,,2025-07-10,github/github-actions[bot],https://github.com/apache/airflow/pull/49470#issuecomment-3057116402,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
Add ALL_DONE_MIN_ONE_SUCCESS trigger rule This trigger rule is useful for cases where we must verify all upstream is done with execution but we only need one of them to be successful. Verified also with example dag: ``` from datetime import datetime from airflow import DAG from airflow.operators.bash import BashOperator from airflow.operators.python import PythonOperator from airflow.providers.standard.operators.empty import EmptyOperator from airflow.utils.trigger_rule import TriggerRule from …,,,,,,Anecdotal,issue,,,,,,,,2025-07-31,github/eladkal,https://github.com/apache/airflow/pull/53959,repo: apache/airflow | keyword: workaround | state: closed
"> I'm not completely against it, but I think we could use something like a branch operator with `all_done`. -0 to this, but would love to hear how everyone think 🙂 There is no branching here. I don't know of any simple way to get the desired behavior. you need to workaround with additional tasks which in big environments creates many unnecessary tasks (more load on the scheduler etc..)",,,,,,Anecdotal,comment,,,,,,,,2025-07-31,github/eladkal,https://github.com/apache/airflow/pull/53959#issuecomment-3139469554,repo: apache/airflow | issue: Add ALL_DONE_MIN_ONE_SUCCESS trigger rule | keyword: workaround
"> Overall LGTM - I could imagine use cases of this trigger rule. For the long term, maybe we should think about changing the trigger rule mechanism so the definition could be more flexible instead of the hardcoded enum (e.g., this TR will be represented as something like `trigger_rule={'done': 'all', 'success': '>=1'}`). We can look into it after Airflow 3.1 if you have a proposal feel free to suggest it in the mailing list. I think this can be a good idea.",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/eladkal,https://github.com/apache/airflow/pull/53959#issuecomment-3142199235,repo: apache/airflow | issue: Add ALL_DONE_MIN_ONE_SUCCESS trigger rule | keyword: workaround
"Yep, what's in my mind is customizable trigger rules as well. @shahar1 let's see whether we can work on something together after 3.1 🙂",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/Lee-W,https://github.com/apache/airflow/pull/53959#issuecomment-3143848305,repo: apache/airflow | issue: Add ALL_DONE_MIN_ONE_SUCCESS trigger rule | keyword: workaround
"> Yep, what's in my mind is customizable trigger rules as well. @shahar1 let's see whether we can work on something together after 3.1 🙂 We should be careful with that. Trigger rules are on the hottest path of scheduler, and we should not have too many of them, also customizability of it is limited as it runs in scheduler. We should generally try to minimise and rather hard-code all the logic here I think,",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/potiuk,https://github.com/apache/airflow/pull/53959#issuecomment-3145182126,repo: apache/airflow | issue: Add ALL_DONE_MIN_ONE_SUCCESS trigger rule | keyword: workaround
"Add `account_host` to extra fields in ADLS `get_fs()` method ### Description This PR addresses [#53333](https://github.com/apache/airflow/issues/53333) by adding support for the `account_host` field in the ADLS `get_fs()` method. The current implementation misses this field, even though it's commonly needed for custom Azure endpoint configurations. ### What is the purpose of this change? To ensure users can configure `account_host` via Airflow's Azure connection extras, aligning the behavior wi…",,,,,,Anecdotal,issue,,,,,,,,2025-07-19,github/samuelkhtu,https://github.com/apache/airflow/pull/53521,repo: apache/airflow | keyword: workaround | state: closed
"use 1 worker for log server <!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applic…",,,,,,Anecdotal,issue,,,,,,,,2025-08-14,github/mattp-,https://github.com/apache/airflow/pull/54510,repo: apache/airflow | keyword: gotcha | state: open
"Congratulations on your first Pull Request and welcome to the Apache Airflow community! If you have any issues or are unsure about any anything please check our Contributors' Guide (https://github.com/apache/airflow/blob/main/contributing-docs/README.rst) Here are some useful points: - Pay attention to the quality of your code (ruff, mypy and type annotations). Our [pre-commits]( https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#prerequisites-for-pre-commit…",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/boring-cyborg[bot],https://github.com/apache/airflow/pull/54510#issuecomment-3188649468,repo: apache/airflow | issue: use 1 worker for log server | keyword: gotcha
> @jason810496 curious - any reason you chose 2 workers specifically for this when you refactored? Airflow use 2 workers before I refactor from Flask to FastAPI with uvicorn. https://github.com/apache/airflow/pull/52581/files#diff-4829755d9e5ddcb1bce89f1cc272181507d40cb38891ce6abd4537e97004d38cL191,,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/jason810496,https://github.com/apache/airflow/pull/54510#issuecomment-3188726474,repo: apache/airflow | issue: use 1 worker for log server | keyword: gotcha
"By the way, how can I reproduce this error? I have run the components ( scheduler, trigger ) with server_logs enable successfully in container.",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/jason810496,https://github.com/apache/airflow/pull/54510#issuecomment-3188741091,repo: apache/airflow | issue: use 1 worker for log server | keyword: gotcha
"Assets do not show up as consumed ### Apache Airflow version 3.0.1 ### If ""Other Airflow 2 version"" selected, which one? _No response_ ### What happened? I am playing around with Assets. This is the generating DAG / Asset (Confusing!) ``` from airflow.sdk import Metadata, asset @asset(uri=""file:///tmp/example.csv"", schedule=None) def write_example(self): with open(""/tmp/example.csv"", ""w"") as f: f.write(""Hallo"") yield Metadata(self, {""row_count"": 0}) ``` I have two consuming DAGs, one ""asset"" sy…",,,,,,Anecdotal,issue,,,,,,,,2025-05-21,github/bolkedebruin,https://github.com/apache/airflow/issues/50873,repo: apache/airflow | keyword: gotcha | state: open
"Currently assets are only shown as consumed if they are used in schedule, not inlets. I feel we might want to do some more thinking instead of just adding those. It’s probably best to have _three_ fields: producing (things that list the asset as outlet), consuming (list as inlet), and scheduled (list in schedule). Not sure about the names here. cc @bbovenzi for ideas.",,,,,,Anecdotal,comment,,,,,,,,2025-05-21,github/uranusjr,https://github.com/apache/airflow/issues/50873#issuecomment-2899480502,repo: apache/airflow | issue: Assets do not show up as consumed | keyword: gotcha
"Ah gotcha, I misread your comment. It is now confusing indeed. To me ""consumed"" means ""used worked on"". We could consider the following alternatives: Data Dependency / Triggers (or Schedules to avoid naming conflicts) or Consumed by / Triggers (or Schedules)",,,,,,Anecdotal,comment,,,,,,,,2025-05-22,github/bolkedebruin,https://github.com/apache/airflow/issues/50873#issuecomment-2900839730,repo: apache/airflow | issue: Assets do not show up as consumed | keyword: gotcha
"Adding: ``` from airflow.sdk import Asset, asset from airflow.sdk import DAG, dag, task write_example = Asset(""write_example"") #@asset(schedule=None) @dag def my_dag(): @task def consume_csv(write_example): print(""Bla"") my_dag() ``` This sets it as a schedule and a it is an inlet. However 1. When triggering the Asset (write_example) this dag does not get kicked off 2. It also does not become visible as ""consumed"" in the Asset I might be doing something unexpected `write_example = Asset(""write_e…",,,,,,Anecdotal,comment,,,,,,,,2025-05-22,github/bolkedebruin,https://github.com/apache/airflow/issues/50873#issuecomment-2901001385,repo: apache/airflow | issue: Assets do not show up as consumed | keyword: gotcha
"I made a mistake in the dag: ``` from airflow.sdk import Asset, asset from airflow.sdk import DAG, dag, task write_example = Asset(""write_example"") #@asset(schedule=None) @dag(schedule=[write_example]) def my_dag(): @task def consume_csv(w=write_example) -> int: print(""Bla"") return 10 @task def consume_xcom(x: int): print(f""received {x}"") y = consume_csv(write_example) consume_xcom(y) my_dag() ``` However it sill doesn't show up. There is however a warning `Cannot activate asset Asset(name=""wri…",,,,,,Anecdotal,comment,,,,,,,,2025-05-22,github/bolkedebruin,https://github.com/apache/airflow/issues/50873#issuecomment-2901051545,repo: apache/airflow | issue: Assets do not show up as consumed | keyword: gotcha
UI-wise. the confusion is just our usage of Consuming and Producing? I am happy to refactor that. It was just the bare minimum info to add,,,,,,Anecdotal,comment,,,,,,,,2025-05-22,github/bbovenzi,https://github.com/apache/airflow/issues/50873#issuecomment-2902534287,repo: apache/airflow | issue: Assets do not show up as consumed | keyword: gotcha
"> However it sill doesn't show up. There is however a warning Are you defining `write_example` elsewhere? You can only use an asset name once. It looks like that other definition is conflicting with this one, and it prevents this asset from being created at all. This is why it does not show up. > the confusion is just our usage of Consuming and Producing? This, and also there are two kind of consuming—using the asset to schedule things, and reading from the asset. We need a way to distinguish t…",,,,,,Anecdotal,comment,,,,,,,,2025-05-22,github/uranusjr,https://github.com/apache/airflow/issues/50873#issuecomment-2902541686,repo: apache/airflow | issue: Assets do not show up as consumed | keyword: gotcha
Ok and we don't yet have a concept if a Task is actually reading an Asset. So we should update copy to reflect that its only scheduled on,,,,,,Anecdotal,comment,,,,,,,,2025-05-22,github/bbovenzi,https://github.com/apache/airflow/issues/50873#issuecomment-2902684603,repo: apache/airflow | issue: Assets do not show up as consumed | keyword: gotcha
"@uranusjr I have included the whole example, so yes it is defined elsewhere in a different DAG-file (see first code snippet). I'd like to refer to the asset in another DAG without including the original DAG (that's the whole point). So the question is how do I do that? @bbovenzi I think the *main* concept is to actually read from an asset rather than just to be dependent on its schedule. So, in other words it is data dependency (data -> data) rather than data-task dependency. This also makes fo…",,,,,,Anecdotal,comment,,,,,,,,2025-05-23,github/bolkedebruin,https://github.com/apache/airflow/issues/50873#issuecomment-2903680695,repo: apache/airflow | issue: Assets do not show up as consumed | keyword: gotcha
I’m going to * Rename `consuming_dags` to `scheduled_dags` (since they actually just use the asset for scheduling) * Add a `consuming_tasks` that include _tasks_ that use the asset as an inlet * These will be reflected in the API response,,,,,,Anecdotal,comment,,,,,,,,2025-06-02,github/uranusjr,https://github.com/apache/airflow/issues/50873#issuecomment-2928945387,repo: apache/airflow | issue: Assets do not show up as consumed | keyword: gotcha
So for the dag-asset dependency graphs. We will show edges for the following: - from producing_tasks to the asset - asset to the consuming_task - asset to scheduled_dags Should all of these edges be styled the same or the task<>asset ones should be shown differently to represent a Data Dependency rather than the normal task scheduling dependency. We can also add a filter to /dag_dependencies to only show asset<>task relationships.,,,,,,Anecdotal,comment,,,,,,,,2025-06-05,github/bbovenzi,https://github.com/apache/airflow/issues/50873#issuecomment-2945491655,repo: apache/airflow | issue: Assets do not show up as consumed | keyword: gotcha
"ExternalTaskSensor can never find External Parent Task <!-- Welcome to Apache Airflow! For a smooth issue process, try to answer the following questions. Don't worry if they're not all applicable; just try to include what you can :-) If you need to include code snippets or logs, please put them in fenced code blocks. If they're super-long, please use the details tag like <details><summary>super-long log</summary> lots of stuff </details> Please delete these comment blocks before submitting the …",,,,,,Anecdotal,issue,,,,,,,,2021-01-14,github/hedrickw,https://github.com/apache/airflow/issues/13681,repo: apache/airflow | keyword: gotcha | state: open
"Why did you manually trigger it? Secondly if you want to manually trigger it for trying it out, run both of them using cli with a specific execution date",,,,,,Anecdotal,comment,,,,,,,,2021-01-14,github/kaxil,https://github.com/apache/airflow/issues/13681#issuecomment-760516463,repo: apache/airflow | issue: ExternalTaskSensor can never find External Parent Task | keyword: gotcha
"The schedule_interval need to be the same for both DAGs for it to work and shouldn't be None. ```python # # Licensed to the Apache Software Foundation (ASF) under one # or more contributor license agreements. See the NOTICE file # distributed with this work for additional information # regarding copyright ownership. The ASF licenses this file # to you under the Apache License, Version 2.0 (the # ""License""); you may not use this file except in compliance # with the License. You may obtain a copy…",,,,,,Anecdotal,comment,,,,,,,,2021-01-14,github/ephraimbuddy,https://github.com/apache/airflow/issues/13681#issuecomment-760531896,repo: apache/airflow | issue: ExternalTaskSensor can never find External Parent Task | keyword: gotcha
@ephraimbuddy Ah maybe its just an outdated file i grabbed example from master and it has schedule of None https://github.com/apache/airflow/blob/master/airflow/example_dags/example_external_task_marker_dag.py,,,,,,Anecdotal,comment,,,,,,,,2021-01-14,github/hedrickw,https://github.com/apache/airflow/issues/13681#issuecomment-760537641,repo: apache/airflow | issue: ExternalTaskSensor can never find External Parent Task | keyword: gotcha
> @ephraimbuddy Ah maybe its just an outdated file i grabbed example from master and it has schedule of None > https://github.com/apache/airflow/blob/master/airflow/example_dags/example_external_task_marker_dag.py Can you make a PR to correct it? That would be nice,,,,,,Anecdotal,comment,,,,,,,,2021-01-14,github/ephraimbuddy,https://github.com/apache/airflow/issues/13681#issuecomment-760538590,repo: apache/airflow | issue: ExternalTaskSensor can never find External Parent Task | keyword: gotcha
"Greetings, IMHO, it's a gotcha for newcomers after testing `TriggerDagRunOperator` successfully. When trying `ExternalTaskSensor`, newcomers need to fight with the following mysterious line. ``` [2024-05-22, 06:18:06 UTC] {external_task.py:274} INFO - Poking for DAG 'extract_tasks' on 2024-05-22T06:16:58.493357+00:00 ... ``` Actually, it means to find the task instance with specific _execution_ timestamp. Maybe just need to put a note in documentation, to suggest to use schedule `* * * * *` whe…",,,,,,Anecdotal,comment,,,,,,,,2024-05-22,github/guhuajun,https://github.com/apache/airflow/issues/13681#issuecomment-2123957732,repo: apache/airflow | issue: ExternalTaskSensor can never find External Parent Task | keyword: gotcha
Datasets: Allow time based force dependency when dataset is not ready. ### Body **The current state:** If DAG is defined with 4 datasets. Airflow will wait for all of them to be ready before scheduling the DAG. This works well and serve use cases where all 4 datasets are curial and must be ready. **The use case we don't currently handle:** It is common for datasets not to be equally important. Sometimes the core datasets are ready yet some minor ones are not (for example if one of the datasets …,,,,,,Anecdotal,issue,,,,,,,,2023-04-30,github/eladkal,https://github.com/apache/airflow/issues/30974,repo: apache/airflow | keyword: gotcha | state: open
"Have you thought about other options as alternatives? e.g.: - Add further attributes to Dataset definition to mark which Datasets have a blocking dependency `schedule=[Dataset(""http://my/important/dataset""), Dataset(""http://my/optional/dataset"", optional=True)]` - Add a timeout to the Dataset so that scheduler takes care for monitoring if something non-important is not always required via `schedule=[Dataset(""http://my/important/dataset""), Dataset(""http://my/other/dataset"", max_delay=60)]` - May…",,,,,,Anecdotal,comment,,,,,,,,2023-04-30,github/jscheffl,https://github.com/apache/airflow/issues/30974#issuecomment-1529115113,repo: apache/airflow | issue: Datasets: Allow time based force dependency when dataset is not ready. | keyword: gotcha
"Hi, just to provide a bit more info on one of the use cases that inspired this issue: * We have two datasets, say A and B, both of which are provided by third parties (and therefore fail occasionally) * A is updated daily, B is updated weekly * Dataset C is effectively a join of A and B * Whenever A is updated, C needs to be updated * We take the newly available A, and the _latest available_ version of B to compute C Our current plan to handle this is to mark only A as an inlet to C, and handle…",,,,,,Anecdotal,comment,,,,,,,,2023-05-01,github/dakshin-k,https://github.com/apache/airflow/issues/30974#issuecomment-1529325649,repo: apache/airflow | issue: Datasets: Allow time based force dependency when dataset is not ready. | keyword: gotcha
"Just a reminder on that one (I personally have no opinion enough to chime in) but - since this is a change How airflow architecture is designed around datasets and their dependencies, whatever is discussed here shoudl be brought to the devlist as proposal: https://airflow.apache.org/community/ -> people might not follow this discussion / feature request here and only a devlist discussion will make it into ""implementable"" change. I guess it's fine to disucss it here to get the idea and concepts …",,,,,,Anecdotal,comment,,,,,,,,2023-05-02,github/potiuk,https://github.com/apache/airflow/issues/30974#issuecomment-1531390960,repo: apache/airflow | issue: Datasets: Allow time based force dependency when dataset is not ready. | keyword: gotcha
"> Usually feature is something that adds a small thing that does not change the internals, in this case we are talking about changing the ways how dependencies are resolved by scheduler We are discussing possible enhancement. Should the implementation plan requires new entity (for example like happened with Timetables) or alter the feature beyond it's original function then AIP is required. I think it's too early to know if AIP is required or not.",,,,,,Anecdotal,comment,,,,,,,,2023-05-02,github/eladkal,https://github.com/apache/airflow/issues/30974#issuecomment-1531405848,repo: apache/airflow | issue: Datasets: Allow time based force dependency when dataset is not ready. | keyword: gotcha
"> Hi, just to provide a bit more info on one of the use cases that inspired this issue: > > * We have two datasets, say A and B, both of which are provided by third parties (and therefore fail occasionally) > * A is updated daily, B is updated weekly > * Dataset C is effectively a join of A and B > > * Whenever A is updated, C needs to be updated > * We take the newly available A, and the _latest available_ version of B to compute C > > Our current plan to handle this is to mark only A as an in…",,,,,,Anecdotal,comment,,,,,,,,2023-05-04,github/chuxiangfeng,https://github.com/apache/airflow/issues/30974#issuecomment-1534409209,repo: apache/airflow | issue: Datasets: Allow time based force dependency when dataset is not ready. | keyword: gotcha
"I think it might be nice if we had something similar to the `TriggerRule` for task instances, on datasets? Not just an enum though, I was thinking something similar to timetables, where we could provide a few default rules like `ALL_DEPENDENCIES_REFRESHED` and `AT_LEAST_ONE_REFRESHED`, and allow users to create custom rules if needed. Any thoughts?",,,,,,Anecdotal,comment,,,,,,,,2023-05-05,github/dakshin-k,https://github.com/apache/airflow/issues/30974#issuecomment-1536033429,repo: apache/airflow | issue: Datasets: Allow time based force dependency when dataset is not ready. | keyword: gotcha
"> Hi, just to provide a bit more info on one of the use cases that inspired this issue: > > * We have two datasets, say A and B, both of which are provided by third parties (and therefore fail occasionally) > * A is updated daily, B is updated weekly > * Dataset C is effectively a join of A and B > > * Whenever A is updated, C needs to be updated > * We take the newly available A, and the _latest available_ version of B to compute C > > Our current plan to handle this is to mark only A as an in…",,,,,,Anecdotal,comment,,,,,,,,2024-02-01,github/cmarteepants,https://github.com/apache/airflow/issues/30974#issuecomment-1921943496,repo: apache/airflow | issue: Datasets: Allow time based force dependency when dataset is not ready. | keyword: gotcha
"@cmarteepants Yes that would solve the problem at hand, however it's not ideal as re-computing `C` multiple times could cause an issue as there's no guarantee that operation is idempotent. For example, in our case the update to`C` is a bit like appending the newly computed dataset to the existing one. There's a separate pipeline which reads the last 30 `C` datasets and performs a monthly aggregation. So we'd now have to make changes to the way `C` itself works, or perform mitigations like de-du…",,,,,,Anecdotal,comment,,,,,,,,2024-02-05,github/dakshin-k,https://github.com/apache/airflow/issues/30974#issuecomment-1926517418,repo: apache/airflow | issue: Datasets: Allow time based force dependency when dataset is not ready. | keyword: gotcha
"Gotcha, makes sense. Paraphrasing: DAG `C`: DAG that contains Task that outputs to dataset `C` The crux of the problem is that DAG `C` cannot leverage dataset events from dataset `B` without setting dataset `B` as a scheduling dependency to DAG `C`. This is due to the strict coupling between data dependencies and scheduling dependencies.",,,,,,Anecdotal,comment,,,,,,,,2024-02-05,github/cmarteepants,https://github.com/apache/airflow/issues/30974#issuecomment-1927291755,repo: apache/airflow | issue: Datasets: Allow time based force dependency when dataset is not ready. | keyword: gotcha
"I think we are getting a bit too broad here. We should start somewhere... Lets try to focus on two use cases: ----------- 1. In the eye of a specific DAG not all Datasets are equally important. We should have a mechanism that says ""I'd like to wait for a dataset but no more than X min"" Something like: ``` with DAG( dag_id=""multiple_datasets_example"", schedule=[ Dataset(""s3://dataset/example1.csv""), Dataset(""s3://dataset/example2.csv""), Dataset(""s3://dataset/example3.csv"", wait_no_longer_than=ti…",,,,,,Anecdotal,comment,,,,,,,,2024-04-12,github/eladkal,https://github.com/apache/airflow/issues/30974#issuecomment-2051650875,repo: apache/airflow | issue: Datasets: Allow time based force dependency when dataset is not ready. | keyword: gotcha
@uranusjr you're closer to this than I am. What is your opinion? I think there's a lot of value in supporting these scenarios - but there's something about the approach that is bothering me. Should these rules be defined as `Dataset` args? A common pattern that I have seen amongst Astronomer customers is for data producers to define datasets in a [consolidated file](https://github.com/astronomer/snowpatrol/blob/main/include/datasets.py) in order to make them discoverable for data consumers. Dat…,,,,,,Anecdotal,comment,,,,,,,,2024-04-12,github/cmarteepants,https://github.com/apache/airflow/issues/30974#issuecomment-2052540712,repo: apache/airflow | issue: Datasets: Allow time based force dependency when dataset is not ready. | keyword: gotcha
"I’ve been thinking about this. Yeah the above `wait_no_longer_than` interface seems to be the way to go (maybe with a different name, say `DatasetTimeout`). I initially considered using a timetable for this, but ultimately this does not work since we may want to configure the “timeout” of each dataset differently, and a timetable would combine very awkwardly since it carries too much other information. (Side note: I think we will need to clean this up a lot as a part of Airflow 3, likely comple…",,,,,,Anecdotal,comment,,,,,,,,2024-04-29,github/uranusjr,https://github.com/apache/airflow/issues/30974#issuecomment-2081854430,repo: apache/airflow | issue: Datasets: Allow time based force dependency when dataset is not ready. | keyword: gotcha
> Should these rules be defined as Dataset args? It's a property of a specific DAG that uses the dataset. It's not a property of the dataset itself. > A common pattern that I have seen amongst Astronomer customers is for data producers to define datasets in a [consolidated file](https://github.com/astronomer/snowpatrol/blob/main/include/datasets.py) in order to make them discoverable for data consumers. Data consumers will then import them to use for scheduling purposes. I agree this is a commo…,,,,,,Anecdotal,comment,,,,,,,,2024-05-01,github/eladkal,https://github.com/apache/airflow/issues/30974#issuecomment-2088039805,repo: apache/airflow | issue: Datasets: Allow time based force dependency when dataset is not ready. | keyword: gotcha
"Create Snowflake Stage AssetWatcher ### Description With the addition of AssetWatchers in Airflow 3.0, there are a number of use-cases that could leverage this new functionality. One of them being Snowflake. I'd like to propose an AssetWatcher/Trigger that monitors a Snowflake Stage. When a new file lands in the Stage, a DAG scheduled using this AssetWatcher would be triggered. This would be implemented to heavily use the `SnowflakeHook`. ### Use case/motivation Snowflake is one of the most com…",,,,,,Anecdotal,issue,,,,,,,,2025-04-07,github/jroachgolf84,https://github.com/apache/airflow/issues/48885,repo: apache/airflow | keyword: gotcha | state: closed
"I think the hardest part about all this will be tracking the stage files that have been ""processed"" so that same file only triggers the dag runs it is supposed to. Naively, maybe the watcher renames the file and stores old and new name as part of asset event metadata?",,,,,,Anecdotal,comment,,,,,,,,2025-04-07,github/cmarteepants,https://github.com/apache/airflow/issues/48885#issuecomment-2783987444,repo: apache/airflow | issue: Create Snowflake Stage AssetWatcher | keyword: gotcha
"> I think the hardest part about all this will be tracking the stage files that have been ""processed"" so that same file only triggers the dag runs it is supposed to. Naively, maybe the watcher renames the file and stores old and new name as part of asset event metadata? I don't know what was eventually decided as I didn't track AIP-82 closely but should be covered by AIP-82 https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=315493498#AIP82ExternaleventdrivenschedulinginAirflow-Avo…",,,,,,Anecdotal,comment,,,,,,,,2025-04-07,github/eladkal,https://github.com/apache/airflow/issues/48885#issuecomment-2784046753,repo: apache/airflow | issue: Create Snowflake Stage AssetWatcher | keyword: gotcha
"> > I think the hardest part about all this will be tracking the stage files that have been ""processed"" so that same file only triggers the dag runs it is supposed to. Naively, maybe the watcher renames the file and stores old and new name as part of asset event metadata? > > I don't know what was eventually decided as I didn't track AIP-82 closely but should be covered by AIP-82 https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=315493498#AIP82ExternaleventdrivenschedulinginAirfl…",,,,,,Anecdotal,comment,,,,,,,,2025-04-07,github/vincbeck,https://github.com/apache/airflow/issues/48885#issuecomment-2784066783,repo: apache/airflow | issue: Create Snowflake Stage AssetWatcher | keyword: gotcha
"I chatted with @RNHTTR earlier this morning. We thought it might make sense to leverage the asset_events table to avoid ""infinite scheduling"". For example, when the trigger runs, it checks for any files in Snowflake stage that have landed since the last Asset Event. Thoughts?",,,,,,Anecdotal,comment,,,,,,,,2025-04-08,github/jroachgolf84,https://github.com/apache/airflow/issues/48885#issuecomment-2786502167,repo: apache/airflow | issue: Create Snowflake Stage AssetWatcher | keyword: gotcha
"> > > I think the hardest part about all this will be tracking the stage files that have been ""processed"" so that same file only triggers the dag runs it is supposed to. Naively, maybe the watcher renames the file and stores old and new name as part of asset event metadata? > > > > > > I don't know what was eventually decided as I didn't track AIP-82 closely but should be covered by AIP-82 https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=315493498#AIP82Externaleventdrivenschedul…",,,,,,Anecdotal,comment,,,,,,,,2025-04-08,github/vincbeck,https://github.com/apache/airflow/issues/48885#issuecomment-2786544128,repo: apache/airflow | issue: Create Snowflake Stage AssetWatcher | keyword: gotcha
"> I chatted with [@RNHTTR](https://github.com/RNHTTR) earlier this morning. We thought it might make sense to leverage the `asset_events` table to avoid ""infinite scheduling"". For example, when the trigger runs, it checks for any files in Snowflake stage that have landed since the last Asset Event. Thoughts? Your flow would be basically like: ``` New file in Snowflake stage -> trigger -> AssetWatcher -> Asset ``` Your trigger would use `asset_events` to only detect files added since the last ev…",,,,,,Anecdotal,comment,,,,,,,,2025-04-08,github/vincbeck,https://github.com/apache/airflow/issues/48885#issuecomment-2786601443,repo: apache/airflow | issue: Create Snowflake Stage AssetWatcher | keyword: gotcha
"Agree re queue working perfectly with event-driven paradigm, but I can appreciate the idea of building up a catalogue of Watchers in Airflow that can be used without extra infrastructure. >Your trigger would use asset_events to only detect files added since the last event. >With this solution you can run into race condition issues, if the asset event is not inserted fast enough in the DB, you can trigger the DAG multiple times before you read the last asset event from the DB. Ah yeah, that's a …",,,,,,Anecdotal,comment,,,,,,,,2025-04-08,github/cmarteepants,https://github.com/apache/airflow/issues/48885#issuecomment-2786635018,repo: apache/airflow | issue: Create Snowflake Stage AssetWatcher | keyword: gotcha
"> Agree re queue working perfectly with event-driven paradigm, but I can appreciate the idea of building up a catalogue of Watchers in Airflow that can be used without extra infrastructure. > > > Your trigger would use asset_events to only detect files added since the last event. > > > With this solution you can run into race condition issues, if the asset event is not inserted fast enough in the DB, you can trigger the DAG multiple times before you read the last asset event from the DB. > > Ah…",,,,,,Anecdotal,comment,,,,,,,,2025-04-08,github/vincbeck,https://github.com/apache/airflow/issues/48885#issuecomment-2786644614,repo: apache/airflow | issue: Create Snowflake Stage AssetWatcher | keyword: gotcha
"I believe that this object/file storage pattern will be one of the more common use-cases for `AssetWatcher`'s, so it's something I'm inclined to dig a bit deeper into. IMO, there are a couple of other ""gotchas"" to keep in mind: * In the Snowflake Stage example here (and others), do I want to process each file individually, or files in a batch since the object store was last ""polled""? * How can we avoid the race condition mentioned above, which is going to be prevalent with just about any object…",,,,,,Anecdotal,comment,,,,,,,,2025-04-08,github/jroachgolf84,https://github.com/apache/airflow/issues/48885#issuecomment-2786807188,repo: apache/airflow | issue: Create Snowflake Stage AssetWatcher | keyword: gotcha
"Closing this for now, there is a larger discussion to be had around patterns for this development. https://github.com/apache/airflow/issues/50369",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/jroachgolf84,https://github.com/apache/airflow/issues/48885#issuecomment-3197145073,repo: apache/airflow | issue: Create Snowflake Stage AssetWatcher | keyword: gotcha
"Add support for S3 dag bundle <!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by appl…",,,,,,Anecdotal,issue,,,,,,,,2025-02-10,github/ismailsimsek,https://github.com/apache/airflow/pull/46621,repo: apache/airflow | keyword: gotcha | state: closed
"Congratulations on your first Pull Request and welcome to the Apache Airflow community! If you have any issues or are unsure about any anything please check our Contributors' Guide (https://github.com/apache/airflow/blob/main/contributing-docs/README.rst) Here are some useful points: - Pay attention to the quality of your code (ruff, mypy and type annotations). Our [pre-commits]( https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#prerequisites-for-pre-commit…",,,,,,Anecdotal,comment,,,,,,,,2025-02-10,github/boring-cyborg[bot],https://github.com/apache/airflow/pull/46621#issuecomment-2647895259,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
"> moved it to provider package. its ready for review. any chance to trigger the test workflows? Triggered workflows again, will review soon :+1:",,,,,,Anecdotal,comment,,,,,,,,2025-04-04,github/o-nikolas,https://github.com/apache/airflow/pull/46621#issuecomment-2779501590,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
code related tests are passing now. remaining test is related to airflow version. the BaseDagBundle code available only in airflow 3 but the test is running with airflow 2.9.x . any idea how to fix it?,,,,,,Anecdotal,comment,,,,,,,,2025-04-13,github/ismailsimsek,https://github.com/apache/airflow/pull/46621#issuecomment-2799864543,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
"Looks reasonable to me, but I'm no Bundles specialist. I talked to Jed before he left on leave and he mentioned to loop in @dstandish on Bundles stuff. Does this look reasonable to you as well Daniel?",,,,,,Anecdotal,comment,,,,,,,,2025-05-06,github/o-nikolas,https://github.com/apache/airflow/pull/46621#issuecomment-2856024573,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
"@dstandish @eladkal @ephraimbuddy is it possible to get this in, if overall it looks good. and followup with improvements.",,,,,,Anecdotal,comment,,,,,,,,2025-05-12,github/ismailsimsek,https://github.com/apache/airflow/pull/46621#issuecomment-2871790391,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
is it possible to run following command with podman? its asking for docker ``` breeze static-checks --all-files --show-diff-on-failure --color always --initialize-environment ```,,,,,,Anecdotal,comment,,,,,,,,2025-05-21,github/ismailsimsek,https://github.com/apache/airflow/pull/46621#issuecomment-2899078102,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
> is it possible to run following command with podman? its asking for docker > > ``` > breeze static-checks --all-files --show-diff-on-failure --color always --initialize-environment > ``` Did you try https://podman-desktop.io/docs/migrating-from-docker/managing-docker-compatibility,,,,,,Anecdotal,comment,,,,,,,,2025-05-21,github/potiuk,https://github.com/apache/airflow/pull/46621#issuecomment-2899117361,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
I would love to see this merged as well. @ismailsimsek the use of caplog needs to be addressed (i.e. removed). Do you need any help with this?,,,,,,Anecdotal,comment,,,,,,,,2025-06-26,github/o-nikolas,https://github.com/apache/airflow/pull/46621#issuecomment-3010579709,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
"> I would love to see this merged as well. @ismailsimsek the use of caplog needs to be addressed (i.e. removed). Do you need any help with this? That would be a great help, thank you! I don't have much time to work on this recently, so I'd really appreciate it.",,,,,,Anecdotal,comment,,,,,,,,2025-06-28,github/ismailsimsek,https://github.com/apache/airflow/pull/46621#issuecomment-3015218710,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
@ismailsimsek I made some changes to use mocking of the log attribute instead of caplog usage (which was breaking the tests). It's difficult to apply changes to a branch I don't own. I added three commits with the fixes (and some others stuff that's happened in main since). I don't think it applied cleanly. But you can look at the commits and update your branch accordingly if not!,,,,,,Anecdotal,comment,,,,,,,,2025-07-04,github/o-nikolas,https://github.com/apache/airflow/pull/46621#issuecomment-3034009768,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
"I have updated my local branch and ran the s3 tests successfully, everything looks good to me. Thank you very much @o-nikolas appreciate it.",,,,,,Anecdotal,comment,,,,,,,,2025-07-04,github/ismailsimsek,https://github.com/apache/airflow/pull/46621#issuecomment-3035328142,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
"Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.",,,,,,Anecdotal,comment,,,,,,,,2025-07-04,github/boring-cyborg[bot],https://github.com/apache/airflow/pull/46621#issuecomment-3037080875,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
"Hi all, Would it be a good idea to introduce a `RemoteDagBundle` interface (or perhaps extend `BaseDagBundle` or add a common utility in the `bundles.base` module) to support **versioning** for remote Dag bundles like `S3DagBundle`? We could support bundle versioning using one of the following approaches (potentially controlled by an `enable_versioning` parameter in `RemoteDagBundle`): 1. **Content-based hash**: Calculate a hash of the bundle by hashing each Dags file and incorporating the dire…",,,,,,Anecdotal,comment,,,,,,,,2025-07-15,github/jason810496,https://github.com/apache/airflow/pull/46621#issuecomment-3072211401,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
"> Hi all, > > Would it be a good idea to introduce a `RemoteDagBundle` interface (or perhaps extend `BaseDagBundle` or add a common utility in the `bundles.base` module) to support **versioning** for remote Dag bundles like `S3DagBundle`? > > We could support bundle versioning using one of the following approaches (potentially controlled by an `enable_versioning` parameter in `RemoteDagBundle`): > > 1. **Content-based hash**: Calculate a hash of the bundle by hashing each Dags file and incorpor…",,,,,,Anecdotal,comment,,,,,,,,2025-07-15,github/o-nikolas,https://github.com/apache/airflow/pull/46621#issuecomment-3074540257,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
"> I was talking to Jed about something very similar last week. I don't even think the interface needs to enforce the ""how"" for the versioning, that can be delegated to each Bundle provider. Cool! Glad to hear we’re on the same page. I don’t have a strong opinion on this, my main concern is making sure we align on the overall changes before moving forward. I agree, this approach might be simpler. > IMHO the interface should expect a string payload back from the bundle provider which is the repre…",,,,,,Anecdotal,comment,,,,,,,,2025-07-16,github/jason810496,https://github.com/apache/airflow/pull/46621#issuecomment-3078208467,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
"> > I was talking to Jed about something very similar last week. I don't even think the interface needs to enforce the ""how"" for the versioning, that can be delegated to each Bundle provider. > > Cool! Glad to hear we’re on the same page. I don’t have a strong opinion on this, my main concern is making sure we align on the overall changes before moving forward. Cool, I'm glad as well! Let's definitely wait for @jedcunningham to weigh in as well. > > I agree, this approach might be simpler. > > …",,,,,,Anecdotal,comment,,,,,,,,2025-07-16,github/o-nikolas,https://github.com/apache/airflow/pull/46621#issuecomment-3079664938,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
"> And I think that payload, and ones for other providers (whatever they decide/need to do for their storage), are exactly what can just be serialized into a string payload and returned through the Bundle interface. Yep. As commented on the devlist. Making it into a generic solution, without special tables for S3 - but leaving the serialization / deserialization to implementation - is a very elegant and nice solution :)",,,,,,Anecdotal,comment,,,,,,,,2025-07-18,github/potiuk,https://github.com/apache/airflow/pull/46621#issuecomment-3087055436,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
"My 2c, giving bundles somewhere to store this extra stuff makes sense. I'd rather we let the bundles own id creation though - if there is one that is meaningful already, we should use that.",,,,,,Anecdotal,comment,,,,,,,,2025-07-18,github/jedcunningham,https://github.com/apache/airflow/pull/46621#issuecomment-3089868511,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
"> IMHO the interface should expect a string payload back from the bundle provider which is the representation of the dag bundle version (which can be whatever mechanism the provider wants/can use to serialize the state of the dags at that time). This is what we have today actually, it's just very limited in size, particularly when you consider the ""I need to store the version of every object in a bucket"" type situation. So, providing somewhere to store it makes sense, but we don't have a natura…",,,,,,Anecdotal,comment,,,,,,,,2025-07-18,github/jedcunningham,https://github.com/apache/airflow/pull/46621#issuecomment-3089885375,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
"@jedcunningham > My 2c, giving bundles somewhere to store this extra stuff makes sense. :heart: > I'd rather we let the bundles own id creation though - if there is one that is meaningful already, we should use that. I think a consistent version id format is better than a wild west of IDs from each provider, but I'm happy to disagree and commit on this one. I see where you're coming from on the id sometimes being useful (but I really only see git as the example of that). > This is what we have …",,,,,,Anecdotal,comment,,,,,,,,2025-07-18,github/o-nikolas,https://github.com/apache/airflow/pull/46621#issuecomment-3090778221,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
@jason810496 I chatted with @jedcunningham last week and we're both strapped for time at the moment. Did you want to take a stab at implementing what we've been discussing here?,,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/o-nikolas,https://github.com/apache/airflow/pull/46621#issuecomment-3134221835,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
"> @jason810496 > > I chatted with @jedcunningham last week and we're both strapped for time at the moment. Did you want to take a stab at implementing what we've been discussing here? Sure, I would like to help finalize the design and handle the implementation as well. However, my bandwidth is limited at the moment as I am resolving an issue with the CloudWatch remote logging handler and preparing for my OSS conference presentation next week. I will only be able to start working on the design a…",,,,,,Anecdotal,comment,,,,,,,,2025-07-30,github/jason810496,https://github.com/apache/airflow/pull/46621#issuecomment-3134753448,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
"Transition of `update` airflowctl config command <!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unle…",,,,,,Anecdotal,issue,,,,,,,,2025-06-13,github/yunchipang,https://github.com/apache/airflow/pull/51704,repo: apache/airflow | keyword: gotcha | state: closed
"Fix: ensure get_df uses SQLAlchemy engine to avoid pandas warning <!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses…",,,,,,Anecdotal,issue,,,,,,,,2025-06-25,github/daesung9511,https://github.com/apache/airflow/pull/52224,repo: apache/airflow | keyword: gotcha | state: closed
"Congratulations on your first Pull Request and welcome to the Apache Airflow community! If you have any issues or are unsure about any anything please check our Contributors' Guide (https://github.com/apache/airflow/blob/main/contributing-docs/README.rst) Here are some useful points: - Pay attention to the quality of your code (ruff, mypy and type annotations). Our [pre-commits]( https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#prerequisites-for-pre-commit…",,,,,,Anecdotal,comment,,,,,,,,2025-06-25,github/boring-cyborg[bot],https://github.com/apache/airflow/pull/52224#issuecomment-3003576716,repo: apache/airflow | issue: Fix: ensure get_df uses SQLAlchemy engine to avoid pandas warning | keyword: gotcha
"Hi, just a kind reminder for the review. I’m here and ready to hear your advice and revise the code. Thanks! @bugraoz93",,,,,,Anecdotal,comment,,,,,,,,2025-07-09,github/daesung9511,https://github.com/apache/airflow/pull/52224#issuecomment-3053994703,repo: apache/airflow | issue: Fix: ensure get_df uses SQLAlchemy engine to avoid pandas warning | keyword: gotcha
"> Hi, just a kind reminder for the review. I’m here and ready to hear your advice and revise the code. Thanks! > @bugraoz93 Thanks for the changes!",,,,,,Anecdotal,comment,,,,,,,,2025-07-10,github/bugraoz93,https://github.com/apache/airflow/pull/52224#issuecomment-3057858188,repo: apache/airflow | issue: Fix: ensure get_df uses SQLAlchemy engine to avoid pandas warning | keyword: gotcha
"Hi, I modified the pyproject.toml and rolled back the code to ensure it passes with the minimal required dependencies and aligns the method signature with its superclass, as required by mypy in the unit tests. Please let me know if anything should be further modified or removed. Otherwise, it looks like the CI has passed. Thanks again for your time and review! @bugraoz93 @potiuk It passed in the previous commit (133408b).",,,,,,Anecdotal,comment,,,,,,,,2025-07-14,github/daesung9511,https://github.com/apache/airflow/pull/52224#issuecomment-3070904873,repo: apache/airflow | issue: Fix: ensure get_df uses SQLAlchemy engine to avoid pandas warning | keyword: gotcha
Thanks a lot @daesung9511! Could you please rebase the branch for a cleaner commit history? I believe it is always merged with the incoming changes. I think we can merge it afterwards. Great work!,,,,,,Anecdotal,comment,,,,,,,,2025-07-15,github/bugraoz93,https://github.com/apache/airflow/pull/52224#issuecomment-3073905121,repo: apache/airflow | issue: Fix: ensure get_df uses SQLAlchemy engine to avoid pandas warning | keyword: gotcha
"After CI is fully green, I am going to merge it :) Just checking one last time for a full CI run to ensure added packages won't break anything from main",,,,,,Anecdotal,comment,,,,,,,,2025-07-16,github/bugraoz93,https://github.com/apache/airflow/pull/52224#issuecomment-3079404517,repo: apache/airflow | issue: Fix: ensure get_df uses SQLAlchemy engine to avoid pandas warning | keyword: gotcha
"Awesome work, congrats on your first merged pull request! You are invited to check our [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.",,,,,,Anecdotal,comment,,,,,,,,2025-07-16,github/boring-cyborg[bot],https://github.com/apache/airflow/pull/52224#issuecomment-3079706147,repo: apache/airflow | issue: Fix: ensure get_df uses SQLAlchemy engine to avoid pandas warning | keyword: gotcha
"Upgrade singledispatch usages to Python 3.10 Remove leftover code for dag.test() compat. Now that we've removed 3.9 support, it is also possible to simply some single-dispatch regiatration code. Unfortunately this actually requires us moving some imports *out of* the type-checking block since those types are now inspected at runtime. I'm not entirely sure this is a net improvement. Discussion needed.",,,,,,Anecdotal,issue,,,,,,,,2025-07-08,github/uranusjr,https://github.com/apache/airflow/pull/53001,repo: apache/airflow | keyword: gotcha | state: closed
"Filter only provided integration paths for breeze integration testing Currently we process all the paths even when we specify specific integration. its bit confusing, lets fitlerout only provided integration paths. Example before: This is for ydb , but it processed all the paths. <img width=""1772"" alt=""image"" src=""https://github.com/user-attachments/assets/7ae7839f-b757-4989-b565-68f50f6c0745"" /> After: <img width=""1755"" alt=""image"" src=""https://github.com/user-attachments/assets/62c5723d-ea6e-…",,,,,,Anecdotal,issue,,,,,,,,2025-06-29,github/gopidesupavan,https://github.com/apache/airflow/pull/52462,repo: apache/airflow | keyword: gotcha | state: closed
"We just need to be EXTRA careful with such changes and look at the logs of the integration tests, to see that they were actually executed and not just **green**.",,,,,,Anecdotal,comment,,,,,,,,2025-06-29,github/potiuk,https://github.com/apache/airflow/pull/52462#issuecomment-3016799844,repo: apache/airflow | issue: Filter only provided integration paths for breeze integration testing | keyword: gotcha
"> We just need to be EXTRA careful with such changes and look at the logs of the integration tests, to see that they were actually executed and not just **green**. Yeah correct, In CI except provided input integration rest all integrations tests skipped. as the integration environment is different :)",,,,,,Anecdotal,comment,,,,,,,,2025-06-29,github/gopidesupavan,https://github.com/apache/airflow/pull/52462#issuecomment-3016802455,repo: apache/airflow | issue: Filter only provided integration paths for breeze integration testing | keyword: gotcha
"> Renamed integration type gremline to tinkerpop, to align with other providers. Yeah. Those were the ""extra-careful"" things :)",,,,,,Anecdotal,comment,,,,,,,,2025-06-29,github/potiuk,https://github.com/apache/airflow/pull/52462#issuecomment-3016820918,repo: apache/airflow | issue: Filter only provided integration paths for breeze integration testing | keyword: gotcha
"### Backport failed to create: v3-0-test. View the failure log <a href='https://github.com/apache/airflow/actions/runs/15958180830'> Run details </a> <table> <tr> <th>Status</th> <th>Branch</th> <th>Result</th> </tr> <tr> <td>❌</td> <td>v3-0-test</td> <td><a href=""https://github.com/apache/airflow/commit/0c947b5300fb267cea8ada4648c7c42ce1d396cb""><img src='https://img.shields.io/badge/Commit-0c947b5-red' alt='Commit Link'></a></td> </tr> </table> You can attempt to backport this manually by runn…",,,,,,Anecdotal,comment,,,,,,,,2025-06-29,github/github-actions[bot],https://github.com/apache/airflow/pull/52462#issuecomment-3016960658,repo: apache/airflow | issue: Filter only provided integration paths for breeze integration testing | keyword: gotcha
"> No need to backport Yeah, but it changed few images and hash value, thought its better to backport to avoid if any future conflicts",,,,,,Anecdotal,comment,,,,,,,,2025-06-29,github/gopidesupavan,https://github.com/apache/airflow/pull/52462#issuecomment-3017073068,repo: apache/airflow | issue: Filter only provided integration paths for breeze integration testing | keyword: gotcha
"@potiuk @gopidesupavan any reason why it is skipping the tests for tinkerpop now? I tried running it locally and it is getting skipped. ``` (apache-airflow) AhmadFarhan@Ahmads-MacBook-Pro airflow % breeze testing providers-integration-tests --integration tinkerpop Good version of Docker: 28.1.1. Good version of docker-compose: 2.36.0 Executable permissions on entrypoints are OK WARN[0000] Warning: No resource found to remove for project ""airflow-test-all"". [+] Running 9/9 ✔ gremlin Pulled 35.4s…",,,,,,Anecdotal,comment,,,,,,,,2025-07-03,github/ahmadtfarhan,https://github.com/apache/airflow/pull/52462#issuecomment-3033078711,repo: apache/airflow | issue: Filter only provided integration paths for breeze integration testing | keyword: gotcha
@potiuk @gopidesupavan you should be able to see this test CI: https://github.com/apache/airflow/actions/runs/15958640668/job/45008319963#step:5:1138 and then this one before the change: https://github.com/apache/airflow/actions/runs/15957152355/job/45005093612#step:5:1166,,,,,,Anecdotal,comment,,,,,,,,2025-07-03,github/ahmadtfarhan,https://github.com/apache/airflow/pull/52462#issuecomment-3033093147,repo: apache/airflow | issue: Filter only provided integration paths for breeze integration testing | keyword: gotcha
I did not change it myself because it will need some engineering more than just renaming. Please can you revert it back? @gopidesupavan,,,,,,Anecdotal,comment,,,,,,,,2025-07-03,github/ahmadtfarhan,https://github.com/apache/airflow/pull/52462#issuecomment-3033095212,repo: apache/airflow | issue: Filter only provided integration paths for breeze integration testing | keyword: gotcha
"@ahmadtfarhan Ah thanks for catching that, The change is to align with provider naming and discover only provided integration tests. see the description above. I have fix here https://github.com/apache/airflow/pull/52826",,,,,,Anecdotal,comment,,,,,,,,2025-07-03,github/gopidesupavan,https://github.com/apache/airflow/pull/52462#issuecomment-3033290174,repo: apache/airflow | issue: Filter only provided integration paths for breeze integration testing | keyword: gotcha
"Docs setup for the Task SDK <!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applic…",,,,,,Anecdotal,issue,,,,,,,,2025-05-28,github/sunank200,https://github.com/apache/airflow/pull/51153,repo: apache/airflow | keyword: gotcha | state: closed
Future work: go in to a lot more detail on the dynamic task mapping and move that out of apache-airflow-core docs to here.,,,,,,Anecdotal,comment,,,,,,,,2025-06-03,github/ashb,https://github.com/apache/airflow/pull/51153#issuecomment-2934366388,repo: apache/airflow | issue: Docs setup for the Task SDK | keyword: gotcha
"> We are missing a dep on `apache-airflow-devel-common[docs]` to pull in the sphinx deps correctly > > I'm not sure if it's just me locally, but there is a huge amount of whitespace all over the shop. > > <img alt=""Screenshot 2025-06-03 at 10 05 05"" width=""2000"" src=""https://private-user-images.githubusercontent.com/34150/450672838-1792f169-4518-4e65-9752-5d68119b509c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6Imt…",,,,,,Anecdotal,comment,,,,,,,,2025-06-03,github/sunank200,https://github.com/apache/airflow/pull/51153#issuecomment-2935061088,repo: apache/airflow | issue: Docs setup for the Task SDK | keyword: gotcha
I have a feeling that the issue I'm seeing is something locally that's affecting me. (I think I've seen it before) When we build the docs in CI do we create an artefact of them that we can download and view?,,,,,,Anecdotal,comment,,,,,,,,2025-06-03,github/ashb,https://github.com/apache/airflow/pull/51153#issuecomment-2935228001,repo: apache/airflow | issue: Docs setup for the Task SDK | keyword: gotcha
"> Future work: go in to a lot more detail on the dynamic task mapping and move that out of apache-airflow-core docs to here. Created issue for this: https://github.com/apache/airflow/issues/51519 > move the tutorials (dags, tasks, taskflow etc etc.) from apache-airflow-core to here. Created issue for this: https://github.com/apache/airflow/issues/51520",,,,,,Anecdotal,comment,,,,,,,,2025-06-09,github/sunank200,https://github.com/apache/airflow/pull/51153#issuecomment-2954772309,repo: apache/airflow | issue: Docs setup for the Task SDK | keyword: gotcha
"> It handles most of my comments, and the PR need not be perfect in the initial cut. I am OK with this, as it implements the initial skeleton for task SDK docs, so in the interest of setting up the basic layer to build things onto, looks good +1 > > Please ensure to handle the comments before merge. Yes I have created follow up issues as well as mentioned in the thread of comments above.",,,,,,Anecdotal,comment,,,,,,,,2025-06-11,github/sunank200,https://github.com/apache/airflow/pull/51153#issuecomment-2962101176,repo: apache/airflow | issue: Docs setup for the Task SDK | keyword: gotcha
AIP-84 Authentications and Permissions Meta issue for Authentications and Permissions for the API. ## I Multiple PRs to add back permissions to all endpoints (the ones from the legacy API that were removed during the migration) ### Public API - [x] assets @rawwar https://github.com/apache/airflow/pull/47136 - [x] asset aliases @rawwar https://github.com/apache/airflow/pull/47241 - [x] associated assets @rawwar https://github.com/apache/airflow/issues/47561 - [x] backfills @vatsrahul1001 https:/…,,,,,,Anecdotal,issue,,,,,,,,2024-09-20,github/pierrejeambrun,https://github.com/apache/airflow/issues/42360,repo: apache/airflow | keyword: gotcha | state: closed
"Hello @jason810496, There has been some update on this one, it is not well scoped and depends on the work that Vincent is doing on the FABAuthManager and JWT backend. I do not recommend to take this one at this point. Removing the `good first issue tag`",,,,,,Anecdotal,comment,,,,,,,,2024-10-22,github/pierrejeambrun,https://github.com/apache/airflow/issues/42360#issuecomment-2428552187,repo: apache/airflow | issue: AIP-84 Authentications and Permissions | keyword: gotcha
This is now unblocked since https://github.com/apache/airflow/issues/44884 has been implemented. We can start implementing the permissions for different endpoints.,,,,,,Anecdotal,comment,,,,,,,,2025-02-24,github/pierrejeambrun,https://github.com/apache/airflow/issues/42360#issuecomment-2678550061,repo: apache/airflow | issue: AIP-84 Authentications and Permissions | keyword: gotcha
"> @jason810496 if you still have some interest for this let me know so I can assign you :) Sure! I can work on this issue, I will look into the context tomorrow then ask if I not sure about the details.",,,,,,Anecdotal,comment,,,,,,,,2025-02-24,github/jason810496,https://github.com/apache/airflow/issues/42360#issuecomment-2678569225,repo: apache/airflow | issue: AIP-84 Authentications and Permissions | keyword: gotcha
"Great. Don't hesitate to do 1 PR per 'resource'. (for instance for DagRun endpoints, for taskinstance endpoints, connection enpoints etc...). I think @rawwar will provide some help too so you can split the work easily. :)",,,,,,Anecdotal,comment,,,,,,,,2025-02-24,github/pierrejeambrun,https://github.com/apache/airflow/issues/42360#issuecomment-2678632160,repo: apache/airflow | issue: AIP-84 Authentications and Permissions | keyword: gotcha
"I will start with the **Dag** endpoint first and provide an update here before working on the other endpoints. I’ll also list all the necessary endpoints later. Before proceeding further, I’d like to double-check something. ~After reviewing #44884, it seems that we might need a general middleware or a `Depends` function (in FastAPI) as an authentication layer to read the Bearer token from authorization request headers. This authentication layer should leverage the `AuthManager` initialized in t…",,,,,,Anecdotal,comment,,,,,,,,2025-02-25,github/jason810496,https://github.com/apache/airflow/issues/42360#issuecomment-2680812942,repo: apache/airflow | issue: AIP-84 Authentications and Permissions | keyword: gotcha
> I’ll also list all the necessary endpoints later. Endpoints: - [ ] configuration - [ ] connection - [ ] dag - [ ] asset - [ ] pool - [ ] variable - [ ] view - [ ] custom_view,,,,,,Anecdotal,comment,,,,,,,,2025-02-25,github/jason810496,https://github.com/apache/airflow/issues/42360#issuecomment-2680902207,repo: apache/airflow | issue: AIP-84 Authentications and Permissions | keyword: gotcha
"> [@jason810496](https://github.com/jason810496) , I'll pick asset, pool and variable. I'll work on them this week. Then I'll take configuration and connection also, if I am fast enough I can take the rest also ! ( Not sure I have enough time currently ) - [ ] dag https://github.com/apache/airflow/pull/47062 - [ ] configuration https://github.com/apache/airflow/pull/47208 - [ ] ~connection~ variable https://github.com/apache/airflow/pull/47211",,,,,,Anecdotal,comment,,,,,,,,2025-02-25,github/jason810496,https://github.com/apache/airflow/issues/42360#issuecomment-2682298523,repo: apache/airflow | issue: AIP-84 Authentications and Permissions | keyword: gotcha
> [@Lee-W](https://github.com/Lee-W) do you want to take the private `/ui` endpoints ? I'll probably start work on this one next week. Anything that has not yet been taken works for me 🙂,,,,,,Anecdotal,comment,,,,,,,,2025-02-26,github/Lee-W,https://github.com/apache/airflow/issues/42360#issuecomment-2684477879,repo: apache/airflow | issue: AIP-84 Authentications and Permissions | keyword: gotcha
"The previous endpoints were listed from `AuthManager`, but we should instead list them from the **Legacy API**, as some entities have not been implemented in `AuthManager`. ```bash grep -rl ""requires_access_"" airflow/api_connexion/endpoints/ | sed -E 's|airflow/api_connexion/endpoints//||; s/_endpoint\.py$//; s|^|- [ ] |' | sort ``` ### `airflow/api_fastapi/core_api/routes/public/` ```bash ls airflow/api_fastapi/core_api/routes/public/ | grep -v __init__ | sed -E 's/\.py$//; s|^|- [ ] |' ``` 28…",,,,,,Anecdotal,comment,,,,,,,,2025-02-28,github/jason810496,https://github.com/apache/airflow/issues/42360#issuecomment-2689687190,repo: apache/airflow | issue: AIP-84 Authentications and Permissions | keyword: gotcha
"Can you instead list them on `api_fastapi` ? Some legacy endpoints were migrated to different resource. custom_view does not exist anymore, we potentially have the `execution_api` to secure behind auth, etc.",,,,,,Anecdotal,comment,,,,,,,,2025-02-28,github/pierrejeambrun,https://github.com/apache/airflow/issues/42360#issuecomment-2690195236,repo: apache/airflow | issue: AIP-84 Authentications and Permissions | keyword: gotcha
"> Can you instead list them on `api_fastapi` ? Some legacy endpoints were migrated to different resource. custom_view does not exist anymore, we potentially have the `execution_api` to secure behind auth, etc. Just updated the above list: - `airflow/api_fastapi/core_api/routes/public/` : 28 endpoints - `airflow/api_fastapi/core_api/routes/ui/` : 8 endpoints - `airflow/api_fastapi/execution_api/routes/` : 6 endpoints total: 42 endpoints I think some `requires_access_<entity>` might be reused for…",,,,,,Anecdotal,comment,,,,,,,,2025-02-28,github/jason810496,https://github.com/apache/airflow/issues/42360#issuecomment-2690708991,repo: apache/airflow | issue: AIP-84 Authentications and Permissions | keyword: gotcha
"@pierrejeambrun , I'll pick these dag_parsing, dag_report, dag_run, dag_sources, dag_stats, dag_tags, dag_versions, dag_warning, task_instances, xcom",,,,,,Anecdotal,comment,,,,,,,,2025-02-28,github/rawwar,https://github.com/apache/airflow/issues/42360#issuecomment-2691049627,repo: apache/airflow | issue: AIP-84 Authentications and Permissions | keyword: gotcha
I'll pick: - [ ] event_logs https://github.com/apache/airflow/pull/47246 - [ ] extra_links https://github.com/apache/airflow/pull/47251 - [ ] import_error https://github.com/apache/airflow/pull/47270 - [ ] job https://github.com/apache/airflow/pull/47271 - [ ] log https://github.com/apache/airflow/pull/47302,,,,,,Anecdotal,comment,,,,,,,,2025-03-01,github/jason810496,https://github.com/apache/airflow/issues/42360#issuecomment-2691916029,repo: apache/airflow | issue: AIP-84 Authentications and Permissions | keyword: gotcha
"@jason810496 @rawwar can you please update the description and indicate which of the APIs are Done, which are WIP and which are yet to be picked up.",,,,,,Anecdotal,comment,,,,,,,,2025-03-04,github/phanikumv,https://github.com/apache/airflow/issues/42360#issuecomment-2696227780,repo: apache/airflow | issue: AIP-84 Authentications and Permissions | keyword: gotcha
"> [@jason810496](https://github.com/jason810496) [@rawwar](https://github.com/rawwar) can you please update the description and indicate which of the APIs are Done, which are WIP and which are yet to be picked up. Hi @phanikumv, I don't have permission to update the meta issue description, but I've added the PR links in my comment above. The links will reflect the current state of the PRs.",,,,,,Anecdotal,comment,,,,,,,,2025-03-04,github/jason810496,https://github.com/apache/airflow/issues/42360#issuecomment-2696313433,repo: apache/airflow | issue: AIP-84 Authentications and Permissions | keyword: gotcha
"@jason810496 @rawwar @vatsrahul1001 , are there any other known dependencies in public auths? It would be nice if we could list them so that we know what to do first 🙂 I'll take a look at the non-assigned ones later. as for the format, you can take a look at the UI part.",,,,,,Anecdotal,comment,,,,,,,,2025-03-07,github/Lee-W,https://github.com/apache/airflow/issues/42360#issuecomment-2706059611,repo: apache/airflow | issue: AIP-84 Authentications and Permissions | keyword: gotcha
"> [@jason810496](https://github.com/jason810496) [@rawwar](https://github.com/rawwar) [@vatsrahul1001](https://github.com/vatsrahul1001) , are there any other known dependencies in public auths? It would be nice if we could list them so that we know what to do first 🙂 I'll take a look at the non-assigned ones later. as for the format, you can take a look at the UI part. except asset_alias,dag_report my rest of the PR's depend on dags",,,,,,Anecdotal,comment,,,,,,,,2025-03-07,github/rawwar,https://github.com/apache/airflow/issues/42360#issuecomment-2706624973,repo: apache/airflow | issue: AIP-84 Authentications and Permissions | keyword: gotcha
"> [@jason810496](https://github.com/jason810496) [@rawwar](https://github.com/rawwar) [@vatsrahul1001](https://github.com/vatsrahul1001) , are there any other known dependencies in public auths? It would be nice if we could list them so that we know what to do first 🙂 I'll take a look at the non-assigned ones later. as for the format, you can take a look at the UI part. Still working on fixing the Kubernetes tests. If not resolved today, I’ll continue over the weekend. I have idea of how to fix…",,,,,,Anecdotal,comment,,,,,,,,2025-03-07,github/jason810496,https://github.com/apache/airflow/issues/42360#issuecomment-2706705120,repo: apache/airflow | issue: AIP-84 Authentications and Permissions | keyword: gotcha
feat(secrets_masker): add minimum secret length and skip masking for common terms closes: #48105 feat(secrets_masker): add minimum secret length and skip masking for common terms Introduce a minimum secret length (MIN_SECRET_LENGTH) to avoid masking short strings that are unlikely to be sensitive. Add a list of common terms (SECRETS_TO_SKIP_MASKING) that should be excluded from masking in production.,,,,,,Anecdotal,issue,,,,,,,,2025-04-04,github/amarlearning,https://github.com/apache/airflow/pull/48791,repo: apache/airflow | keyword: gotcha | state: closed
"> I think masking a password/other sensitive field should be done on structured data _regardless_ of the length, it's only the ""mask when the value appears elsewhere in a string"" that should depend on the list. > > Maybe this is what happens, but I don't think the tests check that behaviour You're right. Structured data fields like passwords and tokens are always masked via `_redact`, regardless of length. For unstructured text, only values exceeding the minimum length are masked in `add_mask`.…",,,,,,Anecdotal,comment,,,,,,,,2025-04-04,github/amarlearning,https://github.com/apache/airflow/pull/48791#issuecomment-2779409446,repo: apache/airflow | issue: feat(secrets_masker): add minimum secret length and skip masking for common terms | keyword: gotcha
"> I can add more tests to verify this behavior thoroughly. Please, it's important behaviour to test and make sure we don't break",,,,,,Anecdotal,comment,,,,,,,,2025-04-04,github/ashb,https://github.com/apache/airflow/pull/48791#issuecomment-2779595820,repo: apache/airflow | issue: feat(secrets_masker): add minimum secret length and skip masking for common terms | keyword: gotcha
"Adding a backcompat shim for baseoperatorlink <!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless …",,,,,,Anecdotal,issue,,,,,,,,2025-03-26,github/amoghrajesh,https://github.com/apache/airflow/pull/48396,repo: apache/airflow | keyword: gotcha | state: closed
"Set `catchup_by_default` config to False by default ""Oh I need to unpause the DAG to run it, gotcha"" *click* Many DAG runs spawn and start running ""Nonono I only wanted one, stop stop"" ⬆️ This was me almost 3 years ago, using Airflow for the first time, obviously not understanding scheduling yet, and thinking of catchup mainly as a condiment. Now I know how start_date works, but I still often set it just a while in the past because time math can be hard. Setting catchup=False has become a defau…",,,,,,Anecdotal,issue,,,,,,,,2025-03-04,github/TJaniF,https://github.com/apache/airflow/pull/47354,repo: apache/airflow | keyword: gotcha | state: closed
Looks like a lot of tests are not happy because they rely on the first run being created against start_date. Let’s just add `catchup_by_default = true` in `airflow/config_templates/unit_tests.cfg` so tests still set catchup by default.,,,,,,Anecdotal,comment,,,,,,,,2025-03-06,github/uranusjr,https://github.com/apache/airflow/pull/47354#issuecomment-2702697740,repo: apache/airflow | issue: Set `catchup_by_default` config to False by default | keyword: gotcha
Wouldn't that defeat the purpose of tests. Should we make `DagMaker` configurable in `devel-common/src/tests_common/pytest_plugin.py` ? And perhaps run for both catchup `True` as well as `False` ?,,,,,,Anecdotal,comment,,,,,,,,2025-03-06,github/abhishekbhakat,https://github.com/apache/airflow/pull/47354#issuecomment-2703002988,repo: apache/airflow | issue: Set `catchup_by_default` config to False by default | keyword: gotcha
"Check `tests/jobs/test_scheduler_job.py::TestSchedulerJob::test_scheduler_task_start_date` Catchup=False (New Default) Task Start Date Handling: - The scheduler ignores the historical significance of task start dates - It immediately schedules all tasks for the most recent interval, regardless of their original start dates - Future task start dates are essentially treated as if they don't exist. **This scheduler ignorance of future task start dates could be breaking behaviour for catch up by de…",,,,,,Anecdotal,comment,,,,,,,,2025-03-06,github/abhishekbhakat,https://github.com/apache/airflow/pull/47354#issuecomment-2703120572,repo: apache/airflow | issue: Set `catchup_by_default` config to False by default | keyword: gotcha
"Yeah, I agree @abhishekbhakat the tests should be testing both settings for the config. I'll make an attempt on that this weekend :)",,,,,,Anecdotal,comment,,,,,,,,2025-03-06,github/TJaniF,https://github.com/apache/airflow/pull/47354#issuecomment-2703233684,repo: apache/airflow | issue: Set `catchup_by_default` config to False by default | keyword: gotcha
"@TJaniF I can make a PR to your branch. I have added many tests that cover both scenarios. There's still a lot to do, I'll probably make a PR today. Will ping you to see if you can merge it into your branch.",,,,,,Anecdotal,comment,,,,,,,,2025-03-06,github/abhishekbhakat,https://github.com/apache/airflow/pull/47354#issuecomment-2703278038,repo: apache/airflow | issue: Set `catchup_by_default` config to False by default | keyword: gotcha
"I fixed all tests except 1: ```bash FAILED tests/always/test_project_structure.py::TestProjectStructure::test_providers_modules_should_have_tests - AssertionError: Detect missing tests in providers module - please add tests assert equals failed [ [] 'providers/fab/tests/unit/fab /www/node_modules/flatted/pytho n/test_flatted.py', ] == 1 failed, 1563 passed, 1 skipped, 1 xfailed, 1 warning in 85.93s (0:01:25) == ``` This should not be related to this PR I think ¯\\\_(ツ)\_/¯.",,,,,,Anecdotal,comment,,,,,,,,2025-03-06,github/abhishekbhakat,https://github.com/apache/airflow/pull/47354#issuecomment-2703768665,repo: apache/airflow | issue: Set `catchup_by_default` config to False by default | keyword: gotcha
"> I fixed all tests except 1: > > ```shell > FAILED tests/always/test_project_structure.py::TestProjectStructure::test_providers_modules_should_have_tests - AssertionError: Detect missing tests in providers module - please add tests > assert equals failed > [ [] > 'providers/fab/tests/unit/fab > /www/node_modules/flatted/pytho > n/test_flatted.py', > ] > == 1 failed, 1563 passed, 1 skipped, 1 xfailed, 1 warning in 85.93s (0:01:25) == > ``` > > This should not be related to this PR I think ¯\_(ツ…",,,,,,Anecdotal,comment,,,,,,,,2025-03-06,github/TJaniF,https://github.com/apache/airflow/pull/47354#issuecomment-2703796286,repo: apache/airflow | issue: Set `catchup_by_default` config to False by default | keyword: gotcha
"Ok I think this is ready, the tests that are failing recently I think are unrelated to this PR (K8s tests) 👀 Also added the config change to `airflow config lint` @sunank200 :)",,,,,,Anecdotal,comment,,,,,,,,2025-03-13,github/TJaniF,https://github.com/apache/airflow/pull/47354#issuecomment-2721445408,repo: apache/airflow | issue: Set `catchup_by_default` config to False by default | keyword: gotcha
"> Not enough knowledge on the lint command and the config default, otherwise lgtm. Thank you @uranusjr! The static check was an extra empty line, sorry, fixed that now :) And yes, I don't think the K8s checks are failing because of something I did... 🤔 EDIT: it is 💚 ! :D",,,,,,Anecdotal,comment,,,,,,,,2025-03-14,github/TJaniF,https://github.com/apache/airflow/pull/47354#issuecomment-2724090326,repo: apache/airflow | issue: Set `catchup_by_default` config to False by default | keyword: gotcha
"Improve caching strategy across the board of CI workflow We are using various caches in our build and so far - due to the way how ""standard"" caching works, PRs from forks could not effectively use the cache from main Airflow repository - because caches are not shared with other repositories - so the PRs builds could only use cache effectively when they were rebased and continued running from the same fork. This PR improves caching strategy using ""stash"" action from the ASF. Unlike `cache` - the…",,,,,,Anecdotal,issue,,,,,,,,2024-12-30,github/potiuk,https://github.com/apache/airflow/pull/45289,repo: apache/airflow | keyword: gotcha | state: closed
> Looks like the stash action does not work exactly as advertised :) think we need to set env variable name `head_name` https://github.com/apache/infrastructure-actions/blob/main/stash/restore/action.yml#L108,,,,,,Anecdotal,comment,,,,,,,,2024-12-30,github/gopidesupavan,https://github.com/apache/airflow/pull/45289#issuecomment-2565432944,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
> > Looks like the stash action does not work exactly as advertised :) > > think we need to set env variable name `head_name` https://github.com/apache/infrastructure-actions/blob/main/stash/restore/action.yml#L108 ah head_name is derived from github ref.,,,,,,Anecdotal,comment,,,,,,,,2024-12-30,github/gopidesupavan,https://github.com/apache/airflow/pull/45289#issuecomment-2565440590,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
"Yeah - it seems that the action likely gets into a race condition, or does not work as advertised: From: https://github.com/apache/infrastructure-actions/tree/main/stash#usage > Using the save action again in the same workflow run will overwrite the existing cache with the same key. This does apply to each invocation in a matrix job as well! If you want to keep the old cache, you can use a different key or set overwrite to false. It seems that we have error 409 conflict not handled (I tried wit…",,,,,,Anecdotal,comment,,,,,,,,2024-12-30,github/potiuk,https://github.com/apache/airflow/pull/45289#issuecomment-2565546644,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
"Evidently it tries with ""overwite: true"": ![image](https://github.com/user-attachments/assets/2936fedb-7c01-4e42-9fff-131b7aa1da29)",,,,,,Anecdotal,comment,,,,,,,,2024-12-30,github/potiuk,https://github.com/apache/airflow/pull/45289#issuecomment-2565548617,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
"If that's the right guess, then we might try to handle it somehow - since we don't care which uploaded artifact will be uploaded - we can actually even ignore 409 when it happens - without retrying it because it means that someone else managed to upload the artifact in parallel and they ""won"".",,,,,,Anecdotal,comment,,,,,,,,2024-12-30,github/potiuk,https://github.com/apache/airflow/pull/45289#issuecomment-2565551877,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
"Wow that's wild, haven't seen that before :D But that's an issue with the artifact backend, nothing really I can change in the action itself.",,,,,,Anecdotal,comment,,,,,,,,2024-12-30,github/assignUser,https://github.com/apache/airflow/pull/45289#issuecomment-2565602157,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
"> Wow that's wild, haven't seen that before :D But that's an issue with the artifact backend, nothing really I can change in the action itself. Let's see... PRs might be coming :)",,,,,,Anecdotal,comment,,,,,,,,2024-12-30,github/potiuk,https://github.com/apache/airflow/pull/45289#issuecomment-2565855217,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
"FYI. @assignUser and @gopidesupavan -> seems that this is a well known ""feature"" of the `@v4` action that caused a number of users a problems when migrating to `@v4`. The proposed `solution` (which IMHO is just a workaround) is to upload each artifact with a different key and use ""merge-multiple"" feature and glob pattern to download and merge all such uploaded artifacts (!??!!#$@#%!%!$%!#%!).... This is even explained here: https://github.com/actions/download-artifact/blob/main/docs/MIGRATION.m…",,,,,,Anecdotal,comment,,,,,,,,2024-12-30,github/potiuk,https://github.com/apache/airflow/pull/45289#issuecomment-2565964463,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
"OK. @assignUser and @gopidesupavan -> I think I found a solution (and actually this is a better one in general for performance, but slightly more ""distributed"" among the .yml files. Instead of heaving a clear save/restore around installation, I only do: * restore * installl And I make sure to have one separate job that is prerequisite of all other jobs (`build-info` which was already there as a single job that kicks-off the rest - for uv cache and `install-pre-commit` that is added as ""needs"" f…",,,,,,Anecdotal,comment,,,,,,,,2024-12-30,github/potiuk,https://github.com/apache/airflow/pull/45289#issuecomment-2565985028,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
"Ha. TIL.... First of all It turns out that `uv` is DAMN FAST. I Installing breeze with uv from scratch = 6s, just downloading breeze cache is 5s <img width=""826"" alt=""Screenshot 2024-12-31 at 00 42 24"" src=""https://github.com/user-attachments/assets/036c15a9-4789-432f-953c-a7f1d146cdb7"" /> Not mentioning upload delay.... So using cache for breeze makes no sense.. I expected a bit different numbers for ""pre-commit"" ... but... Installing all our pre-commits with uv from scratch = 1m45 s, uploadin…",,,,,,Anecdotal,comment,,,,,,,,2024-12-31,github/potiuk,https://github.com/apache/airflow/pull/45289#issuecomment-2566025919,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
Ok. @assignUser @gopidesupavan -> I believe I also found out why caching did not speed up pre-commit. The problem was that `stash/restore` action (unlike download-artifact) did not expand `~` to home directory of the user - and this is where our cache is stored. Effectively the cache was restored to `~/.cache/` directory inside the checked out repository (with actual `~` as directory name) :( .. Took me a bit of time to figure it out. Fix in https://github.com/apache/infrastructure-actions/pull…,,,,,,Anecdotal,comment,,,,,,,,2024-12-31,github/potiuk,https://github.com/apache/airflow/pull/45289#issuecomment-2566058919,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
"> FYI. @assignUser and @gopidesupavan -> seems that this is a well known ""feature"" of the `@v4` action that caused a number of users a problems when migrating to `@v4`. The proposed `solution` (which IMHO is just a workaround) is to upload each artifact with a different key and use ""merge-multiple"" feature and glob pattern to download and merge all such uploaded artifacts (!??!!#$@#%!%!$%!#%!).... > > This is even explained here: https://github.com/actions/download-artifact/blob/main/docs/MIGRA…",,,,,,Anecdotal,comment,,,,,,,,2024-12-31,github/gopidesupavan,https://github.com/apache/airflow/pull/45289#issuecomment-2566094221,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
"> OK. @assignUser and @gopidesupavan -> I think I found a solution (and actually this is a better one in general for performance, but slightly more ""distributed"" among the .yml files. > > Instead of heaving a clear save/restore around installation, I only do: > > * restore > * installl > > And I make sure to have one separate job that is prerequisite of all other jobs (`build-info` which was already there as a single job that kicks-off the rest - for uv cache and `install-pre-commit` that is ad…",,,,,,Anecdotal,comment,,,,,,,,2024-12-31,github/gopidesupavan,https://github.com/apache/airflow/pull/45289#issuecomment-2566094564,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
"> Ha. TIL.... > > First of all It turns out that `uv` is DAMN FAST. I > > Installing breeze with uv from scratch = 6s, just downloading breeze cache is 5s > > <img alt=""Screenshot 2024-12-31 at 00 42 24"" width=""826"" src=""https://private-user-images.githubusercontent.com/595491/399386334-036c15a9-4789-432f-953c-a7f1d146cdb7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzU2MTQ1ODAsIm5iZiI6MTczNTYxN…",,,,,,Anecdotal,comment,,,,,,,,2024-12-31,github/gopidesupavan,https://github.com/apache/airflow/pull/45289#issuecomment-2566095238,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
> Ok. @assignUser @gopidesupavan -> I believe I also found out why caching did not speed up pre-commit. The problem was that `stash/restore` action (unlike download-artifact) did not expand `~` to home directory of the user - and this is where our cache is stored. > > Effectively the cache was restored to `~/.cache/` directory inside the checked out repository (with actual `~` as directory name) :( .. Took me a bit of time to figure it out. > > Fix in [apache/infrastructure-actions#84](https://…,,,,,,Anecdotal,comment,,,,,,,,2024-12-31,github/gopidesupavan,https://github.com/apache/airflow/pull/45289#issuecomment-2566097552,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
"OK. I think i Implemented all workarounds to not have to wait for any of the `stash` action PRs I created, let's see what will be the savings now",,,,,,Anecdotal,comment,,,,,,,,2024-12-31,github/potiuk,https://github.com/apache/airflow/pull/45289#issuecomment-2566554693,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
"There is one more error with tar ... Pre-commit is currently stored with `/hom/runner` prefix, that's why it had no impact on the timing. But K8S is already hugely impactful -> instaling the venv + all the tools is ~ 1 minute. Savingit is 30 seconds, restoring 10 seconds - since we have many k8S jobs, this will be huge improvement",,,,,,Anecdotal,comment,,,,,,,,2024-12-31,github/potiuk,https://github.com/apache/airflow/pull/45289#issuecomment-2566601905,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
"AIP-72: Allow retrieving Connection from Task Context part of https://github.com/apache/airflow/issues/44481 - Added a minimal Connection user-facing object in Task SDK definition for use in the DAG file - Added logic to get Connections in the context. Fixed some bugs in the way related to Connection parsing/serializing! Now, we have following Connection related objects: - `ConnectionResponse` is auto-generated and tightly coupled with the API schema. - `ConnectionResult` is runtime-specific an…",,,,,,Anecdotal,issue,,,,,,,,2024-12-18,github/kaxil,https://github.com/apache/airflow/pull/45043,repo: apache/airflow | keyword: gotcha | state: closed
"Better description on how to build image for k8s <!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unle…",,,,,,Anecdotal,issue,,,,,,,,2024-11-29,github/potiuk,https://github.com/apache/airflow/pull/44492,repo: apache/airflow | keyword: gotcha | state: closed
"<img width=""918"" alt=""Screenshot 2024-11-30 at 00 30 40"" src=""https://github.com/user-attachments/assets/7c969345-f0b4-4ef0-81a5-59a2136a250a"">",,,,,,Anecdotal,comment,,,,,,,,2024-11-29,github/potiuk,https://github.com/apache/airflow/pull/44492#issuecomment-2508733157,repo: apache/airflow | issue: Better description on how to build image for k8s | keyword: gotcha
"Add GZipMiddleware to compress response content. Closes #43640 The change adds GZipMiddleware to compress response content. Minimum response size for compression is greater than 1kB and compress level is 5 with values ranging from 1 to 9 with 1 (fastest, lowest compression) and 9 (slowest, most compression) Docs : https://fastapi.tiangolo.com/advanced/middleware/#gzipmiddleware Dag list page with 75 dags |compress level | original size (kB) | compressed size (kB) | |---|---|---| | 1 | 48.40 | 6…",,,,,,Anecdotal,issue,,,,,,,,2024-11-05,github/tirkarthi,https://github.com/apache/airflow/pull/43707,repo: apache/airflow | keyword: gotcha | state: closed
Add basic endpoints for managing backfill entities More logic will be added for `create` and `cancel`. We'll need to create dag runs and fail them accordingly. But I'll add that logic separately to make it easier to scrutinize it more closely.,,,,,,Anecdotal,issue,,,,,,,,2024-09-24,github/dstandish,https://github.com/apache/airflow/pull/42455,repo: apache/airflow | keyword: gotcha | state: closed
"Add FAB migration commands This PR adds `migrate`, `upgrade` and `reset` db commands to facilitate migrating FAB DBs. FAB upgrade is also integrated into Airflow upgrade such that if airflow db is being upgraded to the heads, FAB migration will also upgrade to the heads. Migration checks to determine if migration has finished now includes checking that FAB migration is also done. Note that downgrading Airflow does not trigger FAB downgrade. FAB downgrade has to be done with FAB downgrade comman…",,,,,,Anecdotal,issue,,,,,,,,2024-08-27,github/ephraimbuddy,https://github.com/apache/airflow/pull/41804,repo: apache/airflow | keyword: gotcha | state: closed
"Try to make dataset objects totally unhashable, redux Re-post of @uranusjr 's change in #42054, with a bug fixed and tests added. Original PR description: > Using the hash property on Dataset (and DatasetAlias) is problematic since subclassing is a possibility, and user code may not correctly implement hashing for Airflow’s purposes. --- **^ Add meaningful description above** Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#…",,,,,,Anecdotal,issue,,,,,,,,2024-09-06,github/topherinternational,https://github.com/apache/airflow/pull/42066,repo: apache/airflow | keyword: gotcha | state: closed
"> This wasn't covered by a test? We should add one. Well it's revealed by some provider tests that look like they weren't run in #42054's checks. I think the challenge is that this is arg defaults all the way down, there's a long chain of `func(kwargs, extra=None)` calls that would all need tests to detect if a call that passed no extras produced a dataset object that matched `Dataset(args)` (w/o an `extra` arg).",,,,,,Anecdotal,comment,,,,,,,,2024-09-06,github/topherinternational,https://github.com/apache/airflow/pull/42066#issuecomment-2333881083,"repo: apache/airflow | issue: Try to make dataset objects totally unhashable, redux | keyword: gotcha"
> This should also include the previous changes since they were reverted. I opened this PR before @potiuk reverted 😓,,,,,,Anecdotal,comment,,,,,,,,2024-09-06,github/topherinternational,https://github.com/apache/airflow/pull/42066#issuecomment-2333882074,"repo: apache/airflow | issue: Try to make dataset objects totally unhashable, redux | keyword: gotcha"
"@uranusjr idk where the dataset `factory` signature is documented, but it should be documented that the `extra` kwarg should be changed from None to {}. e.g. here is the common/io/file one: `def create_dataset(*, path: str, extra=None) -> Dataset:` When #42054 is re-implemented it should be swapped to `def create_dataset(*, path: str, extra={}) -> Dataset:` and so forth for others.",,,,,,Anecdotal,comment,,,,,,,,2024-09-06,github/topherinternational,https://github.com/apache/airflow/pull/42066#issuecomment-2333897091,"repo: apache/airflow | issue: Try to make dataset objects totally unhashable, redux | keyword: gotcha"
"> def create_dataset(*, path: str, extra={}) -> Dataset: Can’t do this because Python… https://docs.python-guide.org/writing/gotchas/#mutable-default-arguments Keeping the default as None and changing it to `{}` should be the way to do. I think I’ll add post-processing code in Dataset so None is converted automatically to `{}` so the interface does not change. (Once the Dataset class is created, there shouldn’t be compatible issues.)",,,,,,Anecdotal,comment,,,,,,,,2024-09-06,github/uranusjr,https://github.com/apache/airflow/pull/42066#issuecomment-2333933159,"repo: apache/airflow | issue: Try to make dataset objects totally unhashable, redux | keyword: gotcha"
"I’ve re-applied the previous changes here, and also added a fix so callers (including HookLineageCollector) can continue to pass in a None. It will be converted automatically to a dict. Can you work on a test to cover the `dataset_factory` code path?",,,,,,Anecdotal,comment,,,,,,,,2024-09-06,github/uranusjr,https://github.com/apache/airflow/pull/42066#issuecomment-2333945799,"repo: apache/airflow | issue: Try to make dataset objects totally unhashable, redux | keyword: gotcha"
"> I’ve re-applied the previous changes here, and also added a fix so callers (including HookLineageCollector) can continue to pass in a None. It will be converted automatically to a dict. > > Can you work on a test to cover the `dataset_factory` code path? I removed my original change, I think it's best if we keep the call stack as it is and use your fix that coerces the None to {} only in the Dataset constructor, less confusing and easier to test that way imo. I added more tests to create_data…",,,,,,Anecdotal,comment,,,,,,,,2024-09-06,github/topherinternational,https://github.com/apache/airflow/pull/42066#issuecomment-2334489099,"repo: apache/airflow | issue: Try to make dataset objects totally unhashable, redux | keyword: gotcha"
Snowflake Provider - Hook's support for not providing a region is broken when using SQLAlchemy ### Apache Airflow Provider(s) snowflake ### Versions of Apache Airflow Providers Versions 2.2.x (since https://github.com/apache/airflow/commit/0a37be3e3cf9289f63f1506bc31db409c2b46738). ### Apache Airflow version 2.2.1 ### Operating System Debian GNU/Linux 10 (buster) ### Deployment Other 3rd-party Helm chart ### Deployment details Bitnami Airflow Helm chart @ version 8.0.2 ### What happened When co…,,,,,,Anecdotal,issue,,,,,,,,2021-12-04,github/mattpolzin,https://github.com/apache/airflow/issues/20032,repo: apache/airflow | keyword: pro tip | state: closed
I appreciate the encouragement! I've got a lot of projects in progress right now so I don't think I'll find time to get set up here as well.,,,,,,Anecdotal,comment,,,,,,,,2021-12-04,github/mattpolzin,https://github.com/apache/airflow/issues/20032#issuecomment-986052872,repo: apache/airflow | issue: Snowflake Provider - Hook's support for not providing a region is broken when using SQLAlchemy | keyword: pro tip
"@mik-laj I have made the change, I have a question on the unit tests. They are failing because the connection sends the region name. Do you have a suggestion on how to test without a region name? https://github.com/harishkrao/airflow/blob/sqlalchemy-snowflake-region/tests/providers/snowflake/hooks/test_snowflake.py#L94",,,,,,Anecdotal,comment,,,,,,,,2021-12-08,github/harishkrao,https://github.com/apache/airflow/issues/20032#issuecomment-988575520,repo: apache/airflow | issue: Snowflake Provider - Hook's support for not providing a region is broken when using SQLAlchemy | keyword: pro tip
"@harishkrao Hi. I improved the tests yesterday and I think now it will be easy to write the new test case to handle this sitution.. See: https://github.com/apache/airflow/pull/20095 Now all you have to do is add a test case here. https://github.com/apache/airflow/blob/545ca59ba9a0b346cbbf28cc6958f9575e5e6b0b/tests/providers/snowflake/hooks/test_snowflake.py#L76-L95 To build a URL, we shouldn't use `string.format`. Instead of it, we should use `snowflake.sqlalchemy.URL`. See: https://github.com/…",,,,,,Anecdotal,comment,,,,,,,,2021-12-08,github/mik-laj,https://github.com/apache/airflow/issues/20032#issuecomment-988647949,repo: apache/airflow | issue: Snowflake Provider - Hook's support for not providing a region is broken when using SQLAlchemy | keyword: pro tip
Thank you @mik-laj. I will write the test cases based on the new tests you pushed yesterday. Also thank you for the SQL Alchemy URL build link. I will use it to construct the URI.,,,,,,Anecdotal,comment,,,,,,,,2021-12-09,github/harishkrao,https://github.com/apache/airflow/issues/20032#issuecomment-989486844,repo: apache/airflow | issue: Snowflake Provider - Hook's support for not providing a region is broken when using SQLAlchemy | keyword: pro tip
"@mik-laj Made the below changes, running pytests pending, will let you know after running them tomorrow: https://github.com/harishkrao/airflow/blob/fix-sqlalchemy-snowflake-region/airflow/providers/snowflake/hooks/snowflake.py#L216-L239 https://github.com/harishkrao/airflow/blob/fix-sqlalchemy-snowflake-region/tests/providers/snowflake/hooks/test_snowflake.py#L268-L279",,,,,,Anecdotal,comment,,,,,,,,2021-12-09,github/harishkrao,https://github.com/apache/airflow/issues/20032#issuecomment-989595540,repo: apache/airflow | issue: Snowflake Provider - Hook's support for not providing a region is broken when using SQLAlchemy | keyword: pro tip
@mik-laj How did you manage dependencies when running pytests? I am facing `WARNING - Exception when importing 'airflow.providers.snowflake.hooks.snowflake.SnowflakeHook' from 'apache-airflow-providers-snowflake' package: No module named 'sqlalchemy.sql.roles'` Both `snowflake-sqlalchemy` and `sqlalchemy` are installed.,,,,,,Anecdotal,comment,,,,,,,,2021-12-10,github/harishkrao,https://github.com/apache/airflow/issues/20032#issuecomment-991330611,repo: apache/airflow | issue: Snowflake Provider - Hook's support for not providing a region is broken when using SQLAlchemy | keyword: pro tip
You can also use constraint files. See: https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html,,,,,,Anecdotal,comment,,,,,,,,2021-12-11,github/mik-laj,https://github.com/apache/airflow/issues/20032#issuecomment-991518172,repo: apache/airflow | issue: Snowflake Provider - Hook's support for not providing a region is broken when using SQLAlchemy | keyword: pro tip
See also complete description here: https://github.com/apache/airflow/blob/main/TESTING.rst#airflow-unit-tests @harishkrao,,,,,,Anecdotal,comment,,,,,,,,2021-12-11,github/potiuk,https://github.com/apache/airflow/issues/20032#issuecomment-991730374,repo: apache/airflow | issue: Snowflake Provider - Hook's support for not providing a region is broken when using SQLAlchemy | keyword: pro tip
"I narrowed down the scope of our issue here. Once I am inside the virtual environment, and I try to import the same SQLAlchemy package, I run into the same error: `root@66ecbec1a1f0:/opt/airflow# python Python 3.8.12 (default, Nov 17 2021, 17:26:17) [GCC 8.3.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> from snowflake.sqlalchemy import URL Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/usr/local/lib/python3.8/site-packages/…",,,,,,Anecdotal,comment,,,,,,,,2021-12-12,github/harishkrao,https://github.com/apache/airflow/issues/20032#issuecomment-991978806,repo: apache/airflow | issue: Snowflake Provider - Hook's support for not providing a region is broken when using SQLAlchemy | keyword: pro tip
"@harishkrao See: https://github.com/snowflakedb/snowflake-sqlalchemy/issues/234 To use URL class, you need to run: ```bash python -m pip install snowflake-sqlalchemy==1.2.4 sqlalchemy==1.3.24 ``` Unfortunately, Snowflake incorrectly marked the version requirements in one release of `snowflake-sqlalchemy` and after fixing the problem, they didn't mark the release as yanked, so pip downloads invalid dependency set.",,,,,,Anecdotal,comment,,,,,,,,2021-12-13,github/mik-laj,https://github.com/apache/airflow/issues/20032#issuecomment-992013437,repo: apache/airflow | issue: Snowflake Provider - Hook's support for not providing a region is broken when using SQLAlchemy | keyword: pro tip
Thanks very much for letting me know and for pushing the fix @mik-laj :) I will use the above versions. Edit: It worked :+1: Thanks a lot!,,,,,,Anecdotal,comment,,,,,,,,2021-12-13,github/harishkrao,https://github.com/apache/airflow/issues/20032#issuecomment-992031060,repo: apache/airflow | issue: Snowflake Provider - Hook's support for not providing a region is broken when using SQLAlchemy | keyword: pro tip
"@mik-laj any suggestions to overcome celery dependency issues? Searched the forum and online and found no open bugs related to it. Happens when running `sudo ./breeze initialize-local-virtualenv --python 3.8` and the virtual environment has the celery version of `5.2.1`. `from celery import Celery, Task, states as celery_states ModuleNotFoundError: No module named 'celery'`",,,,,,Anecdotal,comment,,,,,,,,2021-12-13,github/harishkrao,https://github.com/apache/airflow/issues/20032#issuecomment-992100936,repo: apache/airflow | issue: Snowflake Provider - Hook's support for not providing a region is broken when using SQLAlchemy | keyword: pro tip
"```bash AIRFLOW_VERSION=main PYTHON_VERSION=""$(python --version | cut -d "" "" -f 2 | cut -d ""."" -f 1-2)"" CONSTRAINT_URL=""https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"" pip install celery --constraint ""${CONSTRAINT_URL}"" ``` See: https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html",,,,,,Anecdotal,comment,,,,,,,,2021-12-13,github/mik-laj,https://github.com/apache/airflow/issues/20032#issuecomment-992104938,repo: apache/airflow | issue: Snowflake Provider - Hook's support for not providing a region is broken when using SQLAlchemy | keyword: pro tip
"My apologies for asking again, I am contributing to this repo for the first time. Will follow this and install the dependencies. Thank you for the help and patience @mik-laj.",,,,,,Anecdotal,comment,,,,,,,,2021-12-13,github/harishkrao,https://github.com/apache/airflow/issues/20032#issuecomment-992127514,repo: apache/airflow | issue: Snowflake Provider - Hook's support for not providing a region is broken when using SQLAlchemy | keyword: pro tip
"Just an info - I am working on this, my laptop crashed, so I have to setup the environment in another laptop and continue, will get back soon.",,,,,,Anecdotal,comment,,,,,,,,2021-12-15,github/harishkrao,https://github.com/apache/airflow/issues/20032#issuecomment-994943759,repo: apache/airflow | issue: Snowflake Provider - Hook's support for not providing a region is broken when using SQLAlchemy | keyword: pro tip
"Made changes for the [fix](https://github.com/harishkrao/airflow/commit/f4f499e589ace7cccf307bab12b4aa5d3c641af9). Preparing the PR now. @mik-laj a couple points for clarification. I added a unit test for the fix and it passed. I tried using the `snowflake.sqlalchemy.URL`, but during running `pytest`, it caused issues with generating the URL with nested dict values for session_parameters, so I reverted it to use string format, as is currently being used in the mainline branch.",,,,,,Anecdotal,comment,,,,,,,,2021-12-22,github/harishkrao,https://github.com/apache/airflow/issues/20032#issuecomment-999307613,repo: apache/airflow | issue: Snowflake Provider - Hook's support for not providing a region is broken when using SQLAlchemy | keyword: pro tip
"add more accurate typing for DbApiHook.run method # Overview The `DbApiHook.run` method has incredibly complex typing, which is currently reduced to just the following: ```python def run( self, sql: str | Iterable[str], autocommit: bool = False, parameters: Iterable | Mapping | None = None, handler: Callable | None = None, split_statements: bool = True, return_last: bool = True, ) -> Any | list[Any] | None: ``` This is incomplete typing for a few reasons: - Some of the `bool`s in the kwargs det…",,,,,,Anecdotal,issue,,,,,,,,2023-06-11,github/dwreeves,https://github.com/apache/airflow/pull/31846,repo: apache/airflow | keyword: pro tip | state: closed
"The scope of the PR increased a little bit here. Hope that isn't too bad. - I replaced all `parameters: Iterable | Mapping | None`s with `parameters: Iterable | Mapping[str, Any] | None`s. - I also type-annotated most (but not all-- some I wasn't sure whether they hooked into a SQLAlchemy connection) `parameters` kwargs that were un-annotated that I could find. - I removed two superfluous `run()` methods for `DbApiHook` subclasses. These were just doing `return super().run(...)` with no other c…",,,,,,Anecdotal,comment,,,,,,,,2023-06-13,github/dwreeves,https://github.com/apache/airflow/pull/31846#issuecomment-1589713114,repo: apache/airflow | issue: add more accurate typing for DbApiHook.run method | keyword: pro tip
"@uranusjr Coming back to / checking in on this. ---- I think the main outstanding thing with this PR is in regards of what to do with default arguments in overloads. - **(1) Excluding the defaults entirely** (something you suggested) causes MyPy to fail due to incompatible typing. - The current approach includes **(2) overloaded defaults that match the actual defaults**, as per your initial request. It requires adding some `# type: ignore`s though. - Another approach that MyPy will pass is to u…",,,,,,Anecdotal,comment,,,,,,,,2023-06-18,github/dwreeves,https://github.com/apache/airflow/pull/31846#issuecomment-1595942281,repo: apache/airflow | issue: add more accurate typing for DbApiHook.run method | keyword: pro tip
"(Forgot to respond to the comment) > default arguments in overloads Using `...` is probably the best we can do, and slightly better than `ignore` comments. Not perfect, but unless we explode the overloads (cross-product with and without argument variants) it’s the best we can do.",,,,,,Anecdotal,comment,,,,,,,,2023-06-19,github/uranusjr,https://github.com/apache/airflow/pull/31846#issuecomment-1596612096,repo: apache/airflow | issue: add more accurate typing for DbApiHook.run method | keyword: pro tip
"Clean up f-strings in logging calls This PR cleans up f-string use in logging calls throughout. This is part of initial prep to restrict f-string use in logging calls via CI in the future. --- **^ Add meaningful description above** Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information. In case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRF…",,,,,,Anecdotal,issue,,,,,,,,2022-05-09,github/josh-fell,https://github.com/apache/airflow/pull/23597,repo: apache/airflow | keyword: pro tip | state: closed
It's not entirely clear to me why there are so many svg file changes from running static checks locally. Is this expected with the new Breeze?,,,,,,Anecdotal,comment,,,,,,,,2022-05-09,github/josh-fell,https://github.com/apache/airflow/pull/23597#issuecomment-1121436529,repo: apache/airflow | issue: Clean up f-strings in logging calls | keyword: pro tip
"The problem is that you have still old rich in the breeze environment (and there is a single change in the commands - adding new static check entry). Run `breeze self-upgrade`, remove `images/breeze/output-commands-hash.txt` and run `breeze static-checks --last-commit`. It should help.",,,,,,Anecdotal,comment,,,,,,,,2022-05-09,github/potiuk,https://github.com/apache/airflow/pull/23597#issuecomment-1121443256,repo: apache/airflow | issue: Clean up f-strings in logging calls | keyword: pro tip
"You can also find all places where rich is added in dependencies (there are a few places in .pre-commit.config, in few setup.py and change to `rich>=12.4.1` - that should force everyone to a new rich next time. With this change - it should help in general for anyone modifying breeze code in the future.",,,,,,Anecdotal,comment,,,,,,,,2022-05-09,github/potiuk,https://github.com/apache/airflow/pull/23597#issuecomment-1121445448,repo: apache/airflow | issue: Clean up f-strings in logging calls | keyword: pro tip
With new rich there should also be differences in generateda images but far less of those Just some css hash changes + the static check list updated.,,,,,,Anecdotal,comment,,,,,,,,2022-05-09,github/potiuk,https://github.com/apache/airflow/pull/23597#issuecomment-1121450852,repo: apache/airflow | issue: Clean up f-strings in logging calls | keyword: pro tip
Still strange that the image generated uses old rich. Did you run `breeze self-upgrade` before running the static check? I guess possibly adding rich>=12.4.1 might be needed.,,,,,,Anecdotal,comment,,,,,,,,2022-05-09,github/potiuk,https://github.com/apache/airflow/pull/23597#issuecomment-1121489594,repo: apache/airflow | issue: Clean up f-strings in logging calls | keyword: pro tip
"The PR most likely needs to run full matrix of tests because it modifies parts of the core of Airflow. However, committers might decide to merge it quickly and take the risk. If they don't merge it quickly - please rebase it to the latest main at your convenience, or amend the last commit of the PR, and push it with --force-with-lease.",,,,,,Anecdotal,comment,,,,,,,,2022-05-23,github/github-actions[bot],https://github.com/apache/airflow/pull/23597#issuecomment-1134084723,repo: apache/airflow | issue: Clean up f-strings in logging calls | keyword: pro tip
"Split redshift cluster and redshift sql The first redshift hook was for managing the cluster itself. Later a hook for _using_ the cluster was added to the same module. Better to separate these into distinct modules redshift_sql and redshift_cluster. Here we split the redshift modules for operators, hooks, and sensors.",,,,,,Anecdotal,issue,,,,,,,,2021-12-13,github/dstandish,https://github.com/apache/airflow/pull/20276,repo: apache/airflow | keyword: pro tip | state: closed
"The PR most likely needs to run full matrix of tests because it modifies parts of the core of Airflow. However, committers might decide to merge it quickly and take the risk. If they don't merge it quickly - please rebase it to the latest main at your convenience, or amend the last commit of the PR, and push it with --force-with-lease.",,,,,,Anecdotal,comment,,,,,,,,2021-12-14,github/github-actions[bot],https://github.com/apache/airflow/pull/20276#issuecomment-993535151,repo: apache/airflow | issue: Split redshift cluster and redshift sql | keyword: pro tip
"> The deprecation warnings should be added to ""known"" list and tests are failing unfortunately :) thanks wil work on it",,,,,,Anecdotal,comment,,,,,,,,2021-12-14,github/dstandish,https://github.com/apache/airflow/pull/20276#issuecomment-993670687,repo: apache/airflow | issue: Split redshift cluster and redshift sql | keyword: pro tip
"> The deprecation warnings should be added to ""known"" list and tests are failing unfortunately :) Hey @potiuk , can you point me the way here... Providers manager is warning here: ``` if already_registered.hook_class_name != hook_class_name: log.warning( ""The hook connection type '%s' is registered twice in the"" "" package '%s' with different class names: '%s' and '%s'. "" "" Please fix it!"", hook_info.connection_type, package_name, already_registered.hook_class_name, hook_class_name, ) ``` This c…",,,,,,Anecdotal,comment,,,,,,,,2021-12-14,github/dstandish,https://github.com/apache/airflow/pull/20276#issuecomment-993683446,repo: apache/airflow | issue: Split redshift cluster and redshift sql | keyword: pro tip
> Providers manager is warning here: > if already_registered.hook_class_name != hook_class_name: This warning is not in the CI. I believe this is caused by .pyc file which remained from the Hook being present in the old location when you moved the .py file (in your local development environment). This is one of the main reasons in Breeze we have this: ``` # Disable writing .pyc files - slightly slower imports but not messing around when switching # Python version and avoids problems with root-o…,,,,,,Anecdotal,comment,,,,,,,,2021-12-14,github/potiuk,https://github.com/apache/airflow/pull/20276#issuecomment-993816084,repo: apache/airflow | issue: Split redshift cluster and redshift sql | keyword: pro tip
> > Providers manager is warning here: > > if already_registered.hook_class_name != hook_class_name: > > This warning is not in the CI. I believe this is caused by .pyc file which remained from the Hook being present in the old location when you moved the .py file (in your local development environment). This is one of the main reasons in Breeze we have this: > > ``` > # Disable writing .pyc files - slightly slower imports but not messing around when switching > # Python version and avoids prob…,,,,,,Anecdotal,comment,,,,,,,,2021-12-14,github/dstandish,https://github.com/apache/airflow/pull/20276#issuecomment-993820052,repo: apache/airflow | issue: Split redshift cluster and redshift sql | keyword: pro tip
Ah. i see. See the FULL stack trace there (The summary of error does not have the warning just assert - but in the output of the test you have the whole error message: https://github.com/apache/airflow/runs/4515244414?check_suite_focus=true#step:6:4063,,,,,,Anecdotal,comment,,,,,,,,2021-12-14,github/potiuk,https://github.com/apache/airflow/pull/20276#issuecomment-993836002,repo: apache/airflow | issue: Split redshift cluster and redshift sql | keyword: pro tip
"You need to unfold the failed ""core"" tests to see it all: ``` tests/core/test_providers_manager.py:151: AssertionError ----------------------------- Captured stdout call ----------------------------- [2021-12-14 03:43:05,390] {providers_manager.py:511} WARNING - The hook connection type 'aws' is registered twice in the package 'apache-airflow-providers-amazon' with different class names: 'airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook' and 'airflow.providers.amazon.aws.hooks.redshift_c…",,,,,,Anecdotal,comment,,,,,,,,2021-12-14,github/potiuk,https://github.com/apache/airflow/pull/20276#issuecomment-993837172,repo: apache/airflow | issue: Split redshift cluster and redshift sql | keyword: pro tip
@dstandish would it be possible to handle the class name changes on this PR as well? https://github.com/apache/airflow/issues/20296 The modifications for that are relatively small. We shouldn't have `AwsRedshiftClusterSensor`. It should be: `from airflow.providers.amazon.aws.sensors.redshift_cluster import RedshiftClusterSensor`,,,,,,Anecdotal,comment,,,,,,,,2021-12-14,github/eladkal,https://github.com/apache/airflow/pull/20276#issuecomment-993939869,repo: apache/airflow | issue: Split redshift cluster and redshift sql | keyword: pro tip
> @dstandish There are more deprecation messages that need changing Ok will search again tomorrow thought I got everything,,,,,,Anecdotal,comment,,,,,,,,2021-12-15,github/dstandish,https://github.com/apache/airflow/pull/20276#issuecomment-994388280,repo: apache/airflow | issue: Split redshift cluster and redshift sql | keyword: pro tip
"Add ShortCircuitOperator configurability for respecting downstream trigger rules Closes: #7858 Adding the ability to configure the `ShortCircuitOperator` to respect trigger rules for downstream tasks. Currently this operator ignores all trigger rules and forcibly skips all downstream tasks. However, there are use cases in which downstream tasks from the `ShortCircuitOperator` have trigger rules applied such that said tasks should execute even if upstream tasks are skipped by the operator (e.g. …",,,,,,Anecdotal,issue,,,,,,,,2021-08-04,github/josh-fell,https://github.com/apache/airflow/pull/17421,repo: apache/airflow | keyword: pro tip | state: closed
"The PR most likely needs to run full matrix of tests because it modifies parts of the core of Airflow. However, committers might decide to merge it quickly and take the risk. If they don't merge it quickly - please rebase it to the latest main at your convenience, or amend the last commit of the PR, and push it with --force-with-lease.",,,,,,Anecdotal,comment,,,,,,,,2021-09-02,github/github-actions[bot],https://github.com/apache/airflow/pull/17421#issuecomment-911445602,repo: apache/airflow | issue: Add ShortCircuitOperator configurability for respecting downstream trigger rules | keyword: pro tip
> @josh-fell can you take a look at the test failures? Should be fixed. There were other changes made to `test_python.py` that required the `logging` package which I had removed as part of this PR. Added the import back.,,,,,,Anecdotal,comment,,,,,,,,2021-11-29,github/josh-fell,https://github.com/apache/airflow/pull/17421#issuecomment-981683414,repo: apache/airflow | issue: Add ShortCircuitOperator configurability for respecting downstream trigger rules | keyword: pro tip
~Apparently I shouldn't have rebased from the last commit in the branch which was the merge from `main` to this branch~Something went awry with the rebase. Wiped all of my changes and closed the PR. Can't figure out what but maybe the wrong hash was generated by `merge-base`. Not sure but should have verified. Sad panda. I have opened a new PR - #20044. Sorry about all this gents.,,,,,,Anecdotal,comment,,,,,,,,2021-12-05,github/josh-fell,https://github.com/apache/airflow/pull/17421#issuecomment-986163436,repo: apache/airflow | issue: Add ShortCircuitOperator configurability for respecting downstream trigger rules | keyword: pro tip
"Azure: New sftp to wasb operator <!-- How to write a good git commit message: http://chris.beams.io/posts/git-commit/ --> closes: #9683 This operator helps to extract data from SFTP and load it to an Azure Wasb. --- **^ Add meaningful description above** Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information. In case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/…",,,,,,Anecdotal,issue,,,,,,,,2021-10-10,github/wolvery,https://github.com/apache/airflow/pull/18877,repo: apache/airflow | keyword: pro tip | state: closed
"The PR most likely needs to run full matrix of tests because it modifies parts of the core of Airflow. However, committers might decide to merge it quickly and take the risk. If they don't merge it quickly - please rebase it to the latest main at your convenience, or amend the last commit of the PR, and push it with --force-with-lease.",,,,,,Anecdotal,comment,,,,,,,,2021-12-21,github/github-actions[bot],https://github.com/apache/airflow/pull/18877#issuecomment-998792354,repo: apache/airflow | issue: Azure: New sftp to wasb operator | keyword: pro tip
"Fix mypy errors in example DAGs Part of #19891 --- **^ Add meaningful description above** Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information. In case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed. In case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://…",,,,,,Anecdotal,issue,,,,,,,,2021-12-01,github/josh-fell,https://github.com/apache/airflow/pull/19918,repo: apache/airflow | keyword: pro tip | state: closed
"> @josh-fell - still waiting on this or shall we close it? I had a poorly-worded question to you and wanted to know what you had agreed to above. Is it OK to move forward with adding the `# type: ignore[call-args]` comment to the individual operators in these DAGs (rather than the current ""fix"" of a blanket ignore on the file) or wait for the more proper parallel changes that are happening (something to effect of not needing all the ignore comments if an underlying module was fixed)? If it's th…",,,,,,Anecdotal,comment,,,,,,,,2021-12-14,github/josh-fell,https://github.com/apache/airflow/pull/19918#issuecomment-993073218,repo: apache/airflow | issue: Fix mypy errors in example DAGs | keyword: pro tip
"I think we should do proper-fixes then :) this is not a blocker at all - I just started to think that this one might be closed as soon as all the parallell ""smaller"" fixes are merged, because they will fix everything here ;)",,,,,,Anecdotal,comment,,,,,,,,2021-12-14,github/potiuk,https://github.com/apache/airflow/pull/19918#issuecomment-993447431,repo: apache/airflow | issue: Fix mypy errors in example DAGs | keyword: pro tip
"> I think we should do proper-fixes then :) this is not a blocker at all - I just started to think that this one might be closed as soon as all the parallell ""smaller"" fixes are merged, because they will fix everything here ;) Yeah this might be something we tackle last (i.e. getting MyPy to ""understand"" `default_args` in DAGs as to not throw an argument-call error). I'm inclined to close this for now.",,,,,,Anecdotal,comment,,,,,,,,2021-12-15,github/josh-fell,https://github.com/apache/airflow/pull/19918#issuecomment-994891324,repo: apache/airflow | issue: Fix mypy errors in example DAGs | keyword: pro tip
"Support for Storing and Retrieving Complex Data Types (e.g., Embeddings) in Pandas DataFrames ### Feature Type - [x] Adding new functionality to pandas - [x] Changing existing functionality in pandas - [x] Removing existing functionality in pandas ### Problem Description Currently, Pandas does not provide a native way to store and retrieve complex data types like NumPy arrays (e.g., embeddings) in formats such as CSV without converting them to strings. This results in a loss of structure and re…",,,,,,Anecdotal,issue,,,,,,,,2025-02-09,github/ashishjaimongeorge,https://github.com/pandas-dev/pandas/issues/60895,repo: pandas-dev/pandas | keyword: best practice | state: open
"Thanks for the request, why can't the `dtype` argument be used here: ```python pd.DataFrame({""a"": [1 + 3j, 1.5 + 4.5j, 1/3]}).to_csv(""test.csv"", index=False) print(pd.read_csv(""test.csv"", dtype={""a"": np.complex128}, engine=""python"")) a 0 1.000000+3.000000j 1 1.500000+4.500000j 2 0.333333+0.000000j ``` Note you need to use the engine `python` rather than `c` as the latter does not (yet) support complex. I think adding support for complex to the `c` engine would be welcome.",,,,,,Anecdotal,comment,,,,,,,,2025-02-09,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/60895#issuecomment-2646345348,"repo: pandas-dev/pandas | issue: Support for Storing and Retrieving Complex Data Types (e.g., Embeddings) in Pandas DataFrames | keyword: best practice"
Thank you for the clarification! I am interested in working on adding support for complex numbers to the C engine. Could you please guide me on how to get started or point me to any relevant developer resources? I look forward to contributing.,,,,,,Anecdotal,comment,,,,,,,,2025-02-20,github/ashishjaimongeorge,https://github.com/pandas-dev/pandas/issues/60895#issuecomment-2672154385,"repo: pandas-dev/pandas | issue: Support for Storing and Retrieving Complex Data Types (e.g., Embeddings) in Pandas DataFrames | keyword: best practice"
"Hi, here is an implementation of the suggested solution described, would it be possible to have it reviewed? There is a couple things (like that I am unable to run pytests for some reason) that should be addressed, but it is passing the tests that I have written. https://github.com/pandas-dev/pandas/pull/61157",,,,,,Anecdotal,comment,,,,,,,,2025-03-28,github/Jaspvr,https://github.com/pandas-dev/pandas/issues/60895#issuecomment-2762673242,"repo: pandas-dev/pandas | issue: Support for Storing and Retrieving Complex Data Types (e.g., Embeddings) in Pandas DataFrames | keyword: best practice"
"My apologies for the accidental input earlier. Could you also clarify if the above PR addresses the issue, and whether we're exploring any other approaches?",,,,,,Anecdotal,comment,,,,,,,,2025-04-15,github/ashishjaimongeorge,https://github.com/pandas-dev/pandas/issues/60895#issuecomment-2804973491,"repo: pandas-dev/pandas | issue: Support for Storing and Retrieving Complex Data Types (e.g., Embeddings) in Pandas DataFrames | keyword: best practice"
"As commented on the PR, I am negative on adding a keyword to `read_csv` when it seems to me that `dtype` already supports this. I am not aware of any other work on this issue.",,,,,,Anecdotal,comment,,,,,,,,2025-04-15,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/60895#issuecomment-2805136011,"repo: pandas-dev/pandas | issue: Support for Storing and Retrieving Complex Data Types (e.g., Embeddings) in Pandas DataFrames | keyword: best practice"
"While dtype works, it’s not the most user-friendly solution for everyone. For users who aren’t deeply familiar with read_csv’s options—or the fact that they need to switch to the Python engine for complex types—it can feel a bit complex and unintuitive. They have to: > Know that dtype supports complex types like np.complex128. > Understand that the C engine doesn’t support complex numbers yet, so they must specify engine=""python"". > Manually set up the dtype dictionary for each relevant column.…",,,,,,Anecdotal,comment,,,,,,,,2025-04-16,github/ashishjaimongeorge,https://github.com/pandas-dev/pandas/issues/60895#issuecomment-2808618857,"repo: pandas-dev/pandas | issue: Support for Storing and Retrieving Complex Data Types (e.g., Embeddings) in Pandas DataFrames | keyword: best practice"
"> Why the Proposed Feature is Needed While these alternatives exist, they require either additional dependencies, manual preprocessing, or compromise on format usability. Adding native support for preserving and restoring complex data types in Pandas CSV operations would: > > Eliminate the need for external libraries like JSON, Pickle, or Parquet. Improve usability for machine learning and data science workflows. Keep CSV files human-readable while ensuring data integrity. It feels like the pro…",,,,,,Anecdotal,comment,,,,,,,,2025-04-16,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/60895#issuecomment-2809264092,"repo: pandas-dev/pandas | issue: Support for Storing and Retrieving Complex Data Types (e.g., Embeddings) in Pandas DataFrames | keyword: best practice"
"Thanks @Priya09153 for the interest but there is no agreed way forward here. Both myself and @rhshadrach have effectively rejected the enhancement request so far. I would not be adverse to closing the issue as ""won't fix"" however also happy to keep the issue open to allow further discussion towards a potentially acceptable solution.",,,,,,Anecdotal,comment,,,,,,,,2025-06-08,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/60895#issuecomment-2953999117,"repo: pandas-dev/pandas | issue: Support for Storing and Retrieving Complex Data Types (e.g., Embeddings) in Pandas DataFrames | keyword: best practice"
"I would like this issue to remain open to support complex with `dtype` using the C-engine. Alternatively, I wouldn't be adverse with opening a new issue for this and closing this one. If we did have support with the C-engine, it seems to me we could consider automatically inferring complex data. I have sense as to whether this would be a good idea or not. But I am very much opposed to adding yet another option to `read_csv`.",,,,,,Anecdotal,comment,,,,,,,,2025-06-08,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/60895#issuecomment-2954044378,"repo: pandas-dev/pandas | issue: Support for Storing and Retrieving Complex Data Types (e.g., Embeddings) in Pandas DataFrames | keyword: best practice"
"> I would like this issue to remain open to support complex with dtype using the C-engine. Thanks @rhshadrach for clarifying. To expand on your previous comment https://github.com/pandas-dev/pandas/issues/60895#issuecomment-2646345348 ```python import pandas as pd import io # Create a CSV file in memory csv_data = io.StringIO() csv_data.write(""a\n1.000000+3.000000j\n1.500000+4.500000j\n0.333333+0.000000j\n"") csv_data.seek(0) # Reset the file pointer to the beginning # Read the CSV into Pandas w…",,,,,,Anecdotal,comment,,,,,,,,2025-06-08,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/60895#issuecomment-2954141764,"repo: pandas-dev/pandas | issue: Support for Storing and Retrieving Complex Data Types (e.g., Embeddings) in Pandas DataFrames | keyword: best practice"
"ENH: Make DataFrame.filter accept filters in new formats I think it'd be very nice for users to get this working regarding [filter](https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.filter.html): ```python df.filter(df[""age""] > 18) # same as `df[df[""age""] > 18` df.filter(""age > 18"") # same as `df.query(""age > 18"")`, I think `.query` should be deprecated if this is implemented in `.filter` df.filter(lambda df: df[""age""] > 18) # same as `df[df['age'].apply(lambda x: x > 18)]`, use…",,,,,,Anecdotal,issue,,,,,,,,2025-04-20,github/datapythonista,https://github.com/pandas-dev/pandas/issues/61317,repo: pandas-dev/pandas | keyword: best practice | state: open
"Partial proposal: - Accept `items` (will maybe want to rename this argument?) of type: - Series (will align on index) - Non-Series list-likes (must be same length as df) - strings a la `query` - UDFs to be discussed. - Deprecate `like`, `regex`; offer no alternatives. - Deprecate `axis=1` but add `DataFrame.select` (somewhat talked about in https://github.com/pandas-dev/pandas/issues/55289). For UDFs, it seems to me that the usage in the OP can be readily handled by `pipe`. I would more expect …",,,,,,Anecdotal,comment,,,,,,,,2025-04-20,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2817202254,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
"I like the idea. If I understand correctly, the main use of `df.filter(cond)` where cond is a `Series` will be equivalent to now use `df[cond]`. I think implementing the `like` and `regex` behaviors would be trivial with `df.filter(df[""col""].str.contains(""xxx""))` and same for `regex`, right? It does feel we're offering a very reasonable alternative. I see your point for using `.pipe` to filter, and in a way kind of agree. But it feels like `df.filter(lambda x: x[""age""] > 18)` will be together w…",,,,,,Anecdotal,comment,,,,,,,,2025-04-20,github/datapythonista,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2817212049,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
"> I think implementing the `like` and `regex` behaviors would be trivial with `df.filter(df[""col""].str.contains(""xxx""))` and same for `regex`, right? It does feel we're offering a very reasonable alternative. Agreed - I should have said no _new_ alternatives. :laughing: For UDFs, one reason not to have `df.filter(lambda x: x[""age""] > 18)` operate by row is that it is effectively a transpose (`x` being a Series means it can only have one dtype), one of the behaviors I would love to remove from p…",,,,,,Anecdotal,comment,,,,,,,,2025-04-22,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2822457019,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
"> Overall, I lean toward operate by-row here, but not strongly. `DataFrame.filter` does not filter a Dataframe on its contents, the filter is applied to the labels of the index. The suggestion in the OP is to essentially add value based conditional filtering to this method. If you operate by row, (or by column if the axis argument is retained), then if you passed a Series with the Series.name set to the index label then it would be easier to filter based on the index label and thereby potential…",,,,,,Anecdotal,comment,,,,,,,,2025-04-23,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2824051737,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
"I see your point @rhshadrach, and I think what you propose is very reasonable and maybe even thr best option in theory. In practice, I would be very surprised if most users don't find the pyspark-like API of the function receiving the whole dataframe more intuitive. See [this example](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html) in their docs: ```python df.filter(df.age > 3).show() ``` We can't compare directly with a lazy API, but…",,,,,,Anecdotal,comment,,,,,,,,2025-04-23,github/datapythonista,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2824166423,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
"Maybe I'm missing something, but why deprecate `query()`. I have LOTS of code that uses that. Why not leave `filter` as is - it operates on labels - and maybe expand `query()` to take expressions as proposed here. So that `df.query(df[""age""] > 18)` and `df.query(""age > 18"")` would do the same thing",,,,,,Anecdotal,comment,,,,,,,,2025-04-23,github/Dr-Irv,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2824382437,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
"That's a reasonable option. I think filter is more clear, and is what everybody else is using. If we were to implement the API from scratch now, I think it would be the obvious choice. For backward compatibility query may be better, and we can surely consider it. But I would rather have a very long deprecation timeline, than keep the API IMHO wrong because of a choice we did that now is not ideal.",,,,,,Anecdotal,comment,,,,,,,,2025-04-23,github/datapythonista,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2824439041,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
"> Why not leave filter as is - it operates on labels Because it's at odds with other DataFrame libraries. - PySpark: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html - Polars: https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.filter.html - ibis: https://ibis-project.org/reference/expression-tables.html#ibis.expr.types.relations.Table.filter The exceptions are Modin and dask, but I think they were designed t…",,,,,,Anecdotal,comment,,,,,,,,2025-04-25,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2831321640,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
"> > Why not leave filter as is - it operates on labels > > Because it's at odds with other DataFrame libraries. But those libraries were introduced _after_ `pandas` ! So shouldn't THEY be modifying _their_ API's? (I say this somewhat facetiously) > In addition, I would argue query is an odd choice of a name for a filtering method. Then maybe you think that SQL (Structured Query Language) should be called SFL (Structured Filtering Language) ? (definitely said facetitously) To me `query` is appro…",,,,,,Anecdotal,comment,,,,,,,,2025-04-25,github/Dr-Irv,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2831353001,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
"For a method that returns a subset of rows based on a condition, I think the standard terminology is filter. Query seems more appropriate for a more complex expression that can get data doing operations that not only involve a filter. I think SQL is consistent with this, since it allows to do more than the WHERE clause. And I think query feels inappropriate. `df.where` would be consistent with SQL, but to me filter is clearly the right choice.",,,,,,Anecdotal,comment,,,,,,,,2025-04-25,github/datapythonista,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2831370075,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
> To me query is appropriate if you think of it from the perspective of how SQL works. Queries in SQL can do so much more than filter. `DataFrame.query` can only filter. I think this is supporting my contention that `query` is an odd name.,,,,,,Anecdotal,comment,,,,,,,,2025-04-25,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2831401276,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
"> Why not leave `filter` as is - it operates on labels - and maybe expand `query()` to take expressions as proposed here. That makes sense to me as we would not be mixing label based ""filtering"" with value based ""filtering"" > I think filter is more clear, and is what everybody else is using. If we were to implement the API from scratch now, I think it would be the obvious choice. This also makes sense to me. So the issue is how to make this transition. If we don't mix the label based and value …",,,,,,Anecdotal,comment,,,,,,,,2025-04-26,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2831968914,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
"> When we deprecate we say something like ""x is deprecated. use ... instead"". [@rhshadrach](https://github.com/rhshadrach) can you clarify what the ... would be? `df.filter(df[""col""].index.str.contains(""xxx""))` (or same with square brackets)... this was discussed above in one of the many replies, Richard meant no new specific alternative, and all the existing filter method funcionality is already possible (and personally I'd bet that the alternative using a boolean mask based on the index attri…",,,,,,Anecdotal,comment,,,,,,,,2025-04-26,github/datapythonista,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2831973882,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
"> and personally I'd bet that the alternative using a boolean mask based on the index attribute may already be more popular than the filter method. Yes, boolean indexing is one of the core strengths of pandas and remains the best practice for filtering data. Its clarity and explicit nature make it ideal for developers who want to see exactly which rows or columns are being selected—for example, using expressions like `df[df['age'] > 18]` directly leaves little room for ambiguity. In contrast, c…",,,,,,Anecdotal,comment,,,,,,,,2025-04-26,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2831982440,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
"I think the existing API is already duplicated, as you mention, filter is syntactic sugar for 3 very particular use cases (I personally never used). I don't think the square brackets is a good API for method chaining, so I'm happy with the duplication after the changes proposed here. Also, after having used both pyspark and polars, I find the filter method with a condition one of the essential functionality of a dataframe library. If we manage to implement the syntax below, I think it'll be the…",,,,,,Anecdotal,comment,,,,,,,,2025-04-26,github/datapythonista,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2831992761,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
"> And as a first step it needs the changes proposed in this issue. If DataFrame.filter did not already exist and do something different it would definitely be more straightforward to implement this. > I think the existing API is already duplicated, as you mention, filter is syntactic sugar for 3 very particular use cases (I personally never used). I don't disagree. Let me think on this some more. > I don't think the square brackets is a good API for method chaining, so I'm happy with the duplic…",,,,,,Anecdotal,comment,,,,,,,,2025-04-26,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2831998703,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
pinging @jorisvandenbossche for input as participant https://github.com/pandas-dev/pandas/issues/12401 and the follow up open issue #26642. I'm guessing from https://github.com/pandas-dev/pandas/issues/26642#issuecomment-511080719 that @jorisvandenbossche may want to retain the syntatic sugar that .filter offers but is not adverse to renaming the method.,,,,,,Anecdotal,comment,,,,,,,,2025-04-26,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2832026971,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
"This is an interesting discussion, and I'm glad some of the old issues were linked in to give a historical context. Here is how I see things (and feel free to disagree): 1. Way back when, `pandas` had a `DataFrame.select()` method and `DataFrame.filter()` method, both which operated on labels. `DataFrame.select()` was deprecated and removed because of the functionality of `DataFrame.loc` and `DataFrame.filter()`. 2. There has been debate over time regarding the various ways people can filter da…",,,,,,Anecdotal,comment,,,,,,,,2025-04-27,github/Dr-Irv,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2833609539,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
"> One thing that I want to point out about `query()`, which I find very useful, is that it allows filtering on a combination of labels and data. `.filter` will behave exactly the same on string input. And I am fine with keeping `query`. I have a bit of a preference to have a long period of discouraging it and then deprecating, but I'm fine with it remaining an alias for `filter` (on string inputs) indefinitely. > how to create a transition path for those using `filter()` for filtering on labels…",,,,,,Anecdotal,comment,,,,,,,,2025-04-28,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2836593049,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
"BUG: ""str.contains"" match groups UserWarning discourages best practices & slows performance ### Pandas version checks - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas. ### Re…",,,,,,Anecdotal,issue,,,,,,,,2024-01-09,github/bionicles,https://github.com/pandas-dev/pandas/issues/56798,repo: pandas-dev/pandas | keyword: best practice | state: open
"follow up: for easy wins in pandas performance, a review of all the logical conditions for warning in pandas might be warranted, if we're compiling stuff just to check if we ought to warn people, then we could remove these warnings where possible to speed everything up and simplify the codebase",,,,,,Anecdotal,comment,,,,,,,,2024-01-09,github/bionicles,https://github.com/pandas-dev/pandas/issues/56798#issuecomment-1882959700,"repo: pandas-dev/pandas | issue: BUG: ""str.contains"" match groups UserWarning discourages best practices & slows performance | keyword: best practice"
Thanks for the report. There can be a performance hit on the actual operation when using capturing vs non-capturing groups however. I think this might be another good case for #55385.,,,,,,Anecdotal,comment,,,,,,,,2024-01-09,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/56798#issuecomment-1883857328,"repo: pandas-dev/pandas | issue: BUG: ""str.contains"" match groups UserWarning discourages best practices & slows performance | keyword: best practice"
"+1 to this request. This issue still exists in April 2025. I am using `str.contains` to test whether anything matches. The `contains` method explicitly does not capture anything, so having a warning about the regex containing capturing groups is pointless.",,,,,,Anecdotal,comment,,,,,,,,2025-04-30,github/bestekov,https://github.com/pandas-dev/pandas/issues/56798#issuecomment-2843561274,"repo: pandas-dev/pandas | issue: BUG: ""str.contains"" match groups UserWarning discourages best practices & slows performance | keyword: best practice"
"API: Design questions for HDFStore.append I'm new and am trying to put together a PR for documentation improvements surrounding `HDFStore`. I've been looking at `HDFStore.append()` and have encountered several instances where I am unsure of what the best practice is: - Many parameters are unused or have no effect by nature. Do we keep these and document them with an additional statement that they are unused, or do we remove them/un-document them? - Some parameters do have effects but their vari…",,,,,,Anecdotal,issue,,,,,,,,2025-02-12,github/JakeTT404,https://github.com/pandas-dev/pandas/issues/60920,repo: pandas-dev/pandas | keyword: best practice | state: open
"Thanks for the report! I think what you're doing is raising the question of whether or not there are certain deficiencies that should be corrected or merely documented. That's the right approach, but pandas uses `QST` for questions from users about using pandas. I've reworked this as a API report. > Many parameters are unused or have no effect by nature. Can you detail these? > Some parameters do have effects but their variable names are misleading such as `min_itemsize` This goes back to https…",,,,,,Anecdotal,comment,,,,,,,,2025-02-12,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/60920#issuecomment-2654998248,repo: pandas-dev/pandas | issue: API: Design questions for HDFStore.append | keyword: best practice
"> Is there a point in having an append parameter Ah, I confused `df.to_hdf` with `HDFStore.append`; I think you're talking about the latter. This is less clear to me, will need to dig into it more. I think this also clears up my confusion on unused parameters. I haven't checked, but could the signature include unused parameters for a consistent signature across many functions, others of which do use the parameters? I have not checked.",,,,,,Anecdotal,comment,,,,,,,,2025-02-12,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/60920#issuecomment-2655003476,repo: pandas-dev/pandas | issue: API: Design questions for HDFStore.append | keyword: best practice
"I have put together the following notes regarding the parameters for the append function. - `format` for append can only be ""table"" as ""fixed"" does not support appending (remove?). - `axes` parameter must be an iterable containing a single value for the selection of a dataframe axis. It can only be two values `[0]` or `[1]` which change the axis used for indexing. from a user perspective, this only affects on which axis data is appended (document. rename possible but probably best not to). - `i…",,,,,,Anecdotal,comment,,,,,,,,2025-02-13,github/JakeTT404,https://github.com/pandas-dev/pandas/issues/60920#issuecomment-2655424948,repo: pandas-dev/pandas | issue: API: Design questions for HDFStore.append | keyword: best practice
"DOC: Comparing .loc/.iloc to tuples and chained indexing ### Pandas version checks - [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/) ### Location of the documentation https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#indexing-view-versus-copy ### Documentation problem ``` import pandas as pd # Creating a DataFrame with some sample data data = { 'Name': ['Jason', 'Emma', 'Alex', 'Sarah'], …",,,,,,Anecdotal,issue,,,,,,,,2024-12-31,github/joansigh,https://github.com/pandas-dev/pandas/issues/60632,repo: pandas-dev/pandas | keyword: best practice | state: open
"Thanks for the report! > The documentation mentions how .iloc/.loc is a better option. For example, something such as the following. > > `df.loc[df['Name'] == 'Jason', 'Age'] = 29` > > However it is not clear about best practices regarding tuples, such as the following. > > `df[('Age', df['Name'] == 'Jason')] = 29 ` The following two lines are equivalent: ```python df[('Age', df['Name'] == 'Jason')] = 29 df['Age', df['Name'] == 'Jason'] = 29 ``` That is, the argument in the 2nd line above being…",,,,,,Anecdotal,comment,,,,,,,,2024-12-31,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/60632#issuecomment-2566584368,repo: pandas-dev/pandas | issue: DOC: Comparing .loc/.iloc to tuples and chained indexing | keyword: best practice
"Gotcha, so anything with a comma is a tuple? Even though my two examples both involve tuples, the example that included loc would still be preferred because it is faster than the other example that didn't use loc?",,,,,,Anecdotal,comment,,,,,,,,2025-01-01,github/joansigh,https://github.com/pandas-dev/pandas/issues/60632#issuecomment-2567140936,repo: pandas-dev/pandas | issue: DOC: Comparing .loc/.iloc to tuples and chained indexing | keyword: best practice
"Suggestion: remove tests from the distribution Would it make sense to remove `tests` folder from the pandas distribution? It takes roughly 33% of the whole package weight. It is especially important when using pandas inside the AWS Lambdas, where the deployment package size is limited to 50 MB zipped and 5 MB might really make a difference. ``` # Uncompressed du -h -s pandas* 46.5M pandas 30.9M pandas_no_tests # Compressed du -h -s pandas* 14.7M pandas.zip 10.1M pandas_no_tests.zip ```",,,,,,Anecdotal,issue,,,,,,,,2020-01-06,github/vfilimonov,https://github.com/pandas-dev/pandas/issues/30741,repo: pandas-dev/pandas | keyword: best practice | state: open
"I think we've talked about that in the past. We do have `pandas.test()` as part of the public API however, so we'd need to consider that. A couple options: 1. Provide a separate distribution like `pandas-slim` or something that excludes these files (and docs) 2. Have `pandas.test()` fetch the source files on demand. That seems a bit messy though. Just as a note: we do exclude the test data files that are present in the git repository. So we're only talking about source files.",,,,,,Anecdotal,comment,,,,,,,,2020-01-06,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-571189614,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
Hello @TomAugspurger `pandas-slim` sounds like an good workaround. It looks like docs are not a part of the distribution. And right it's tests code only without the data files - in terms of size they are second to `_libs` and almost equal to the rest of the code. ``` 8.0K ./arrays 16K ./errors 24K ./api 40K ./__pycache__ 76K ./compat 88K ./_config 248K ./tseries 308K ./util 440K ./plotting 2.3M ./io 6.7M ./core 17M ./tests 20M ./_libs ``` And what is the reason of having tests as a part of an A…,,,,,,Anecdotal,comment,,,,,,,,2020-01-06,github/vfilimonov,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-571289513,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"Thanks for correcting me on the docs stuff. I'm not sure why all these projects tend to include tests. As a guess, I suspect that we have more issues related to how the packages are built (compilers, flags, libraries we link to) than they do. On Mon, Jan 6, 2020 at 1:56 PM Vladimir Filimonov <notifications@github.com> wrote: > Hello @TomAugspurger <https://github.com/TomAugspurger> > > pandas-slim sounds like an good workaround. > > It looks like docs are not a part of the distribution. > And r…",,,,,,Anecdotal,comment,,,,,,,,2020-01-06,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-571302141,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"I particularly used to run the tests for numpy, scikitlearn and matplotlib after installing, since at times I'd have them fail on Windows. However this was quite some time ago, 4 years ago perhaps. Perhaps other users were doing the same?",,,,,,Anecdotal,comment,,,,,,,,2020-01-07,github/stonecharioteer,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-571420076,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"I'm a package manager for nixpkgs, and I'm against removing tests from the sdist package, however, removing from the wheel would make sense from a packaging standpoint. It's considered best practice in FOSS that if you distribute source, you also distribute tests along side it. EDIT: tests are a nice guarantee that the package is working as intended. We could also checkout the github repo for tests. However, quickly looking at the setup.py, pandas was meant to have the CI set correct metadata s…",,,,,,Anecdotal,comment,,,,,,,,2020-01-09,github/jonringer,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-572685347,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"Hi. i reported this elsewhere, so I'm pasting my comment here. **My use case** After a pip install pandas the `lib/site-packages/pandas/tests/` includes a lot of testing code which is definitely not relevant for me and many other end users of `pandas`. This bloats the installation and makes installation slower. I'm working on packaging a python environment to distribute with a preinstalled set of modules and application and there are too many popular 3rd-party modules which include unneeded tes…",,,,,,Anecdotal,comment,,,,,,,,2020-01-26,github/joaoe,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-578532230,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"A small remark: as a part of recent [commit to pyarrow](https://github.com/apache/arrow/pull/7334#issuecomment-637846927) @wesm removed `pyarrow.tests` from the wheel which to my understanding contributed **2.3 MB** of **~60 MB** installed size. In case of pandas tests folder contributed (as of version 1.0.3) tests folder contributed **17.9 MB** out of **49 MB** installed size. So I'd like to bring the question back to the discussion and perhaps, @wesm could comment on that?",,,,,,Anecdotal,comment,,,,,,,,2020-06-03,github/vfilimonov,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-638000968,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
I think it would be a good idea to not ship the tests in wheels. If you want users to be able to run the tests against their production installs perhaps the tests can be packaged as a separate source wheel. Install size is becoming a problem because of size constraints in things like AWS Lambda.,,,,,,Anecdotal,comment,,,,,,,,2020-06-03,github/wesm,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-638158062,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
https://uwekorn.com/2020/10/28/trimming-down-pyarrow-conda-2-of-x.html has some information. I think I've come around to the idea that we can just not ship the test files in the main `pandas` distributions. We can have a separate `pandas-tests` so that `pip install pandas-tests` package that's just 1. The test files 2. A small `__init__.py` file ties things together We could even update `pandas.test()` to check for the presence of the `pandas-tests` package.,,,,,,Anecdotal,comment,,,,,,,,2020-10-30,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-719609290,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"In https://github.com/rasterio/rasterio-wheels/issues/59, I'm requesting some kind of shared-libs solution that might help the python package ecosystem for binary wheels and a similar request could be made for shared testing libs. In the context of testing, it might/could help to have separate `numpy.testing` and `pandas.testing` projects that might be integrated into a parent `sci-testing` or similar uber-testing package with shared dependencies (it likely impacts other projects that are also …",,,,,,Anecdotal,comment,,,,,,,,2020-10-30,github/dazza-codes,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-719843812,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"I don't use pandas at all, but I saw the link from the numpy issue. It's worth noting that with [implicit namespace packages](https://www.python.org/dev/peps/pep-0420/) you could technically ship `pandas.tests` in a separate wheel and wouldn't break backwards compatibility (other than the new requirement). I've found the primary downside of namespace packages is that editable installations have annoying caveats, but in an installed system it seems to be supported just fine.",,,,,,Anecdotal,comment,,,,,,,,2020-10-31,github/virtuald,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-719874618,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"For the conda side, @xhochy did this for pyarrow here https://github.com/conda-forge/arrow-cpp-feedstock/pull/186, so that could be inspiration for doing the same for the pandas conda package.",,,,,,Anecdotal,comment,,,,,,,,2020-11-03,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-721108878,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"It sounds like maintainers here are generally supportive of cutting tests and docs out of the package. I recently went through this exercise in LightGBM (https://github.com/microsoft/LightGBM/pull/3639, https://github.com/microsoft/LightGBM/pull/3685), and I'd be happy to do the work here to reduce the package size. Would the maintainers here be open to PRs if I started working on this? ### Details & Background I think Wes's comment about AWS Lambda is an important one: https://github.com/panda…",,,,,,Anecdotal,comment,,,,,,,,2020-12-30,github/jameslamb,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-752751976,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"Thanks! I've opened #38846 with a proposal to remove docs. It reduces the size of the sdist package by about 1MB. I think that removing tests needs to be done by maintainers, though, if it will involve distributing a new `pandas-tests` package (https://github.com/pandas-dev/pandas/issues/30741#issuecomment-719609290). I really hope that is picked up, as it would have a significant impact on the size of the package. | | `master` | #38846 | #38846 + removing tests | |:-----------------------:|:--…",,,,,,Anecdotal,comment,,,,,,,,2020-12-31,github/jameslamb,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-752838856,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"> Thanks! I've opened #38846 with a proposal to remove docs. It reduces the size of the sdist package by about 1MB. > > I think that removing tests needs to be done by maintainers, though, if it will involve distributing a new `pandas-tests` package ([#30741 (comment)](https://github.com/pandas-dev/pandas/issues/30741#issuecomment-719609290)). > > I really hope that is picked up, as it would have a significant impact on the size of the package. > > `master` #38846 #38846 + removing tests > sdis…",,,,,,Anecdotal,comment,,,,,,,,2021-01-05,github/Dr-Irv,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-754933575,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"I just tried removing that directory completely before running the size checks. The results are the same ```shell # check size rm -rf pandas.egg-info rm -rf dist python setup.py sdist du -a -h dist/ # check size without tests/io/sas/data/ rm -rf pandas/tests/io/sas/data/ rm -rf pandas.egg-info rm -rf dist python setup.py sdist du -a -h dist/ ``` I also tried this with `prune pandas/tests` added to `MANIFEST.in` (which is how I originally calculated the ""without tests"" numbers). with `pandas/tes…",,,,,,Anecdotal,comment,,,,,,,,2021-01-05,github/jameslamb,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-754942806,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"@jameslamb I was surprised that in your original numbers, with the uncompressed version, the result lowered from 19M to 7.7M, so that's why I thought the SAS data was the culprit. If I look at my installation of pandas 1.1.3, the uncompressed installed tests directory contains just over 9MB. Still justifies figuring out if we can leave it out of the dist.",,,,,,Anecdotal,comment,,,,,,,,2021-01-06,github/Dr-Irv,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-755405199,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"Providing an update here, it seems that distributions of `pandas` have grown by maybe `~15MB` (uncompressed) since I last calculated it in December (https://github.com/pandas-dev/pandas/issues/30741#issuecomment-752838856). On my laptop (running Python 3.7 and Ubuntu 18.04), running the following ```shell # install pandas 1.2.3 pip install --upgrade pandas # check sizes du -a ${HOME}/miniconda3/lib/python3.7/site-packages/pandas \ | sort -n -r \ | head -n 10 ``` results in ```text 68548 pandas …",,,,,,Anecdotal,comment,,,,,,,,2021-03-29,github/jameslamb,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-809469931,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"I'm also +1 on removing tests from the distribution (and optionally pulling them down on demand for `pandas.test()`), but won't have time to work on it.",,,,,,Anecdotal,comment,,,,,,,,2021-03-29,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-809472141,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"Hi, I was following along with PR #38846 which was recently merged, and I wanted to discuss the idea that I repeatedly saw @WillAyd bring up: specifically the idea of implementing a `pandas[no-doc]` PEP-508-compliant dependency specification as a way to give users an option to opt out of documentation in their packages. **Unfortunately, I think this idea is technically infeasible given the mechanics of PEP 508 dependencies.** (Would be very happy to be proven wrong because this has been an anno…",,,,,,Anecdotal,comment,,,,,,,,2021-04-09,github/jayqi,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-816837610,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"One potential difficulty before implementing this: We note for EA authors that there is a set of test that can be run to check EA compatibility: https://pandas.pydata.org/docs/development/extending.html#testing-extension-arrays So if tests were removed from the distribution, a subset of `pandas/tests/extensions/` probably should not be removed",,,,,,Anecdotal,comment,,,,,,,,2022-09-21,github/mroeschke,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-1253040812,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
Splitting the pandas library and tests would be really useful. We are using this library in our serverless deployment. and there is size restriction to upload the package into AWS lambda of 250 MB. Removing tests file will reduce the size of our package.,,,,,,Anecdotal,comment,,,,,,,,2023-09-07,github/viccsjain,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-1709718597,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"Making docs and tests optional would be great. In my cloud deployments I repackage it without the tests; 15 MB do make a difference for me. I've seem many other packages including tests, but never that big.",,,,,,Anecdotal,comment,,,,,,,,2023-09-25,github/jbsilva,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-1733436647,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"I was checking the size of one of my docker images, and found that tests are about 50% of the size of installed package: ![изображение](https://github.com/pandas-dev/pandas/assets/4661021/9fc4621e-900d-4d33-a28a-5dc7fec2be9a) Completely waste of space for me.",,,,,,Anecdotal,comment,,,,,,,,2024-05-30,github/dolfinus,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-2139463981,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"According to https://pypistats.org/packages/pandas the package has 240 million downloads per month. Now if the 32MB tests folder from the package would be removed, the package size would be halved. Currently the wheels are roughly 13MB large, so let's say the wheels would be 7MB after removing the tests, then pypi would save ~1.7 Petabyte of Bandwidth per Month, and could have saved roughly ~90 Petabytes of traffic since this issue was opened...",,,,,,Anecdotal,comment,,,,,,,,2024-08-03,github/jonas-w,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-2267131502,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"I just noticed that on one of our filesystems, which is not set up for lots of small files, pandas' tests end up taking 300 MB of installed size. Would it work as an initial step to make a script which splits the tests out of the wheels to be uploaded, and makes separate pandas-test wheels which can be uploaded separately? This is obviously not the most elegant way to do it, but I think I can see more or less how to make that work, whereas I'm not sure I can commit the time to figuring out how …",,,,,,Anecdotal,comment,,,,,,,,2024-09-17,github/takluyver,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-2356392445,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
PERF: Memory leak when returning subset of DataFrame and deleting the rest ### Pandas version checks - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [X] I have confirmed this issue exists on the main branch of pandas. ### Reproducible Example I think the simplest way is for me to just post this bare example which shows the memory leak (I had a sim…,,,,,,Anecdotal,issue,,,,,,,,2022-11-08,github/mar-ses,https://github.com/pandas-dev/pandas/issues/49582,repo: pandas-dev/pandas | keyword: best practice | state: open
"Yes, the values are quite different, but the issue is the same. Partly, the reason for the different values could be because I ran the main example in Jupyter, while this one I ran directly as a file script. I didn't fully understand why I get this big 2GB offset in the main example. Here is the output: ``` Pandas version: 1.5.1 Initial memory usage: 62.513152 MB Memory usage after iteration: 478.826496 MB Memory usage after iteration: 1270.603776 MB Memory usage after iteration: 1547.554816 MB…",,,,,,Anecdotal,comment,,,,,,,,2022-11-08,github/mar-ses,https://github.com/pandas-dev/pandas/issues/49582#issuecomment-1307436102,repo: pandas-dev/pandas | issue: PERF: Memory leak when returning subset of DataFrame and deleting the rest | keyword: best practice
"Also, isn't it a bit weird how the increments in memory usage are not regular? Sometimes it's 600 MB, sometimes 800 MB, sometimes 200. Even though each step should be identical. Do you konw why this could be? Could it just be random timing of the garbage collector? Also, if you run this on your system, what values do you get?",,,,,,Anecdotal,comment,,,,,,,,2022-11-08,github/mar-ses,https://github.com/pandas-dev/pandas/issues/49582#issuecomment-1307439864,repo: pandas-dev/pandas | issue: PERF: Memory leak when returning subset of DataFrame and deleting the rest | keyword: best practice
"@mar-ses the memory leak happens because you are doing the most common pandas bad practice - continuously mutating dataframe in a loop, one cell at a time. if you rewrite your lines in def get_df(N): ``` for i in df.index: df.at[i, ""c""] = {f""blabla_{j}"": j for j in range(i)} ``` into below, the memory leak will disappear. ``` df[""c""] = pd.Series([[{f""blabla_{j}"": j for j in range(i)}] for i in df.index]) ``` I am not sure how pandas team can fix this issue, but underlying problem is users are u…",,,,,,Anecdotal,comment,,,,,,,,2023-02-27,github/somurzakov,https://github.com/pandas-dev/pandas/issues/49582#issuecomment-1446941027,repo: pandas-dev/pandas | issue: PERF: Memory leak when returning subset of DataFrame and deleting the rest | keyword: best practice
"Very surprised to see that actually; it was my impression that it was actually recommended that you first create the dataframe, with its full size allocated but empty, and then modify its elements, instead of adding/appending rows. Because adding rows results in constant array creation and is less efficient. At least that's what I thought I heard from places like stackoverflow, perhaps I was mistaken.",,,,,,Anecdotal,comment,,,,,,,,2023-02-27,github/mar-ses,https://github.com/pandas-dev/pandas/issues/49582#issuecomment-1447021471,repo: pandas-dev/pandas | issue: PERF: Memory leak when returning subset of DataFrame and deleting the rest | keyword: best practice
"@mar-ses modifying df is definitely not recommended, because when you modify one cell - it invalidates entire memory block behind it, that stores values other nearby cells. Doing it in a loop for every cell - and you can see how much waste will be created by allocating and invalidating memory blocks at each iteration. For details about BlockManager you can read this blog https://uwekorn.com/2020/05/24/the-one-pandas-internal.html this blog post also contains few other really good recommendation…",,,,,,Anecdotal,comment,,,,,,,,2023-02-27,github/somurzakov,https://github.com/pandas-dev/pandas/issues/49582#issuecomment-1447048848,repo: pandas-dev/pandas | issue: PERF: Memory leak when returning subset of DataFrame and deleting the rest | keyword: best practice
I'm now seeing ``` Initial memory usage: 79.9744 MB Memory usage after iteration: 442.945536 MB Memory usage after iteration: 247.074816 MB Memory usage after iteration: 292.450304 MB Memory usage after iteration: 345.153536 MB Memory usage after iteration: 343.887872 MB Memory usage after iteration: 369.041408 MB Memory usage after iteration: 362.8032 MB Memory usage after iteration: 409.530368 MB Memory usage after iteration: 554.610688 MB Memory usage after iteration: 431.632384 MB ``` @somu…,,,,,,Anecdotal,comment,,,,,,,,2023-04-21,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/49582#issuecomment-1518031560,repo: pandas-dev/pandas | issue: PERF: Memory leak when returning subset of DataFrame and deleting the rest | keyword: best practice
"Regarding the first question, I don't remember anymore and I don't think I have the example that was causing this at hand. I did try other ways of doing this than the example I gave, but I don't think I tried to create the series with such an inner list comprehension. Regarding the second point, it's no typo, though I know it's discouraged. In this case, I was dealing with data from a database that includes a lot of ""metadata"" which is stored in json files, and I actually need almost all of the…",,,,,,Anecdotal,comment,,,,,,,,2023-04-21,github/mar-ses,https://github.com/pandas-dev/pandas/issues/49582#issuecomment-1518343081,repo: pandas-dev/pandas | issue: PERF: Memory leak when returning subset of DataFrame and deleting the rest | keyword: best practice
"STY: avoid accessing private attributes/methods/functions In 2020 we made some progress on avoiding importing private functions/classes, enforced by validate_unwanted_patterns.private_import_across_module. Along the same lines (and frankly way more difficult), it would be Best Practice to avoid accessing private attributes/methods at runtime, e.g. in NDFrame.xs we call index._get_loc_level and ideally we would use a public MultiIndex method instead. I imagine a code check could grep (many code …",,,,,,Anecdotal,issue,,,,,,,,2023-10-11,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/55493,repo: pandas-dev/pandas | keyword: best practice | state: open
"TYP: clean notations in the wrong place https://github.com/pandas-dev/pandas/blob/master/pandas/core/groupby/groupby.py#L1294 ``` # To track operations that expand dimensions, like ohlc OutputFrameOrSeries = TypeVar(""OutputFrameOrSeries"", bound=NDFrame) ``` at the very least this should be in `pandas._typing`. puzzled why we need this at *all*, as this is the same as `FrameOrSeries`",,,,,,Anecdotal,issue,,,,,,,,2021-01-11,github/jreback,https://github.com/pandas-dev/pandas/issues/39108,repo: pandas-dev/pandas | keyword: best practice | state: open
"> at the very least this should be in `pandas._typing`. puzzled why we need this at _all_, as this is the same as `FrameOrSeries` IIRC FrameOrSeries is already in use there, so another typevar is needed",,,,,,Anecdotal,comment,,,,,,,,2021-01-11,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/39108#issuecomment-757985818,repo: pandas-dev/pandas | issue: TYP: clean notations in the wrong place | keyword: best practice
"The issue is that FrameOrSeries is bound to the Groupby class. So any method of a subclass cannot use FrameOrSeries freely. For example, a helper method that takes in the result and reindexes it. We'd like the typing to reflect that the argument type to this method is the same as the return type. Personally, I think the code would be more clear if we added a TypeVar to bind to the GroupBy class and then use FrameOrSeries in addition when needed. In other words, switch the roles of FrameOrSeries…",,,,,,Anecdotal,comment,,,,,,,,2021-01-11,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/39108#issuecomment-758238955,repo: pandas-dev/pandas | issue: TYP: clean notations in the wrong place | keyword: best practice
"> Personally, I think the code would be more clear if we added a TypeVar to bind to the GroupBy class and then use FrameOrSeries in addition when needed. In other words, switch the roles of FrameOrSeries and OutputFrameOrSeries (and rename the latter). This would more closely parallel code outside of groupby. sgtm",,,,,,Anecdotal,comment,,,,,,,,2021-01-12,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/39108#issuecomment-758533069,repo: pandas-dev/pandas | issue: TYP: clean notations in the wrong place | keyword: best practice
"@jreback: Does my suggestion above satisfy this issue? If so, maybe we could align on a name/location for the TypeVar. Using `GroupByFrameOrSeries` seems a bit verbose to me; I'd suggest `GBFrameOrSeries` or `GroupByObj` or `GroupByData`. From comments above, it seems like it'd be appropriate to add to `_typing` with a comment to the effect that it is to only be used to bind to GroupBy.",,,,,,Anecdotal,comment,,,,,,,,2021-01-15,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/39108#issuecomment-760642897,repo: pandas-dev/pandas | issue: TYP: clean notations in the wrong place | keyword: best practice
"> If so, maybe we could align on a name/location for the TypeVar. from https://github.com/microsoft/pyright/blob/master/docs/typed-libraries.md#best-practices-for-inlined-types > Typically, a TypeVar should be private to the file that declares it, and should therefore begin with an underscore.",,,,,,Anecdotal,comment,,,,,,,,2021-01-15,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/39108#issuecomment-760809493,repo: pandas-dev/pandas | issue: TYP: clean notations in the wrong place | keyword: best practice
"IIRC that's the argument why we don't currently add a preceding underscore. We don't currently have a comprehensive style guide for type annotations, so I would prefer to use something like https://github.com/microsoft/pyright/blob/master/docs/typed-libraries.md#best-practices-for-inlined-types where we don't document our style (I also like the layout of the google Python style https://google.github.io/styleguide/pyguide.html with a background to why an appropriate style is chosen, I think adop…",,,,,,Anecdotal,comment,,,,,,,,2021-01-15,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/39108#issuecomment-761116984,repo: pandas-dev/pandas | issue: TYP: clean notations in the wrong place | keyword: best practice
"Sub-nanosecond datetime support This is a place to discuss whether supporting datetime logic with sub-nanosecond precision is desirable and feasible. ### Motivation I'm using pandas to interact with neuroscientific data—specifically, electrophysiology data. These data are sampled at very precise frequencies that cannot be expressed as an integer number of nanoseconds. Often the datasets span a large enough duration that rounding the sampling frequency to the nearest nanosecond would result an u…",,,,,,Anecdotal,issue,,,,,,,,2020-01-08,github/wmayner,https://github.com/pandas-dev/pandas/issues/30823,repo: pandas-dev/pandas | keyword: best practice | state: open
"@wmayner pandas' datetime support is built on top of numpy's datetime64, which does support picosecond and attosecond units (and would be much more performant than a decimal-based implementation) Would being limited to 64 bits be a problem for your use case? i.e. for attoseconds np.timedelta64 only supports timedeltas of about \pm 9.2 seconds (or timestamps within 9.2 seconds of 1970-01-01 00:00:00) non-nanosecond support is a pretty common request, and I think we'd be open to it if someone ste…",,,,,,Anecdotal,comment,,,,,,,,2020-01-08,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/30823#issuecomment-572290768,repo: pandas-dev/pandas | issue: Sub-nanosecond datetime support | keyword: best practice
an extension array that implemented unit support for date times / time delta would actually be somewhat straightforward in the current framework,,,,,,Anecdotal,comment,,,,,,,,2020-01-09,github/jreback,https://github.com/pandas-dev/pandas/issues/30823#issuecomment-572322421,repo: pandas-dev/pandas | issue: Sub-nanosecond datetime support | keyword: best practice
"@jbrockmendel Thanks. 64 bits is enough, but I'm confused by the following behavior, which seems to show a loss of precision when casting to `pd.Timestamp` and then back to `np.datetime64`: ```python import numpy as np import pandas as pd # Sampling frequency (Hz) fs = 256.9901428222656 # Sampling period t = 0.0038911998297600074 # Convert to int of attoseconds t = int(t * 1e18) t = np.datetime64(t, 'as') print('np.datetime64:'.rjust(22), t) t = pd.Timestamp(t) print('pd.Timestamp:'.rjust(22), …",,,,,,Anecdotal,comment,,,,,,,,2020-01-09,github/wmayner,https://github.com/pandas-dev/pandas/issues/30823#issuecomment-572745543,repo: pandas-dev/pandas | issue: Sub-nanosecond datetime support | keyword: best practice
"> I'm still confused that casting back to np.datetime64 appears to lose precision as well. When pd.Timestamp gets a non-nano np.datetime64 object, it casts it to nanoseconds, which is lossy in this use case. We could probably issue a warning about loss of precision. A PR to do so would be welcome.",,,,,,Anecdotal,comment,,,,,,,,2020-01-10,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/30823#issuecomment-573116077,repo: pandas-dev/pandas | issue: Sub-nanosecond datetime support | keyword: best practice
"Yes, I think a warning would be helpful. I can try to put that together if I have the time! But my confusion is about the cast from `pd.Timestamp` to `np.datetime64`: that seems to lose precision too. In the example above the `pd.Timestamp` object has nanosecond precision, so why does the `np.datetime64` seem to have only microsecond precision?",,,,,,Anecdotal,comment,,,,,,,,2020-01-10,github/wmayner,https://github.com/pandas-dev/pandas/issues/30823#issuecomment-573119882,repo: pandas-dev/pandas | issue: Sub-nanosecond datetime support | keyword: best practice
"> so why does the np.datetime64 seem to have only microsecond precision? That is going on inside the np.datetime64 constructor, and i can only speculate that it is treating the Timestamp as a datetime, which we would expect to have microsecond precision. To retain precision when converting from Timestamp, try `ts.to_datetime64()`",,,,,,Anecdotal,comment,,,,,,,,2020-01-10,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/30823#issuecomment-573125691,repo: pandas-dev/pandas | issue: Sub-nanosecond datetime support | keyword: best practice
"In 2.0 we support non-nano resolutions ""s"", ""ms"", ""us"", but not sub-nano resolutions. If someone wants to implement sub-nano support, it wouldn't be _that_ difficult. I do think warning of precision loss when when constructing a Timestamp from a sub-nano datetime64 might be worthwile.",,,,,,Anecdotal,comment,,,,,,,,2023-03-26,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/30823#issuecomment-1483972010,repo: pandas-dev/pandas | issue: Sub-nanosecond datetime support | keyword: best practice
"DOC: Data Editing Samples/Guide #### Location of the documentation N/A #### Documentation problem There seems to be general community confusion over when the use `iloc`, `loc`, `at`, `iat` or the other methods that allow you to update rows and columns. What is the best way to add a single new row? Multiple rows `pd.concat` something else? It would be nice if a guide or doc was created to point to the best practices for data editing on a DataFrame.",,,,,,Anecdotal,issue,,,,,,,,2020-07-22,github/achapkowski,https://github.com/pandas-dev/pandas/issues/35378,repo: pandas-dev/pandas | keyword: best practice | state: open
I think the relevant docs are: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html Is there something missing there?,,,,,,Anecdotal,comment,,,,,,,,2020-07-26,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/35378#issuecomment-664021609,repo: pandas-dev/pandas | issue: DOC:  Data Editing Samples/Guide | keyword: best practice
"These two sets of docs are relevant to the case, but if I need to do this: ```python df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'], 'B': ['B0', 'B1', 'B2', 'B3'], 'C': ['C0', 'C1', 'C2', 'C3'], 'D': ['D0', 'D1', 'D2', 'D3']}, index=[0, 1, 2, 3]) # add this single row to df1 row = ['A4', 'B4', 'C4', 'D4'] ``` What is the most efficient way to do this? Do I create a DataFrame for the single row and concat it? Do I use append? One of the `loc/iloc` methods?",,,,,,Anecdotal,comment,,,,,,,,2020-07-27,github/achapkowski,https://github.com/pandas-dev/pandas/issues/35378#issuecomment-664239295,repo: pandas-dev/pandas | issue: DOC:  Data Editing Samples/Guide | keyword: best practice
"I think adding a section in the documentation discussing this operation makes sense; I couldn't find one on it and hopefully didn't miss anything. Two suggestions: - There has been much support for deprecating `append` (#35407). - Regardless of the method used, it is more efficient to gather data up front (e.g. as a dictionary/list of dictionaries) and then create a single `DataFrame` rather than having many appends/concats/locs.",,,,,,Anecdotal,comment,,,,,,,,2020-08-01,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/35378#issuecomment-667544547,repo: pandas-dev/pandas | issue: DOC:  Data Editing Samples/Guide | keyword: best practice
"@rhshadrach thanks for pointing out the deprecation! There are so many issues, it's hard to keep track of them all. I know the best practice is to have all your data in hand, but sometimes your dataframe needs a few more rows of data and especially for large datasets (millions of rows) recreating the whole dataframe is overkill.",,,,,,Anecdotal,comment,,,,,,,,2020-08-03,github/achapkowski,https://github.com/pandas-dev/pandas/issues/35378#issuecomment-667916143,repo: pandas-dev/pandas | issue: DOC:  Data Editing Samples/Guide | keyword: best practice
"Add a ""Best Practices"" document I'd like to have a document that describes how we think people should write pandas code. This introduces a bit of friction when documenting something, since you'll need to decide ""does it go in best practices or the user guide?"" But I think the idea of a ""best practices"" document with opinionated, short examples and prose, linking back to the user guide and API docs, is valuable. I've started a notebook at https://mybinder.org/v2/gh/TomAugspurger/pandas-best-prac…",,,,,,Anecdotal,issue,,,,,,,,2019-07-19,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/27483,repo: pandas-dev/pandas | keyword: best practice | state: open
"What about: - Avoid chained indexing - Avoid iteration - Use boolean masks If you are searching stackoverflow, still lots of questions do chained indexing. Additionally, in lots of questions people want to iterate, which most of the times can be avoided using vectorisation, boolean masks etc. I would put this under this under tidy data, since people often just come up with awfully formated data, we could emphasize how easy tasks are if data are well formatted. (Think of lists of strings or tupl…",,,,,,Anecdotal,comment,,,,,,,,2019-10-02,github/datajanko,https://github.com/pandas-dev/pandas/issues/27483#issuecomment-537678672,"repo: pandas-dev/pandas | issue: Add a ""Best Practices"" document | keyword: best practice"
"+1 for avoid iterations, boolean masks. From interviews, I can confirm a majority of newbies are bad at both. On a broader point, I think ""how you should write pandas code"" falls into two buckets: - Syntactic sugar: What style do we suggest is the most readable - Efficiency: What methods optimise runtime/memory I think both are valuable, and a good best-practices document would be helpful for the community at large. Syntactic sugar can be addressed by an opinionated doc with short examples like…",,,,,,Anecdotal,comment,,,,,,,,2019-12-24,github/JMBurley,https://github.com/pandas-dev/pandas/issues/27483#issuecomment-568768942,"repo: pandas-dev/pandas | issue: Add a ""Best Practices"" document | keyword: best practice"
"BUG: to_clipboard() incorrectly claims I have specified a non-single character separator in excel mode. ### - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [x] I have confirmed this bug exists on the master branch of pandas. ### Reproducible Example ```python import pandas as pd df = pd.DataFrame({'numbers': [1, 2, 3], 'colors': ['red', 'white', 'bl…",,,,,,Anecdotal,issue,,,,,,,,2021-10-20,github/simonbance,https://github.com/pandas-dev/pandas/issues/44120,repo: pandas-dev/pandas | keyword: best practice | state: open
"Part of the issue here might be coding best practice; there are multiple statements within the try/except, and this makes it difficult to understand what is wrong. I'm going to stick my neck out and suggest that there should be no try/except here. If the problem really is with to_csv() then let to_csv() throw us an error that we can get useful information from. I don't know if trying to catch a TypeError is the correct approach here.",,,,,,Anecdotal,comment,,,,,,,,2021-10-21,github/simonbance,https://github.com/pandas-dev/pandas/issues/44120#issuecomment-948677119,repo: pandas-dev/pandas | issue: BUG: to_clipboard() incorrectly claims I have specified a non-single character separator in excel mode. | keyword: best practice
"> Part of the issue here might be coding best practice; there are multiple statements within the try/except, and this makes it difficult to understand what is wrong. I'm going to stick my neck out and suggest that there should be no try/except here. If the problem really is with to_csv() then let to_csv() throw us an error that we can get useful information from. I don't know if trying to catch a TypeError is the correct approach here. Thanks for the feedback. The clipboard code was vendored fr…",,,,,,Anecdotal,comment,,,,,,,,2021-10-21,github/fangchenli,https://github.com/pandas-dev/pandas/issues/44120#issuecomment-948875025,repo: pandas-dev/pandas | issue: BUG: to_clipboard() incorrectly claims I have specified a non-single character separator in excel mode. | keyword: best practice
"Hi @fangchenli thanks for responding to my issue. I appreciate the explanation. So you're saying that this is likely an error with pypaperclip. I wonder if the try/except that surrounds that part of the code should be modified, so that everything that's currently on lines 120 to 128 should be outside. Perhaps we can make a separate check for a valid ""sep"" argument before calling to_csv(), not enclose it in the same try/except as the presumed problematic call to pypaperclip. At the moment the wa…",,,,,,Anecdotal,comment,,,,,,,,2021-10-21,github/simonbance,https://github.com/pandas-dev/pandas/issues/44120#issuecomment-948900703,repo: pandas-dev/pandas | issue: BUG: to_clipboard() incorrectly claims I have specified a non-single character separator in excel mode. | keyword: best practice
"@simonbance I made this modification and run your example. ```python except TypeError as err: warnings.warn( f""to_clipboard in excel mode requires a single character separator. {err}"" ) ``` I got this ``` /Users/fangchenli/Workspace/pandas-fangchenli/pandas/io/clipboards.py:135: UserWarning: to_clipboard in excel mode requires a single character separator. to_csv() got an unexpected keyword argument 'index_names' ```",,,,,,Anecdotal,comment,,,,,,,,2021-10-21,github/fangchenli,https://github.com/pandas-dev/pandas/issues/44120#issuecomment-948990387,repo: pandas-dev/pandas | issue: BUG: to_clipboard() incorrectly claims I have specified a non-single character separator in excel mode. | keyword: best practice
"Actually I was thinking something like this (sorry can't test it right now): ```python if excel: buf = StringIO() if sep is None: sep = ""\t"" elif len(sep) > 1: warnings.warn(""to_clipboard in excel mode requires a single character separator."") # Do something appropriate here! Maybe set sep = ""\t"" or slice sep to length 1 ? (would be unpredictable!) # ..... # clipboard_set (pyperclip) expects unicode obj.to_csv(buf, sep=sep, encoding=""utf-8"", **kwargs) text = buf.getvalue() try: clipboard_set(tex…",,,,,,Anecdotal,comment,,,,,,,,2021-10-21,github/simonbance,https://github.com/pandas-dev/pandas/issues/44120#issuecomment-949009479,repo: pandas-dev/pandas | issue: BUG: to_clipboard() incorrectly claims I have specified a non-single character separator in excel mode. | keyword: best practice
"read_csv should default to index_col = 0 #### Code Sample ```python df.to_csv(file_path) df = pd.read_csv(file_path) ``` #### Problem description Currently, the default CSV writing behaviour is to write the index column. The default reading behaviour, however, is to assume there is no index column in the file. This is not intuitive when writing and reading files. The expected behaviour is that a file which is written without any index option it should be able to be read without any index option…",,,,,,Anecdotal,issue,,,,,,,,2018-12-28,github/itko,https://github.com/pandas-dev/pandas/issues/24468,repo: pandas-dev/pandas | keyword: best practice | state: open
"At this point, both of those defaults are unlikely to be changed. `pd.DataFrame.from_csv` exists for easier round-tripping, though deprecated, see e.g. #10163 Because it is schema-less, csv is never a particularly safe format for round-tripping, consider using something binary like parquet of HDF5 instead",,,,,,Anecdotal,comment,,,,,,,,2018-12-28,github/chris-b1,https://github.com/pandas-dev/pandas/issues/24468#issuecomment-450406181,repo: pandas-dev/pandas | issue: read_csv should default to index_col = 0 | keyword: best practice
"> At this point, both of those defaults are unlikely to be changed. Agreed. Though might entertain this option more in a super-breaking release like `1.0`.",,,,,,Anecdotal,comment,,,,,,,,2018-12-30,github/gfyoung,https://github.com/pandas-dev/pandas/issues/24468#issuecomment-450539549,repo: pandas-dev/pandas | issue: read_csv should default to index_col = 0 | keyword: best practice
"This has been rejected before in #12627 with the following reply: > Has been this way almost since the beginning. > > The idea is that .to_csv and .from_csv are inverses > > Essentially impossible to change at this point. But to be honest its actually a sensible default. Indexes are more and more important. If you are not using them you should. However, `.from_csv` has been deprecated in favor of `.read_csv`, which would only be the inverse of `.to_csv` if this default were changed. I can only …",,,,,,Anecdotal,comment,,,,,,,,2019-03-22,github/JeroenDelcour,https://github.com/pandas-dev/pandas/issues/24468#issuecomment-475684493,repo: pandas-dev/pandas | issue: read_csv should default to index_col = 0 | keyword: best practice
"just personally, I'd be more sympathetic to changing the default on `to_csv` to `index=False`, but that has its own set of problems",,,,,,Anecdotal,comment,,,,,,,,2019-03-22,github/chris-b1,https://github.com/pandas-dev/pandas/issues/24468#issuecomment-475691478,repo: pandas-dev/pandas | issue: read_csv should default to index_col = 0 | keyword: best practice
"> but that has its own set of problems Agreed. But the point is well taken that we should pick a suitable (and consistent) default for both to avoid the confusions described above. That being said, it would be good to get some more opinions on what people generally use in the wild (i.e. with an index, without) before settling on one. > Defaults have been changed before, is there a specific reason this one shouldn't? We aren't saying that it shouldn't, but asking for us to do this in `0.25.0` is…",,,,,,Anecdotal,comment,,,,,,,,2019-03-22,github/gfyoung,https://github.com/pandas-dev/pandas/issues/24468#issuecomment-475738659,repo: pandas-dev/pandas | issue: read_csv should default to index_col = 0 | keyword: best practice
"> asking for us to do this in `0.25.0` is rushing things IMO. Sorry, I didn't mean to rush it. I'm not very familiar with the Pandas release cycle. As long as it's not pushed back to `1.0.0` - I don't think it's that breaking. > just personally, I'd be more sympathetic to changing the default on to_csv to index=False, but that has its own set of problems I'm leaning towards this, too, if nothing else because it would match what most users I know already do.",,,,,,Anecdotal,comment,,,,,,,,2019-03-22,github/JeroenDelcour,https://github.com/pandas-dev/pandas/issues/24468#issuecomment-475743177,repo: pandas-dev/pandas | issue: read_csv should default to index_col = 0 | keyword: best practice
Agreed with https://github.com/pandas-dev/pandas/issues/24468#issuecomment-450406181 that this is too large of a change for us at this point. What do you think @itko?,,,,,,Anecdotal,comment,,,,,,,,2019-12-11,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/24468#issuecomment-564676633,repo: pandas-dev/pandas | issue: read_csv should default to index_col = 0 | keyword: best practice
"I tend to agree with @JeroenDelcour. Not only have I seen multiple users get confused by the appearance of unnamed columns, I myself often forget to set the index arguments. It seems to me we should be prioritizing usability and intuitiveness. Can't we schedule this for release and add a FutureWarning?",,,,,,Anecdotal,comment,,,,,,,,2019-12-11,github/itko,https://github.com/pandas-dev/pandas/issues/24468#issuecomment-564775127,repo: pandas-dev/pandas | issue: read_csv should default to index_col = 0 | keyword: best practice
"Error when converting df to json table (utc timezone date time object causes the error) When converting df.to_json(orient=""table"",index=False) and there are datetime.now(timezone.utc) objects in the table it causes the following error (Without orient table it works though.): `--------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-122-4a9c9a4c89de> in <module> ----> 1 x = current_opportunites.to_json(orient=""tab…",,,,,,Anecdotal,issue,,,,,,,,2021-02-01,github/franz101,https://github.com/pandas-dev/pandas/issues/39537,repo: pandas-dev/pandas | keyword: best practice | state: open
"test = pd.DataFrame([{""name"":""foo"",""time"":datetime.now(timezone.utc)}]).to_json( orient=""table"") this works like a charm pd.DataFrame([{""name"":""foo"",""time"":datetime.now(timezone.utc)}]).to_json( orient=""records"")",,,,,,Anecdotal,comment,,,,,,,,2021-02-02,github/franz101,https://github.com/pandas-dev/pandas/issues/39537#issuecomment-771275969,repo: pandas-dev/pandas | issue: Error when converting df to json table (utc timezone date time object causes the error) | keyword: best practice
"Not sure what the best practise is with datetime, json and pandas. pd.DataFrame([{""name"":""foo"",""time"":datetime.now(timezone.utc)}]).to_json( orient=""records"") won't parse the dates... pd.read_json(test) also won't work with date_format='iso' this will return the datetimeformat... json.dumps(pd.DataFrame([{""name"":""foo"",""time"":datetime.now(timezone.utc)}]).to_dict(""records"" ),default=str) Maybe I made some small mistakes. what is the best practice here with datetime and utc timezones?",,,,,,Anecdotal,comment,,,,,,,,2021-02-02,github/franz101,https://github.com/pandas-dev/pandas/issues/39537#issuecomment-771278772,repo: pandas-dev/pandas | issue: Error when converting df to json table (utc timezone date time object causes the error) | keyword: best practice
"`to_json` and `read_json` with timezone support only works with `orient=""table""`. The code may render with other `orient` types but the timezone info will be lost, i.e. converted to UTC. The error you are getting can be patched with: ``` #_table_schema.py # line 123 elif is_datetime64tz_dtype(dtype): try: field[""tz""] = dtype.tz.zone except AttributeError: field[""tz""] = dtype.name[15:len(dtype.name)-1] ``` Apparently the way you are creating a DateTime object, e.g.: ``` val = datetime.now(timezo…",,,,,,Anecdotal,comment,,,,,,,,2021-02-02,github/attack68,https://github.com/pandas-dev/pandas/issues/39537#issuecomment-771672988,repo: pandas-dev/pandas | issue: Error when converting df to json table (utc timezone date time object causes the error) | keyword: best practice
"ASV Benchmark - Time Standards TLDR - I think we need to cap our benchmarks at a maximum of .2 seconds. That's a long way off though, so I think should start with a cap of 1 second per benchmark Right now we have some very long running benchmarks: https://pandas.pydata.org/speed/pandas/#summarylist?sort=1&dir=desc I haven't seen a definitive answer, but I think ASV leverages the builtin timeit functionality to figure out how long a given benchmark should run. https://docs.python.org/3.7/library…",,,,,,Anecdotal,issue,,,,,,,,2019-10-22,github/WillAyd,https://github.com/pandas-dev/pandas/issues/29165,repo: pandas-dev/pandas | keyword: best practice | state: open
"The timeit documentation does not apply as is, see here for details: https://asv.readthedocs.io/en/stable/benchmarks.html#timing-benchmarks On October 22, 2019 5:59:32 PM UTC, William Ayd <notifications@github.com> wrote: >TLDR - I think we need to cap our benchmarks at a maximum of .2 >seconds. That's a long way off though, so I think should start with a >cap of 1 second per benchmark > >Right now we have some very long running benchmarks: > >https://pandas.pydata.org/speed/pandas/#summarylist…",,,,,,Anecdotal,comment,,,,,,,,2019-10-22,github/pv,https://github.com/pandas-dev/pandas/issues/29165#issuecomment-545140934,repo: pandas-dev/pandas | issue: ASV Benchmark - Time Standards | keyword: best practice
"Thanks for the link - reading through it definitely gives more guidance. So we if we track something that itself takes more than 10 milliseconds to run do you know the `number` of times it is run within a `sample`? The documentation mentions that asv selects a `number` by approximation how many runs it will take to reach the `sample_time`, but its not clear what happens if one run exceeds sample_time altogether Alternately do you have thoughts here on general best practices? Right now our bench…",,,,,,Anecdotal,comment,,,,,,,,2019-10-22,github/WillAyd,https://github.com/pandas-dev/pandas/issues/29165#issuecomment-545153750,repo: pandas-dev/pandas | issue: ASV Benchmark - Time Standards | keyword: best practice
"If it takes longer that `sample_time`, `number = 1`. You probably want to adjust `repeat`, as the default `(2, 10, 20.0)` runs until 10 samples are collected or 20 seconds elapsed --- you can e.g. make the max time shorter.",,,,,,Anecdotal,comment,,,,,,,,2019-10-23,github/pv,https://github.com/pandas-dev/pandas/issues/29165#issuecomment-545626662,repo: pandas-dev/pandas | issue: ASV Benchmark - Time Standards | keyword: best practice
"@WillAyd It appears there's a few issues: - The benchmark takes 20s per run, there's not much `asv` can do as long as that's the case. - It's being run over 48 parameter combinations (half fast/half slow). - These two factors mean 8 minutes for a `n=1` run (24 * 20s), so it's slow and noisy - The pydata speed site is using an older version of `asv` that includes memory addresses in run names: https://pandas.pydata.org/speed/pandas/#rolling.Apply.time_rolling?p-function=%3Cbuilt-in%20function%20…",,,,,,Anecdotal,comment,,,,,,,,2019-10-26,github/qwhelan,https://github.com/pandas-dev/pandas/issues/29165#issuecomment-546639116,repo: pandas-dev/pandas | issue: ASV Benchmark - Time Standards | keyword: best practice
"I'll get the asv updated in the env running these. On Sat, Oct 26, 2019 at 4:02 PM Christopher Whelan <notifications@github.com> wrote: > @WillAyd <https://github.com/WillAyd> It appears there's a few issues: > > - The benchmark takes 20s per run, there's not much asv can do as long > as that's the case. > - It's being run over 48 parameter combinations (half fast/half slow). > - These two factors mean 8 minutes for a n=1 run (24 * 20s), so it's > slow and noisy > - The pydata speed site is usi…",,,,,,Anecdotal,comment,,,,,,,,2019-10-28,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/29165#issuecomment-546767870,repo: pandas-dev/pandas | issue: ASV Benchmark - Time Standards | keyword: best practice
"DOC: Remove and Update out of date Docker Image issue with #61511 - [ ] Addresses & closes [DOC: Docker image provided on ""Debugging C extensions"" is out of date #61511](https://github.com/pandas-dev/pandas/issues/61511) - [ ] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit).",,,,,,Anecdotal,issue,,,,,,,,2025-06-04,github/jacksnnn,https://github.com/pandas-dev/pandas/pull/61550,repo: pandas-dev/pandas | keyword: best practice | state: closed
"@WillAyd do you mind having a look? I think we should also remove the container if it's not maintained, not only the docs.",,,,,,Anecdotal,comment,,,,,,,,2025-06-04,github/datapythonista,https://github.com/pandas-dev/pandas/pull/61550#issuecomment-2939686376,repo: pandas-dev/pandas | issue: DOC: Remove and Update out of date Docker Image issue with  #61511 | keyword: best practice
This pull request is stale because it has been open for thirty days with no activity. Please [update](https://pandas.pydata.org/pandas-docs/stable/development/contributing.html#updating-your-pull-request) and respond to this comment if you're still interested in working on this.,,,,,,Anecdotal,comment,,,,,,,,2025-07-05,github/github-actions[bot],https://github.com/pandas-dev/pandas/pull/61550#issuecomment-3037457110,repo: pandas-dev/pandas | issue: DOC: Remove and Update out of date Docker Image issue with  #61511 | keyword: best practice
"Thanks for the pull request, but it appears to have gone stale. If interested in continuing, please merge in the main branch, address any review comments and/or failing tests, and we can reopen.",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/mroeschke,https://github.com/pandas-dev/pandas/pull/61550#issuecomment-3128236893,repo: pandas-dev/pandas | issue: DOC: Remove and Update out of date Docker Image issue with  #61511 | keyword: best practice
"`pandas.__version__` is `2.3.0+4.g1dfc98e16a` in pandas 2.3.0 and python 3.9, not `2.3.0` ## How to reproduce: ``` docker run --rm python:3.9 bash -c ""pip install pandas && python -c 'import pandas; print(pandas.__version__)'"" ``` ## Output: ```sh % docker run --rm python:3.9 bash -c ""pip install pandas && python -c 'import pandas; print(pandas.__version__)'"" ... # pip install logs 2.3.0+4.g1dfc98e16a ``` Seems related to https://github.com/pandas-dev/pandas/issues/61563#issuecomment-2947099734",,,,,,Anecdotal,issue,,,,,,,,2025-06-06,github/harupy,https://github.com/pandas-dev/pandas/issues/61579,repo: pandas-dev/pandas | keyword: best practice | state: closed
I can confirm this. This breaks our pandas version check function in our sdk https://github.com/shinnytech/tqsdk-ci/actions/runs/15481907073/job/43589180749 ![Image](https://github.com/user-attachments/assets/f981017f-65f0-4713-8e34-87aa2c2b62f8),,,,,,Anecdotal,comment,,,,,,,,2025-06-06,github/shinny-taojiachun,https://github.com/pandas-dev/pandas/issues/61579#issuecomment-2948194373,"repo: pandas-dev/pandas | issue: `pandas.__version__` is `2.3.0+4.g1dfc98e16a` in pandas 2.3.0 and python 3.9, not `2.3.0` | keyword: best practice"
"@harupy Thank you for your assistant. We are aware what we use in the past may not be the best practice. Newer release of our sdk should change to better version check. But releasing newer version takes a lot of time. Meanwhile, this issue is blocking our users from using our sdk, especially new installation and upgrading of our sdk.",,,,,,Anecdotal,comment,,,,,,,,2025-06-06,github/shinny-taojiachun,https://github.com/pandas-dev/pandas/issues/61579#issuecomment-2948238675,"repo: pandas-dev/pandas | issue: `pandas.__version__` is `2.3.0+4.g1dfc98e16a` in pandas 2.3.0 and python 3.9, not `2.3.0` | keyword: best practice"
"I can confirm this happens to me, only for 3.9 and pandas installed via pip (installed via conda-forge has the right version)",,,,,,Anecdotal,comment,,,,,,,,2025-06-06,github/datapythonista,https://github.com/pandas-dev/pandas/issues/61579#issuecomment-2948969583,"repo: pandas-dev/pandas | issue: `pandas.__version__` is `2.3.0+4.g1dfc98e16a` in pandas 2.3.0 and python 3.9, not `2.3.0` | keyword: best practice"
Broken `ydata-profiling` integration due to this error. ``` ../../../venv3.9/lib/python3.9/site-packages/ydata_profiling/__init__.py:10: in <module> from ydata_profiling.compare_reports import compare # isort:skip # noqa ../../../venv3.9/lib/python3.9/site-packages/ydata_profiling/compare_reports.py:12: in <module> from ydata_profiling.profile_report import ProfileReport ../../../venv3.9/lib/python3.9/site-packages/ydata_profiling/profile_report.py:26: in <module> from visions import VisionsTyp…,,,,,,Anecdotal,comment,,,,,,,,2025-06-06,github/yaricom,https://github.com/pandas-dev/pandas/issues/61579#issuecomment-2949583364,"repo: pandas-dev/pandas | issue: `pandas.__version__` is `2.3.0+4.g1dfc98e16a` in pandas 2.3.0 and python 3.9, not `2.3.0` | keyword: best practice"
"Apologies, yes this is unexpected. This was due to having to release 3.9 wheels in an unorthodox way https://github.com/pandas-dev/pandas/pull/61569 with context in https://github.com/pandas-dev/pandas/issues/61563#issuecomment-2945331441 (Removing the `good first issue` since the fix just involves releasing pandas though our normal mechanisms)",,,,,,Anecdotal,comment,,,,,,,,2025-06-06,github/mroeschke,https://github.com/pandas-dev/pandas/issues/61579#issuecomment-2949810635,"repo: pandas-dev/pandas | issue: `pandas.__version__` is `2.3.0+4.g1dfc98e16a` in pandas 2.3.0 and python 3.9, not `2.3.0` | keyword: best practice"
"API (string dtype): implement hierarchy (NA > NaN, pyarrow > python) for consistent comparisons between different string dtypes Closes https://github.com/pandas-dev/pandas/issues/60639 This does not yet handle the case of comparison to object dtype. - [ ] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature - [ ] Added an entry in the latest `doc/source/whatsnew/vX.X.X.rst` file if fixing…",,,,,,Anecdotal,issue,,,,,,,,2025-03-17,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/pull/61138,repo: pandas-dev/pandas | keyword: best practice | state: closed
"@jorisvandenbossche - I've merged main and pushed a commit here. If you have any objections, I can pull it off. - Adds whatsnew to 2.3. - Simplifies conditionals in a test. - Fixes behavior of ArrowExtensionArray and adds tests for it. For the last one, previously ArrowExtensionArray vs Nan-Python was giving back NumPy bool. This was the only case where ArrowExtensionArray was not resulting in ArrowExtensionArray. > This does not yet handle the case of comparison to object dtype. object dtype l…",,,,,,Anecdotal,comment,,,,,,,,2025-05-10,github/rhshadrach,https://github.com/pandas-dev/pandas/pull/61138#issuecomment-2868849120,"repo: pandas-dev/pandas | issue: API (string dtype): implement hierarchy (NA > NaN, pyarrow > python) for consistent comparisons between different string dtypes | keyword: best practice"
"> > object dtype looks correct to me here. > > Hmm, not entirely sure anymore what I meant with that object dtype was not yet covered. I thought maybe the case where the object dtype does not contain just strings, but also that seems to work fine One case related to object dtype that is still failing is comparing with an object series that has mixed types: ```python In [3]: ser1 = pd.Series([""a"", None, ""b""], dtype=pd.StringDtype(""pyarrow"", na_value=np.nan)) In [4]: ser2 = pd.Series([""a"", None, …",,,,,,Anecdotal,comment,,,,,,,,2025-05-14,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/pull/61138#issuecomment-2881073720,"repo: pandas-dev/pandas | issue: API (string dtype): implement hierarchy (NA > NaN, pyarrow > python) for consistent comparisons between different string dtypes | keyword: best practice"
"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions: 1. Checkout backport branch and update it. ``` git checkout 2.3.x git pull ``` 2. Cherry pick the first parent branch of the this PR on top of the older branch: ``` git cherry-pick -x -m1 6177e2233974643d17d1560a4d61804800534900 ``` 3. You will likely have some merge/cherry-pick conflict here, fix them and commit: ``` git commit -am 'Backport PR #61138: API (string dtype):…",,,,,,Anecdotal,comment,,,,,,,,2025-05-19,github/lumberbot-app[bot],https://github.com/pandas-dev/pandas/pull/61138#issuecomment-2891518674,"repo: pandas-dev/pandas | issue: API (string dtype): implement hierarchy (NA > NaN, pyarrow > python) for consistent comparisons between different string dtypes | keyword: best practice"
"It appears that this has not been included in the 2.3 release as intended. see #61570 we probably need to now move the release note to 2.3.1 on main and backport that too. probably also best to have a PR to create a 2.3.1 whatsnew just in case anyway. If we abandon a release, we just change the milestones and move the entries in the whatsnew in a separate PR.",,,,,,Anecdotal,comment,,,,,,,,2025-06-09,github/simonjayhawkins,https://github.com/pandas-dev/pandas/pull/61138#issuecomment-2955882770,"repo: pandas-dev/pandas | issue: API (string dtype): implement hierarchy (NA > NaN, pyarrow > python) for consistent comparisons between different string dtypes | keyword: best practice"
"And I see that there are a few more issues with ""Still Needs Manual Backport"" label (https://github.com/pandas-dev/pandas/issues?q=label%3A%22Still+Needs+Manual+Backport%22+is%3Aclosed) that we should check if that is indeed still the case",,,,,,Anecdotal,comment,,,,,,,,2025-06-09,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/pull/61138#issuecomment-2956039442,"repo: pandas-dev/pandas | issue: API (string dtype): implement hierarchy (NA > NaN, pyarrow > python) for consistent comparisons between different string dtypes | keyword: best practice"
ENH: Implement DataFrame.select - [X] closes #61522 - [X] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature - [X] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit). - [X] Added [type annotations](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#type-hints) to new arguments/methods/functi…,,,,,,Anecdotal,issue,,,,,,,,2025-05-31,github/datapythonista,https://github.com/pandas-dev/pandas/pull/61527,repo: pandas-dev/pandas | keyword: best practice | state: closed
"For reference, [PySpark](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.select.html) uses `*cols`, and [Polars](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.select.html) uses `*exprs`, but also supports passing a list. If in the future we implement `pandas.col` (as discussed for `filter`), and we'd like to support selecting expressions, the syntax of using `**kwargs` in my opinion is even better than `kwargs` co…",,,,,,Anecdotal,comment,,,,,,,,2025-06-04,github/datapythonista,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2939323050,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"> I'm personally not convinced by the reasons to use a list so far. Being consistent with `groupby` doesn't seem so important, since `by` is way more complicated than what's implemented here, and groupby has many other parameters, which makes a difference. It's not just `groupby`. Here's a list of some other methods where we pass lists/sequences of columns: - `reset_index()` - `drop()` with `columns` argument - `sort_values()` - `value_counts()` > > I feel that we're making the API more complic…",,,,,,Anecdotal,comment,,,,,,,,2025-06-04,github/Dr-Irv,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2940733211,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"Thanks @Dr-Irv for putting together the list, I really appreciate, and it's very helpful. I'm not convinced, as all them are different cases. You can do `df.sort_values(""my_col"")`, `df.value_counts(""my_col"")`,... The pattern in them the way I understand it is that the base case it's expecting a single column, but a list is also accepted. We could also consider this for `select`, assume that the base case is for it to select a single column, allow `df.select(""my_column"")`, and for multiple colum…",,,,,,Anecdotal,comment,,,,,,,,2025-06-04,github/datapythonista,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2940903494,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"> We could also consider this for `select`, assume that the base case is for it to select a single column, allow `df.select(""my_column"")`, and for multiple columns use the same pattern as the methods you shared, `df.select([""col1"", ""col2""])`. I'd be fine with having one column or a list of columns. But this also raises the following. Compare the following possibilities: - `df.select(""col"")` vs. `df[[""col""]]` - `df.select(""col1"", ""col2"")` vs. `df[[""col1"", ""col2""]]` - df.select([""col1"", ""col2""]) …",,,,,,Anecdotal,comment,,,,,,,,2025-06-04,github/Dr-Irv,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2941142752,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"`select` will offer a much better syntax with method chaining, that's the motivation for having both `select` and `filter`. I can show you with an example if needed.",,,,,,Anecdotal,comment,,,,,,,,2025-06-04,github/datapythonista,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2941160748,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"> `select` will offer a much better syntax with method chaining, that's the motivation for having both `select` and `filter`. I can show you with an example if needed. Maybe I'm missing something, but using `df[[""col1"", ""col2""]]` in a method chain is equivalent to using `df.select(""col1"", ""col2"")` in a method chain.",,,,,,Anecdotal,comment,,,,,,,,2025-06-04,github/Dr-Irv,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2941164980,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"> Maybe I'm missing something, but using `df[[""col1"", ""col2""]]` in a method chain is equivalent to using `df.select(""col1"", ""col2"")` in a method chain. You're right, it's the inconsistency with everything else being a method that I don't think it's great: ```python import pandas (pandas.read_csv(""taxi_data/yellow_tripdata_2015-01.csv"") .rename(columns={""trip_distance"": ""trip_miles""}) [[""pickup_longitude"", ""pickup_latitude"", ""trip_miles""]] .assign(distance_kms=lambda df: df.trip_miles * 1.60934)…",,,,,,Anecdotal,comment,,,,,,,,2025-06-04,github/datapythonista,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2941275626,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"`.select(""pickup_longitude"", ""pickup_latitude"", distance_kms=pandas.col(""trip_miles"") * 1.60934)` I'm not comfortable with having a named-keyword assignment in here. IIUC from the dev call there was another keyword from `filter` being discussed as being moved to `select`, so then we'd have both explicit keywords and implicit keyword-assignment being mixed-and-matched.",,,,,,Anecdotal,comment,,,,,,,,2025-06-04,github/jbrockmendel,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2941342851,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"> You're right, it's the inconsistency with everything else being a method that I don't think it's great: So this is just about a syntax preference. I'd probably continue to use the current methodology of ` [[""pickup_longitude"", ""pickup_latitude"", ""trip_miles""]]` instead of `select()` because I'm used to the former. But if you are trying to get people coming from Polars or PySpark to use pandas instead, then your proposal makes sense, although I think the API should accept either a `*args` OR a…",,,,,,Anecdotal,comment,,,,,,,,2025-06-04,github/Dr-Irv,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2941361233,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"Thanks for the feedback. This is something that Polars allows and I wanted to show as it makes the example cleaner. But surely not part of this PR, or any plan for the short term. We should have `pandas.col` implemented to have this discussion. I personally think we shouldn't introduce keyword arguments to `select`, even if as you say it was discussed. For me the problem with implementing this is that keyword arguments must be after positional arguments, making the user life difficult when tryi…",,,,,,Anecdotal,comment,,,,,,,,2025-06-04,github/datapythonista,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2941363773,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"You are correct @Dr-Irv, and of course `df[[""col1"", ""col2""]]` will stay. I wouldn't say it's for people coming from PySpark or Polars, even if it surely will make life easier to users using both. I think for someone new to pandas, in particular the ones who will use method chaining, the new syntax can make it easier to learn. `df[foo]` is nice in a way, but it's quite overloaded and probably difficult to understand for beginners. Selecting columns being a method like any other DataFrame method …",,,,,,Anecdotal,comment,,,,,,,,2025-06-04,github/datapythonista,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2941380091,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"> But the main reason to have this to me is that I think it brings consistency and readability to pipelines with method chaining as in the example. So if you look at our docs at: https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#astype There is this example: ```python dft[[""a"", ""b""]] = dft[[""a"", ""b""]].astype(np.uint8) ``` If you are encouraging readability, would you suggest we update the docs/tutorials that use a pattern like the above to: ```python dft[[""a"", ""b""]] = dft.sele…",,,,,,Anecdotal,comment,,,,,,,,2025-06-04,github/Dr-Irv,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2941461827,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"It's just for method chaining that I think it's better. I don't think in isolation select is better. It's more explicit, but I don't think we should rewrite examples or encourage select. I think `df[[...]]` is totally fine, it's just for cases like the example I share that I think it complicates things, and select will be useful. Good point about select for groupby, it does seem as a good idea, as a pipeline with method chaining and a group by will also be clearer and more consistent with selec…",,,,,,Anecdotal,comment,,,,,,,,2025-06-04,github/datapythonista,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2941503192,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"To me the real issue here is that in method chains you'd like one operation on each line. But when you format your code, you get this: ```python ( pandas.read_csv(""taxi_data/yellow_tripdata_2015-01.csv"") .rename(columns={""trip_distance"": ""trip_miles""})[[""pickup_longitude"", ""pickup_latitude"", ""trip_miles""]] .assign(distance_kms=lambda df: df.trip_miles * 1.60934) .drop(""trip_miles"", axis=1) .pipe(lambda df: df[df.distance_kms > 10.]) .to_parquet(""long_taxi_pickups.parquet"")) ) ``` In addition, t…",,,,,,Anecdotal,comment,,,,,,,,2025-06-04,github/rhshadrach,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2941776829,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"We discussed this in today's call, and while not the perfect solution, everybody agreed that allowing both `*args` and a list is better than the alternatives. I fully agree that supporting two different APIs is something that we would want to avoid. But at the same time, forcing a list makes the API more complicated and less Pythonic (`.select(""col1"", ""col2"")` is clearly simpler and more pythonic than `.select([""col1"", ""col2""])`). Also, not allowing `*args` would be counterintuitive for users o…",,,,,,Anecdotal,comment,,,,,,,,2025-06-11,github/datapythonista,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2964314103,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"I'm -1 on supporting both, but I missed today's meeting and I'm +1 on over-weighting the opinions of people who show up. Was there discussion of how to disambiguate a sequence versus a column label that is a tuple?",,,,,,Anecdotal,comment,,,,,,,,2025-06-11,github/jbrockmendel,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2964450480,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"Thanks @jbrockmendel for the comment. We discussed your point of view which you previously shared, on wanting just one way. I think everybody in the call was also -1 in supporting both (including myself). It's just that in this particular case not supporting both seems even worse, for the reasons in my previous comment. We did discuss about ambiguity and tuples. I'll make sure I'll test every possible case in that sense, but if I don't miss anything, the dual API doesn't make things worse If a …",,,,,,Anecdotal,comment,,,,,,,,2025-06-12,github/datapythonista,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2965627225,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"FWIW, my uninformed opinion matches @jbrockmendel's. Especially when all it costs to avoid that ambiguity is a `*` to unpack the sequence (example 2 below). ``` df.select(""a"", ""b"") # 1. select the two columns: ""a"", ""b"" df.select([""a"", ""b""]) # 2. select the two columns: ""a"", ""b"" df.select(*[""a"", ""b""]) # 3. select the two columns: ""a"", ""b"" df.select((""a"", ""b"")) # 4. select the *one* column: (""a"", ""b"") ``` But this isn't the only place pandas treats tuples differently from other containers, and I …",,,,,,Anecdotal,comment,,,,,,,,2025-06-12,github/TomAugspurger,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2966300070,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"If I understand correctly that it'd be better to not support example 2 (if example 1 is considered the best API, which I think in isolation mostly everybody thinks), then I think there is agreement. The reason to still support it is that it was considered confusing and inconsistent for users who are used to `.groupby([""col1"", ""col2""])`, `.sort_values([""col1"", ""col2""])`, `.reset_index([""col1"", ""col2""])` to not support `.select([""col1"", ""col2""])... Based on that, we have next options: 1. Only `.s…",,,,,,Anecdotal,comment,,,,,,,,2025-06-12,github/datapythonista,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2966542218,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"If anyone else has interest in the `select` / `filter` stuff, please go ahead, happy to hand over it. I already spent more time that I wanted in this, and it doesn't seem to be taking us anywhere.",,,,,,Anecdotal,comment,,,,,,,,2025-06-20,github/datapythonista,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2992691964,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
BUG: Fix pyarrow categoricals not working for pivot and multiindex - [X] closes #53051 - [x] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature - [x] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit). - [x] Added [type annotations](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#type-hin…,,,,,,Anecdotal,issue,,,,,,,,2025-03-28,github/robin-mader-bis,https://github.com/pandas-dev/pandas/pull/61193,repo: pandas-dev/pandas | keyword: best practice | state: closed
"Hey @mroeschke, Thanks for the Review! All comments should be adressed. Could you have another look if everything looks good?",,,,,,Anecdotal,comment,,,,,,,,2025-04-09,github/robin-mader-bis,https://github.com/pandas-dev/pandas/pull/61193#issuecomment-2789640030,repo: pandas-dev/pandas | issue: BUG: Fix pyarrow categoricals not working for pivot and multiindex | keyword: best practice
"BUG: fix read_json ignoring the dtype with the pyarrow engine - Added code to check if dtypes have been passed in, if they are, read the JSON with the schema built from those dtypes - Split read JSON function up for the respective engines - [X] closes #59516 (Replace xxxx with the GitHub issue number) - [X] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature - [X] All [code checks passed…",,,,,,Anecdotal,issue,,,,,,,,2025-02-24,github/will-larkin,https://github.com/pandas-dev/pandas/pull/60997,repo: pandas-dev/pandas | keyword: best practice | state: closed
I believe that issue appears in pyarrow < 16. In your test you can check that pyarrow version and filter the warning if that is detected,,,,,,Anecdotal,comment,,,,,,,,2025-03-05,github/WillAyd,https://github.com/pandas-dev/pandas/pull/60997#issuecomment-2701777991,repo: pandas-dev/pandas | issue: BUG: fix read_json ignoring the dtype with the pyarrow engine | keyword: best practice
"> I believe that issue appears in pyarrow < 16. In your test you can check that pyarrow version and filter the warning if that is detected Looks like that has worked, thanks!",,,,,,Anecdotal,comment,,,,,,,,2025-03-05,github/will-larkin,https://github.com/pandas-dev/pandas/pull/60997#issuecomment-2701901970,repo: pandas-dev/pandas | issue: BUG: fix read_json ignoring the dtype with the pyarrow engine | keyword: best practice
STY: Enable shellcheck pre-commit hook Shellcheck ensures that our shell scripts follow some best practices https://www.shellcheck.net/,,,,,,Anecdotal,issue,,,,,,,,2025-01-29,github/mroeschke,https://github.com/pandas-dev/pandas/pull/60817,repo: pandas-dev/pandas | keyword: best practice | state: closed
"ENH: .isin() method should use __contains__ rather than __iter__ for user-defined classes to determine presence. ### Feature Type - [X] Adding new functionality to pandas - [X] Changing existing functionality in pandas - [ ] Removing existing functionality in pandas ### Problem Description Right now, if you would define a user class: ```python class MyClass: def __init__(self): self.collection = [1, 2, 3] self.another_collection = [4, 5, 6] def __contains__(self, item): return item in self.coll…",,,,,,Anecdotal,issue,,,,,,,,2024-06-18,github/f3ss1,https://github.com/pandas-dev/pandas/issues/59041,repo: pandas-dev/pandas | keyword: best practice | state: closed
`isin` uses a hash table for performance - this would not work with `__contains__`. Is there a real-world example where the results would differ? I'd be okay with leaking a bit of the implementation details in the docstring - that `isin` iterates through the values in the passed list-like.,,,,,,Anecdotal,comment,,,,,,,,2024-06-19,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/59041#issuecomment-2179475719,repo: pandas-dev/pandas | issue: ENH: .isin() method should use __contains__ rather than __iter__ for user-defined classes to determine presence. | keyword: best practice
"I believe the real-life example I have might not be the best programming practice. In my scenario, I have a dictionary that maps item names to item instances, and the `__contains__` method works for both class names and instances. However, I'm starting to doubt if this is a good design choice in fact. Still, I think that the current `isin` implementation and its documentation need some adjustments as they seem a bit misleading. Although it mentions that the object should be `Iterable`, implying…",,,,,,Anecdotal,comment,,,,,,,,2024-06-24,github/f3ss1,https://github.com/pandas-dev/pandas/issues/59041#issuecomment-2185761598,repo: pandas-dev/pandas | issue: ENH: .isin() method should use __contains__ rather than __iter__ for user-defined classes to determine presence. | keyword: best practice
"> Although it mentions that the object should be `Iterable`, implying that `__contains__` might not be used, it also suggests using a `dict`, which contradicts this. Can you elaborate what the contradiction is?",,,,,,Anecdotal,comment,,,,,,,,2024-06-24,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/59041#issuecomment-2187435216,repo: pandas-dev/pandas | issue: ENH: .isin() method should use __contains__ rather than __iter__ for user-defined classes to determine presence. | keyword: best practice
"I believe my comment might be misleading :) It's not the contradiction in general for Python (of course `dict` is iterable as well) rather than in my personal opinion: when I see a `dict`, the first thing I think about is hashing and doing stuff like `my_dict[""a""]` which leads to thinking that it would use `__contains__`.",,,,,,Anecdotal,comment,,,,,,,,2024-06-26,github/f3ss1,https://github.com/pandas-dev/pandas/issues/59041#issuecomment-2190984087,repo: pandas-dev/pandas | issue: ENH: .isin() method should use __contains__ rather than __iter__ for user-defined classes to determine presence. | keyword: best practice
"Hello, > Is there a real-world example where the results would differ? I think there is yes: **bloom filters**. I would like to filter a pandas dataframe with a bloom filter, but stumbled on this issue: ``` TypeError: only list-like objects are allowed to be passed to isin(), you passed a `Bloom` ``` The problem is that you cannot iterate on such data structures, it doesn't make sense. The only thing you can do is test the membership of an element.",,,,,,Anecdotal,comment,,,,,,,,2025-01-13,github/Ezibenroc,https://github.com/pandas-dev/pandas/issues/59041#issuecomment-2588189215,repo: pandas-dev/pandas | issue: ENH: .isin() method should use __contains__ rather than __iter__ for user-defined classes to determine presence. | keyword: best practice
"My main opposition to supporting this in isin is that it would require making isin use a non-vectorized approach, which makes the semantics more confusing.",,,,,,Anecdotal,comment,,,,,,,,2025-01-14,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/59041#issuecomment-2588529705,repo: pandas-dev/pandas | issue: ENH: .isin() method should use __contains__ rather than __iter__ for user-defined classes to determine presence. | keyword: best practice
DOC: PyArrow documentation for round trip conversions with pandas ### Pandas version checks - [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/) ### Location of the documentation https://pandas.pydata.org/pandas-docs/stable/reference/arrays.html#pyarrow ### Documentation problem This issue was first brought up: https://github.com/pandas-dev/pandas/issues/50074 The documentation mentions two different approaches…,,,,,,Anecdotal,issue,,,,,,,,2025-01-01,github/joansigh,https://github.com/pandas-dev/pandas/issues/60638,repo: pandas-dev/pandas | keyword: best practice | state: closed
BUG: inconsistant parsing between Timestamp and to_datetime ### Pandas version checks - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas. ### Reproducible Example ```python pd.…,,,,,,Anecdotal,issue,,,,,,,,2023-03-24,github/arnaudlegout,https://github.com/pandas-dev/pandas/issues/52167,repo: pandas-dev/pandas | keyword: best practice | state: closed
"thanks for the report this is expected: `to_datetime` can work on an array of elements, so it needs to check that they can all be parsed with the same format, whereas `Timestamp` only parses a single element so there's no risk of inconsistency between elements having said that, the warning could probably be extended to say ``` Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.…",,,,,,Anecdotal,comment,,,,,,,,2023-03-24,github/MarcoGorelli,https://github.com/pandas-dev/pandas/issues/52167#issuecomment-1483100006,repo: pandas-dev/pandas | issue: BUG: inconsistant parsing between Timestamp and to_datetime | keyword: best practice
"I still don't get it. I understand that `to_datetime` has to possibly parse a sequence of dates. It tries to infer the format from the first date and apply the same formatting to others (unless the risky `format='mixed'` option is used). But in my case, there is a single date passed to `to_datetime` and there is no ambiguity on this first date. I don't understand why there should be a `UserWarning` here.",,,,,,Anecdotal,comment,,,,,,,,2023-03-24,github/arnaudlegout,https://github.com/pandas-dev/pandas/issues/52167#issuecomment-1483144261,repo: pandas-dev/pandas | issue: BUG: inconsistant parsing between Timestamp and to_datetime | keyword: best practice
arguably we could only warn if there are at least two non-null elements maybe that's worth doing - interested in submitting a PR?,,,,,,Anecdotal,comment,,,,,,,,2023-03-24,github/MarcoGorelli,https://github.com/pandas-dev/pandas/issues/52167#issuecomment-1483156253,repo: pandas-dev/pandas | issue: BUG: inconsistant parsing between Timestamp and to_datetime | keyword: best practice
"I had a look at the code. It looks a bit harder than I thought. The exception is produced by `core.tools.datetimes._guess_datetime_format_for_array`. This method simply tries to guess the datetime format from the first non-NaN element. If this element format cannot be guessed the warning is raised. So this warning is not raised when two elements in the same array do not have the same format, but when the first non-NaN element format cannot be guessed. Suppressing this warning in this method whe…",,,,,,Anecdotal,comment,,,,,,,,2023-03-24,github/arnaudlegout,https://github.com/pandas-dev/pandas/issues/52167#issuecomment-1483237226,repo: pandas-dev/pandas | issue: BUG: inconsistant parsing between Timestamp and to_datetime | keyword: best practice
"By the way, is the message of the warning correct. I have no idea what ""falling back to 'dateutil'"" means and implies. I would personally just drop this sentence from the warning message as it seems uninformative for a user.",,,,,,Anecdotal,comment,,,,,,,,2023-03-24,github/arnaudlegout,https://github.com/pandas-dev/pandas/issues/52167#issuecomment-1483241587,repo: pandas-dev/pandas | issue: BUG: inconsistant parsing between Timestamp and to_datetime | keyword: best practice
"I'd suggest making the change within `_guess_datetime_format_for_array`, i.e. ```diff - warnings.warn( - ""Could not infer format, so each element will be parsed "" - ""individually, falling back to `dateutil`. To ensure parsing is "" - ""consistent and as-expected, please specify a format."", - UserWarning, - stacklevel=find_stack_level(), - ) + if tslib.first_non_null(arr[first_non_null+1:]) != -1: + warnings.warn( + ""Could not infer format, so each element will be parsed "" + ""individually, falling…",,,,,,Anecdotal,comment,,,,,,,,2023-03-24,github/MarcoGorelli,https://github.com/pandas-dev/pandas/issues/52167#issuecomment-1483243159,repo: pandas-dev/pandas | issue: BUG: inconsistant parsing between Timestamp and to_datetime | keyword: best practice
"Do you have any specific reasons for this suggestion? `_guess_datetime_format_for_array` is called to know whether the format can be guessed, the length of the passed array should be irrelevant to the behavior of this method. I expect this method to return either a format or None, not to raise a warning linked to the length of the array. According to me it is more relevant to raise the warning in `_convert_listlike_datetimes`.",,,,,,Anecdotal,comment,,,,,,,,2023-03-24,github/arnaudlegout,https://github.com/pandas-dev/pandas/issues/52167#issuecomment-1483266640,repo: pandas-dev/pandas | issue: BUG: inconsistant parsing between Timestamp and to_datetime | keyword: best practice
"because in there we're already doing the check for the first non-null item if you wanted to do it one level up, you'd need to do that check again, or return the index of the first non-null item",,,,,,Anecdotal,comment,,,,,,,,2023-03-24,github/MarcoGorelli,https://github.com/pandas-dev/pandas/issues/52167#issuecomment-1483274854,repo: pandas-dev/pandas | issue: BUG: inconsistant parsing between Timestamp and to_datetime | keyword: best practice
"@MarcoGorelli you were faster than me! I got your point on checking for non-null item. Still have one comment on the warning message. ``` ""Could not infer format, so each element will be parsed "" ""individually, falling back to `dateutil`. To ensure parsing is "" ""consistent and as-expected, please specify a format."" ``` When I read this message, I understand that the date cannot be parsed. Indeed, if you cannot find the format of a date, how it can be parsed anyway later. This ""Could not infer f…",,,,,,Anecdotal,comment,,,,,,,,2023-03-25,github/arnaudlegout,https://github.com/pandas-dev/pandas/issues/52167#issuecomment-1483781817,repo: pandas-dev/pandas | issue: BUG: inconsistant parsing between Timestamp and to_datetime | keyword: best practice
"> @MarcoGorelli you were faster than me! Apologies - I'd normally prefer to mentor people through contributions, but with the final 2.0.0 release on the horizon (possibly next week!), I figured I'd just go ahead and try to get this in Regarding the warning - I'll try to explain what's going on: - first, pandas tries to infer the format. It can't guess all formats, but if it can guess the format of the given element, then it will be parsed using pandas' own parsers - if pandas can't infer the fo…",,,,,,Anecdotal,comment,,,,,,,,2023-03-25,github/MarcoGorelli,https://github.com/pandas-dev/pandas/issues/52167#issuecomment-1483783288,repo: pandas-dev/pandas | issue: BUG: inconsistant parsing between Timestamp and to_datetime | keyword: best practice
"Thanks for the clarification, I would suggest : `""Could not infer format. Falling back to a slower and possibly inconsistent per-date parsing. Specifying the format is recommended for faster and consistent parsing."" (24 words, 164 chars vs. 26 words, 170 chars in the original message)` > I wish 😄 But the legendary @WillAyd has some blog posts on debugging C extensions at https://willayd.com/ great, thanks!",,,,,,Anecdotal,comment,,,,,,,,2023-03-25,github/arnaudlegout,https://github.com/pandas-dev/pandas/issues/52167#issuecomment-1483790506,repo: pandas-dev/pandas | issue: BUG: inconsistant parsing between Timestamp and to_datetime | keyword: best practice
"Hey! I have the same problem, if I use `read_csv` `data = pd.read_csv('data.csv', index_col='Date', parse_dates=True)` The data in `data.csv` are ``` Date,Fremont Bridge Total,Fremont Bridge East Sidewalk,Fremont Bridge West Sidewalk 11/01/2019 12:00:00 AM,12,7,5 11/01/2019 01:00:00 AM,7,0,7 11/01/2019 02:00:00 AM,1,0,1 11/01/2019 03:00:00 AM,6,6,0 11/01/2019 04:00:00 AM,6,5,1 11/01/2019 05:00:00 AM,20,9,11 11/01/2019 06:00:00 AM,97,43,54 ``` The warning message what I get then: ``` UserWarning…",,,,,,Anecdotal,comment,,,,,,,,2023-07-17,github/ax-va,https://github.com/pandas-dev/pandas/issues/52167#issuecomment-1637503247,repo: pandas-dev/pandas | issue: BUG: inconsistant parsing between Timestamp and to_datetime | keyword: best practice
"You are right. ``` pd.read_csv('data.csv',` index_col='Date', parse_dates=True, date_format=""%m/%d/%Y %I:%M:%S `%p"") ``` parses correctly",,,,,,Anecdotal,comment,,,,,,,,2023-07-17,github/ax-va,https://github.com/pandas-dev/pandas/issues/52167#issuecomment-1637663349,repo: pandas-dev/pandas | issue: BUG: inconsistant parsing between Timestamp and to_datetime | keyword: best practice
"hi everyone, im new using pandas, I am trying to analyze some data and I get the following UserWarning: UserWarning: Could not infer format, so each element will be parsed individually, falling back to dateutil. To ensure parsing is consistent and as-expected, please specify a format. df['Fecha'] = pd.to_datetime(df.Date) this is my dataframe code: df['Fecha'] = pd.to_datetime(df.Date) df.drop(columns='Date', inplace=True) Does anyone have a solution?",,,,,,Anecdotal,comment,,,,,,,,2023-08-07,github/Chinaskidev,https://github.com/pandas-dev/pandas/issues/52167#issuecomment-1667024544,repo: pandas-dev/pandas | issue: BUG: inconsistant parsing between Timestamp and to_datetime | keyword: best practice
"@MarcoGorelli I also have an issue with this date parsing message. I am trying to parse a date series which has 2 formats merged together: `08-06-2024 00:00:00` and `8/14/2024 12:00:00 AM` (I know that this is an odd case, but can't do anything about it). When I try to parse this, I get the error mentioned above. What do you suggest is the best way for me to handle this case as I don't completely understand how mixed type are handled. Someone on stack over flow suggested to read data in both fo…",,,,,,Anecdotal,comment,,,,,,,,2024-11-12,github/ReetiMauryaCrest,https://github.com/pandas-dev/pandas/issues/52167#issuecomment-2470222906,repo: pandas-dev/pandas | issue: BUG: inconsistant parsing between Timestamp and to_datetime | keyword: best practice
"I have the same problem as @ReetiMauryaCrest. I don't have control over how the data is entered, so I get a mixture of formats for dates. I want to be sure I'm using best practices, what would y'all suggest?",,,,,,Anecdotal,comment,,,,,,,,2024-11-13,github/afranklin238,https://github.com/pandas-dev/pandas/issues/52167#issuecomment-2473861273,repo: pandas-dev/pandas | issue: BUG: inconsistant parsing between Timestamp and to_datetime | keyword: best practice
hey > I don't completely understand how mixed type are handled if you pass `format='mixed'` then formats are inferred row-by-row but personally i'd suggest doing some validation before your data reaches pandas,,,,,,Anecdotal,comment,,,,,,,,2024-11-13,github/MarcoGorelli,https://github.com/pandas-dev/pandas/issues/52167#issuecomment-2473901768,repo: pandas-dev/pandas | issue: BUG: inconsistant parsing between Timestamp and to_datetime | keyword: best practice
"Thanks @MarcoGorelli I ended up making my custom parsing function based on 2 date formats I was receiving and applying it on the row. My main concern was that it would be too slow but I haven't found any large delays till now, so I think it would work for me for now.",,,,,,Anecdotal,comment,,,,,,,,2024-11-16,github/ReetiMauryaCrest,https://github.com/pandas-dev/pandas/issues/52167#issuecomment-2480488115,repo: pandas-dev/pandas | issue: BUG: inconsistant parsing between Timestamp and to_datetime | keyword: best practice
BUG: Failed to import pandas <2.1.0 witn numpy >=2.0.0 ### Pandas version checks - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas. ### Reproducible Example In a clean python …,,,,,,Anecdotal,issue,,,,,,,,2024-06-19,github/yfa-vagelis,https://github.com/pandas-dev/pandas/issues/59052,repo: pandas-dev/pandas | keyword: best practice | state: closed
pandas 2.2.2 is the first version that is compatible with numpy 2.0. I'm not sure how feasible it is to edit the metadata to add in an upper bound on numpy version to an already released version. cc @lithomas1 https://pandas.pydata.org/docs/whatsnew/v2.2.2.html#pandas-2-2-2-is-now-compatible-with-numpy-2-0,,,,,,Anecdotal,comment,,,,,,,,2024-06-19,github/asishm,https://github.com/pandas-dev/pandas/issues/59052#issuecomment-2179034499,repo: pandas-dev/pandas | issue: BUG: Failed to import pandas <2.1.0 witn numpy >=2.0.0 | keyword: best practice
"We cannot retroactively change packaging metadata on previously released versions. (IIRC, a pin was put in place in time for the pandas 2.1.x series) You should pin your numpy version to less than 2, or install pandas 2.2.2 as @asishm mentions. Pinning numpy for future releases of pandas is something that we'll definitely consider (but figuring out which version of numpy to pin to is a non-trivial)",,,,,,Anecdotal,comment,,,,,,,,2024-06-19,github/lithomas1,https://github.com/pandas-dev/pandas/issues/59052#issuecomment-2179213305,repo: pandas-dev/pandas | issue: BUG: Failed to import pandas <2.1.0 witn numpy >=2.0.0 | keyword: best practice
https://stackoverflow.com/questions/78650222/valueerror-numpy-dtype-size-changed-may-indicate-binary-incompatibility-expec,,,,,,Anecdotal,comment,,,,,,,,2024-07-08,github/RubTalha,https://github.com/pandas-dev/pandas/issues/59052#issuecomment-2215036995,repo: pandas-dev/pandas | issue: BUG: Failed to import pandas <2.1.0 witn numpy >=2.0.0 | keyword: best practice
">Pinning numpy for future releases of pandas is something that we'll definitely consider (but figuring out which version of numpy to pin to is a non-trivial) The best practice is to at least set the upper bound to prevent usage of next major version (e.g. `numpy<2.0.0` or `numpy~=1.4`). Otherwise, it's only a matter of time until all your released packages break forever.",,,,,,Anecdotal,comment,,,,,,,,2024-08-21,github/Ark-kun,https://github.com/pandas-dev/pandas/issues/59052#issuecomment-2299964429,repo: pandas-dev/pandas | issue: BUG: Failed to import pandas <2.1.0 witn numpy >=2.0.0 | keyword: best practice
"> We cannot retroactively change packaging metadata on previously released versions. I may be dense, but isn't this what the z in version x.y.z is for? If the 2.0 series has the problem, and latest version of 2.0 is 2.0.3, is there anything wrong with releasing a 2.0.4 where the only change is a changed numpy dependency? > (IIRC, a pin was put in place in time for the pandas 2.1.x series) I experienced the problem with 2.1.1. I don't know if it would have been different with 2.1.4. (And I can't…",,,,,,Anecdotal,comment,,,,,,,,2024-09-05,github/AllanHOlesenBW,https://github.com/pandas-dev/pandas/issues/59052#issuecomment-2330730889,repo: pandas-dev/pandas | issue: BUG: Failed to import pandas <2.1.0 witn numpy >=2.0.0 | keyword: best practice
"BUG: can't support numpy2.2,how to deal with it ### Pandas version checks - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas. ### Reproducible Example ```python A module that w…",,,,,,Anecdotal,issue,,,,,,,,2024-06-17,github/Franklyn1987,https://github.com/pandas-dev/pandas/issues/59023,repo: pandas-dev/pandas | keyword: best practice | state: closed
"This may be because a new major version of numpy was [released](https://pypi.org/project/numpy/#history) on June 16, and in pandas pyproject.toml (version 2.2.2) there is: ``` ""numpy>=1.22.4; python_version<'3.11'"", ""numpy>=1.23.2; python_version=='3.11'"", ""numpy>=1.26.0; python_version>='3.12'"", ``` which was causing problems in my case - I had to pin version of numpy to <2 to fix it.",,,,,,Anecdotal,comment,,,,,,,,2024-06-17,github/mbroton,https://github.com/pandas-dev/pandas/issues/59023#issuecomment-2172899166,"repo: pandas-dev/pandas | issue: BUG: can't support numpy2.2,how to deal with it | keyword: best practice"
"> This may be because a new major version of numpy was [released](https://pypi.org/project/numpy/#history) on June 16, and in pandas pyproject.toml (version 2.2.2) there is: > > ``` > ""numpy>=1.22.4; python_version<'3.11'"", > ""numpy>=1.23.2; python_version=='3.11'"", > ""numpy>=1.26.0; python_version>='3.12'"", > ``` > > which was causing problems in my case - I had to pin version of numpy to <2 to fix it. Wondering what the reason was for not setting an upper limit. It doesn't make sense to assum…",,,,,,Anecdotal,comment,,,,,,,,2024-06-17,github/adodo-adoli,https://github.com/pandas-dev/pandas/issues/59023#issuecomment-2173153190,"repo: pandas-dev/pandas | issue: BUG: can't support numpy2.2,how to deal with it | keyword: best practice"
"Just to clarify, this is a `bottleneck` issue, not a pandas one. Need to avoid `bottleneck`, install the pre-release version `'bottleneck>= 1.4.0rc5'`, or downgrade NumPy for now. (I assume `bottleneck` will do a release very soon, but there should be nothing to do in pandas.)",,,,,,Anecdotal,comment,,,,,,,,2024-06-17,github/seberg,https://github.com/pandas-dev/pandas/issues/59023#issuecomment-2173315070,"repo: pandas-dev/pandas | issue: BUG: can't support numpy2.2,how to deal with it | keyword: best practice"
"> Just to clarify, this is a `bottleneck` issue, not a pandas one. Need to avoid `bottleneck`, install the pre-release version `'bottleneck>= 1.4.0rc5'`, or downgrade NumPy for now. (I assume `bottleneck` will do a release very soon, but there should be nothing to do in pandas.) It is still not a good practice to define dependencies the way it is, unless there's a reason for it, right?",,,,,,Anecdotal,comment,,,,,,,,2024-06-17,github/adodo-adoli,https://github.com/pandas-dev/pandas/issues/59023#issuecomment-2173523169,"repo: pandas-dev/pandas | issue: BUG: can't support numpy2.2,how to deal with it | keyword: best practice"
"Yes, just to say the version bounds are correct here to the degree possible. Not saying it is may not be good to do it for other dependencies, but you can't do much for depdencies using calver anyway (they somewhat promise to not do major releases). For larger projects (like pandas), there is also the idea to pin to a version ~1 year in the future as allowing sufficient flexibility while preventing this type of thing and if someone uses semver things are fine. But, sloppy versioning has its adv…",,,,,,Anecdotal,comment,,,,,,,,2024-06-17,github/seberg,https://github.com/pandas-dev/pandas/issues/59023#issuecomment-2173547401,"repo: pandas-dev/pandas | issue: BUG: can't support numpy2.2,how to deal with it | keyword: best practice"
"Bottleneck has done its release, if you update it, things may just work for you now (assuming there is no other thing missing).",,,,,,Anecdotal,comment,,,,,,,,2024-06-18,github/seberg,https://github.com/pandas-dev/pandas/issues/59023#issuecomment-2175306674,"repo: pandas-dev/pandas | issue: BUG: can't support numpy2.2,how to deal with it | keyword: best practice"
"I had this same issue while working with OpenCV, you can give the solutions a try for Pandas instead. Save your files(if needed) and follow the drill **First try** downgrading NumPy as newer versions sometimes creates the problem if it does not solve your issue then go for the second try **Second try** Uninstall Anaconda (if you are using it) and install an older version of the same *as you have to install OpenCV then NumPy gets installed along with it (may happen with other packages too) and i…",,,,,,Anecdotal,comment,,,,,,,,2024-09-03,github/Handique-lab,https://github.com/pandas-dev/pandas/issues/59023#issuecomment-2327212936,"repo: pandas-dev/pandas | issue: BUG: can't support numpy2.2,how to deal with it | keyword: best practice"
BUILD: Pandas 1.5.3 is unusable due to incompatibility with Numpy 2.0.0 ### Installation check - [X] I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas). ### Platform macOS-14.5-arm64-arm-64bit ### Installation Method pip install ### pandas Version 1.5.3 ### Python Version 3.10.13 ### Installation Logs Installing `pandas==1.5.3` with either Pip or Poetry on Python >= 3.9 leads to an error upon the first import of Pand…,,,,,,Anecdotal,issue,,,,,,,,2024-07-18,github/rsayn,https://github.com/pandas-dev/pandas/issues/59270,repo: pandas-dev/pandas | keyword: best practice | state: closed
"You need pandas 2.2.2 to use numpy 2.0. Unfortunately, I don't think the release infrastructure still exists for pandas 1.5.3, so we aren't going to release another pandas in the 1.5.x series that fixes this. (the build system is completely different) We will try to pin numpy in future pandas , but figuring out which version of numpy to pin too requires some thought.",,,,,,Anecdotal,comment,,,,,,,,2024-07-18,github/lithomas1,https://github.com/pandas-dev/pandas/issues/59270#issuecomment-2237137396,repo: pandas-dev/pandas | issue: BUILD: Pandas 1.5.3 is unusable due to incompatibility with Numpy 2.0.0 | keyword: best practice
">Unfortunately, I don't think the release infrastructure still exists for pandas 1.5.3, so we aren't going to release another pandas in the 1.5.x series that fixes this. Is it possible to just take the last 1.x wheel, unpack it, change the requirement, bump the version and upload the fixed wheel to PyPI?",,,,,,Anecdotal,comment,,,,,,,,2024-08-23,github/Ark-kun,https://github.com/pandas-dev/pandas/issues/59270#issuecomment-2306064298,repo: pandas-dev/pandas | issue: BUILD: Pandas 1.5.3 is unusable due to incompatibility with Numpy 2.0.0 | keyword: best practice
">We will try to pin numpy in future pandas , but figuring out which version of numpy to pin too requires some thought. I'm not sure exact pinning is required. The best practice is to at least set the upper bound to prevent usage of next major version (e.g. `numpy<2.0.0` or `numpy~=1.4`). Otherwise, it's only a matter of time until all your released packages break forever (something that has just happened).",,,,,,Anecdotal,comment,,,,,,,,2024-08-23,github/Ark-kun,https://github.com/pandas-dev/pandas/issues/59270#issuecomment-2306065372,repo: pandas-dev/pandas | issue: BUILD: Pandas 1.5.3 is unusable due to incompatibility with Numpy 2.0.0 | keyword: best practice
"BUG: DataFrame.query can not work well with string ### Pandas version checks - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [ ] I have confirmed this bug exists on the main branch of pandas. ### Reproducible Example ```python # example1 USERS = pd.DataFrame( {'email':['a@g.com','b@g.com','b@g.com','c@g.com','d@g.com']}) print (USERS) # email # 0 a@…",,,,,,Anecdotal,issue,,,,,,,,2022-09-05,github/yanyu2015,https://github.com/pandas-dev/pandas/issues/48391,repo: pandas-dev/pandas | keyword: best practice | state: closed
"What is your main issue here? Why Example 1 errs out or undesired Example 2 result? See below explanation at the correct behavior of this method. Example 1 error In example 1, you are comparing a 5-row DataFrame to a 2-row DataFrame. So, the `DataFrame.query` call compares different length Series (5 values to 2 values). However, in example 2, you compare a 3-row DataFrame with a 3-row DataFrame. So the `DataFrame.query` call compares equal length Series of 3 values and hence no error is raised.…",,,,,,Anecdotal,comment,,,,,,,,2022-09-08,github/ParfaitG,https://github.com/pandas-dev/pandas/issues/48391#issuecomment-1240081348,repo: pandas-dev/pandas | issue: BUG: DataFrame.query can not work well with string | keyword: best practice
"thx, the key reason is **the query call is comparing row to row**. So, if I want to get the expected result. What's the best practice?",,,,,,Anecdotal,comment,,,,,,,,2022-09-08,github/yanyu2015,https://github.com/pandas-dev/pandas/issues/48391#issuecomment-1240114166,repo: pandas-dev/pandas | issue: BUG: DataFrame.query can not work well with string | keyword: best practice
"It is unclear what you are trying to do. Best practice is to use the appropriate method for the job. `DataFrame.query` works best for simple logical filters not really for matching values across DataFrames. Consider `DataFrame.merge`, `DataFrame.join`, or `Series.isin` for this need: ```python USERS.merge(EXCLUDE, on=""email"") USERS.set_index(""email"").join( EXCLUDE.set_index(""email""), how=""inner"" ) USERS[USERS[""email""].isin(EXCLUDE[""email""].tolist())] ``` Actually, you can use `DataFrame.query` …",,,,,,Anecdotal,comment,,,,,,,,2022-09-10,github/ParfaitG,https://github.com/pandas-dev/pandas/issues/48391#issuecomment-1242576398,repo: pandas-dev/pandas | issue: BUG: DataFrame.query can not work well with string | keyword: best practice
"BUG: DataFrame(data, ...) creates a copy when 'data' is a NumPy array (pandas 3.0+) ### Pandas version checks - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas. ### Reproducib…",,,,,,Anecdotal,issue,,,,,,,,2024-06-04,github/jameslamb,https://github.com/pandas-dev/pandas/issues/58913,repo: pandas-dev/pandas | keyword: best practice | state: closed
Thanks for the report. It appears this was changed/enforced with copy-on-write in https://github.com/pandas-dev/pandas/pull/57254 cc @phofl I _think_ it's because with copy-on-write it was also decided that the underlying numpy array should be made read-only which would require a copy to not affect the input. ```python In [1]: import numpy as np ...: import pandas as pd + /opt/miniconda3/envs/pandas-dev/bin/ninja [1/1] Generating write_version_file with a custom command In [2]: arr = np.array([…,,,,,,Anecdotal,comment,,,,,,,,2024-06-04,github/mroeschke,https://github.com/pandas-dev/pandas/issues/58913#issuecomment-2148105125,"repo: pandas-dev/pandas | issue: BUG: DataFrame(data, ...) creates a copy when 'data' is a NumPy array (pandas 3.0+) | keyword: best practice"
"Thanks very much @mroeschke . I'm not familiar enough with the tradeoffs, so don't have a strong opinion about whether the `pandas` 2.x behavior for this case should be restored. I wasn't sure if this was intentional or not, so just wanted to report it.",,,,,,Anecdotal,comment,,,,,,,,2024-06-05,github/jameslamb,https://github.com/pandas-dev/pandas/issues/58913#issuecomment-2148643351,"repo: pandas-dev/pandas | issue: BUG: DataFrame(data, ...) creates a copy when 'data' is a NumPy array (pandas 3.0+) | keyword: best practice"
"IIRC the read-only behavior was also needed in order to avoid mutations of a pandas object by modifying the underlying numpy array (e.g. `df.values[0, 0] = ""new_value""`) which would sidestep the reference counting mechanism behind copy on write. I agree it would be nice to not have to always copy a numpy array. I'm not sure if there exists a work-around to only copy if the array is about to be written to.",,,,,,Anecdotal,comment,,,,,,,,2024-06-05,github/mroeschke,https://github.com/pandas-dev/pandas/issues/58913#issuecomment-2148663591,"repo: pandas-dev/pandas | issue: BUG: DataFrame(data, ...) creates a copy when 'data' is a NumPy array (pandas 3.0+) | keyword: best practice"
"The copy behaviour change in `pd.DataFrame` was indeed intentional. Apart from making the array read-only in `.values` / `to_numpy`, the main reason for doing this by default is that we want to be able to give (at least by default) the guarantee of _""changing values in a specific `df` can only be done by specifically mutating that `df` (e.g. `df.lioc[..] = ..`), and doing so only affects that `df`""_ (which is one of the main behaviour aspects of the CoW proposal). If `df = pd.DataFrame(arr)` di…",,,,,,Anecdotal,comment,,,,,,,,2024-06-05,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/58913#issuecomment-2150515599,"repo: pandas-dev/pandas | issue: BUG: DataFrame(data, ...) creates a copy when 'data' is a NumPy array (pandas 3.0+) | keyword: best practice"
"In the Copy-on-Write migration guide (https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html) there is a small section on this ""Constructors now copy NumPy arrays by default"", which says: > The Series and DataFrame constructors now copies a NumPy array by default when not otherwise specified. This was changed to avoid mutating a pandas object when the NumPy array is changed inplace outside of pandas. You can set `copy=False` to avoid this copy. Would it be useful to expand that? (note…",,,,,,Anecdotal,comment,,,,,,,,2024-06-05,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/58913#issuecomment-2150531992,"repo: pandas-dev/pandas | issue: BUG: DataFrame(data, ...) creates a copy when 'data' is a NumPy array (pandas 3.0+) | keyword: best practice"
"I'm not sure whether this is relevant or interesting, but it looks like polars decided not to make a copy by default. ```python import numpy as np import polars as pl X = np.arange(4).reshape(2, 2, order=""F"") df = pl.DataFrame(X) print(df) X[0, 0] = 42 print(df) ``` ``` shape: (2, 2) ┌──────────┬──────────┐ │ column_0 ┆ column_1 │ │ --- ┆ --- │ │ i32 ┆ i32 │ ╞══════════╪══════════╡ │ 0 ┆ 2 │ │ 1 ┆ 3 │ └──────────┴──────────┘ shape: (2, 2) ┌──────────┬──────────┐ │ column_0 ┆ column_1 │ │ --- ┆ …",,,,,,Anecdotal,comment,,,,,,,,2024-06-06,github/s-banach,https://github.com/pandas-dev/pandas/issues/58913#issuecomment-2152956444,"repo: pandas-dev/pandas | issue: BUG: DataFrame(data, ...) creates a copy when 'data' is a NumPy array (pandas 3.0+) | keyword: best practice"
"Interesting. It indeed doesn't copy the numpy array, so as you show polars isn't protected from the array getting modified. But it still seems to ensure when you modify the dataframe, another dataframe viewing the same data doesn't change (I assume it does a copy on write at that point): ```python import numpy as np import polars as pl X = np.arange(4).reshape(2, 2, order=""F"") df1 = pl.DataFrame(X) df2 = pl.DataFrame(X) df1[0, ""column_0""] = 42 print(df1) print(df2) ``` ``` shape: (2, 2) ┌──────…",,,,,,Anecdotal,comment,,,,,,,,2024-06-07,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/58913#issuecomment-2155139132,"repo: pandas-dev/pandas | issue: BUG: DataFrame(data, ...) creates a copy when 'data' is a NumPy array (pandas 3.0+) | keyword: best practice"
"> *In the Copy-on-Write migration guide (https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html) there is a small section on this ""Constructors now copy NumPy arrays by default""* Thank you so much! I hadn't found that while searching, but that **exactly** answers my question here. And I **really** appreciate our explanation in https://github.com/pandas-dev/pandas/issues/58913#issuecomment-2150515599, totally makes sense to me why the default behavior is changing. Thanks very much for …",,,,,,Anecdotal,comment,,,,,,,,2024-06-09,github/jameslamb,https://github.com/pandas-dev/pandas/issues/58913#issuecomment-2156331592,"repo: pandas-dev/pandas | issue: BUG: DataFrame(data, ...) creates a copy when 'data' is a NumPy array (pandas 3.0+) | keyword: best practice"
DOC: Edited/proofread the website's getting started page for readability - [ ] closes #xxxx (Replace xxxx with the GitHub issue number) - [ ] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature - [ ] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit). - [ ] Added [type annotations](https://pandas.pydata.org/pandas-docs/de…,,,,,,Anecdotal,issue,,,,,,,,2024-05-28,github/shindelr,https://github.com/pandas-dev/pandas/pull/58853,repo: pandas-dev/pandas | keyword: best practice | state: closed
"I don't believe they're new, I think they're already part of the project. I had accidentally staged them and was unable to remove them from my commit. I didn't make any modifications to any images, or add/remove any images. The only thing I meant to be in this pr were my proposed changes to `index.rst`. Sorry about that!",,,,,,Anecdotal,comment,,,,,,,,2024-05-30,github/shindelr,https://github.com/pandas-dev/pandas/pull/58853#issuecomment-2138668962,repo: pandas-dev/pandas | issue: DOC: Edited/proofread the website's getting started page for readability | keyword: best practice
"All good. You can git rebase to keep the commit history clean. On Wed, May 29, 2024 at 21:44 Robin Shindelman ***@***.***> wrote: > I don't believe they're new, I think they're already part of the project. > I had accidentally staged them and was unable to remove them from my > commit. I didn't make any modifications to any images, or add/remove any > images. The only thing I meant to be in this pr were my proposed changes to > index.rst. Sorry about that! > > — > Reply to this email directly, …",,,,,,Anecdotal,comment,,,,,,,,2024-05-30,github/chaityacshah,https://github.com/pandas-dev/pandas/pull/58853#issuecomment-2138804397,repo: pandas-dev/pandas | issue: DOC: Edited/proofread the website's getting started page for readability | keyword: best practice
"Yes, please! On Thu, May 30, 2024 at 11:38 AM Robin Shindelman ***@***.***> wrote: > Great, should I go ahead an close this pr? Then re-commit rebased with a > clean commit history, then open a new pr? > > — > Reply to this email directly, view it on GitHub > <https://github.com/pandas-dev/pandas/pull/58853#issuecomment-2140622756>, > or unsubscribe > <https://github.com/notifications/unsubscribe-auth/ADLDVCDT2V5N7BUD4ODXE6TZE5W2VAVCNFSM6AAAAABIN27NHOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCN…",,,,,,Anecdotal,comment,,,,,,,,2024-05-31,github/chaityacshah,https://github.com/pandas-dev/pandas/pull/58853#issuecomment-2141130596,repo: pandas-dev/pandas | issue: DOC: Edited/proofread the website's getting started page for readability | keyword: best practice
"> Great, should I go ahead an close this pr? Then re-commit rebased with a clean commit history, then open a new pr? You can just run ```bash git checkout upstream/main doc/ git commit -m""Undo doc changes"" git push origin HEAD ``` And update this PR",,,,,,Anecdotal,comment,,,,,,,,2024-05-31,github/mroeschke,https://github.com/pandas-dev/pandas/pull/58853#issuecomment-2142779358,repo: pandas-dev/pandas | issue: DOC: Edited/proofread the website's getting started page for readability | keyword: best practice
"Okay, I followed the steps from @mroeschke and updated the PR with a rebase, so the second commit has the changes to index.rst but does not have the random images. Is this correct?",,,,,,Anecdotal,comment,,,,,,,,2024-06-01,github/shindelr,https://github.com/pandas-dev/pandas/pull/58853#issuecomment-2143589635,repo: pandas-dev/pandas | issue: DOC: Edited/proofread the website's getting started page for readability | keyword: best practice
"It looks like your latest push still shows a diff for the images. I guess I would recommend creating a new pull request with just the changes to `doc/source/getting_started/index.rst`. (Also as a best practice, I would recommend creating a branch from your `main` branch and create a pull request from that branch)",,,,,,Anecdotal,comment,,,,,,,,2024-06-03,github/mroeschke,https://github.com/pandas-dev/pandas/pull/58853#issuecomment-2145769810,repo: pandas-dev/pandas | issue: DOC: Edited/proofread the website's getting started page for readability | keyword: best practice
"Okay, I went ahead and reset my main, checked out a new branch, and made the changes again with what I'm hoping is a clean pull request: https://github.com/pandas-dev/pandas/pull/58919 Thanks for your patience and assistance! Feel free to close this PR, or let me know it's okay to close it.",,,,,,Anecdotal,comment,,,,,,,,2024-06-04,github/shindelr,https://github.com/pandas-dev/pandas/pull/58853#issuecomment-2147992816,repo: pandas-dev/pandas | issue: DOC: Edited/proofread the website's getting started page for readability | keyword: best practice
Update Styler documentation for escaping HTML Adds a note to the docs about `Styler` and `escape` on untrusted HTML input.,,,,,,Anecdotal,issue,,,,,,,,2024-02-11,github/TomAugspurger,https://github.com/pandas-dev/pandas/pull/57365,repo: pandas-dev/pandas | keyword: best practice | state: closed
DOC: Add notice and example for `CategoricalDtype` with different ``categories_dtype`` - [x] closes #57259 (Replace xxxx with the GitHub issue number) - [ ] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature - [ ] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit). - [ ] Added [type annotations](https://pandas.pydata.org…,,,,,,Anecdotal,issue,,,,,,,,2024-02-06,github/luke396,https://github.com/pandas-dev/pandas/pull/57273,repo: pandas-dev/pandas | keyword: best practice | state: closed
"Thanks @luke396 ! I was hoping that this part would be edited: ```rst Two instances of :class:`~pandas.api.types.CategoricalDtype` compare equal whenever they have the same categories and order. When comparing two unordered categoricals, the order of the ``categories`` is not considered. ``` to be a complete and definitive definition, covering all cases right here. (instead of adding examples and having the user then guess the actual rules)",,,,,,Anecdotal,comment,,,,,,,,2024-02-06,github/VladimirFokow,https://github.com/pandas-dev/pandas/pull/57273#issuecomment-1929049214,repo: pandas-dev/pandas | issue: DOC: Add notice and example for `CategoricalDtype` with different ``categories_dtype`` | keyword: best practice
"More generally, I am confused: ```rst A categorical's type is fully described by 1. ``categories``: a sequence of unique values and no missing values 2. ``ordered``: a boolean ``` ... If the `CategoricalDtype` is ""fully described"" by ``categories`` and ``order``, and ``categories`` is defined as a ""sequence of unique values and no missing values"" --> `[]` is a ""sequence of unique values and no missing values"" (yes, it's empty, but it is a sequence of 0 unique values and doesn't contain NaNs (th…",,,,,,Anecdotal,comment,,,,,,,,2024-02-06,github/VladimirFokow,https://github.com/pandas-dev/pandas/pull/57273#issuecomment-1929072485,repo: pandas-dev/pandas | issue: DOC: Add notice and example for `CategoricalDtype` with different ``categories_dtype`` | keyword: best practice
"> ``` > Two instances of :class:`~pandas.api.types.CategoricalDtype` compare equal > whenever they have the same categories and order. When comparing two > unordered categoricals, the order of the ``categories`` is not considered. > ``` Thank you, @VladimirFokow, for your advice on the PR. I hadn't considered it as thoroughly as you did. At the moment, the example in the file doesn't suit my needs well, which is why the PR is still in progress. Have to admit it, I actually don't have the same a…",,,,,,Anecdotal,comment,,,,,,,,2024-02-06,github/luke396,https://github.com/pandas-dev/pandas/pull/57273#issuecomment-1929182717,repo: pandas-dev/pandas | issue: DOC: Add notice and example for `CategoricalDtype` with different ``categories_dtype`` | keyword: best practice
"Maybe they meant that a `Categorical`👈 (not `CategoricalDtype`) is partly described by `.categories`... But in this case, `categories` is NOT a sequence. It's an `Index` (containing the categories, and of a certain `dtype` which is determined... automatically? - even if constructed from a numpy array of objects yet all integers - the dtype of this index will be int, not object... How is this dtype determined - completely disregarding the numpy dtype and just looking at the actual values? How to…",,,,,,Anecdotal,comment,,,,,,,,2024-02-06,github/VladimirFokow,https://github.com/pandas-dev/pandas/pull/57273#issuecomment-1929226661,repo: pandas-dev/pandas | issue: DOC: Add notice and example for `CategoricalDtype` with different ``categories_dtype`` | keyword: best practice
ENH: Add check for character limmit - [x] closes #56954 (Replace xxxx with the GitHub issue number) - [x] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature - [ ] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit). - [ ] Added [type annotations](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.…,,,,,,Anecdotal,issue,,,,,,,,2024-01-27,github/luke396,https://github.com/pandas-dev/pandas/pull/57103,repo: pandas-dev/pandas | keyword: best practice | state: closed
"As discussed in the context of https://github.com/pandas-dev/pandas/issues/56954#issuecomment-1911937560, we have incorporated a validation for the maximum row/column number within the code. https://github.com/pandas-dev/pandas/blob/9008ee5810c09bc907b5fdc36fc3c1dff4a50c55/pandas/io/formats/excel.py#L929-L934 Furthermore, I have introduced a character limit check and included corresponding notifications in the documentation.",,,,,,Anecdotal,comment,,,,,,,,2024-01-27,github/luke396,https://github.com/pandas-dev/pandas/pull/57103#issuecomment-1913072979,repo: pandas-dev/pandas | issue: ENH: Add check for character limmit | keyword: best practice
"The CI test failure appears to be linked to the changes made in the pull request, but I'm unable to reproduce it on my local machine (WSL2). Assistance may be needed.",,,,,,Anecdotal,comment,,,,,,,,2024-01-27,github/luke396,https://github.com/pandas-dev/pandas/pull/57103#issuecomment-1913159325,repo: pandas-dev/pandas | issue: ENH: Add check for character limmit | keyword: best practice
"> The CI test failure appears to be linked to the changes made in the pull request Caused by warnings from other packages, and should be resolved by incorporating `raise_on_extra_warnings=False`",,,,,,Anecdotal,comment,,,,,,,,2024-01-28,github/luke396,https://github.com/pandas-dev/pandas/pull/57103#issuecomment-1913455984,repo: pandas-dev/pandas | issue: ENH: Add check for character limmit | keyword: best practice
"JSON code removals and cleanups Not sure this code ever did anything - seems to be optimized out. Still more to be done but this works for a first pass As far as the functions go, adding `static` is a best practice for functions that aren't meant to be exported from a shared library (C defaults functions to extern visibility, which you could argue is unfortunate)",,,,,,Anecdotal,issue,,,,,,,,2023-11-30,github/WillAyd,https://github.com/pandas-dev/pandas/pull/56252,repo: pandas-dev/pandas | keyword: best practice | state: closed
"DISC: Validating internal docstring presence, completeness and style We currently have guidelines/checks for docstrings https://pandas.pydata.org/docs/development/contributing_docstring.html for the api facing methods/functions following the numpydoc docstring guide https://numpydoc.readthedocs.io/en/latest/format.html. since we also use sphinx for building the on-line documentation. Many of our internal functions don't have docstrings and this could be a barrier to new contributions. For the m…",,,,,,Anecdotal,issue,,,,,,,,2020-03-17,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/32773,repo: pandas-dev/pandas | keyword: lesson learned | state: open
"To start the discussion, I propose that we adopt google style guide for internal docstrings/code comments. http://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings We could also considering using pydocstyle http://www.pydocstyle.org/en/latest/ to validate docstrings against this style. (although may involve technically difficulties having two standards)",,,,,,Anecdotal,comment,,,,,,,,2020-03-17,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-600035771,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"What aspect of the google style guide do you propose to adopt? Or the full section about docstring comments? If that is the case, I don't really see why we would start using a different format to document parameters for internal functions vs public functions.",,,,,,Anecdotal,comment,,,,,,,,2020-03-17,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-600050117,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"The driver for adopting a different format, would be primarily to make it easier to have code checks for docstring coverage/quality for all of the codebase not just the public api. Is it feasible to use the current validation on all docstrings? The google style also includes styles for module docstrings and pydocstyle allows these to be validated. With the introduction of type annotations, having the types in the docstring is redundant (for internal functions) and duplication can allow inconsis…",,,,,,Anecdotal,comment,,,,,,,,2020-03-17,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-600064248,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"I would like the outcome of this discussion to be either the current ""pandas docstring guide"" renamed ""pandas api docstring guide"" with a second, much more lightweight guide.. ""pandas internal docstring guide"" or rewrite the current guide to acknowledge the distinction. https://github.com/pandas-dev/pandas/pull/32033#issuecomment-592517783",,,,,,Anecdotal,comment,,,,,,,,2020-03-17,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-600089934,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"> Or the full section about docstring comments? adopting ""section 3.8 Comments and Docstrings"" in it's entirety means that ""pandas internal docstring guide"" is effectively already written with pydocstyle providing the validation.",,,,,,Anecdotal,comment,,,,,,,,2020-03-17,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-600091527,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"I opened this issue primarily in order to improve coverage, so my motivation is to achieve greater breadth rather than depth. I therefore feel that having a more lightweight standard is more likely to achieve that goal. I do see issues with having a different standard and therefore opened the discussion with the google suggestion for feedback. I see some middle ground with the single standard approach, by just adopting a subset of the current standard for the internal docstrings. Potentially, w…",,,,,,Anecdotal,comment,,,,,,,,2020-03-17,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-600248502,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"> so my motivation is to achieve greater breadth rather than depth I'm not sure I understand the distinction. Can you give an example? > We could take the subset approach a bit further changing say the requirement for types where type annotations exist. This relates to a topic I've been thinking about: should we treat annotations as ""this is what you _should_ pass"" or ""you can safely assume that this is what _is_ passed""? e.g. in core.indexing we do the latter",,,,,,Anecdotal,comment,,,,,,,,2020-03-17,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-600250253,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"> I'm not sure I understand the distinction. Can you give an example? > ``` def sanitize_array( data, index, dtype=None, copy: bool = False, raise_cast_failure: bool = False ): """""" Sanitize input data to an ndarray, copy if specified, coerce to the dtype if specified. """""" ``` vs ``` def is_sequence(obj) -> bool: """""" Check if the object is a sequence of objects. String types are not included as sequences here. Parameters ---------- obj : The object to check Returns ------- is_sequence : bool Whe…",,,,,,Anecdotal,comment,,,,,,,,2020-03-17,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-600253788,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"> This relates to a topic I've been thinking about: should we treat annotations as ""this is what you _should_ pass"" or ""you can safely assume that this is what _is_ passed""? There's also another.. ""this is what you _could_ pass"". When I'm adding annotations, i go though the function and the inputs of the functions called to try and establish what types are acceptable. I think there maybe some merit in being more restrictive on the type accepted, i.e. _should_ pass. This is not the place for tha…",,,,,,Anecdotal,comment,,,,,,,,2020-03-17,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-600256785,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"> ""you can safely assume that this is what _is_ passed"" that is effectively what MonkeyType does. https://pypi.org/project/MonkeyType/",,,,,,Anecdotal,comment,,,,,,,,2020-03-17,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-600257668,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"First, I am -1 on using a different (conflicting) style guide for internal docstrings than for external ones. I personally don't see the advantage of that. > I think that time/effort creating/reviewing docstings for functions that are less complex/more obvious would be better spent on more complex/less obvious functions. That's a valid concern, but how does that relate to the styling of the docstrings? Because you have the feeling that with numpydoc style too much time goes to formatting detail…",,,,,,Anecdotal,comment,,,,,,,,2020-03-17,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-600271229,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"> That's a valid concern, but how does that relate to the styling of the docstrings? Because you have the feeling that with numpydoc style too much time goes to formatting details? And with google style this would be less? > mainly just against the formatting of args requiring types and conforming to the numpydoc type notation which is a different standard to the python typing notation used for type annotations. > Note that with numpydoc standard, we can also have a subset of the rules to check…",,,,,,Anecdotal,comment,,,,,,,,2020-03-17,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-600281371,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"> mainly just against the formatting of args requiring types and conforming to the numpydoc type notation Numpydoc doesn't require types (see https://numpydoc.readthedocs.io/en/latest/format.html#sections, it has an example of a parameter without type), it's only our internal validation that requires this. We could perfectly well decide to relax this. > That's basically the aim of this issue and what i'd like to get consensus agreement on. A more lightweight standard. i.e. not to need to meet a…",,,,,,Anecdotal,comment,,,,,,,,2020-03-17,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-600284371,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"> First, I am -1 on using a different (conflicting) style guide for internal docstrings than for external ones. I personally don't see the advantage of that. I share this same concern. There's already a rather large cognitive load to contributions in terms of guidelines and checks that is a lot to remember as a maintainer, and probably off-putting to new contributors. So OK with validating internal docstrings, but I think should play by the same rules as all other docstrings in the code base",,,,,,Anecdotal,comment,,,,,,,,2020-03-18,github/WillAyd,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-600367344,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"> There's already a rather large cognitive load to contributions in terms of guidelines and checks that is a lot to remember as a maintainer Agreed. This issue is intended to address that. whether a solution is to have a more lightweight standard or use the existing standard for internal docstrings, adding automated validation of internal docstrings is key to this discussion and the checks need to be part of the CI so that maintainers don't need to check conformance to docstring style. I should…",,,,,,Anecdotal,comment,,,,,,,,2020-03-18,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-600513873,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
">So OK with validating internal docstrings, but I think should play by the same rules as all other docstrings in the code base If we keep the current standard/validation for docstrings, I wonder whether it is feasible to run validate_docstrings on any methods/functions touched in a PR on CI. I suspect this could be quite disruptive.",,,,,,Anecdotal,comment,,,,,,,,2020-03-18,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-600588978,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"> I think what would be interesting for the discussion then is to do a concrete proposal of which rules you want to relax or which rules ot keep. In the first instance, I think i'll run validate_docstrings on internal docstrings on a the odd PR from time to time that changes an internal docstring and see what we feel is inappropriate and maybe we can discuss which checks could be relaxed using specific cases, rather than select a subset from the outset. see #32769",,,,,,Anecdotal,comment,,,,,,,,2020-03-18,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-600620117,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"Did we ever fix our user facing docstrings to adhere to the docstring standards? I know @datapythonista used to run metrics on that; not sure if there is a current list FWIW I think worth finishing that off before diving into standards for the internal functions, as there is probably lessons to be learned",,,,,,Anecdotal,comment,,,,,,,,2020-03-19,github/WillAyd,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-601368447,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"A lot of progress was made, but we're far from done with the public API. @martinagvilas and @galuhsahid have a dashboard on the current state. Can you share the link please?",,,,,,Anecdotal,comment,,,,,,,,2020-03-19,github/datapythonista,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-601372110,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"> FWIW I think worth finishing that off before diving into standards for the internal functions, as there is probably lessons to be learned I think there are plenty of on-line resources and experience shared on how to use pandas. On the other hand, my concern is from a project sustainability perspective. > Many of our internal functions don't have docstrings and this could be a barrier to new contributions. For the more complex/undocumented internals, I am concerned that this may also affect th…",,,,,,Anecdotal,comment,,,,,,,,2020-03-19,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-601379368,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"> for me, getting the internal functions documented is just as important as the external api. But what is important for those internal functions is that they have a proper, understandable explanation of what they are doing / what they are used for. This is something that needs human review. On the other hand, most of the things that a style checker does (is there a space after the colon, did it use proper punctuation and capitalization, whitespace, ...), I care about much less for internal docs…",,,,,,Anecdotal,comment,,,,,,,,2020-03-19,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-601382284,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
[Here](https://github.com/pandanistas/docstring_notebooks) is the link to the dashboard inspecting docstrings errors in the public API that @datapythonista mentioned. The current errors are updated every hour.,,,,,,Anecdotal,comment,,,,,,,,2020-03-19,github/martinagvilas,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-601384808,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"> But what is important for those internal functions is that they have a proper, understandable explanation of what they are doing / what they are used for. This is something that needs human review. > > On the other hand, most of the things that a style checker does (is there a space after the colon, did it use proper punctuation and capitalization, whitespace, ...), I care about much less for internal docstrings (they also don't need to render properly online). > (maybe checking that all para…",,,,,,Anecdotal,comment,,,,,,,,2020-03-19,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-601394120,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
> [Here](https://github.com/pandanistas/docstring_notebooks) is the link to the dashboard inspecting docstrings errors in the public API that @datapythonista mentioned. The current errors are updated every hour. Thanks for posting @martinagvilas,,,,,,Anecdotal,comment,,,,,,,,2020-03-19,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-601396411,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
"Thanks @simonjayhawkins for mentioning this on the call today. I'd like to make the following proposal. - All functions/methods are to follow numpy docstrings, and are required to have the following. - A 1 line short description - Arguments and a description for each. Types are optional. - Return if applicable. - Any optional sections of the docstring (e.g. examples, notes), if they are present, are to following the standard rules. - All new functions/methods added should have docstrings. This …",,,,,,Anecdotal,comment,,,,,,,,2021-05-13,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/32773#issuecomment-840216855,"repo: pandas-dev/pandas | issue: DISC: Validating internal docstring presence, completeness and style | keyword: lesson learned"
Make pandas/io/sql.py work with sqlalchemy 2.0 - [x] closes #40686 - [ ] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature - [x] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit). - [ ] Added [type annotations](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#type-hints) to new arguments…,,,,,,Anecdotal,issue,,,,,,,,2022-09-15,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576,repo: pandas-dev/pandas | keyword: lesson learned | state: closed
"> I'm sorry I'm not so familiar with the SQLAlchemy changes, but do you mind explaining why we should filter `RemovedIn20Warnings` instead of updating the code? I assume we want to be compatible with both 1.4 and 2.0 which are not compatible. But feels like it'd make more sense to for now use `if` statements to support both versions. My approach is to update pandas/io/sql.py so that it can support both versions. I also had to update pandas/tests/io/test_sql.py, because pandas users will need ru…",,,,,,Anecdotal,comment,,,,,,,,2022-09-16,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1249231454,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"Thanks a lot for all the information @cdcadman, that's useful. There is something I still don't fully understand. Let me explain in detail, and please correct me if I'm wrong, I can surely be missing something. Let me use this example: ```python # 1.4 session.query(User).get(42) # 2.0 session.get(User, 42) ``` I see two different cases. **Case A: 2.0 syntax also works in 1.4** This case is easy, we should simply replace one by the other, right? **Case B: 2.0 syntax doesn't work on 1.4** Then, w…",,,,,,Anecdotal,comment,,,,,,,,2022-09-17,github/datapythonista,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1250058091,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"@datapythonista I agree with everything you wrote. I don't think we should censor `RemovedIn20Warning`. The only warnings filter I put into any code is this code in pandas/tests/io/test_sql.py: ``` if SQLALCHEMY_INSTALLED: # This only matters if the environment variable SQLALCHEMY_WARN_20 is set to 1. pytestmark = pytest.mark.filterwarnings(""error::sqlalchemy.exc.RemovedIn20Warning"") ``` This causes tests to fail if they raise RemovedIn20Warning. Your comment about checking sqlalchemy versions …",,,,,,Anecdotal,comment,,,,,,,,2022-09-17,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1250079650,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"@datapythonista I modified this PR's code, title, and original comment, and it is again ready for review. Since I am now testing 2.0-style sqlalchemy connectables instead of checking for warnings, I think this can close #40686. Since I did make a lot of changes, would you prefer that I start a new PR instead?",,,,,,Anecdotal,comment,,,,,,,,2022-09-23,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1256456148,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"Is it known when SQLAlchemy 2.0 will be released? If it comes before pandas 2.0 (late 2022/early 2023), maybe we can just bump the minimum version of SQLAlchemy to 2.0 and adopt the new syntax. I'm not in love in duplicating all the testing & having to support 2 versions of an optional dependency with different syntax. xref: https://github.com/pandas-dev/pandas/issues/44823",,,,,,Anecdotal,comment,,,,,,,,2022-09-23,github/mroeschke,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1256475116,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"@mroeschke I haven't seen a release date for sqlalchemy 2.0. Instead of duplicating all the tests, I could make the `future` argument of `create_engine` depend on either a global constant in test_sql.py or an environment variable. The `future` argument will be supported in sqlalchemy 2.0 and required to be `True`. I could also put this PR on hold until the beta release of sqlalchemy 2.0 comes out.",,,,,,Anecdotal,comment,,,,,,,,2022-09-23,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1256713833,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"I'm planning to make some changes to this PR. Firstly, I noticed that pandas.io.sql.execute is documented, right above this line: https://pandas.pydata.org/docs/user_guide/io.html?highlight=sql%20execute#engine-connection-examples . As it stands, my PR would make this return a context manager instead of a Results Iterable, and I don't think I need to make this change, so I will change it back. I plan to make `SQLDatabase` accept only a SQLAlchemy `Connection` and not an `Engine`. I would change…",,,,,,Anecdotal,comment,,,,,,,,2022-10-28,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1294907916,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"@datapythonista @mroeschke Based on your feedback, I took out all the test duplication, and instead ran the tests with sqlalchemy 2.0.0b2 installed to ensure that this can close #40686. This PR will allow pandas to work with sqlalchemy 1.4.16 (the documented minimum version) and higher, even after sqlalchemy 2.0 is released. I found a note here (written 10/13/2022) on the timing of sqlalchemy 2.0: https://www.sqlalchemy.org/blog/2022/10/13/sqlalchemy-2.0.0b1-released/ >we will likely move from …",,,,,,Anecdotal,comment,,,,,,,,2022-10-31,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1297071602,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
Thanks for your work on this @cdcadman. IMO I would still be partial on just supporting sqlalchemy 2.0 syntax/tests/min version when it becomes available.,,,,,,Anecdotal,comment,,,,,,,,2022-10-31,github/mroeschke,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1297416747,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"@mroeschke As a pandas/sqlalchemy user, it would be really helpful to get these changes into pandas sooner, so that I can get my code ready for sqlalchemy 2.0 sooner. This PR is making the kind of changes envisioned in the first paragraph of the sqlalchemy 2.0 migration document: https://docs.sqlalchemy.org/en/14/changelog/migration_20.html#overview . >The SQLAlchemy 2.0 transition presents itself in the SQLAlchemy 1.4 release as a series of steps that allow an application of any size or comple…",,,,,,Anecdotal,comment,,,,,,,,2022-11-01,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1298783273,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"Thanks @cdcadman. I misunderstood that sqlalchemy 1.4 already has 2.0 functionality, so we can adopt 2.0 syntax/functionality while pandas min version is 1.4. > I made three types of changes to pandas/tests/io/test_sql.py: Could you split these into 3 separate PRs? It's difficult for me to determine which changes correspond with a certain objective. Generally, 1 PR targeting 1 type of change is easier for review.",,,,,,Anecdotal,comment,,,,,,,,2022-11-03,github/mroeschke,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1302463388,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"@mroeschke , I split this into two commits. The tests pass after the first commit with sqlalchemy 1.4.44, but I had to modify `test_sql.py` to make it work with sqlalchemy 2.0.0b3.",,,,,,Anecdotal,comment,,,,,,,,2022-11-19,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1320788745,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"Unfortunately, I introduced a bug in #49531 . Now iteration through the result of `read_sql_query` with a `chunksize` argument can fail, since the connection already closed. I will create a new PR to fix that.",,,,,,Anecdotal,comment,,,,,,,,2022-11-29,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1330157296,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"@mroeschke, I found another bug in addition to the one I intend to resolve with #49967. The second bug is with `pandas.io.sql.execute`. If the caller passes a query that returns a resultset, then the connection might close before the caller can fetch the results. I'm having trouble finding a good workaround. Would it be ok to stop allowing either a sqlalchemy `Engine` or `str` to be passed to this method? It is not a well-documented method. The only mention I found of it is above this line: htt…",,,,,,Anecdotal,comment,,,,,,,,2022-12-09,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1344756733,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"Another potential solution to the `pandas.io.sql.execute` bug is to use `freeze` to fetch all the results before cleaning up. This is inefficient, and I will update the usage docs to point that out, but it will allow downstream code to continue working if it does not pull large amounts of data using `pandas.io.sql.execute`. It will also require updating the minimum version of sqlalchemy to 1.4.45, which is not yet released: https://github.com/sqlalchemy/sqlalchemy/discussions/8962",,,,,,Anecdotal,comment,,,,,,,,2022-12-09,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1344843261,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"Status update: I think this is ready, but it has two PRs in front of it (#50177 and #49967) and I'm still not getting the CI checks to all pass.",,,,,,Anecdotal,comment,,,,,,,,2022-12-13,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1349184851,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"There is one failing test unrelated to the changes I am making. I welcome any suggestions: ``` pandas/tests/io/parser/common/test_file_buffer_url.py::test_file_descriptor_leak[c_high] - AssertionError: ([], [pconn(fd=17, family=<AddressFamily.AF_INET: 2>, type=<SocketKind.SOCK_STREAM: 1>, laddr=addr(ip='10.1.0.145', port=36288), raddr=addr(ip='140.82.112.4', port=443), status='ESTABLISHED')]) ```",,,,,,Anecdotal,comment,,,,,,,,2023-01-06,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1373907318,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"@phofl , the first commit is split off in a separate PR: #49967 . I could make a new PR for the second commit, as it is a separate item: Make PandasSQL.execute arguments more precise. Some of my changes were to get mypy to work with the sqlalchemy 2.0 beta version - for example removing the import of `to_instance` from `sqlalchemy.types`.",,,,,,Anecdotal,comment,,,,,,,,2023-01-06,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1374029941,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"Ah got you, thx. Not necessary. Could you activate the warning here though? To check that ci passes. Set the env variable as in my pr",,,,,,Anecdotal,comment,,,,,,,,2023-01-06,github/phofl,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1374034068,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"@moritzmeister , I was hoping it could be included in pandas 1.5.3, but the decision was not to include it: https://github.com/pandas-dev/pandas/issues/49857#issuecomment-1374889050 . For your issue, I am surrounding the query string with `sqlalchemy.text`.",,,,,,Anecdotal,comment,,,,,,,,2023-01-30,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1408935831,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"Thanks for the link @cdcadman, then I suppose it will end up in the 2.0 release. Yeah I also fell back to wrapping the text. I am suprised that the sqlalchemy version was not pinned to <2.0.0 on 1.5.3 in the meantime.",,,,,,Anecdotal,comment,,,,,,,,2023-01-30,github/moritzmeister,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1409385794,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
ENH: make guess_datetime_format public - [X] closes #54727 (Replace xxxx with the GitHub issue number) - [X] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature - [X] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit). - [NA] Added [type annotations](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codeb…,,,,,,Anecdotal,issue,,,,,,,,2023-09-09,github/nnlnr,https://github.com/pandas-dev/pandas/pull/55079,repo: pandas-dev/pandas | keyword: lesson learned | state: closed
"> Could you add this to the API reference documentation? is the API reference documentation in` doc/source/reference/api`? If so, I'm seeing that path in the .gitignore file - is there a procedure to update the API reference under this context?",,,,,,Anecdotal,comment,,,,,,,,2023-09-13,github/nnlnr,https://github.com/pandas-dev/pandas/pull/55079#issuecomment-1717404624,repo: pandas-dev/pandas | issue: ENH: make guess_datetime_format public | keyword: lesson learned
Thanks for the review and comments @mroeschke and @MarcoGorelli - added an entry to doc/source/reference/general_functions.rst ✅ - updated warning in docstring as sphinx directive ✅ - added examples to docstring ✅,,,,,,Anecdotal,comment,,,,,,,,2023-09-14,github/nnlnr,https://github.com/pandas-dev/pandas/pull/55079#issuecomment-1718648994,repo: pandas-dev/pandas | issue: ENH: make guess_datetime_format public | keyword: lesson learned
👋 I'm getting a bit stuck with these failing checks - which seem to be mostly due to the following attribute error: `AttributeError: module 'pandas' has no attribute 'guess_datetime_format'` I updated the docstring in parsing.pyx with a sphinx `..warning::` directive and added two examples (learned that formatting was important which was a good lesson); however not sure how to address the failing jobs - any feedback or advice on how to troubleshoot?,,,,,,Anecdotal,comment,,,,,,,,2023-09-16,github/nnlnr,https://github.com/pandas-dev/pandas/pull/55079#issuecomment-1722319942,repo: pandas-dev/pandas | issue: ENH: make guess_datetime_format public | keyword: lesson learned
This pull request is stale because it has been open for thirty days with no activity. Please [update](https://pandas.pydata.org/pandas-docs/stable/development/contributing.html#updating-your-pull-request) and respond to this comment if you're still interested in working on this.,,,,,,Anecdotal,comment,,,,,,,,2023-10-20,github/github-actions[bot],https://github.com/pandas-dev/pandas/pull/55079#issuecomment-1771865785,repo: pandas-dev/pandas | issue: ENH: make guess_datetime_format public | keyword: lesson learned
"website build looks good: ![image](https://github.com/pandas-dev/pandas/assets/33491632/ce87e0b4-f2ac-417f-a5ba-e67c020370a1) merging then, thanks @nnlnr !",,,,,,Anecdotal,comment,,,,,,,,2023-10-26,github/MarcoGorelli,https://github.com/pandas-dev/pandas/pull/55079#issuecomment-1781597182,repo: pandas-dev/pandas | issue: ENH: make guess_datetime_format public | keyword: lesson learned
"ENH: add `regex=False` option to pandas.Series.str.match and fullmatch (like in str.contains) ### Feature Type - [ ] Adding new functionality to pandas - [X] Changing existing functionality in pandas - [ ] Removing existing functionality in pandas ### Problem Description pandas.Series.str.contains accepts the parameter `regex` which is a boolean defaulting to True, meaning that the provided pattern is treated as a regex expression. It is useful that this behaviour can be turned of, for instance…",,,,,,Anecdotal,issue,,,,,,,,2022-08-15,github/robna,https://github.com/pandas-dev/pandas/issues/48086,repo: pandas-dev/pandas | keyword: lesson learned | state: closed
"Regarding your alternative solutions, you can rely on [`re.escape(...)`](https://docs.python.org/3/library/re.html#re.escape) to programmatically escape a string in preparation for an exact regex match. A `regex` parameter does not make much sense on `match` and `fullmatch` methods, which clearly refer to the regular expression methods of the same name, and refer to matching a regular expression pattern. Furthermore, I don't exactly see what functionality is missing currently. You are looking f…",,,,,,Anecdotal,comment,,,,,,,,2022-08-19,github/tomaarsen,https://github.com/pandas-dev/pandas/issues/48086#issuecomment-1220680487,repo: pandas-dev/pandas | issue: ENH: add `regex=False` option to pandas.Series.str.match and fullmatch (like in str.contains) | keyword: lesson learned
"I agree with @tomaarsen , unless I am missing something. I would just use the `pd.Series.str.find` , or just use `re.escape`. I'm not sure how strict the match needs to be, but either approach should work fine.",,,,,,Anecdotal,comment,,,,,,,,2022-08-23,github/goeb86,https://github.com/pandas-dev/pandas/issues/48086#issuecomment-1224868718,repo: pandas-dev/pandas | issue: ENH: add `regex=False` option to pandas.Series.str.match and fullmatch (like in str.contains) | keyword: lesson learned
"The idea was that people might not know that `pd.Series.str.match` and `pd.Series.str.fullmatch` are intended to be used with regex primarily, and may assume that they can apply it to non-regex strings in the same way they can do with `pd.Series.str.contains`. But for me the way with using `re.escape` is simple enough. I will close this issue, maybe it will help some others too.",,,,,,Anecdotal,comment,,,,,,,,2022-08-25,github/robna,https://github.com/pandas-dev/pandas/issues/48086#issuecomment-1227164946,repo: pandas-dev/pandas | issue: ENH: add `regex=False` option to pandas.Series.str.match and fullmatch (like in str.contains) | keyword: lesson learned
"I ran into the same issue as @robna , took a while to troubleshoot but in the end I found out that the parentheses in my match string were the issue. If the regex=false was implemented in this function, it would have saved me a lot of time and worries. The suggestion of @tomaarsen is of course a simpler way of achieving the same thing and I will be using that method from now on. Lesson learned the hard way I guess.",,,,,,Anecdotal,comment,,,,,,,,2023-03-02,github/MMK-IBSEN,https://github.com/pandas-dev/pandas/issues/48086#issuecomment-1451919273,repo: pandas-dev/pandas | issue: ENH: add `regex=False` option to pandas.Series.str.match and fullmatch (like in str.contains) | keyword: lesson learned
perf/minor performance improvement in `is_*_dtype` and `get_block_type` methods Improvements: * use of `classes` in `is_*_dtype` avoid duplicate calls and will use a slightly simpler codepath in `isinstance` when possible * `get_block_type` has unnecessary assignment removed - benchmarks added - [x] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit). - [x] Added an entry in the latest `doc/source/whatsnew/vX.X.X.rst` file if fix…,,,,,,Anecdotal,issue,,,,,,,,2022-10-15,github/Code0x58,https://github.com/pandas-dev/pandas/pull/49112,repo: pandas-dev/pandas | keyword: lesson learned | state: closed
"> Could you split this in 2 PR, 1 for typing and 1 for caching? > > Also the whatsnew note should go in `v2.0.0.rst`. I've stripped out caching as it generally wasn't beneficial, but left a note on https://github.com/pandas-dev/pandas/issues/48212#issuecomment-1288145916 about what where might be good to pursue for further gains without losses.",,,,,,Anecdotal,comment,,,,,,,,2022-10-23,github/Code0x58,https://github.com/pandas-dev/pandas/pull/49112#issuecomment-1288146346,repo: pandas-dev/pandas | issue: perf/minor performance improvement in `is_*_dtype` and `get_block_type` methods | keyword: lesson learned
"> So generally this PR should be typing only? If so, could you undo all the refactoring? Oh right, I thought you meant separating some bits in different modules. Is there some policy around type annotation additions? I originally added some after the pre-commit hook/CI complained about typing, The `get_block_type` annotation just moves the type from internally to externally, and the refinement on `classes*` appear trivial, and that's all that is left in after simplifying the change.",,,,,,Anecdotal,comment,,,,,,,,2022-10-25,github/Code0x58,https://github.com/pandas-dev/pandas/pull/49112#issuecomment-1290043998,repo: pandas-dev/pandas | issue: perf/minor performance improvement in `is_*_dtype` and `get_block_type` methods | keyword: lesson learned
"Typically typing PRs don't involve a lot of refactoring of code - mainly just adding annotations. Since this PR involves benchmark & whatsnew & typing annotations & refactoring, it would be preferable if this pull requests only targets one item at a time.",,,,,,Anecdotal,comment,,,,,,,,2022-10-25,github/mroeschke,https://github.com/pandas-dev/pandas/pull/49112#issuecomment-1290940330,repo: pandas-dev/pandas | issue: perf/minor performance improvement in `is_*_dtype` and `get_block_type` methods | keyword: lesson learned
"Thanks for the pull request, but it appears to have gone stale. If interested in continuing, please merge in the main branch, address any review comments and/or failing tests, and we can reopen.",,,,,,Anecdotal,comment,,,,,,,,2022-12-17,github/mroeschke,https://github.com/pandas-dev/pandas/pull/49112#issuecomment-1356405105,repo: pandas-dev/pandas | issue: perf/minor performance improvement in `is_*_dtype` and `get_block_type` methods | keyword: lesson learned
"Towards ""pandas 1.0"" Here's our roadmap document: https://docs.google.com/document/d/151ct8jcZWwh7XStptjbLsda6h2b3C0IuiH_hfZnUA58/edit# Just because it is a nice round number :-) Or maybe we can use it to discuss how we imagine a possible pandas 1.0 .. --- Some clarification (from @shoyer): This is not the place to make new feature requests -- please continue to make separate GitHub issues for those. Almost every new feature can be added without a 1.0 release. If there is a change you think wou…",,,,,,Anecdotal,issue,,,,,,,,2015-04-27,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/10000,repo: pandas-dev/pandas | keyword: lesson learned | state: closed
"My wish list for pandas 1.0: 1. Fix `[]`/`__getitem__` (#9595) 2. Make the index/column distinction less painful (#5677, #8162) I also have a fantasy world where the pandas Index becomes entirely optional, but that might be too big of a break even for pandas 1.0.",,,,,,Anecdotal,comment,,,,,,,,2015-04-27,github/shoyer,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-96782860,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
"What if, every pnl, df, s, had a mode, that changed the slicing/**getitem** behavior. One could set the default in the options, and change it on a per-object basis when necessary? It could allow old-new to transition smoother, plus, get more creative where desired.",,,,,,Anecdotal,comment,,,,,,,,2015-04-27,github/jnmclarty,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-96792204,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
"@jnmclarty A better option would be some sort of flag that could be set per module, similar to a future statement -- changing the way in which a specific DataFrame is queried is just begging for someone to pass it off to an incompatible function. In fact, I just asked if this is possible on StackOverflow: http://stackoverflow.com/questions/29905278/using-future-style-imports-for-module-specific-features-in-python/",,,,,,Anecdotal,comment,,,,,,,,2015-04-27,github/shoyer,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-96824720,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
"Congrats on the great package:D My wish is: - cythonize internals (#163) - Parallelize option for some ops, such as groupby / aggregation",,,,,,Anecdotal,comment,,,,,,,,2015-05-01,github/sinhrks,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-98040042,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
I've been working on problems recently where having groupbys run in parallel would have been great (I think). Also `map`s / `apply`s.,,,,,,Anecdotal,comment,,,,,,,,2015-05-01,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-98213106,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
"These may be too small, but since this is a wishlist I would like to see some improvements in the consistency of the API. Some example: - More consistent usage of singular vs. plural, for example `index`/`indexes`, `column`/`columns`, and `level`/`levels`. This includes both the names and whether they accept single values, multiple values, or both. - Make sure the `axis` argument is available wherever operations are applied across along an axis. - Go through related functions and make sure they…",,,,,,Anecdotal,comment,,,,,,,,2015-06-09,github/toddrjen,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-110361598,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
"For the record, I'm strongly -1 on @toddrjen's suggestion to rename methods to make the use of underscores more consistent. Even Python 3 didn't clean things up like that.",,,,,,Anecdotal,comment,,,,,,,,2015-06-09,github/shoyer,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-110364651,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
"@bwillers I added a xref to an existing issue where that was discussed @benjello is there already a github issue for adding weights? If not, please make one :). @bwillers @benjello The good news is that I don't think either of your suggestions require pandas 1.0. Both could be done incrementally.",,,,,,Anecdotal,comment,,,,,,,,2015-06-12,github/shoyer,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-111582390,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
"@benjello I think we can discuss this further at #10030. That issue is now only about the mean, but would be good to discuss there to which methods we would want to add this functionality.",,,,,,Anecdotal,comment,,,,,,,,2015-06-13,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-111694677,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
"I wasn't entirely sure where to put this but I've written up a short gist as an IPython notebook on the current state of `MultiIndex`ing with `DataFrames.loc`. https://nbviewer.jupyter.org/gist/tgarc/6c40a65f648302b6b9d7# What is particularly relevant to this discussion is in the [last section](https://nbviewer.jupyter.org/gist/tgarc/6c40a65f648302b6b9d7#ambiguities). Specifically pandas allows, `df.loc[('foo','bar'), ('one','two'), ('three','four')]` (1) To be taken to mean `df.loc[(('foo','ba…",,,,,,Anecdotal,comment,,,,,,,,2015-07-14,github/tgarc,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-121096119,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
"This may or may not be a good idea, but it may at least be worth thinking about. Considering that `PanelND` has always been marked as ""experimental"" and not all features support it, and considering the work that has been going on in xray, is `PanelND` something that could be deprecated or dropped for 1.0?",,,,,,Anecdotal,comment,,,,,,,,2015-07-14,github/toddrjen,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-121157447,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
"@tgarc Nice overview notebook! (by the way, if you would like to submit parts of that to improve the docs, very welcome!) Part of what you describe is also discussed here (collapsing index levels or not): #10552 For the allowing of 'incomplete' indexing on frames, there is already a warning in docs for this: http://pandas.pydata.org/pandas-docs/stable/advanced.html#using-slicers (the red warning box). So it is explicitly ""allowed, although warned for because of possible ambiguities"" (so not a b…",,,,,,Anecdotal,comment,,,,,,,,2015-07-14,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-121212752,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
"@toddrjen @shoyer and I have had discussion about this. The proposal is to rename `Xray` -> `pandas-nd`. We can discuss further consolidation at some later point. I think we would then deprecate Panelnd (e.g. 4D and higher) and point to `pandas-nd`. Their are a couple of API issues if we also did this for `Panel`. Mainly I think we would need some conversions, e.g. `to_nd` as a mainline function.",,,,,,Anecdotal,comment,,,,,,,,2015-07-14,github/jreback,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-121228635,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
@tgarc this was added quite a long time ago as a convenience / magic feature. It is specifically warned about and is a limitation of the python syntax. There are times when it can be detected and other times it is ambiguous. I am not sure that we can do anything about it. If people don't read the docs what can you do.,,,,,,Anecdotal,comment,,,,,,,,2015-07-14,github/jreback,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-121228843,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
"@jorisvandenbossche Thanks, I'll look to see if there's an appropriate place to add documentation. Thanks for pointing me to that warning - I admit I didn't know it was there. @jreback I realize that this is an established feature and that there is a warning about it in the docs but as we were discussing pulling back on the complexity of indexing in the future of pandas, modifying this particular feature seemed like a good opportunity to simplify existing code and restrict the number of ways us…",,,,,,Anecdotal,comment,,,,,,,,2015-07-14,github/tgarc,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-121365610,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
"This is pretty fundamental - probably too much so: changing `.values` to `.data`, so a `DataFrame` can be more pythonic in its dict-like interface. ref https://github.com/pydata/pandas/issues/12056",,,,,,Anecdotal,comment,,,,,,,,2016-01-15,github/max-sixty,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-172090251,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
"Pinging here on github as well, as I am not sure everybody is aware of the pandas-dev mailing list. But there is currently a thread started by Wes on a pandas 1.0 / future roadmap, and you are certainly welcome to also provide feedback or share ideas. https://mail.python.org/pipermail/pandas-dev/2016-July/000512.html cc @chris-b1 @gfyoung @MaximilianR @kawochen @janschulz",,,,,,Anecdotal,comment,,,,,,,,2016-07-27,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-235746241,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
"One other major breaking change to consider: We should consider making arithmetic between a Series and a DataFrame broadcast across the _columns_ of the dataframe, i.e., aligning `series.index` with `df.index`, rather than the current behavior aligning `series.index` with `df.columns`. I think this would be far more useful than the current behavior, because it's much more common to want to do arithmetic between a series and all columns of a DataFrame. This would make broadcasting in pandas inco…",,,,,,Anecdotal,comment,,,,,,,,2016-07-29,github/shoyer,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-236238297,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
"Some questions for the next couple releases... - Is the idea for 1.0 to stabilize the 0.x API, or to drop a handful of larger API-breaking changes? Or are we pushing the API-breaking changes (e.g. fixing `__getitem__`) till 2.0? Actually, that's really my only question. I guess the only followup would be ""what falls into that bucket of large API-breaking changes that are actually feasible?"" I think now that 1.0 is upon us, we should refocus this issue from ""wishlist"" to ""stuff that's actually g…",,,,,,Anecdotal,comment,,,,,,,,2016-08-10,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-238956123,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
"> Is the idea for 1.0 to stabilize the 0.x API, or to drop a handful of larger API-breaking changes? Or are we pushing the API-breaking changes (e.g. fixing **getitem**) till 2.0? As it is now discussed on the pandas-dev mailing list, I think the conclusion is indeed how you state it here: 1.0 as a stabilization of the current 0.x API, and 2.0 with an internal refactor / larger API changes (eg getitem) > we should refocus this issue from ""wishlist"" to ""stuff that's actually going to happen for …",,,,,,Anecdotal,comment,,,,,,,,2016-08-10,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-239031550,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
"here's why I have the tags set this way. We have approx 1000 issues under `next major release`. This is really just a placeholder for things to do, that otherwise are not categorized as pie-in-sky `Someday`. The way things have been working is to pull issues off of this to a numbered release. IOW, when someone submits a pull-request I mark the issue. Then when the PR is actually merged it gets set with the version number. Otherwise you get a bunch of stale PR's that have version numbers and you…",,,,,,Anecdotal,comment,,,,,,,,2016-08-10,github/jreback,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-239037604,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
"pandas has basically been operating in Kanban style since its beginning. Issues are marked as ""on deck"" (here: ""next major release"" -- perhaps we could give this a better name like ""approved"", ""on deck"", ""fair game"" -- some issues may be either pie-in-the-sky or have not yet reached consensus about the path forward) with potentially an additional level of prioritization (e.g. blocker) It may be a good idea to start thinning down the 1.0 TODO list to things that absolutely must get done. We also…",,,,,,Anecdotal,comment,,,,,,,,2016-08-10,github/wesm,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-239039378,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
Pass method in __finalize__ This passes `method` everywhere we use `__finalize__`. I'm trying to get a better sense for 1. When we call finalize (and when we fail too) 2. When we finalize multiple times 3. The overhead of finalize I haven't called it anywhere new yet (followup PR with that coming though).,,,,,,Anecdotal,issue,,,,,,,,2020-04-03,github/TomAugspurger,https://github.com/pandas-dev/pandas/pull/33273,repo: pandas-dev/pandas | keyword: lesson learned | state: closed
"maybe a silly question, but for this to be useful, doesnt it entail putting e.g. `_getitem_multilevel`-specific logic into `__finalize__`, when that logic would make more sense in `_getitem_multilevel`?",,,,,,Anecdotal,comment,,,,,,,,2020-04-03,github/jbrockmendel,https://github.com/pandas-dev/pandas/pull/33273#issuecomment-608640679,repo: pandas-dev/pandas | issue: Pass method in __finalize__ | keyword: lesson learned
"> when that logic would make more sense in _getitem_multilevel? Depends on just how specific the logic is to `_getitem_multilevel`. A motivating use case is my `allows_duplicate_labels` PR. In that I would do something like ```python if method in {""concat"", ""merge"", ""align"", ...} # methods involving multiple NDFrames: self.allows_duplicate_labels = all(ndframe.allows_duplicate_labels for ndframe in others) ``` in this case, there's not any logic specific to `concat`. But we use the `method` to …",,,,,,Anecdotal,comment,,,,,,,,2020-04-03,github/TomAugspurger,https://github.com/pandas-dev/pandas/pull/33273#issuecomment-608652571,repo: pandas-dev/pandas | issue: Pass method in __finalize__ | keyword: lesson learned
"Makes sense, thanks for clarifying On Fri, Apr 3, 2020 at 1:52 PM Tom Augspurger <notifications@github.com> wrote: > when that logic would make more sense in _getitem_multilevel? > > Depends on just how specific the logic is to _getitem_multilevel. A > motivating use case is my allows_duplicate_labels PR. In that I would do > something like > > if method in {""concat"", ""merge"", ""align"", ...} # methods involving multiple NDFrames: > self.allows_duplicate_labels = all(ndframe.allows_duplicate_labe…",,,,,,Anecdotal,comment,,,,,,,,2020-04-03,github/jbrockmendel,https://github.com/pandas-dev/pandas/pull/33273#issuecomment-608670535,repo: pandas-dev/pandas | issue: Pass method in __finalize__ | keyword: lesson learned
"Do we want such specific method names? Because this is kind of ""public"" interface, we should maybe try to limit to a set of ""categories"" (but I didn't really think about whether that defeats the purpose of it or not)",,,,,,Anecdotal,comment,,,,,,,,2020-04-04,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/pull/33273#issuecomment-608988828,repo: pandas-dev/pandas | issue: Pass method in __finalize__ | keyword: lesson learned
"For > Because this is kind of ""public"" interface > can you make method a required parameter in __finalize__ to avoid this problem in the future. and > also needs a whatsnew note Do we make any claim that `__finalize__` is public? There's just a couple whatsnew mentions in the docs. And the docstring doesn't say anything about it being public. I'm OK with these specific names and making `method` required but not if it's currently part of the public API.",,,,,,Anecdotal,comment,,,,,,,,2020-04-06,github/TomAugspurger,https://github.com/pandas-dev/pandas/pull/33273#issuecomment-609747875,repo: pandas-dev/pandas | issue: Pass method in __finalize__ | keyword: lesson learned
"> Do we make any claim that __finalize__ is public? Looking at the docs, it's indeed not mentioned in the docs on subclassing. But personally, I always assumed this is part of our sublcassing API. In any case, geopandas is using it for that reason (so that's probably the reason I was assuming that ;)).",,,,,,Anecdotal,comment,,,,,,,,2020-04-06,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/pull/33273#issuecomment-609753718,repo: pandas-dev/pandas | issue: Pass method in __finalize__ | keyword: lesson learned
"> I always assumed this is part of our subclassing API. I was also assuming that, since it does interact with `metadata`. So my compromise was to not make `method` required, in case a subclass' method was calling it without a `method`. But I will push up a doc note that the values in `method` aren't stable yet.",,,,,,Anecdotal,comment,,,,,,,,2020-04-06,github/TomAugspurger,https://github.com/pandas-dev/pandas/pull/33273#issuecomment-609759329,repo: pandas-dev/pandas | issue: Pass method in __finalize__ | keyword: lesson learned
"Regarding part of our subclassing API or not: since we actually don't use `methods` internally ourselves, I suppose it's indeed the purpose for subclasses to be able to override finalize to use `methods`. (or do other custom handling of metadata) @TomAugspurger is it also your goal that external users can implement similar metadata propagation logic as you are experimenting for the allow_duplicate_labels? (I seem to remember we were discussing this at some point I think)",,,,,,Anecdotal,comment,,,,,,,,2020-04-06,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/pull/33273#issuecomment-609958127,repo: pandas-dev/pandas | issue: Pass method in __finalize__ | keyword: lesson learned
"> is it also your goal that external users can implement similar metadata propagation logic as you are experimenting for the allow_duplicate_labels I'm going back and forth on this. I have a few ideas for what an API would look like, but I'm having trouble deciding what will work well in practice. If I had to guess, I'll propose handling `allow_duplicate_labels` explicitly in `__finalize__`, and use lessons learned there to inform an API design.",,,,,,Anecdotal,comment,,,,,,,,2020-04-06,github/TomAugspurger,https://github.com/pandas-dev/pandas/pull/33273#issuecomment-609962640,repo: pandas-dev/pandas | issue: Pass method in __finalize__ | keyword: lesson learned
"w/r/t the private/public/experimental aspect, the only opinion i have is that we retain flexibility with this (and not need to wait until 2.0 if minds are changed)",,,,,,Anecdotal,comment,,,,,,,,2020-04-06,github/jbrockmendel,https://github.com/pandas-dev/pandas/pull/33273#issuecomment-609963592,repo: pandas-dev/pandas | issue: Pass method in __finalize__ | keyword: lesson learned
"I think this PR is compatible with that. The only ""API change"" is a clarification that the actual `method` passed by pandas isn't currently stable.",,,,,,Anecdotal,comment,,,,,,,,2020-04-06,github/TomAugspurger,https://github.com/pandas-dev/pandas/pull/33273#issuecomment-609965628,repo: pandas-dev/pandas | issue: Pass method in __finalize__ | keyword: lesson learned
"We should still guarantee some stability *for the places where we already specify methods*, though. But fully ok with the others being regarded as experimental.",,,,,,Anecdotal,comment,,,,,,,,2020-04-06,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/pull/33273#issuecomment-609971325,repo: pandas-dev/pandas | issue: Pass method in __finalize__ | keyword: lesson learned
"That makes me hesitant to include `method` in some of the methods here, which in turn makes me think that's it's premature to say `method` is stable if it's been in a release. I'd rather work with subclassers to find what they're relying on. But since we're in an API gray zone that may be hard.",,,,,,Anecdotal,comment,,,,,,,,2020-04-06,github/TomAugspurger,https://github.com/pandas-dev/pandas/pull/33273#issuecomment-609974641,repo: pandas-dev/pandas | issue: Pass method in __finalize__ | keyword: lesson learned
"In an effort to unstick things, I've removed the addition of `method=""_*""`. So I've only added `method=` to public methods, which I hope is uncontroversial. I opened https://github.com/pandas-dev/pandas/issues/33338 as a general discussion about `__finalize__` and whether it's public API.",,,,,,Anecdotal,comment,,,,,,,,2020-04-06,github/TomAugspurger,https://github.com/pandas-dev/pandas/pull/33273#issuecomment-610003829,repo: pandas-dev/pandas | issue: Pass method in __finalize__ | keyword: lesson learned
"Fix #25099 set na_rep values before converting to string to prevent data truncation Hi, The code in this pull request fixes #25099, and all tests pass. However, I am not sure if this is the best (or even an elegant) solution. The issue happened because the float `NaN` is converted to `str` (i.e. ""nan""), which gets stored as a `<U3`. When the user-provided `na_rep` is used, it is first truncated down to the data type length (i.e. ""myn""). I tried moving the `na_rep` to before it is converted to s…",,,,,,Anecdotal,issue,,,,,,,,2019-02-03,github/kinow,https://github.com/pandas-dev/pandas/pull/25103,repo: pandas-dev/pandas | keyword: lesson learned | state: closed
# [Codecov](https://codecov.io/gh/pandas-dev/pandas/pull/25103?src=pr&el=h1) Report > Merging [#25103](https://codecov.io/gh/pandas-dev/pandas/pull/25103?src=pr&el=desc) into [master](https://codecov.io/gh/pandas-dev/pandas/commit/f75a220ff1e5e027ef2b070430fd7f4490cdcbf0?src=pr&el=desc) will **increase** coverage by `<.01%`. > The diff coverage is `100%`. [![Impacted file tree graph](https://codecov.io/gh/pandas-dev/pandas/pull/25103/graphs/tree.svg?width=650&token=eZ4WkYLtcO&height=150&src=pr)…,,,,,,Anecdotal,comment,,,,,,,,2019-02-03,github/codecov[bot],https://github.com/pandas-dev/pandas/pull/25103#issuecomment-460036982,repo: pandas-dev/pandas | issue: Fix #25099 set na_rep values before converting to string to prevent data truncation | keyword: lesson learned
# [Codecov](https://codecov.io/gh/pandas-dev/pandas/pull/25103?src=pr&el=h1) Report > :exclamation: No coverage uploaded for pull request base (`master@e0c63b4`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit). > The diff coverage is `100%`. [![Impacted file tree graph](https://codecov.io/gh/pandas-dev/pandas/pull/25103/graphs/tree.svg?width=650&token=eZ4WkYLtcO&height=150&src=pr)](https://codecov.io/gh/pandas-dev/pandas/pull/2510…,,,,,,Anecdotal,comment,,,,,,,,2019-02-03,github/codecov[bot],https://github.com/pandas-dev/pandas/pull/25103#issuecomment-460036983,repo: pandas-dev/pandas | issue: Fix #25099 set na_rep values before converting to string to prevent data truncation | keyword: lesson learned
"Hello @kinow! Thanks for updating this PR. We checked the lines you've touched for [PEP 8](https://www.python.org/dev/peps/pep-0008) issues, and found: There are currently no PEP 8 issues detected in this Pull Request. Cheers! :beers: ##### Comment last updated at 2019-09-10 00:01:49 UTC",,,,,,Anecdotal,comment,,,,,,,,2019-02-03,github/pep8speaks,https://github.com/pandas-dev/pandas/pull/25103#issuecomment-460090774,repo: pandas-dev/pandas | issue: Fix #25099 set na_rep values before converting to string to prevent data truncation | keyword: lesson learned
"@jreback just finished running asv with the following command ```bash asv continuous -f 1.1 upstream/master HEAD -b ^io.csv ``` Complete output in this [gist](https://gist.github.com/kinow/9e3760ddfa036b82df0f587323e17ee6), but near the bottom of its output, it says ``` + 9.92±0.1ms 11.7±0.6ms 1.18 io.csv.ReadCSVFloatPrecision.time_read_csv_python_engine(',', '_', None) + 119±1ms 135±4ms 1.14 io.csv.ReadCSVCategorical.time_convert_direct + 171±5ms 190±4ms 1.11 io.csv.ReadCSVCategorical.time_con…",,,,,,Anecdotal,comment,,,,,,,,2019-02-05,github/kinow,https://github.com/pandas-dev/pandas/pull/25103#issuecomment-460461924,repo: pandas-dev/pandas | issue: Fix #25099 set na_rep values before converting to string to prevent data truncation | keyword: lesson learned
"@jreback added a whatsnew entry as requested - happy to amend if text is incorrect or too simple. I resolved the outdated discussions, except for one that is still pertinent. The initial approach was to calculate the maximum length when it was a string, and then just create a type of that length. That way the masked values were not truncated. The pending discussion is around that approach, where we tried instead to change the order of where the mask gets applied. But several other tests failed …",,,,,,Anecdotal,comment,,,,,,,,2019-03-20,github/kinow,https://github.com/pandas-dev/pandas/pull/25103#issuecomment-474780388,repo: pandas-dev/pandas | issue: Fix #25099 set na_rep values before converting to string to prevent data truncation | keyword: lesson learned
"Thanks for the review @WillAyd >Can you merge master? Done. Rebased, fixed conflicts, and tried to address the feedback, except the parameterized test as that was requested before (I think the intention was to add more test scenarios). Cheers Bruno",,,,,,Anecdotal,comment,,,,,,,,2019-04-10,github/kinow,https://github.com/pandas-dev/pandas/pull/25103#issuecomment-481645165,repo: pandas-dev/pandas | issue: Fix #25099 set na_rep values before converting to string to prevent data truncation | keyword: lesson learned
"Hi @WillAyd >Looks like this went for a bit without review - @kinow any chance this is still active? Sure thing, will just need to re-read last comments and rebase the code. Thanks for the review, I will try to update it today or over the weekend 👍",,,,,,Anecdotal,comment,,,,,,,,2019-09-05,github/kinow,https://github.com/pandas-dev/pandas/pull/25103#issuecomment-528609349,repo: pandas-dev/pandas | issue: Fix #25099 set na_rep values before converting to string to prevent data truncation | keyword: lesson learned
"Rebased, now will try to find some time this weekend to troubleshoot the Travis errors, and see if I can fix them and them look at `PyUnicode_GET_LENGTH` too.",,,,,,Anecdotal,comment,,,,,,,,2019-09-06,github/kinow,https://github.com/pandas-dev/pandas/pull/25103#issuecomment-528691643,repo: pandas-dev/pandas | issue: Fix #25099 set na_rep values before converting to string to prevent data truncation | keyword: lesson learned
"@WillAyd rebased, fixed conflicts, tried to address your feedback the best I could, and squashed commits too. CI passing, so should be ready for review again 👍 thanks",,,,,,Anecdotal,comment,,,,,,,,2019-09-09,github/kinow,https://github.com/pandas-dev/pandas/pull/25103#issuecomment-529707843,repo: pandas-dev/pandas | issue: Fix #25099 set na_rep values before converting to string to prevent data truncation | keyword: lesson learned
">lgtm. By the way @kinow no need to squash your commits we do so before merge Oh, thanks for the heads-up, I thought I would be asked to squash them later. And I realized I had made a mess in the last commits, so decided to tidy it up a bit. Lesson learned for the next PR. Thanks!",,,,,,Anecdotal,comment,,,,,,,,2019-09-10,github/kinow,https://github.com/pandas-dev/pandas/pull/25103#issuecomment-529716845,repo: pandas-dev/pandas | issue: Fix #25099 set na_rep values before converting to string to prevent data truncation | keyword: lesson learned
"dataframe's index is blank, ![image](https://user-images.githubusercontent.com/17948924/63857928-26588d80-c9d7-11e9-9a4f-1454979d6723.png) ![image](https://user-images.githubusercontent.com/17948924/63857961-3a03f400-c9d7-11e9-8e41-efcd684293e1.png) a daraframe have valida shape number, but its index is a blank list, which start from 0,end at 0. it is so weird!!!",,,,,,Anecdotal,issue,,,,,,,,2019-08-28,github/ChandlerBBT,https://github.com/pandas-dev/pandas/issues/28191,repo: pandas-dev/pandas | keyword: lesson learned | state: closed
"> pls show a reproducible example not images are you saying offer related data? sorry, I can't, this is our company's data. if you are referring to offer code, the image it is. I encounter this weird situation last night, can not use for loop on a valid dataframe for its index is blank however, even me can't reproduce this bug again on other toy dataframe.... btw, my pandas vesion is 0.23",,,,,,Anecdotal,comment,,,,,,,,2019-08-29,github/ChandlerBBT,https://github.com/pandas-dev/pandas/issues/28191#issuecomment-526010721,"repo: pandas-dev/pandas | issue: dataframe's index is blank, | keyword: lesson learned"
"> # 2.1 check not yet activate merchants > ``` > in_time_stat_but_not_yet_act = in_time_stat[in_time_stat.F_activate_time.isnull()] > in_time_stat_but_not_yet_act_list = in_time_stat_but_not_yet_act.F_merchant_id.tolist() # this list might be None > > if in_time_stat_but_not_yet_act_list is not None: > # 2.1.1 check user's activate time > in_time_stat_but_not_yet_act_now_act = df[df.F_merchant_id.isin(in_time_stat_but_not_yet_act_list)].groupby( > ""F_merchant_id"", as_index=False).agg({""F_time"":…",,,,,,Anecdotal,comment,,,,,,,,2019-08-29,github/ChandlerBBT,https://github.com/pandas-dev/pandas/issues/28191#issuecomment-526012219,"repo: pandas-dev/pandas | issue: dataframe's index is blank, | keyword: lesson learned"
"> It's tough to follow but perhaps try df.reset_index(drop=True) yep,that's my first try, but hard to believe this also fail. I tried below ways: 1. reset_index() 2. reset_index(drop=True) 3. reindex() 4. index = range(len(df)) however, all fail on this particular dataframe.",,,,,,Anecdotal,comment,,,,,,,,2019-08-29,github/ChandlerBBT,https://github.com/pandas-dev/pandas/issues/28191#issuecomment-526037520,"repo: pandas-dev/pandas | issue: dataframe's index is blank, | keyword: lesson learned"
"> We're looking at two different dataframes here. That may be the problem? Gee... what a shame! I will try to re-go this situation again to see if bug will happen on same df. Frankly say, I am afraid it is my mistake... After all, I hv learned my lesson, don' put the object's name too long to distinguish",,,,,,Anecdotal,comment,,,,,,,,2019-08-29,github/ChandlerBBT,https://github.com/pandas-dev/pandas/issues/28191#issuecomment-526038756,"repo: pandas-dev/pandas | issue: dataframe's index is blank, | keyword: lesson learned"
"TST: fixturize series/test_alter_axes.py - [x] prep for #22225, in the sense that it preempts (and splits off) the test-related changes that have been required on the DataFrame-side of the PR (see #22236) - [x] tests modified / passed - [x] passes `git diff upstream/master -u -- ""*.py"" | flake8 --diff` In particular, it takes the fixture-like attributes of `tests/series/common.TestData` and transforms them into fixtures in `tests/series/conftest.py`, with the eventual goal of replacing all the …",,,,,,Anecdotal,issue,,,,,,,,2018-08-28,github/h-vetinari,https://github.com/pandas-dev/pandas/pull/22526,repo: pandas-dev/pandas | keyword: lesson learned | state: closed
# [Codecov](https://codecov.io/gh/pandas-dev/pandas/pull/22526?src=pr&el=h1) Report > Merging [#22526](https://codecov.io/gh/pandas-dev/pandas/pull/22526?src=pr&el=desc) into [master](https://codecov.io/gh/pandas-dev/pandas/commit/f82c10ab4741b274b55c327362c1f3aec29fbde9?src=pr&el=desc) will **increase** coverage by `<.01%`. > The diff coverage is `n/a`. [![Impacted file tree graph](https://codecov.io/gh/pandas-dev/pandas/pull/22526/graphs/tree.svg?width=650&token=eZ4WkYLtcO&height=150&src=pr)]…,,,,,,Anecdotal,comment,,,,,,,,2018-08-28,github/codecov[bot],https://github.com/pandas-dev/pandas/pull/22526#issuecomment-416475850,repo: pandas-dev/pandas | issue: TST: fixturize series/test_alter_axes.py | keyword: lesson learned
"@jreback > can you add doc-strings and meaningful names for these. (e.g. ts -> datetime_series) This is the same issue as in #22236 - these ""fixture-like"" `TestData`-attributes will need to be replaced in all Series tests, and renaming them now will make it harder to simply replace them (e.g. if someone were to work on a different module, to know that `self.series` needs to be replaced with `string_series`). One possible approach to satisfy both demands would be to open up a sister issue to #22…",,,,,,Anecdotal,comment,,,,,,,,2018-08-30,github/h-vetinari,https://github.com/pandas-dev/pandas/pull/22526#issuecomment-417208270,repo: pandas-dev/pandas | issue: TST: fixturize series/test_alter_axes.py | keyword: lesson learned
"@jreback I added the ""translations"" as requested - it's gonna make transitioning the other modules a fair bit harder, but for only four fixtures, I think it's doable.",,,,,,Anecdotal,comment,,,,,,,,2018-08-31,github/h-vetinari,https://github.com/pandas-dev/pandas/pull/22526#issuecomment-417563198,repo: pandas-dev/pandas | issue: TST: fixturize series/test_alter_axes.py | keyword: lesson learned
"Hello @h-vetinari! Thanks for updating the PR. Cheers ! There are no PEP8 issues in this Pull Request. :beers: ##### Comment last updated on August 31, 2018 at 13:49 Hours UTC",,,,,,Anecdotal,comment,,,,,,,,2018-08-31,github/pep8speaks,https://github.com/pandas-dev/pandas/pull/22526#issuecomment-417668186,repo: pandas-dev/pandas | issue: TST: fixturize series/test_alter_axes.py | keyword: lesson learned
"@jreback All green. This is very similar to #22236 - can I please ask you to take a look there again? I incorporated all your feedback (fixturize, split off warnings into new PR, etc.), and don't know what's left to do - it's been a very drawn-out process to eke out further instructions for what could have been a simple test clean-up. Based on this review here, I now also added docstrings to the fixtures there; I didn't rename them yet (but added a note in each fixture), because otherwise the f…",,,,,,Anecdotal,comment,,,,,,,,2018-08-31,github/h-vetinari,https://github.com/pandas-dev/pandas/pull/22526#issuecomment-417783768,repo: pandas-dev/pandas | issue: TST: fixturize series/test_alter_axes.py | keyword: lesson learned
it’s long and drawn out because the PRs have too much unrelated changes in them 1 thing per PR will get u merged much faster with less comments even things as simple as changing unrelated whatsnew notes the more code the more i have to go over with a fine toothed comb further responding to my comments with further questions - while for sure ask if things are unclear can certainly make things take longer,,,,,,Anecdotal,comment,,,,,,,,2018-08-31,github/jreback,https://github.com/pandas-dev/pandas/pull/22526#issuecomment-417787203,repo: pandas-dev/pandas | issue: TST: fixturize series/test_alter_axes.py | keyword: lesson learned
"@jreback For sure, and I appreciate the effort you put in. But I've learned my lesson about the unrelated changes, and I've been splitting them into smaller and smaller pieces. In #22236, the ""unrelated"" changes were - respectfully - mostly your requests at cleaning up things not directly related to the PR (except that it was in the same module), and I split off the new warnings as requested as well. The diffs of the last few commits are not very large at all (except the added docstrings now, w…",,,,,,Anecdotal,comment,,,,,,,,2018-08-31,github/h-vetinari,https://github.com/pandas-dev/pandas/pull/22526#issuecomment-417792649,repo: pandas-dev/pandas | issue: TST: fixturize series/test_alter_axes.py | keyword: lesson learned
"@jreback anything left to do on this? Someone wants to already start working on fixturising the rest of the series tests, see #22550",,,,,,Anecdotal,comment,,,,,,,,2018-09-04,github/h-vetinari,https://github.com/pandas-dev/pandas/pull/22526#issuecomment-418252114,repo: pandas-dev/pandas | issue: TST: fixturize series/test_alter_axes.py | keyword: lesson learned
"@jreback > @h-vetinari the pings are fine, but if someone wants to work on things based on this nothing is stopping them, they can rebase off of this PR, this is a quite common workflow. This will be merged after all comments are satisfied. Pls be patient. We have a tremendous amount of PRs Yeah, I get there's lots of traffic. I mentioned this because this PR was basically finished and it's not a ""beginner workflow"" to rebase off an open PR.",,,,,,Anecdotal,comment,,,,,,,,2018-09-04,github/h-vetinari,https://github.com/pandas-dev/pandas/pull/22526#issuecomment-418546232,repo: pandas-dev/pandas | issue: TST: fixturize series/test_alter_axes.py | keyword: lesson learned
"read_html: Handle colspan and rowspan This is essentially a rebased and squashed #17054 (mad props to @jowens for doing all the hard thinking). My tweaks: * test_computer_sales_page (see #17074) no longer tests for ParserError, because the ParserError was a bug caused by missing colspan support. Now, test that MultiIndex works as expected. * I respectfully removed the fill_rowspan argument from #17073. Instead, the virtual cells created by rowspan/colspan are always copies of the real cells' te…",,,,,,Anecdotal,issue,,,,,,,,2018-06-14,github/adamhooper,https://github.com/pandas-dev/pandas/pull/21487,repo: pandas-dev/pandas | keyword: lesson learned | state: closed
"Hello @adamhooper! Thanks for updating the PR. Cheers ! There are no PEP8 issues in this Pull Request. :beers: ##### Comment last updated on July 05, 2018 at 17:45 Hours UTC",,,,,,Anecdotal,comment,,,,,,,,2018-06-14,github/pep8speaks,https://github.com/pandas-dev/pandas/pull/21487#issuecomment-397430634,repo: pandas-dev/pandas | issue: read_html: Handle colspan and rowspan | keyword: lesson learned
# [Codecov](https://codecov.io/gh/pandas-dev/pandas/pull/21487?src=pr&el=h1) Report > Merging [#21487](https://codecov.io/gh/pandas-dev/pandas/pull/21487?src=pr&el=desc) into [master](https://codecov.io/gh/pandas-dev/pandas/commit/62a0ebc5837a4a1f7cc925e13344be1adf1acc59?src=pr&el=desc) will **increase** coverage by `<.01%`. > The diff coverage is `97.89%`. [![Impacted file tree graph](https://codecov.io/gh/pandas-dev/pandas/pull/21487/graphs/tree.svg?width=650&height=150&src=pr&token=eZ4WkYLtc…,,,,,,Anecdotal,comment,,,,,,,,2018-06-15,github/codecov[bot],https://github.com/pandas-dev/pandas/pull/21487#issuecomment-397658373,repo: pandas-dev/pandas | issue: read_html: Handle colspan and rowspan | keyword: lesson learned
"@jreback Thanks for the review! There was already an entry in whatsnew, but I rephrased it such that users will better be able to predict how the change will affect their data. Do you think that's enough, or should I add a subsection?",,,,,,Anecdotal,comment,,,,,,,,2018-06-15,github/adamhooper,https://github.com/pandas-dev/pandas/pull/21487#issuecomment-397679473,repo: pandas-dev/pandas | issue: read_html: Handle colspan and rowspan | keyword: lesson learned
"@jreback, @WillAyd I've implemented all the changes you requested. Could you please take a look? @WillAyd I tested out an alternative algorithm, as you had concerns about the one in this pull request. The alternative is at https://github.com/adamhooper/pandas/commit/67dea69f604251d6de8185c8dd8c82f15eedfa2a. The diff is +53, -38. Maybe you find it more readable, though?",,,,,,Anecdotal,comment,,,,,,,,2018-06-19,github/adamhooper,https://github.com/pandas-dev/pandas/pull/21487#issuecomment-398421243,repo: pandas-dev/pandas | issue: read_html: Handle colspan and rowspan | keyword: lesson learned
Need to clean up merge conflicts and maybe add whatsnew reference for proper indexing of tables with NA data. Otherwise lgtm,,,,,,Anecdotal,comment,,,,,,,,2018-06-26,github/WillAyd,https://github.com/pandas-dev/pandas/pull/21487#issuecomment-400168375,repo: pandas-dev/pandas | issue: read_html: Handle colspan and rowspan | keyword: lesson learned
Urgh. I did a rebase on upstream/master and pushed with `--force` and now Appveyor is complaining. Was that a bad idea on my part? How should I fix?,,,,,,Anecdotal,comment,,,,,,,,2018-06-26,github/adamhooper,https://github.com/pandas-dev/pandas/pull/21487#issuecomment-400367576,repo: pandas-dev/pandas | issue: read_html: Handle colspan and rowspan | keyword: lesson learned
"I don't think the AppVeyor issue is a result of your rebase. That said, you'd be better off fetching master and merging it into your branch rather than rebase (maintains better comment history). Looks like you have some file conflicts still anyway, so might was well resolve them locally and try merging from master into your branch this time before next push",,,,,,Anecdotal,comment,,,,,,,,2018-06-26,github/WillAyd,https://github.com/pandas-dev/pandas/pull/21487#issuecomment-400487089,repo: pandas-dev/pandas | issue: read_html: Handle colspan and rowspan | keyword: lesson learned
"Indeed -- I rebased over adamhooper/master instead of pandas/master. I've learned my lesson: merge, don't rebase. Fixed by ... er ... rebasing. Correctly, this time.",,,,,,Anecdotal,comment,,,,,,,,2018-06-27,github/adamhooper,https://github.com/pandas-dev/pandas/pull/21487#issuecomment-400683621,repo: pandas-dev/pandas | issue: read_html: Handle colspan and rowspan | keyword: lesson learned
^^ I've noticed a few other pull requests cause ResourceWarning failures on Travis. Does that error relate to this pull request?,,,,,,Anecdotal,comment,,,,,,,,2018-06-27,github/adamhooper,https://github.com/pandas-dev/pandas/pull/21487#issuecomment-400705895,repo: pandas-dev/pandas | issue: read_html: Handle colspan and rowspan | keyword: lesson learned
"Hello, when will this be included into a release? I see it's being included in v0.24, but when will that happen? I'm having this exact problem and would love to benefit from this fix. Cheers!",,,,,,Anecdotal,comment,,,,,,,,2018-07-16,github/clytemnestra,https://github.com/pandas-dev/pandas/pull/21487#issuecomment-405235974,repo: pandas-dev/pandas | issue: read_html: Handle colspan and rowspan | keyword: lesson learned
"Proposal: New Index type for binned data (IntervalIndex) ### Design The idea is to have a natural representation of the grids that ubiquitously appear in simulations and measurements of physical systems. Instead of referencing a single value, a grid cell references a _range_ of values, based on the chosen discretization. Typically, cells boundaries would be specified by floating point numbers. In one dimension, a grid cell corresponds to an _interval_, the name we use here. The key feature of `…",,,,,,Anecdotal,issue,,,,,,,,2014-07-01,github/shoyer,https://github.com/pandas-dev/pandas/issues/7640,repo: pandas-dev/pandas | keyword: lesson learned | state: closed
"@shoyer all for this! I know you are against this, but I would encorage you to inherit from `Index`. OR create a new base class that is ABC like which we can eventually use as a base class for `Index`.",,,,,,Anecdotal,comment,,,,,,,,2014-07-02,github/jreback,https://github.com/pandas-dev/pandas/issues/7640#issuecomment-47766256,repo: pandas-dev/pandas | issue: Proposal: New Index type for binned data (IntervalIndex) | keyword: lesson learned
"Thanks for the support! I'm not sure when I'll get around to implementing this, but I will add it to my source open backlog :). @jreback Agreed, for an new index class _inside_ pandas, it is OK to subclass from `Index`. I haven't thought too much about the details of implementing this in pandas yet. @cpcloud Also agreed, `IntervalIndex` is a better name for the described functionality. I will update the first comment. `CellIndex` makes more sense for an index that is actually constrained to a g…",,,,,,Anecdotal,comment,,,,,,,,2014-07-02,github/shoyer,https://github.com/pandas-dev/pandas/issues/7640#issuecomment-47806968,repo: pandas-dev/pandas | issue: Proposal: New Index type for binned data (IntervalIndex) | keyword: lesson learned
"This would be very useful for me, too. Currently I'm using a `DatetimeIndex` that's one longer than my data, which are padded with a row of `nan`s at the end, so that `df.index[i]:df.index[i+1]` is the ""index"" corresponding to `iloc[i]`. It seemed clever when I started the project. This also seems like it will help make contiguous groupby (#5494) easier, since it gives a natural choice of index for the groupings.",,,,,,Anecdotal,comment,,,,,,,,2014-07-28,github/ischwabacher,https://github.com/pandas-dev/pandas/issues/7640#issuecomment-50395026,repo: pandas-dev/pandas | issue: Proposal: New Index type for binned data (IntervalIndex) | keyword: lesson learned
"@shoyer if u (or anyone else) could post test pairs for this would really help it along essentially test cases for everything from construction to various indexing ops that define as much behavior as possible eg for Int64Index result = Index([1,2,3]) expected = [1,2,3] assert_almost_equal(result,expected)",,,,,,Anecdotal,comment,,,,,,,,2014-07-28,github/jreback,https://github.com/pandas-dev/pandas/issues/7640#issuecomment-50396632,repo: pandas-dev/pandas | issue: Proposal: New Index type for binned data (IntervalIndex) | keyword: lesson learned
"@shoyer Thanks for including me; sorry I didn't notice earlier (mail filter was throwing github alerts out). Indeed, I think a general interval index is probably a great addition; although, I lack the breadth in vision to see a general solution. I did actually implement a hacky version of an interval index in pyuvvis that converts a datetime index to intervals of seconds, minutes etc... The main lesson I learned is that your interval index should be able to map back to the original data. To do …",,,,,,Anecdotal,comment,,,,,,,,2014-07-28,github/hughesadam87,https://github.com/pandas-dev/pandas/issues/7640#issuecomment-50400395,repo: pandas-dev/pandas | issue: Proposal: New Index type for binned data (IntervalIndex) | keyword: lesson learned
"@hugadams Looking at your notebook, it appears you may be thinking of a [TimedeltaIndex](https://github.com/pydata/pandas/issues/3009)? The idea behind IntervalIndex is somewhat distinct -- although I can imagine that an IntervalIndex wrapping a TimedeltaIndex could be useful in some cases. @jreback Sounds like a good idea, when I get the chance I will start writing some test cases and add them to this issue.",,,,,,Anecdotal,comment,,,,,,,,2014-07-28,github/shoyer,https://github.com/pandas-dev/pandas/issues/7640#issuecomment-50410132,repo: pandas-dev/pandas | issue: Proposal: New Index type for binned data (IntervalIndex) | keyword: lesson learned
"Ha ya exactly! Thanks, never even saw this thread. I'll post my notebook there for reference as well. I must not understand the intervalindex then.",,,,,,Anecdotal,comment,,,,,,,,2014-07-28,github/hughesadam87,https://github.com/pandas-dev/pandas/issues/7640#issuecomment-50410757,repo: pandas-dev/pandas | issue: Proposal: New Index type for binned data (IntervalIndex) | keyword: lesson learned
Here are a bunch of test cases: https://github.com/shoyer/pandas/commit/838a59767a0611ea7a07f80f4634cf39d2743046 I can open a PR if that makes things easier.,,,,,,Anecdotal,comment,,,,,,,,2014-07-30,github/shoyer,https://github.com/pandas-dev/pandas/issues/7640#issuecomment-50589328,repo: pandas-dev/pandas | issue: Proposal: New Index type for binned data (IntervalIndex) | keyword: lesson learned
"I have updated the first post with some revisions to implementation details (per by test-cases). Basically, I realized that there is no a strong need to require that intervals be contiguous, and dropping that requirement should add some nice flexibility (e.g., the ability to subsample intervals with `idx[::step]`). @jreback Haha, I thought that was your job? ;) In all seriousness, I will probably get around to this at some point but the existing Index objects are pretty complex. #5080 would hel…",,,,,,Anecdotal,comment,,,,,,,,2014-07-30,github/shoyer,https://github.com/pandas-dev/pandas/issues/7640#issuecomment-50649670,repo: pandas-dev/pandas | issue: Proposal: New Index type for binned data (IntervalIndex) | keyword: lesson learned
"For those of you not following along in #901 (which is honestly a dup of this issue), I am now thinking that the implementation here should probably use an actual interval-tree rather than relying on sortedness. Also, for future reference: a suitable data-structure for an index of multi-dimensional intervals (an `NDIntervalIndex`) is an [""R-tree""](http://en.wikipedia.org/wiki/R-tree). And in fact, this is quite a handy data-structure for GIS queries -- there is an R-tree now implemented in Geop…",,,,,,Anecdotal,comment,,,,,,,,2015-01-28,github/shoyer,https://github.com/pandas-dev/pandas/issues/7640#issuecomment-71794646,repo: pandas-dev/pandas | issue: Proposal: New Index type for binned data (IntervalIndex) | keyword: lesson learned
"I'm not sure if this belongs here or elsewhere. However, I'm trying to not clutter everything uselessly by just adding to the ever growing list of issues. If this belongs elsewhere, I'm happy to move it. Is there a reason `pd.cut` returns a `CategoricalIndex` instead of an `IntervalIndex`? The current behavior is ```python >>> pd.cut(np.linspace(0,` 100), bins=np.linspace(0, 101, 10)).value_counts().sort_index().index CategoricalIndex([ (0.0, 11.222], (11.222, 22.444], (22.444, 33.667], (33.667…",,,,,,Anecdotal,comment,,,,,,,,2018-05-07,github/blalterman,https://github.com/pandas-dev/pandas/issues/7640#issuecomment-387091110,repo: pandas-dev/pandas | issue: Proposal: New Index type for binned data (IntervalIndex) | keyword: lesson learned
"I understand. However, a categorical index does not have the same methods and properties available as an interval index. Is it at all reasonable to return an interval index when the categories are purely numeric? Are there reasons to use a categorical over an interval? Ben --------------------- B. L. Alterman Candidate, Applied Physics Solar and Heliospheric Research Group Climate and Space Sciences and Engineering University of Michigan balterma@umich.edu On Mon, May 7, 2018 at 7:19 PM, Jeff R…",,,,,,Anecdotal,comment,,,,,,,,2018-05-07,github/blalterman,https://github.com/pandas-dev/pandas/issues/7640#issuecomment-387241562,repo: pandas-dev/pandas | issue: Proposal: New Index type for binned data (IntervalIndex) | keyword: lesson learned
"@bla1089 I did consider return an ``IntervalIndex`` from cut/qcut. But rejected as: - it broken backward compat in a big way - the implementation of II is not efficient when stored in a Series (going to better in 0.24 with ExtensionArray) - indexing is quite a bit simpler So in theory it is possible, but I don't really see a compelling reason to switch. cats are a nicer holder type of data like this. What exactly is the issue?",,,,,,Anecdotal,comment,,,,,,,,2018-05-08,github/jreback,https://github.com/pandas-dev/pandas/issues/7640#issuecomment-387242703,repo: pandas-dev/pandas | issue: Proposal: New Index type for binned data (IntervalIndex) | keyword: lesson learned
"@jreback I find myself doing the following type of thing rather often: ```python cut = pd.cut(np.linspace(0, 100), bins=np.linspace(0, 101, 10)).value_counts().sort_index() cut.index = pd.IntervalIndex(cut.index).mid.astype(float) cut.plot(drawstyle=""steps-mid"") ``` I hadn't seen a particular issue for it and I was wondering if I was missing something. The backwards compat issue is certainly relevant.",,,,,,Anecdotal,comment,,,,,,,,2018-05-08,github/blalterman,https://github.com/pandas-dev/pandas/issues/7640#issuecomment-387243460,repo: pandas-dev/pandas | issue: Proposal: New Index type for binned data (IntervalIndex) | keyword: lesson learned
support sql transactions this allows the `con` argument of `pd.read_sql`/`pd.read_sql_query` to be a sqlalchemy [`Session`](http://docs.sqlalchemy.org/en/rel_1_0/orm/session.html) object. this allows for the use of temporary tables.,,,,,,Anecdotal,issue,,,,,,,,2015-07-18,github/adamgreenhall,https://github.com/pandas-dev/pandas/pull/10617,repo: pandas-dev/pandas | keyword: lesson learned | state: closed
"@adamgreenhall Thanks for the contribution! Can you provide a bit more context? (I never used the ORM part of SQLAlchemy, only the Core part) But as a `Session` has a `begin` method and `execute` method, I suppose `read_sql` would just work regardless you provide it a Session or connection? (as the support for connections was just added in #10105, which should already allow temporary tables when using a Connection) Why does it need sqlalchemy 1.0.0 ? The reason your tests fail, is because it se…",,,,,,Anecdotal,comment,,,,,,,,2015-07-19,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/pull/10617#issuecomment-122649728,repo: pandas-dev/pandas | issue: support sql transactions | keyword: lesson learned
"@jorisvandenbossche - #10105 does the trick with `sqlalchemy.engine.Connectable`. I didn't check the docs on master before adding this - learned my lesson for next time. Thanks! ps. Not sure what the sqlalchemy recommended way to handle transactions is now - I think sessions got added in 1.0 - that was just the first thing I found in the docs, but the `with engine.begin():` code seems simpler.",,,,,,Anecdotal,comment,,,,,,,,2015-07-20,github/adamgreenhall,https://github.com/pandas-dev/pandas/pull/10617#issuecomment-122746051,repo: pandas-dev/pandas | issue: support sql transactions | keyword: lesson learned
"But ``` In [46]: isinstance(session, sqlalchemy.engine.Connectable) Out[46]: False ``` so the current code is not going to work for a Session. (I never used Sessions, but I can imagine that support for it would be useful for people using it)",,,,,,Anecdotal,comment,,,,,,,,2015-07-20,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/pull/10617#issuecomment-122768590,repo: pandas-dev/pandas | issue: support sql transactions | keyword: lesson learned
"Allow deprecate_kwarg to transform arguments This will allow deprecated kwargs to be transformed before being passed to a function. For example, in #7963, one could write ``` python @deprecate_kwarg(old_arg_name='infer_dst', new_arg_name='ambiguous', mapping={True: 'infer', False: 'raise'}) def tz_localize(self, tz, ambiguous=None): ... ``` Open issues: - should we test for a bad mapping at creation time? (in this PR: yes) - how much error checking should we do? (in this PR: pass args unrecogni…",,,,,,Anecdotal,issue,,,,,,,,2014-08-11,github/ischwabacher,https://github.com/pandas-dev/pandas/pull/7991,repo: pandas-dev/pandas | keyword: lesson learned | state: closed
"I've been having free time in such short blocks lately that the bottleneck here has been, embarrassingly enough, getting a working python3 installation on my laptop. Pip is being a recalcitrant pipsqueak. I would like to get this done in the next 24 hours, but I'm decreasingly convinced that's going to happen.",,,,,,Anecdotal,comment,,,,,,,,2014-09-08,github/ischwabacher,https://github.com/pandas-dev/pandas/pull/7991#issuecomment-54901233,repo: pandas-dev/pandas | issue: Allow deprecate_kwarg to transform arguments | keyword: lesson learned
Hrm... Somehow I get the feeling that this rebase was not as clean as intended. Also I do not think that these changes warrant changes to that wiki. Though I wish I'd found the wiki sooner. Advice?,,,,,,Anecdotal,comment,,,,,,,,2014-09-09,github/ischwabacher,https://github.com/pandas-dev/pandas/pull/7991#issuecomment-54938059,repo: pandas-dev/pandas | issue: Allow deprecate_kwarg to transform arguments | keyword: lesson learned
"Yes, indeed. For only rebasing, this should normally work: ``` git fetch upstream git rebase upstream/master ``` BTW, I am enjoying `conda` a lot, almost no more `pip` for me.",,,,,,Anecdotal,comment,,,,,,,,2014-09-09,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/pull/7991#issuecomment-54938728,repo: pandas-dev/pandas | issue: Allow deprecate_kwarg to transform arguments | keyword: lesson learned
"@jreback Got it. @jorisvandenbossche I don't see how that creates a branch that's pushable to `origin/patch-2`, since that branch is way behind `upstream/master`.",,,,,,Anecdotal,comment,,,,,,,,2014-09-09,github/ischwabacher,https://github.com/pandas-dev/pandas/pull/7991#issuecomment-54971603,repo: pandas-dev/pandas | issue: Allow deprecate_kwarg to transform arguments | keyword: lesson learned
"But you have to do those commands 'from' that branch, so, if 'patch-2' is your feature branch: ``` git checkout patch-2 git fetch upstream git rebase upstream/master git push -f origin patch-2 ``` This will update your local 'patch-2' branch to be up to date with upstream master, and if you push this to the remote origin (your fork) version, then that version and automaticall the PR will be up to date. But you possibly have to do a force push (`-f`)",,,,,,Anecdotal,comment,,,,,,,,2014-09-09,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/pull/7991#issuecomment-54977082,repo: pandas-dev/pandas | issue: Allow deprecate_kwarg to transform arguments | keyword: lesson learned
"Yup, it was the force push that had eluded me. Instead I had merged in `origin/master` and created a rebase conflict. Lesson learned! @jreback Here's the text I would add to the wiki, but I still don't see a good place to put it. Maybe in a section immediately following [Testing for Warnings](https://github.com/pydata/pandas/wiki/Testing#testing-for-warnings)? > ### Deprecating keyword arguments > > If your PR renames a keyword argument or adds a new kwarg to a function such that the functional…",,,,,,Anecdotal,comment,,,,,,,,2014-09-09,github/ischwabacher,https://github.com/pandas-dev/pandas/pull/7991#issuecomment-54997263,repo: pandas-dev/pandas | issue: Allow deprecate_kwarg to transform arguments | keyword: lesson learned
In here (just add a sub-section in the For Devvelopers): https://github.com/pydata/pandas/wiki mainly this is to point people to when they need to do something (e.g. deprecate a kwarg!),,,,,,Anecdotal,comment,,,,,,,,2014-09-09,github/jreback,https://github.com/pandas-dev/pandas/pull/7991#issuecomment-54998995,repo: pandas-dev/pandas | issue: Allow deprecate_kwarg to transform arguments | keyword: lesson learned
"[Wiki updated](/pydata/pandas/wiki/A-quick-overview-of-pandas.util), though maybe that isn't the most obvious place for people to look.",,,,,,Anecdotal,comment,,,,,,,,2014-09-09,github/ischwabacher,https://github.com/pandas-dev/pandas/pull/7991#issuecomment-55012763,repo: pandas-dev/pandas | issue: Allow deprecate_kwarg to transform arguments | keyword: lesson learned
"DOC: make io.rst utf8 only https://github.com/pydata/pandas/issues/5142 @JanSchulz , can you test whether this solves the problem for you?",,,,,,Anecdotal,issue,,,,,,,,2014-01-13,github/ghost,https://github.com/pandas-dev/pandas/pull/5926,repo: pandas-dev/pandas | keyword: lesson learned | state: closed
"Maybe this can solve the windows building issue (I will also test), but aside: do we want this in the docs? Because the example by itself does work, it's only the building that does not work (as far as I understand).",,,,,,Anecdotal,comment,,,,,,,,2014-01-13,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/pull/5926#issuecomment-32199230,repo: pandas-dev/pandas | issue: DOC: make io.rst utf8 only | keyword: lesson learned
"I think we want users/contributors to be able to build the docs, yeah. Even if they're on windows/diff locale Do you feel the workaround clutter detracts much from the example? It took a lot of effort to get pandas to play nice wth unicode and one lesson learned is not to mix encodings. The docs should be utf8-clean IMO.",,,,,,Anecdotal,comment,,,,,,,,2014-01-13,github/ghost,https://github.com/pandas-dev/pandas/pull/5926#issuecomment-32199985,repo: pandas-dev/pandas | issue: DOC: make io.rst utf8 only | keyword: lesson learned
"I tried it, and it does not solve the issue. And in retrospect, that is maybe also logical: the problem in windows is in the building of the rst with unicode to html, and it is the output generated by the code example which causes this. With your changes, the output of the code example still contains special characters (which is also the point of the code example), and so causes the build on windows to stop. I think @JanSchulz had another approach as a kind of hack: something along the lines of…",,,,,,Anecdotal,comment,,,,,,,,2014-01-13,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/pull/5926#issuecomment-32208600,repo: pandas-dev/pandas | issue: DOC: make io.rst utf8 only | keyword: lesson learned
"Then I misunderstood the issue. There is a definite difference: ``` s1='word,length\nTr\xc3\xa4umen,7\nGr\xc3\xbc\xc3\x9fe,5' s2=s1.decode('utf8').encode('latin-1') s1.decode('utf8') Out[33]: u'word,length\nTr\xe4umen,7\nGr\xfc\xdfe,5' s2.decode('utf8') --------------------------------------------------------------------------- UnicodeDecodeError Traceback (most recent call last) <ipython-input-34-7c1601a98c33> in <module>() ----> 1 s2.decode('utf8') /usr/lib64/python2.7/encodings/utf_8.pyc in …",,,,,,Anecdotal,comment,,,,,,,,2014-01-13,github/ghost,https://github.com/pandas-dev/pandas/pull/5926#issuecomment-32209080,repo: pandas-dev/pandas | issue: DOC: make io.rst utf8 only | keyword: lesson learned
"btw, there was some `decode` action in our hacked version of ipython_directive, #5925 may actually solve the problem by sheer coincidence.",,,,,,Anecdotal,comment,,,,,,,,2014-01-13,github/ghost,https://github.com/pandas-dev/pandas/pull/5926#issuecomment-32209214,repo: pandas-dev/pandas | issue: DOC: make io.rst utf8 only | keyword: lesson learned
"See also here https://github.com/pydata/pandas/issues/5142#issuecomment-26761295. There was indeed a `.decode('utf8')` in our version of the ipython directive for some other reason, but that broke the building on windows.",,,,,,Anecdotal,comment,,,,,,,,2014-01-13,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/pull/5926#issuecomment-32209535,repo: pandas-dev/pandas | issue: DOC: make io.rst utf8 only | keyword: lesson learned
"Unintuitive default behavior with wide DataFrames in the IPython notebook In the IPython notebook, HTML output it the default and whether summary view is displayed should not be governed by hypothetical line width. I ran into this problem in a demo recently and it took me a minute to figure out what was wrong, definitely a bad change in 0.11.",,,,,,Anecdotal,issue,,,,,,,,2013-05-11,github/wesm,https://github.com/pandas-dev/pandas/issues/3573,repo: pandas-dev/pandas | keyword: lesson learned | state: closed
"wes, sorry to hear you had a snafu in front of an audience. The updated behaviour has a consistent story and took a _lot_ of noodling to make sure it behaved sanely under a substantial number of corner cases. It seems the end result of that is crappy UX from your POV. Disappointing to hear considering how little fun I had trying to make it work. An easy fix is to just set the width to something large on ipnb by default. To let users opt in to activating the limit rather then loosening it. That …",,,,,,Anecdotal,comment,,,,,,,,2013-05-20,github/ghost,https://github.com/pandas-dev/pandas/issues/3573#issuecomment-18153744,repo: pandas-dev/pandas | issue: Unintuitive default behavior with wide DataFrames in the IPython notebook | keyword: lesson learned
"Sorry @yp I didn't mean to put down your work which is definitely much appreciated. Satisfying many concerns simultaneously is not an easy thing. It was just basically ""this used to print out fine in the notebook without any option-tweaking, what happened?"" -- the fix could actually be just to always print the full-width HTML repr whenever the table is under max_rows and that could get put all here in the ipython hook: ``` def _repr_html_(self): """""" Return a html representation for a particular…",,,,,,Anecdotal,comment,,,,,,,,2013-05-20,github/wesm,https://github.com/pandas-dev/pandas/issues/3573#issuecomment-18165089,repo: pandas-dev/pandas | issue: Unintuitive default behavior with wide DataFrames in the IPython notebook | keyword: lesson learned
"@y-p _enough is enough_ ... apparently not :-), very brave of you to tackle this once more. Our idea of a double bounding box (display width/height and object data shape) was/is not really a success. Maybe the unintuitive part is that by default they are both active. I hope your changes will resolve the confusion.",,,,,,Anecdotal,comment,,,,,,,,2013-05-21,github/lodagro,https://github.com/pandas-dev/pandas/issues/3573#issuecomment-18231806,repo: pandas-dev/pandas | issue: Unintuitive default behavior with wide DataFrames in the IPython notebook | keyword: lesson learned
"python crash without exception I'm using pandas 0.9.0. I have a function that returns a DataFrame. The resulting DataFrame is 'unstable': many operations I try on it, interactively in IPython, lead to a complete python.exe crash. I get no exception or indication on what's going wrong which is very frustrating. Some operations do work however, like df.index or df.values. In the screenshot below, r is a custom-built class with method to_dataframe() that generates the dataframe. In [6]: df=r.to_da…",,,,,,Anecdotal,issue,,,,,,,,2012-11-05,github/saroele,https://github.com/pandas-dev/pandas/issues/2182,repo: pandas-dev/pandas | keyword: lesson learned | state: closed
"Can you make a smaller reproduction and post more details on how you constructed the DataFrame (e.g., what's in r, what is the input data, how did you call the constructor, etc)?",,,,,,Anecdotal,comment,,,,,,,,2012-11-05,github/changhiskhan,https://github.com/pandas-dev/pandas/issues/2182#issuecomment-10088896,repo: pandas-dev/pandas | issue: python crash without exception | keyword: lesson learned
"A few minutes later I found out that other pandas functionality I normally use bugged too. This was since I upgraded to pandas 0.9. This upgrade did not exit successfully with pip install --upgrade pandas, so I downloaded the zipped pandas 0.9 source and ran python setup.py install Then the bug started. I was unable to repair the mess, so I uninstalled my complete python installation and reinstalled the fresh python(xy) 2.7.3 (comes with pandas 0.9). And hurray, all works again. So sorry for th…",,,,,,Anecdotal,comment,,,,,,,,2012-11-05,github/saroele,https://github.com/pandas-dev/pandas/issues/2182#issuecomment-10091018,repo: pandas-dev/pandas | issue: python crash without exception | keyword: lesson learned
I used to get problems like this when I turned off bounds checking in Cython and there was an out of bounds array access. The interpreter would keep going but if I tried to print the array it would crash. If you're using Cython outside of Pandas check to make sure you're not going out of bounds when accessing an array.,,,,,,Anecdotal,comment,,,,,,,,2012-11-06,github/cpcloud,https://github.com/pandas-dev/pandas/issues/2182#issuecomment-10110283,repo: pandas-dev/pandas | issue: python crash without exception | keyword: lesson learned
"@wesm : no idea what you mean. I invoke a compiler when I type python setup.py install in the pandas folder? How can I check which compiler is used (I have a few different MS visual C++ express editions for example) @cpcloud : I did nothing in Cython, just using the basic pandas functions to create DataFrames, merge, etc.",,,,,,Anecdotal,comment,,,,,,,,2012-11-06,github/saroele,https://github.com/pandas-dev/pandas/issues/2182#issuecomment-10128085,repo: pandas-dev/pandas | issue: python crash without exception | keyword: lesson learned
"@saroele Your compiler will be invoked when you compile Cython code and (roughly) whenever you're using a statically typed language such as Java, C, or C++. I say roughly because, there are exceptions to such a rule (some languages blur the lines between interpretation and compilation). [Just-in-time compilation](http://en.wikipedia.org/wiki/Just-in-time_compilation) is an example of this. Python itself is an interpreted language, but the CPython interpreter is written in C. There are other imp…",,,,,,Anecdotal,comment,,,,,,,,2012-11-06,github/cpcloud,https://github.com/pandas-dev/pandas/issues/2182#issuecomment-10129575,repo: pandas-dev/pandas | issue: python crash without exception | keyword: lesson learned
"Is this build transcript stored somewhere? My system is working now, so i'm not really eager to try breaking it again :-) On Tue, Nov 6, 2012 at 11:08 PM, Wes McKinney notifications@github.comwrote: > I would be interested to see the full build transcript when you did python > setup.py install > > — > Reply to this email directly or view it on GitHubhttps://github.com/pydata/pandas/issues/2182#issuecomment-10129847.",,,,,,Anecdotal,comment,,,,,,,,2012-11-07,github/saroele,https://github.com/pandas-dev/pandas/issues/2182#issuecomment-10140714,repo: pandas-dev/pandas | issue: python crash without exception | keyword: lesson learned
"According to [this](http://www.microsoft.com/resources/documentation/windows/xp/all/proddocs/en-us/redirection.mspx?mfr=true) you can redirect the output of a command to a file in Windows, just like you can in UNIX based systems, it even uses the same syntax. You could do something like ``` python setup.py install>c:\path\to\current\directory\my_build_script.txt ``` on the command line. I think I should mention that the `>` character is what indicates to the OS that you'd like to redirect `stdo…",,,,,,Anecdotal,comment,,,,,,,,2012-11-07,github/cpcloud,https://github.com/pandas-dev/pandas/issues/2182#issuecomment-10148595,repo: pandas-dev/pandas | issue: python crash without exception | keyword: lesson learned
ENH: Rename internal `DataFrame._append()` ### Feature Type - [ ] Adding new functionality to pandas - [ ] Changing existing functionality in pandas - [X] Removing existing functionality in pandas ### Problem Description This comment on the python discussion is interesting: https://discuss.python.org/t/name-suggestions-for-attributeerrors-and-possibly-nameerrors-should-not-include-names-with-single-leading-underscores/48588 Example code (from main): ```python >>> pd.DataFrame.append() Traceback…,,,,,,Anecdotal,issue,,,,,,,,2024-03-20,github/Dr-Irv,https://github.com/pandas-dev/pandas/issues/57936,repo: pandas-dev/pandas | keyword: workaround | state: open
Is the proposal to do this with all internal methods that are similar to removed external names? I'm pretty opposed here - I think we should be free to chose internal names however we see fit.,,,,,,Anecdotal,comment,,,,,,,,2024-03-20,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/57936#issuecomment-2010667383,repo: pandas-dev/pandas | issue: ENH:  Rename internal `DataFrame._append()`  | keyword: workaround
"> Is the proposal to do this with all internal methods that are similar to removed external names? I'm pretty opposed here - I think we should be free to chose internal names however we see fit. No, but in this case, since we deprecated `DataFrame.append()`, we don't want people using `DataFrame._append()` as a workaround so we need to hide it better.",,,,,,Anecdotal,comment,,,,,,,,2024-03-20,github/Dr-Irv,https://github.com/pandas-dev/pandas/issues/57936#issuecomment-2010702518,repo: pandas-dev/pandas | issue: ENH:  Rename internal `DataFrame._append()`  | keyword: workaround
"~~I'd me somewhat +1 - see this SO answer -https://stackoverflow.com/a/76449334/2214597 while it does have a disclaimer, it's still pretty heavily upvoted.~~ Nvm - this SO question is already mentioned in the Discourse thread",,,,,,Anecdotal,comment,,,,,,,,2024-03-20,github/asishm,https://github.com/pandas-dev/pandas/issues/57936#issuecomment-2010809329,repo: pandas-dev/pandas | issue: ENH:  Rename internal `DataFrame._append()`  | keyword: workaround
I would definitely wait until https://github.com/python/cpython/issues/116871 has a resolution upstream before preemptively acting here.,,,,,,Anecdotal,comment,,,,,,,,2024-03-20,github/mroeschke,https://github.com/pandas-dev/pandas/issues/57936#issuecomment-2010890298,repo: pandas-dev/pandas | issue: ENH:  Rename internal `DataFrame._append()`  | keyword: workaround
"No opposition if this is a unique case, but I'd like to not play whack-a-mole with the CPython interpreter and our internal names. That seems like too much code churn to me.",,,,,,Anecdotal,comment,,,,,,,,2024-04-25,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/57936#issuecomment-2078213508,repo: pandas-dev/pandas | issue: ENH:  Rename internal `DataFrame._append()`  | keyword: workaround
"> No opposition if this is a unique case, but I'd like to not play whack-a-mole with the CPython interpreter and our internal names. That seems like too much code churn to me. I think the issue here is that we used to have `DataFrame.append()`, and now CPython is suggesting `DataFrame._append()`, so people are using it. If we remove methods in the future, then we should be aware of the similarity of the removed name with any ""private"" methods.",,,,,,Anecdotal,comment,,,,,,,,2024-04-25,github/Dr-Irv,https://github.com/pandas-dev/pandas/issues/57936#issuecomment-2078228848,repo: pandas-dev/pandas | issue: ENH:  Rename internal `DataFrame._append()`  | keyword: workaround
"I'd also be fine with changing the name internally. Agree with @jbrockmendel we will be waiting a long time for the fix to make its way through different versions of Python. There will be some breakage if people have decided to use this, but the longer we wait the worse that handcuff becomes",,,,,,Anecdotal,comment,,,,,,,,2024-07-18,github/WillAyd,https://github.com/pandas-dev/pandas/issues/57936#issuecomment-2237210711,repo: pandas-dev/pandas | issue: ENH:  Rename internal `DataFrame._append()`  | keyword: workaround
"Metadata generation failed when installing PyQt5 This error comes from running the dockerfile and reaches the PyQt5 in the requirements-dev.txt. From stackoverflow, this problem occur when installing PyQt5 with version after 5.15.6 in virtual environment. Not labeling bug as it's bug from PyQt5, but since this could affect dockerfile building I'll create this issue. If this is not proper please close this issue. Reference: https://stackoverflow.com/questions/70936664/metadata-generation-failed-…",,,,,,Anecdotal,issue,,,,,,,,2025-03-03,github/PenguinPen,https://github.com/pandas-dev/pandas/issues/61037,repo: pandas-dev/pandas | keyword: workaround | state: open
"Thanks for the report! > Not labeling bug as it's bug from PyQt5, but since this could affect dockerfile building I'll create this issue. Could or does affect the Dockerfile? The issue you linked to seems to all refer to Mac OS and Windows, but our Dockerfile is based on Ubuntu.",,,,,,Anecdotal,comment,,,,,,,,2025-03-03,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/61037#issuecomment-2695747379,repo: pandas-dev/pandas | issue: Metadata generation failed when installing PyQt5 | keyword: workaround
"> Could or does affect the Dockerfile? The issue you linked to seems to all refer to Mac OS and Windows, but our Dockerfile is based on Ubuntu. I encountered this issue when I run the docker file in the python virtual environment, and I am on Mac. Error message: <img width=""637"" alt=""Image"" src=""https://github.com/user-attachments/assets/c56354c9-032e-4932-988b-377eac1adee8"" />",,,,,,Anecdotal,comment,,,,,,,,2025-03-04,github/PenguinPen,https://github.com/pandas-dev/pandas/issues/61037#issuecomment-2695846922,repo: pandas-dev/pandas | issue: Metadata generation failed when installing PyQt5 | keyword: workaround
"@PenguinPen This happened to me while trying to build it on a mac with an M chip. Even if you resolve the problem with PyQt, further issues will arise with a number of other packages. Use the commands below, in order to build the docker image with correct platform specifications: build your image with the modified command below (forcing amd64 architecture) - `docker build --platform=linux/amd64 --no-cache -t pandas-dev .` this runs the previously built image and moves you inside the running con…",,,,,,Anecdotal,comment,,,,,,,,2025-05-09,github/surenpoghosian,https://github.com/pandas-dev/pandas/issues/61037#issuecomment-2866324832,repo: pandas-dev/pandas | issue: Metadata generation failed when installing PyQt5 | keyword: workaround
"@rhshadrach I think this is worth highlighting in the documentation. If it's good, we can add it during our upcoming PyData Yerevan Sprint.",,,,,,Anecdotal,comment,,,,,,,,2025-05-09,github/surenpoghosian,https://github.com/pandas-dev/pandas/issues/61037#issuecomment-2866332026,repo: pandas-dev/pandas | issue: Metadata generation failed when installing PyQt5 | keyword: workaround
BUG: many dataframe operations broken when a column contains numpy structured array ### Pandas version checks - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas. ### Reproducib…,,,,,,Anecdotal,issue,,,,,,,,2024-10-26,github/digitalsignalperson,https://github.com/pandas-dev/pandas/issues/60108,repo: pandas-dev/pandas | keyword: workaround | state: open
"also worth mentioning the problems go away if you allow the structured array to have an object dtype e.g. ```python import numpy as np import pandas as pd N = 10 df = pd.DataFrame(data={'other_stuff':np.zeros(N)}) idx = [0, 1] hash_dtype = np.dtype([(f'h{i}', np.uint64) for i in range(4)]) df.loc[idx, 'hashes'] = np.ones(len(idx), dtype=hash_dtype) # here df['hashes'] coerced to object data type ```",,,,,,Anecdotal,comment,,,,,,,,2024-10-26,github/digitalsignalperson,https://github.com/pandas-dev/pandas/issues/60108#issuecomment-2439162649,repo: pandas-dev/pandas | issue: BUG: many dataframe operations broken when a column contains numpy structured array | keyword: workaround
"Thanks for the report. Indeed, pandas does not support NumPy structured arrays. I do not think it is feasible to support these.",,,,,,Anecdotal,comment,,,,,,,,2024-10-26,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/60108#issuecomment-2439670421,repo: pandas-dev/pandas | issue: BUG: many dataframe operations broken when a column contains numpy structured array | keyword: workaround
"Should we rather error when a user creates such a Series with an unsupported dtype, instead of allowing to create it but then fail later on in various confusing ways? Or is there enough that works that people would want to use a Series/DataFrame container with such data?",,,,,,Anecdotal,comment,,,,,,,,2024-10-30,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/60108#issuecomment-2446468235,repo: pandas-dev/pandas | issue: BUG: many dataframe operations broken when a column contains numpy structured array | keyword: workaround
"For sure it would be helpful to fail earlier, it took me a while to figure out what led to creating this issue. My workaround was to view the numpy array as bytes. In my example the structured type is 32 bytes and `.astype('S32')` makes each element appear as a 32 bytes string. Pandas seems fine with this data type. Is there any trick where pandas could do a similar thing, just treat structured arrays as opaque ""element is N bytes"" or even internally storing as a `np.dtype(f'S{element width')` …",,,,,,Anecdotal,comment,,,,,,,,2024-10-30,github/digitalsignalperson,https://github.com/pandas-dev/pandas/issues/60108#issuecomment-2447459800,repo: pandas-dev/pandas | issue: BUG: many dataframe operations broken when a column contains numpy structured array | keyword: workaround
"I'm +1 on failing in the constructor. > Is there any trick where pandas could do a similar thing, just treat structured arrays as opaque ""element is N bytes"" or even internally storing as a `np.dtype(f'S{element width')` type? I don't think this should happen silently.",,,,,,Anecdotal,comment,,,,,,,,2024-10-30,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/60108#issuecomment-2448358005,repo: pandas-dev/pandas | issue: BUG: many dataframe operations broken when a column contains numpy structured array | keyword: workaround
"> I don't think this should happen silently. I was thinking more in terms of something pandas could do internally, but I don't know the internals for why it trips on structured types or if this is a practical idea. For example maybe some bookkeeping happens so that internally pandas sees a ndarray.view of dtype('S{item size}') but to the user it appears like their custom data type. ``` N = 10 hash_dtype = np.dtype([(f'h{i}', np.uint64) for i in range(4)]) hashes = np.zeros(N, dtype=hash_dtype) …",,,,,,Anecdotal,comment,,,,,,,,2024-10-30,github/digitalsignalperson,https://github.com/pandas-dev/pandas/issues/60108#issuecomment-2448655025,repo: pandas-dev/pandas | issue: BUG: many dataframe operations broken when a column contains numpy structured array | keyword: workaround
"> For example maybe some bookkeeping happens so that internally pandas sees a ndarray.view of dtype('S{item size}') but to the user it appears like their custom data type. If this were possible, would essentially any operation other than getting, setting, and reshaping just raise? E.g. groupby.sum. I'm quite negative on partial support for a dtype like this.",,,,,,Anecdotal,comment,,,,,,,,2024-10-31,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/60108#issuecomment-2450746638,repo: pandas-dev/pandas | issue: BUG: many dataframe operations broken when a column contains numpy structured array | keyword: workaround
"Another similar bug report in https://github.com/pandas-dev/pandas/issues/42739, with a _masked_ structured array, where for this case the DataFrame constructor already fails, although it could use a more informative error message. ``` import numpy as np import pandas as pd import numpy.ma as ma # create a masked, structured array a = np.ma.array([(1, 2.2), (42, 5.5)], dtype=[('a',int),('b',float)], mask=[(True,False),(False,True)]) b = pd.DataFrame(a) ``` Currently gives `TypeError: Iterator o…",,,,,,Anecdotal,comment,,,,,,,,2024-11-12,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/60108#issuecomment-2470269737,repo: pandas-dev/pandas | issue: BUG: many dataframe operations broken when a column contains numpy structured array | keyword: workaround
"Some changes I noticed in pandas 2.3.1 / numpy 2.3.2 (was previously on pandas 2.2.3 / numpy 2.3.0) I was building a tool making use of the `np.dtype(f'S{element width')` workaround above, but currently this no longer works for some dataframe operations. ```python df = pd.DataFrame({'x':np.arange(10), 'y' : np.array(['12345678'] * 10, dtype='S8')}) df['y'].dtype # dtype('O') # Curious why it coerced to object type df.groupby('x').first() # But this currently works y x 0 b'12345678' 1 b'12345678…",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/digitalsignalperson,https://github.com/pandas-dev/pandas/issues/60108#issuecomment-3187119542,repo: pandas-dev/pandas | issue: BUG: many dataframe operations broken when a column contains numpy structured array | keyword: workaround
"ENH: cumcount() for DataFrames and Series Unless I am missing something obvious, we can do this: `df.groupby('x').cumcount()` but not: `df.cumcount()` Obviously, there are workarounds, but seems like we should be able to do on a dataframe/series if we can do on a groupby object?",,,,,,Anecdotal,issue,,,,,,,,2016-03-16,github/johne13,https://github.com/pandas-dev/pandas/issues/12648,repo: pandas-dev/pandas | keyword: workaround | state: open
"@johne13 you aren't confusing it with `cumsum`? Besides obtaining it from an index, you can also just do `range(len(df))`",,,,,,Anecdotal,comment,,,,,,,,2016-03-17,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-197615940,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
"It is not a super-common event, to be sure, but there are cases like this: that I bump into somewhat regularly: http://stackoverflow.com/questions/36044890/how-to-speed-up-a-very-slow-pandas-apply-function/36047836#36047836 Simpler example: ``` df = pd.DataFrame({ 'x':[1,2,1,2] }) #df[ df.x == 2 ].cumcount() # doesn't work of course df.loc[ df.x == 2, 'y'] = 1 # so I do a two step with cumsum as 2nd step df.y.cumsum() Out[428]: 0 NaN 1 1.0 2 NaN 3 2.0 ``` So yeah, lots of easy ways to do this o…",,,,,,Anecdotal,comment,,,,,,,,2016-03-17,github/johne13,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-197619108,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
"I guess I am missing something. I couldn't do this, for example, `df['new'] = df[ df.x == 2 ].reset_index()` because obviously I have reset the index so can't correctly align it with the original dataset. I should have been more clear about that aspect. I am generally trying to create a new column when I mistakenly attempt to use the nonexistent cumcount method.",,,,,,Anecdotal,comment,,,,,,,,2016-03-17,github/johne13,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-197621936,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
ok not averse to adding this method for consistency (among other cum. methods) but should return a Series pretty trivial impl if u would like to submit a PR,,,,,,Anecdotal,comment,,,,,,,,2016-03-17,github/jreback,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-197623957,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
"signature would be identical to `.cum*` (at end of `core/generic.py`) ``` In [1]: pd.DataFrame.cumsum? Signature: pd.DataFrame.cumsum(self, axis=None, dtype=None, out=None, skipna=True, **kwargs) Docstring: Return cumulative sum over requested axis. Parameters ---------- axis : {index (0), columns (1)} skipna : boolean, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA Returns ------- sum : Series File: ~/pandas/pandas/core/generic.py Type: instancemethod…",,,,,,Anecdotal,comment,,,,,,,,2016-03-17,github/jreback,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-197624489,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
"Potentially not as fast (?), and handles NAs differently from the case above, but very idiomatic: ``` In [12]: df.where(df==2).expanding().count() Out[12]: x 0 0.0 1 1.0 2 1.0 3 2.0 ```",,,,,,Anecdotal,comment,,,,,,,,2016-03-17,github/max-sixty,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-197637008,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
Taken a step further: we could offer a argument for changing the [NaN behavior](https://github.com/pydata/pandas/blame/master/doc/source/computation.rst#L617) and offer all the `cum-x` behavior with `.expanding()`,,,,,,Anecdotal,comment,,,,,,,,2016-03-17,github/max-sixty,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-197638307,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
"@MaximilianR actually that's a good point. would you make a PR to: - cross-link doc-strings for `.cum*` to the `.expanding().*` (for `sum,prod,max,min`, which are only supported). - if you find a nice place in computation.rst where you might want to mention these similarities. (I see you linked there), just review this then.",,,,,,Anecdotal,comment,,,,,,,,2016-03-17,github/jreback,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-197643264,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
"maybe give a tiny example ``` In [7]: s = Series([1,2,np.nan,3,np.nan,4]) In [8]: s.cumsum() Out[8]: 0 1.0 1 3.0 2 NaN 3 6.0 4 NaN 5 10.0 dtype: float64 In [9]: s.expanding().sum() Out[9]: 0 1.0 1 3.0 2 3.0 3 6.0 4 6.0 5 10.0 dtype: float64 In [10]: s.cumsum().ffill() Out[10]: 0 1.0 1 3.0 2 3.0 3 6.0 4 6.0 5 10.0 dtype: float64 ```",,,,,,Anecdotal,comment,,,,,,,,2016-03-17,github/jreback,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-197644471,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
"> @MaximilianR actually that's a good point. would you make a PR to: I am jammed at the moment (although free enough to cruise the feed, it seems), but I can add that if no one else picks it up over the next few weeks",,,,,,Anecdotal,comment,,,,,,,,2016-03-17,github/max-sixty,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-197650536,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
@MaximilianR I think you can get the same behavior as in my earlier example by using a boolean mask instead of `where`: ``` In [504]: df['new'] = df.x[ df.x == 2 ].expanding().count() In [505]: df Out[505]: x new 0 1 NaN 1 2 1.0 2 1 NaN 3 2 2.0 ```,,,,,,Anecdotal,comment,,,,,,,,2016-03-17,github/johne13,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-197675532,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
"Here's a quick speed test, `cumsum` about 4x faster than `expanding().count()` ``` In [515]: df = pd.DataFrame({ 'x':np.random.randn(10000) }) In [516]: %timeit df['new1'] = df.x[ df.x > 0 ].expanding().count() 100 loops, best of 3: 4.57 ms per loop In [517]: %%timeit ...: df.loc[ df.x > 0, 'new2' ] = 1 ...: df['new2'] = df['new2'].cumsum() 1000 loops, best of 3: 1.12 ms per loop ```",,,,,,Anecdotal,comment,,,,,,,,2016-03-17,github/johne13,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-197677097,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
try assigning to a range ITS much faster still. this is the impl we were not suggesting using expanding.count here it's doing a lot more work and very careful about min periods and such,,,,,,Anecdotal,comment,,,,,,,,2016-03-17,github/jreback,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-197677514,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
"@jreback So do you still think `cumcount` is worthwhile or is `expanding().count()` the way to go? Sorry, don't understand what you mean by ""assigning to a range""? If you still like the cumcount idea I'll give it a shot though I have not done a PR before. Of course if someone else wants to do it I'd gladly defer to them.",,,,,,Anecdotal,comment,,,,,,,,2016-03-17,github/johne13,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-197678231,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
"OK, I think I have an initial attempt and will post the code and example in a moment, but first want to list other similar functions as a point of comparison: - `cumsum()` -- outputs series or dataframe, dtype=float - `(groupby)cumcount()` -- outputs a series, numbered 0,1,2, ... n-1, dtype=int - `expanding().count()` -- outputs series or dataframe, numbered 1,2,3, ... n, dtype=float Starting from there, it made the most sense to me to aim for something roughly consistent with cumsum() and expa…",,,,,,Anecdotal,comment,,,,,,,,2016-03-19,github/johne13,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-198721215,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
"Here's the code, added to generic, just after cummax (and also cumsum for comparison). ``` cls.cumcount = _make_cum_function( 'count', name, name2, axis_descr, ""cumulative count"", lambda y, axis: np.cumsum(~np.isnan(y), axis).astype(float), np.nan, np.nan) cls.cumsum = _make_cum_function( 'sum', name, name2, axis_descr, ""cumulative sum"", lambda y, axis: y.cumsum(axis), 0., np.nan) ``` It's a little less natural than the other cum functions, but that was the best way I could come up with that fi…",,,,,,Anecdotal,comment,,,,,,,,2016-03-19,github/johne13,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-198724773,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
"The problem with making it consistent with `groupby().cumcount()` is that that function does not take the values into accound (whether they are NaN or not), it just numbers the entries in the group regardless of its values. That is also the reason that it returns a Series. So if we want to make `Series/DataFrame.cumcount` consistent with the existing method, then it should also not disregard NaN values (as you do in your example implementation). But, I suppose this defeats part of the reason yo…",,,,,,Anecdotal,comment,,,,,,,,2016-03-19,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-198754369,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
"@jorisvandenbossche I really hadn't thought about how exactly a Series/DataFrame.cumcount would work until I started comparing it to all the existing functions (count/cumsum/groupby.cumcount). Then I realized it couldn't be consistent with all of them, only some. Out of all the comparison functions, it seems to me that it ought to work approximately the same as expanding().count(), right? And conversely, the way groupby.cumcount works makes the least sense to me, but of course I'll gladly defer…",,,,,,Anecdotal,comment,,,,,,,,2016-03-19,github/johne13,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-198757523,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
"This is almost a trivial function, should be something like: ``` def cumcount(self, axis=0, skipna=True): ax = self._get_axis(axis) if skipna and ax.hasnans: result = ax.isnull().cumsum() else: result = range(len(ax)) return Series(result, index=ax) ``` this is really just a range; if the index has nans then its slightly different.",,,,,,Anecdotal,comment,,,,,,,,2016-03-19,github/jreback,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-198759969,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
I guess it technically should have an `ascending` arg to be consistent. Further `.groupby.cumcount()` should have `skipna`. This should mirror the `.cum*` signatures.,,,,,,Anecdotal,comment,,,,,,,,2016-03-19,github/jreback,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-198760073,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
"@jreback But I would also find it very unfortunate that `groupby().cumcount()` and `groupby().apply(lambda x: x.cumcount())` would not be the same .. And it is, as far as I can see, not possible to have both that and have it consistent with `Series.cumsum` and others",,,,,,Anecdotal,comment,,,,,,,,2016-03-19,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-198773998,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
"Actually, to phrase the question in another way: **should `cumcount` work on the index or on the values?** The example implementation of @johne13 works on the values (ignoring NaNs there), while the example implementation of @jreback works on the index.",,,,,,Anecdotal,comment,,,,,,,,2016-03-19,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-198774279,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
"Sorry, some of these details are over my head -- have never really delved into pandas internals before. I tried to make it work as similarly to cum\* as possible, that's why it is done via `_make_cum_function` like the other cum functions rather than a standalone. Signature is comparable to cumsum. ``` In [28]: pd.DataFrame.cumcount? Signature: pd.DataFrame.cumcount(self, axis=None, dtype=None, out=None, skipna=True, **kwargs) Docstring: Return cumulative count over requested axis. Parameters -…",,,,,,Anecdotal,comment,,,,,,,,2016-03-19,github/johne13,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-198775987,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
"And also regarding `groupby.cumcount()`, it actually doesn't agree with `groupby.count()` in that its ending value is n-1 whereas count returns n.",,,,,,Anecdotal,comment,,,,,,,,2016-03-19,github/johne13,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-198776873,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
"BUG: If you add _metadata to a custom subclass of Series, the sequence name is lost when indexing ### Pandas version checks - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas. …",,,,,,Anecdotal,issue,,,,,,,,2025-05-24,github/vitalizzare,https://github.com/pandas-dev/pandas/issues/61491,repo: pandas-dev/pandas | keyword: workaround | state: open
"I've found that a `Series` object has `_metadata = ['_name']` by default. This means that when manually defining `_metadata` in a custom `Series` subclass, we need to explicitly add `'_name'` to it as well. I couldn't find this information in the documentation. Maybe it should be mentioned here: https://pandas.pydata.org/pandas-docs/stable/development/extending.html#define-original-properties. What do you think?",,,,,,Anecdotal,comment,,,,,,,,2025-05-25,github/vitalizzare,https://github.com/pandas-dev/pandas/issues/61491#issuecomment-2907681693,"repo: pandas-dev/pandas | issue: BUG: If you add _metadata to a custom subclass of Series, the sequence name is lost when indexing | keyword: workaround"
BUG: Series.str.isdigit with pyarrow dtype doesn't honor unicode superscripts ### Pandas version checks - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas. ### Reproducible Exa…,,,,,,Anecdotal,issue,,,,,,,,2025-05-20,github/GarrettWu,https://github.com/pandas-dev/pandas/issues/61466,repo: pandas-dev/pandas | keyword: workaround | state: open
"@rhshadrach The issue stems from `pyarrow.compute.utf8_is_digit` not recognizing non-ASCII Unicode digits (e.g., `'³'`). To align with `str.isdigit()`'s behavior and pandas docs, I propose replacing the Arrow compute call in `_str_isdigit()` with ``` def _str_isdigit(self): values = self.to_numpy(na_value=None) data = [] mask = [] for val in values: if val is None: data.append(False) mask.append(True) else: data.append(val.isdigit()) mask.append(False) from pandas.core.arrays.boolean import Boo…",,,,,,Anecdotal,comment,,,,,,,,2025-05-24,github/iabhi4,https://github.com/pandas-dev/pandas/issues/61466#issuecomment-2906583717,repo: pandas-dev/pandas | issue: BUG: Series.str.isdigit with pyarrow dtype doesn't honor unicode superscripts | keyword: workaround
"Looks like this is getting fixed upstream (thanks!). Assuming that to be the case, my preference would be to leave pandas as-is. cc @WillAyd @jorisvandenbossche for any thoughts.",,,,,,Anecdotal,comment,,,,,,,,2025-05-30,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/61466#issuecomment-2923130365,repo: pandas-dev/pandas | issue: BUG: Series.str.isdigit with pyarrow dtype doesn't honor unicode superscripts | keyword: workaround
"Thanks @iabhi4 for the upstream fix https://github.com/apache/arrow/issues/46589. It solves the superscripts issue, but introduces another discrepancy: ``` // '¾' (vulgar fraction) is treated as a digit by utf8proc 'No' ``` Any chance we can fix it too? Otherwise str.isdigit is still different on python string and pyarrow string types.",,,,,,Anecdotal,comment,,,,,,,,2025-06-04,github/GarrettWu,https://github.com/pandas-dev/pandas/issues/61466#issuecomment-2940842397,repo: pandas-dev/pandas | issue: BUG: Series.str.isdigit with pyarrow dtype doesn't honor unicode superscripts | keyword: workaround
"> Looks like this is getting fixed upstream (thanks!). Assuming that to be the case, my preference would be to leave pandas as-is. If this comes in the next pyarrow version, I think we could still add a fallback based on the version. I.e. use pyarrow for recent versions, otherwise still fallback to the python implemenation. Potentially, assuming the cases that behave differently are all in unicode, we could also first do a check if all elements are ascii, and if so always use the pyarrow versio…",,,,,,Anecdotal,comment,,,,,,,,2025-06-11,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/61466#issuecomment-2963166780,repo: pandas-dev/pandas | issue: BUG: Series.str.isdigit with pyarrow dtype doesn't honor unicode superscripts | keyword: workaround
"I am doing a PR for what I mentioned above (still fallback to python for pyarrow<21) at https://github.com/pandas-dev/pandas/pull/61962, but that also shows that this now introduces a new inconsistency .. The superscript is now seen as a digit, but the ⅕ as well, while that is not the case for python: ```python >>> '⅕'.isdigit() False ``` vs ``` >>> import pyarrow.compute as pc >>> pa.__version__ '21.0.0' >>> pc.utf8_is_digit(['⅕']) <pyarrow.lib.BooleanArray object at 0x7f28ca52da80> [ true ] `…",,,,,,Anecdotal,comment,,,,,,,,2025-07-26,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/61466#issuecomment-3121827923,repo: pandas-dev/pandas | issue: BUG: Series.str.isdigit with pyarrow dtype doesn't honor unicode superscripts | keyword: workaround
"What do we want to do here? Stay as close as possible to the Python behaviour? (in that case we could fast-check if all values are ascii, and in that case still using the faster pyarrow algorithm, but otherwise fall back to Python) Or accept that this is one of the differences between both engines and document it as such? (it's also not clear to me if one of both behaviours is ""more correct"" than the other ..)",,,,,,Anecdotal,comment,,,,,,,,2025-07-30,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/61466#issuecomment-3135510770,repo: pandas-dev/pandas | issue: BUG: Series.str.isdigit with pyarrow dtype doesn't honor unicode superscripts | keyword: workaround
"I think that there are endless possibilities for differences and ambiguities on the compute interpretation of unicode sequences, so I'd rather just document that we are using Arrow/utf8proc. I think the only real alternatives are playing ""whack-a-mole"" as people report issues, or always pushing everything to a PyObject and continuing to use Python's interpretation. That would definitely be the ""pure"" backwards-compat approach, but I'm not sure it is all that practical",,,,,,Anecdotal,comment,,,,,,,,2025-07-30,github/WillAyd,https://github.com/pandas-dev/pandas/issues/61466#issuecomment-3136302242,repo: pandas-dev/pandas | issue: BUG: Series.str.isdigit with pyarrow dtype doesn't honor unicode superscripts | keyword: workaround
"Yes, I think I agree for differences that are unicode corner cases like this one. I updated the PR to test the different behaviour, and then we should also document this as one of the known differences (in addition to the upper case of ß)",,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/61466#issuecomment-3185525979,repo: pandas-dev/pandas | issue: BUG: Series.str.isdigit with pyarrow dtype doesn't honor unicode superscripts | keyword: workaround
"ENH: add `atol` to pd.DataFrame.compare() ### Feature Type - [X] Adding new functionality to pandas - [X] Changing existing functionality in pandas - [ ] Removing existing functionality in pandas ### Problem Description When comparing pandas dataframes with floating point numbers, it can be extremely useful to compare with an absolute tolerance (`atol`) as we see in [pandas.testing.assert_frame_equal](https://pandas.pydata.org/docs/reference/api/pandas.testing.assert_frame_equal.html). ### Feat…",,,,,,Anecdotal,issue,,,,,,,,2023-08-21,github/JonahBreslow,https://github.com/pandas-dev/pandas/issues/54677,repo: pandas-dev/pandas | keyword: workaround | state: open
"@mroeschke Hi! I would love to work on this enhancement, would it be ok to start working on it even if it has not yet been reviewed? Also if someone could in the meanwhile review it I would appreciate. Thank you!",,,,,,Anecdotal,comment,,,,,,,,2024-04-25,github/tomhoq,https://github.com/pandas-dev/pandas/issues/54677#issuecomment-2077445903,repo: pandas-dev/pandas | issue: ENH: add `atol` to pd.DataFrame.compare() | keyword: workaround
I would say any issue that has not been triaged yet should not be worked on until a core team member has reviewed the issue,,,,,,Anecdotal,comment,,,,,,,,2024-04-25,github/mroeschke,https://github.com/pandas-dev/pandas/issues/54677#issuecomment-2077773157,repo: pandas-dev/pandas | issue: ENH: add `atol` to pd.DataFrame.compare() | keyword: workaround
Floating-point difference tolerance would be nice in `DataFrame.eq()` and `DataFrame.equals()` as well as `DataFrame.compare()`.,,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/jlefeaux,https://github.com/pandas-dev/pandas/issues/54677#issuecomment-3179691545,repo: pandas-dev/pandas | issue: ENH: add `atol` to pd.DataFrame.compare() | keyword: workaround
"ENH adding metadata argument to DataFrame.to_parquet #### Code Sample, a copy-pastable example if possible Please comsider merging https://github.com/pandas-dev/pandas/compare/master...JacekPliszka:master #### Problem description Currently pandas can not add custom metadata to parquet file. This patch add metadata argument to DataFrame.to_parquet that allows for that. Warning is issued when pandas key is present in the dictionary passed.",,,,,,Anecdotal,issue,,,,,,,,2018-03-28,github/JacekPliszka,https://github.com/pandas-dev/pandas/issues/20521,repo: pandas-dev/pandas | keyword: workaround | state: open
The user given dictionary updates current key value file metadata. If user gives pandas key then it overwrites pandas_metadata but warning.warn is issued. Purpose: User metadata is very needed when: 1. processing is done in several stages and you want to keep information about version/algorithm used on each stage so you can debug it later 2. processing is done with different parameters and you want to keep parameters used with the file 3. you need to add extra custom information e.g. sometimes …,,,,,,Anecdotal,comment,,,,,,,,2018-03-29,github/JacekPliszka,https://github.com/pandas-dev/pandas/issues/20521#issuecomment-377196611,repo: pandas-dev/pandas | issue: ENH  adding metadata argument to DataFrame.to_parquet | keyword: workaround
Note for readers: the PR was closed but mentions a work-around that can be used for now if you need this: https://github.com/pandas-dev/pandas/pull/20534#issuecomment-453236538,,,,,,Anecdotal,comment,,,,,,,,2019-05-02,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/20521#issuecomment-488584231,repo: pandas-dev/pandas | issue: ENH  adding metadata argument to DataFrame.to_parquet | keyword: workaround
"I have been thinking about this and am wondering what the general thoughts are to use [DataFrame.attrs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.attrs.html#pandas-dataframe-attrs) and [Series.attrs](https://pandas.pydata.org/pandas-docs/stable//reference/api/pandas.Series.attrs.html#pandas-series-attrs) for reading and writing metadata to/from parquet? For example, here is how the metadata would be written: ```python pdf = pandas.DataFrame({""a"": [1]}) pdf.attrs = {""name"": ""…",,,,,,Anecdotal,comment,,,,,,,,2021-05-18,github/snowman2,https://github.com/pandas-dev/pandas/issues/20521#issuecomment-843199489,repo: pandas-dev/pandas | issue: ENH  adding metadata argument to DataFrame.to_parquet | keyword: workaround
"Here is a hack to get the attrs to work with pyarrow: ```python def _write_attrs(table, pdf): schema_metadata = table.schema.metadata or {} pandas_metadata = json.loads(schema_metadata.get(b""pandas"", ""{}"")) column_attrs = {} for col in pdf.columns: attrs = pdf[col].attrs if not attrs or not isinstance(col, str): continue column_attrs[col] = attrs pandas_metadata.update( attrs=pdf.attrs, column_attrs=column_attrs, ) schema_metadata[b""pandas""] = json.dumps(pandas_metadata) return table.replace_sc…",,,,,,Anecdotal,comment,,,,,,,,2021-05-18,github/snowman2,https://github.com/pandas-dev/pandas/issues/20521#issuecomment-843274429,repo: pandas-dev/pandas | issue: ENH  adding metadata argument to DataFrame.to_parquet | keyword: workaround
"> Is this something that would need to be done in pandas or pyarrow/fastparquet? Ideally, I think this would actually be done in pyarrow/fastparquet, as it is in those libraries that the ""pandas"" metadata item gets constructed currently",,,,,,Anecdotal,comment,,,,,,,,2021-06-06,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/20521#issuecomment-855356186,repo: pandas-dev/pandas | issue: ENH  adding metadata argument to DataFrame.to_parquet | keyword: workaround
so... can we have simple something to work with df.attrs ? The goal is to replace multiple pseudo-csv formats which add #-prefixed comments in the beginning of a file with something systematic. I believe everyone would agree that's 1) a common usecase 2) supportable by parquet 3) should work without hassle for reader (I'm ok with hassle for writer),,,,,,Anecdotal,comment,,,,,,,,2022-03-09,github/arogozhnikov,https://github.com/pandas-dev/pandas/issues/20521#issuecomment-1063164447,repo: pandas-dev/pandas | issue: ENH  adding metadata argument to DataFrame.to_parquet | keyword: workaround
"> I believe everyone would agree that's 1) a common usecase 2) supportable by parquet 3) should work without hassle for reader (I'm ok with hassle for writer) Yes, and a contribution to add this functionality is welcome, I think. https://github.com/pandas-dev/pandas/pull/41545 tried to do this but was only closed because it also wanted to store column-level attrs (which was the main driver for the PR author), not because we don't want this in general. A PR focusing on storing/restoring DataFram…",,,,,,Anecdotal,comment,,,,,,,,2022-03-23,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/20521#issuecomment-1076239880,repo: pandas-dev/pandas | issue: ENH  adding metadata argument to DataFrame.to_parquet | keyword: workaround
"**Edit** don't need this ⬇️ since `2.1.0` ⚠️ ~My workaround (assuming `fastparquet`)~: ```py # write df.to_parquet(path) meta = {'foo':'bar'} fastparquet.update_file_custom_metadata(path, meta) # read pf = fastparquet.ParquetFile(path) df_ = pf.to_pandas() meta_ = pf.key_value_metadata ``` Note `meta` **must** be `dict[str, str]` (so no nested dicts without bring-your-own serialization).",,,,,,Anecdotal,comment,,,,,,,,2023-05-17,github/davetapley,https://github.com/pandas-dev/pandas/issues/20521#issuecomment-1551991545,repo: pandas-dev/pandas | issue: ENH  adding metadata argument to DataFrame.to_parquet | keyword: workaround
BUG: read_parquet converts pyarrow list type to numpy dtype ### Pandas version checks - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas. ### Reproducible Example ```python imp…,,,,,,Anecdotal,issue,,,,,,,,2023-04-30,github/danielhanchen,https://github.com/pandas-dev/pandas/issues/53011,repo: pandas-dev/pandas | keyword: workaround | state: open
"I found during the Pyarrow conversion, if you pass in a `types_mapper` and setting `ignore_metadata` to `False`, it works! ``` mapping = {schema.type : pd.ArrowDtype(schema.type) for schema in data.schema} data.to_pandas(types_mapper = mapping.get, ignore_metadata = True) ```",,,,,,Anecdotal,comment,,,,,,,,2023-05-01,github/danielhanchen,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1529733038,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"From the traceback, it appears that pyarrow tries to convert this type to a numpy dtype by default, so I think an appropriate fix would be for pyarrow to just return an `ArrowDtype` here ```python File /opt/miniconda3/envs/pandas-dev/lib/python3.10/site-packages/pyarrow/pandas_compat.py:812, in table_to_blockmanager(options, table, categories, ignore_metadata, types_mapper) 809 table = _add_any_metadata(table, pandas_metadata) 810 table, index = _reconstruct_index(table, index_descriptors, 811 …",,,,,,Anecdotal,comment,,,,,,,,2023-05-01,github/mroeschke,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1529976106,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"Hmm so I looked at the Pandas code, and not sure if using `pd.ArrowDtype(dtype)` will work. The issue is `data.schema.pandas_metadata['columns'][7][""numpy_type""]` is a `str` and not an actual `type` object, and pd.ArrowDtype does not accept strings. eg: ``` dt = A.schema.pandas_metadata['columns'][7][""numpy_type""] ``` returns: ``` 'list<element: struct<rank: uint8, subtype: dictionary<values=string, indices=int32, ordered=0>, caption: string, credit: string, type: dictionary<values=string, indi…",,,,,,Anecdotal,comment,,,,,,,,2023-05-02,github/danielhanchen,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1531949440,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"@phofl Oh oops I forgot to mention I tried `pd.read_parquet(..., dtype_backend = ""pyarrow"")`, and the `TypeError` still exists. The error is exactly the same, since it passes the dtype to `np.dtype`",,,,,,Anecdotal,comment,,,,,,,,2023-05-03,github/danielhanchen,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1532423393,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"Confirmed it still fails: ``` import pandas as pd import pyarrow as pa pyarrow_list_of_strings = pd.ArrowDtype(pa.list_(pa.string())) data = pd.DataFrame({ ""Pyarrow"" : pd.Series([[""a""], [""a"", ""b""]], dtype = pyarrow_list_of_strings), }) data.to_parquet(""data.parquet"") # SUCCESS pd.read_parquet(""data.parquet"", dtype_backend = ""pyarrow"") # *** FAIL ```",,,,,,Anecdotal,comment,,,,,,,,2023-05-03,github/danielhanchen,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1532612176,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"Interesting, This one works: ``` data = pd.DataFrame({ ""Pyarrow"" : pd.Series([[""a""], [""a"", ""b""]]), }) data.to_parquet(""data.parquet"") pd.read_parquet(""data.parquet"", dtype_backend = ""pyarrow"") ```",,,,,,Anecdotal,comment,,,,,,,,2023-05-03,github/phofl,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1532615035,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"Ye that works since it's an `object` - Pyarrow indeed saves the data inside the parquet file as `list[string]`. The issue is if you explicity parse `list[string]` directly, it does not work. Ie: ``` data = pd.DataFrame({ ""Pyarrow"" : pd.Series([[""a""], [""a"", ""b""]]), }) data.dtypes ``` returns ``` Pyarrow object dtype: object ```",,,,,,Anecdotal,comment,,,,,,,,2023-05-03,github/danielhanchen,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1532684763,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"In fact the `object` schema is converted: ``` pa.parquet.read_table(""data.parquet"") ``` returns ``` pyarrow.Table Pyarrow: list<item: string> child 0, item: string ---- Pyarrow: [[[""a""],[""a"",""b""]]] ```",,,,,,Anecdotal,comment,,,,,,,,2023-05-03,github/danielhanchen,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1532686839,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"Maybe a `try` `except` so to not break other parts of the Pandas repo? https://github.com/apache/arrow/blob/a77aab07b02b7d0dd6bd9c9a11c4af067d26b674/python/pyarrow/pandas_compat.py#L855 Maybe a `try` `except` so to not break other parts of the Pandas repo? ``` # infer the extension columns from the pandas metadata >>for col_meta, field in zip(columns_metadata, table.schema): try: name = col_meta['field_name'] except KeyError: name = col_meta['name'] dtype = col_meta['numpy_type'] if dtype not i…",,,,,,Anecdotal,comment,,,,,,,,2023-05-03,github/danielhanchen,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1532731403,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"Run into the same issue: ```py df = pd.DataFrame({'a': pd.Series([['a'], ['a', 'b']], dtype=pd.ArrowDtype(pa.list_(pa.string())))}) df.to_parquet('test.parquet') # SUCCESS pd.read_parquet('test.parquet') # *** FAIL df.to_parquet('test.parquet') # SUCCESS pq.read_table('test.parquet').to_pandas(ignore_metadata=True, types_mapper=pd.ArrowDtype) # SUCCESS df.to_parquet('test.parquet', store_schema=False) # SUCCESS pd.read_parquet('test.parquet') # SUCCESS ``` I think the last case was not mentione…",,,,,,Anecdotal,comment,,,,,,,,2023-05-10,github/takacsd,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1542822676,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"@danielhanchen I think the problem is in the pandas specific metadata. If the parquet file was created with something else (e.g.: AWS Athena) it could read it just fine. ```py pq.write_table(pa.table({'a': pa.array([['a'], ['a', 'b']], type=pa.list_(pa.string()))}), 'test.parquet') # SUCCESS pd.read_parquet('test.parquet') # SUCCESS pq.write_table(pa.Table.from_pandas(df), 'test.parquet') # SUCCESS pd.read_parquet('test.parquet') # *** FAIL pq.write_table(pa.Table.from_pandas(df).replace_schema…",,,,,,Anecdotal,comment,,,,,,,,2023-05-11,github/takacsd,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1543868578,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"@takacsd oh yep your reasoning sounds right - so I think adding a simple try except might be a simple maybe? Try calling numpy then if it fails, call pd.ArrowDtype",,,,,,Anecdotal,comment,,,,,,,,2023-05-12,github/danielhanchen,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1545779927,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"The main issue I think is because`dtype` is a string I guess. I'm not 100% sure about how `_pandas_api.pandas_dtype` works, but presumably it's a large `dict` mapping types in string form to the correct type. Due to the infinite nature of possible Arrow datatypes, I guess its not feasible to update the dictionary, so maybe the try except solution is the only reasonable solution? Just my two cents.",,,,,,Anecdotal,comment,,,,,,,,2023-05-12,github/danielhanchen,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1545785525,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"> The main issue I think is because`dtype` is a string I guess. I'm not 100% sure about how `_pandas_api.pandas_dtype` works, but presumably it's a large `dict` mapping types in string form to the correct type. It seems a little more complicated than that: `pandas_dtype` [looks up a registry](https://github.com/pandas-dev/pandas/blob/b033ca94e7ae6e1320c9d65a8163bd0a6049f40a/pandas/core/dtypes/common.py#L1614). This [iterates trough](https://github.com/pandas-dev/pandas/blob/b033ca94e7ae6e1320c9…",,,,,,Anecdotal,comment,,,,,,,,2023-05-13,github/takacsd,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1546600583,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"@takacsd The issue though timestamps can be reasonably easy to construct from text. The below could all be possible though: ``` list[list[struct[int, float]]] list[int] struct[list[datetime]] ``` Constructing Arrow dtypes from that could be potentially problematic. I guess in theory one can iterate through the string, and create a string which you can then call `eval` on ie: `list[struct[int32, string]]` is `pa.list_(pa.struct((pa.int32(), pa.string()))` then you can eval on it. I think a wiser…",,,,,,Anecdotal,comment,,,,,,,,2023-05-13,github/danielhanchen,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1546608810,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"@danielhanchen your approach only works here, and it just ignores the metadata. I'm not a pandas developer but I suppose they generated that metadata for a reason, so it may break some things if we just ignore it. Properly parsing the string is obviously harder, but I still think it is the better solution...",,,,,,Anecdotal,comment,,,,,,,,2023-05-13,github/takacsd,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1546638599,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"@takacsd agreed parsing the metadata string is the correct way. I thought about how one would go about doing it. Eg take: `list<element: struct<rank: uint8, subtype: dictionary<values=string, indices=int32, ordered=0>, caption: string, credit: string, type: dictionary<values=string, indices=int32, ordered=0>, url: string, height: uint16, width: uint16, subType: dictionary<values=string, indices=int32, ordered=0>, crop_name: dictionary<values=string, indices=int32, ordered=0>>>[pyarrow]` You'll …",,,,,,Anecdotal,comment,,,,,,,,2023-05-13,github/danielhanchen,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1546661429,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"Actually a simpler solution is to directly all `.replace` on the string and replace `list<element:` to `pa.list_((` etc. However, this doesnt work with `struct` data-types, since struct also keeps note of each field name. This means a struct field name could have `dictionary` as it's name, which means using `eval` will fail. This probably means string parsing won't work for `struct`s, but works for everything else. I still believe a `try except` is the simplest solution. Obviously now Python 3.…",,,,,,Anecdotal,comment,,,,,,,,2023-05-13,github/danielhanchen,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1546670382,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"Yeah, after some experimenting, I think we need to gave up on parsing the type string: These two: ```py pd.Series([{'a': 1, 'b': 1}], dtype=pd.ArrowDtype(pa.struct({'a': pa.int64(), 'b': pa.int64()}))) pd.Series([{'a: int64, b': 1}], dtype=pd.ArrowDtype(pa.struct({'a: int64, b': pa.int64()}))) ``` both have the following type string: `struct<a: int64, b: int64>[pyarrow]`. But even if we disallow such cases, it is just too hard: I tried to write a recursive parser with some regexp, but I gave up…",,,,,,Anecdotal,comment,,,,,,,,2023-05-15,github/takacsd,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1548199622,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"I was bored: ```py class ParseFail(Exception): pass class Parsed(NamedTuple): type: pa.DataType end: int class TypeStringParser: BASIC_TYPE_MATCHER = re.compile(r'\w+(\[[^\]]+\])?') TIMESTAMP_MATCHER = re.compile(r'timestamp\[([^,]+), tz=([^\]]+)\]') NAME_MATCHER = re.compile(r'\w+') # this can be r'[^:]' to support weird names in struct def __init__(self, type_str: str) -> None: self.type_str = type_str def parse(self) -> pa.DataType: try: parsed = self.type(0) except ParseFail: raise ValueErr…",,,,,,Anecdotal,comment,,,,,,,,2023-05-15,github/takacsd,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1548581760,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"@takacsd Nice work on the parser! :) Ye `struct` is the biggest issue with it being able to have column names. It gets worse if `struct<struct : uint8>` exists - yikes that'll be a painful pain. Also I just noticed but https://github.com/apache/arrow/blob/8be70c137289adba92871555ce74055719172f56/python/pyarrow/pandas_compat.py#L870 actually does in fact parse Arrow Dtypes! The issue is the code previous to it breaks, and it never gets there. ``` for field in table.schema: typ = field.type if is…",,,,,,Anecdotal,comment,,,,,,,,2023-05-16,github/danielhanchen,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1548942282,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"I just hit this today trying to read a parquet file made by someone else, where they had used the pyarrow backend. Here is another minimal example to add to the mix that fails on reading `df2`. ```python import io import numpy as np import pandas as pd import pyarrow as pa def main(): df0 = pd.DataFrame( [ {""foo"": {""bar"": True, ""baz"": np.float32(1)}}, {""foo"": {""bar"": True, ""baz"": None}}, ], ) schema = pa.schema( [ pa.field( ""foo"", pa.struct( [ pa.field(""bar"", pa.bool_(), nullable=False), pa.fie…",,,,,,Anecdotal,comment,,,,,,,,2023-10-12,github/bretttully,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1758894825,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
I'm running into this issue as well: ![Screenshot 2024-02-07 at 19 50 26](https://github.com/pandas-dev/pandas/assets/6324489/2c202ee4-0595-46de-935a-53b5b905679a),,,,,,Anecdotal,comment,,,,,,,,2024-02-08,github/giftculture,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1933241453,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
INSTALLED VERSIONS ------------------ commit : f538741432edf55c6b9fb5d0d496d2dd1d7c2457 python : 3.11.7.final.0 python-bits : 64 OS : Linux OS-release : 4.13.9-300.fc27.x86_64 Version : #1 SMP Mon Oct 23 13:41:58 UTC 2017 machine : x86_64 processor : x86_64 byteorder : little LC_ALL : None LANG : en_US.UTF-8 LOCALE : en_US.UTF-8 pandas : 2.2.0 numpy : 1.26.3 pytz : 2023.4 dateutil : 2.8.2 setuptools : 69.0.3 pip : 23.3.2 Cython : None pytest : None hypothesis : None sphinx : None blosc : None f…,,,,,,Anecdotal,comment,,,,,,,,2024-02-08,github/giftculture,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1933243310,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"The only ""workaround"" at the pandas-level I've found is to set `df.to_parquet(..., store_schema=False)` for a `df` containing complex/nested types like a categorical. Are there any plans to get successful dtype roundtripping going forward? Is there anything in the pyarrow library we can leverage here? Versions: <details> ``` INSTALLED VERSIONS ------------------ commit : bdc79c146c2e32f2cab629be240f01658cfb6cc2 python : 3.11.6.final.0 python-bits : 64 OS : Windows OS-release : 10 Version : 10.0…",,,,,,Anecdotal,comment,,,,,,,,2024-04-04,github/jborman-stonex,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-2037057509,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"> I would recommend using `dtype_backend=""pyarrow""` @phoff, not sure if you saw from my screenshot, but I did apply the dypte_backend=""pyarrow"" to the read_parquet method and it still fails, unless I am misunderstanding your suggestion",,,,,,Anecdotal,comment,,,,,,,,2024-04-04,github/giftculture,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-2037217236,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"The only workaround I have found so far is the following (which works in all cases I have thought of, except round-tripping an empty dataframe with a struct or list type, setting the schema, and _not_ using `dtype_backend=""pyarrow""` when reading back in). Would def welcome suggested improvements to this workaround! Obv you can write this differently if you don't want a byte string returned, but for us that's what we want. ```python def serialize(self, data: pd.DataFrame, **kwargs) -> bytes: """"""…",,,,,,Anecdotal,comment,,,,,,,,2024-04-04,github/bretttully,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-2038370398,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"Missing Values and Categoricals - inconsistent dtypes #### Code Sample, a copy-pastable example if possible ```python In [1]: import pandas as pd In [2]: import numpy as np In [3]: from pandas.api.types import CategoricalDtype In [4]: s1 = pd.Series([np.nan, np.nan]).astype('category') In [5]: s1 Out[5]: 0 NaN 1 NaN dtype: category Categories (0, float64): [] In [6]: s2 = pd.Series([np.nan, np.nan]).astype(CategoricalDtype([])) In [7]: s2 Out[7]: 0 NaN 1 NaN dtype: category Categories (0, objec…",,,,,,Anecdotal,issue,,,,,,,,2018-10-19,github/Dr-Irv,https://github.com/pandas-dev/pandas/issues/23242,repo: pandas-dev/pandas | keyword: workaround | state: open
"> You can't specify the dtype of the underlying categories in the CategoricalDtype constructor You can with ```python In [24]: pd.api.types.CategoricalDtype(categories=pd.Index([], dtype=int)).categories Out[24]: Int64Index([], dtype='int64') ``` CategoricalDtype.categories is just an index. Would you want to accept a `dtype` parameter in `CategoricalDtype` that's passed through? > are both category dtype, and each Series was constructed using astype('category') I think that's the root issue. `…",,,,,,Anecdotal,comment,,,,,,,,2018-10-22,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/23242#issuecomment-431985473,repo: pandas-dev/pandas | issue: Missing Values and Categoricals - inconsistent dtypes | keyword: workaround
"> Would you want to accept a dtype parameter in CategoricalDtype that's passed through? Yes, I think that would help. The other thing that would help is if `union_categoricals` would accept the union of two categories where the `dtype` was different, and one of the categories was empty. Then the result could have the `dtype` of the category that had items in it. The reason I need this is that I'm reading a large file in chunks, and I know which columns are category columns, and want to keep doi…",,,,,,Anecdotal,comment,,,,,,,,2018-10-22,github/Dr-Irv,https://github.com/pandas-dev/pandas/issues/23242#issuecomment-432000675,repo: pandas-dev/pandas | issue: Missing Values and Categoricals - inconsistent dtypes | keyword: workaround
"I'd prefer to avoid special casing empty / all-NaN columns. I think adding a `dtype` keyword to the CategoricalDtype constructor would be fine, with a default of float for backwards compatibility. On Mon, Oct 22, 2018 at 4:45 PM Dr. Irv <notifications@github.com> wrote: > Would you want to accept a dtype parameter in CategoricalDtype that's > passed through? > > Yes, I think that would help. > > The other thing that would help is if union_categoricals would accept the > union of two categories …",,,,,,Anecdotal,comment,,,,,,,,2018-10-23,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/23242#issuecomment-432280616,repo: pandas-dev/pandas | issue: Missing Values and Categoricals - inconsistent dtypes | keyword: workaround
"> I think adding a `dtype` keyword to the CategoricalDtype constructor would > be fine, with a default of float for backwards compatibility. I think the default would have to be `infer`, since if you pass no NaNs, then the dtype is inferred from the type of the passed categories. Then if all the values are NaN, it defaults to float.",,,,,,Anecdotal,comment,,,,,,,,2018-10-23,github/Dr-Irv,https://github.com/pandas-dev/pandas/issues/23242#issuecomment-432306360,repo: pandas-dev/pandas | issue: Missing Values and Categoricals - inconsistent dtypes | keyword: workaround
"BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit ### Pandas version checks - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas. ### …",,,,,,Anecdotal,issue,,,,,,,,2023-09-27,github/scinteeb,https://github.com/pandas-dev/pandas/issues/55305,repo: pandas-dev/pandas | keyword: workaround | state: open
"Hi scinteeb, I am facing exact same issue with Embedded Linux system build by Yocto 4,0 (kirkstone) running on armv7 32bit target. I have setup test environment on RaspberryPi 4 32bit. My test is the spearman correlation test see also: - [closed BUG: Spearman correlation is broken... on 32-bit platforms](https://github.com/pandas-dev/pandas/issues/43588) When installing 64bit it works fine. When using 32bit it fails. OS details: ``` root@raspberrypi4:~# uname -a Linux raspberrypi4 5.15.34-v7l #…",,,,,,Anecdotal,comment,,,,,,,,2023-10-07,github/mweitner,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1751682182,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"Hello Michael, My target device is an embedded unit with scarce storage resources. That's why I cannot do much testing on it. What I did was to create a virtual ARMv7 32bit system using qemu and in there I did the following experiment: 1. install python3.10 2. install build-essential tools 3. using pip3 install numpy v1.22.3 and pandas v1.4.2 4. run the testing What I found was that when the numpy and pandas were installed using pip3 (practically compiled on the target) everything worked fine. …",,,,,,Anecdotal,comment,,,,,,,,2023-10-09,github/scinteeb,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1752629835,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"hello Bogdan, I have the same problem as you, and I would like to consult with you. Has your problem been resolved? How was it ultimately resolved? Thank you very much for seeing my message and providing assistance. Thank you. happy everyday for you... My records are as follows： 1.errors： Traceback (most recent call last): File ""pyabcs"", line 952, in _thread_loop if func(loopcnt=cnt, looptime=t - t0, loopintv=t - tlst) if fargs else func(): File ""mdc_equip"", line 140, in loop_mdcboard self.mdc_…",,,,,,Anecdotal,comment,,,,,,,,2023-10-09,github/avan051,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1753274650,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"Hi Bogdan, just did the verification on Raspberry 4 (32bit) official Debian Bullseye OS (32bit): ``` pi@sheep:~$ getconf LONG_BIT 32 pi@sheep:~$ file /lib/systemd/systemd /lib/systemd/systemd: ELF 32-bit LSB pie executable, ARM, EABI5 version 1 (SYSV), \ dynamically linked, interpreter /lib/ld-linux-armhf.so.3, \ BuildID[sha1]=c8e472c9a12568fbde5035980497ffc8e4a857cd, for GNU/Linux 3.2.0, \ stripped ``` The test was successful: ``` >>> d = pd.DataFrame([1.0, 2.0]) >>> d.corr(method='spearman') …",,,,,,Anecdotal,comment,,,,,,,,2023-10-09,github/mweitner,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1753296814,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"@mweitner ok，thanks for your reply！ BTW, the same configuration is okay for me to run on an ubuntu for x86 platform。 but ,cross compiler to arm,is not okay.",,,,,,Anecdotal,comment,,,,,,,,2023-10-09,github/avan051,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1753308057,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"Hello Michael, There are two more tests that we can do to narrow down further the issue. I am planning to run them tomorrow or a day after tomorrow after I'll build a qemu image. If you have time the tests are quite simple: 1. install on the rasp device the Yocto rpm for numpy and install the pandas with pip3 and run the test 2. install on the rasp device the Yocto rpm for pandas and the numpy with pip3 and run the test Before each test we need to uninstall the numpy and pandas either by pip3 o…",,,,,,Anecdotal,comment,,,,,,,,2023-10-09,github/scinteeb,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1753481511,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"Hi Bogdan, it seems Yocto build of pandas is the bad one as tested Test 1. successfully: - numpy==1.22.3 installed by yocto pandas==1.4.2 installed by pip3 on target ``` root@raspberrypi4:~# pip3 install pandas==1.4.2 --no-use-pep517 -vvv ... Running setup.py install for pandas ... done Successfully installed pandas-1.4.2 python-dateutil-2.8.2 pytz-2023.3.post1 six-1.16.0 ``` Verification: ``` >>> import numpy as np >>> np.__version__ '1.22.3' >>> import pandas as pd >>> pd.__version__ '1.4.2' …",,,,,,Anecdotal,comment,,,,,,,,2023-10-10,github/mweitner,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1755525193,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
I will dive further into how Yocto build compiles python3-pandas package and what is the difference between on-target pip build and Yocto cross compile build? So far I identified Options missing on Yocto build: ``` -pipe -feliminate-unused-debug-types \ -DHAVE_BROKEN_POSIX_SEMAPHORES \ -feliminate-unused-debug-types \ -fPIC -DNPY_NO_DEPRECATED_API=0 ``` However adding those to TOOLCHAIN_OPTIONS in order to have them at cross compiler CC did not help. Is there anyone who knows pip build process …,,,,,,Anecdotal,comment,,,,,,,,2023-10-11,github/mweitner,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1757272319,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"While I am building the qemu image, I kept looking at the source code. I believe that the the issue is coming from the mismatch definition of the intp_t type in pandas, numpy AND cython, definitely caused by some compiling parameter in Yocto. I guess that in one (or more) recipe the architecture is not properly detected using 64-bit platform instead of 32-bit. Also, checking out the WHEEL in the dist-info folder for the packages (i.e./usr/lib/python3.10/site-packages/pandas-1.4.2.dist-info/WHEE…",,,,,,Anecdotal,comment,,,,,,,,2023-10-11,github/scinteeb,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1758189838,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"Hi Bogdan, just stumbled over issue labels ... it could make sense we tag this issue with following labels: - https://github.com/pandas-dev/pandas/labels/32bit - https://github.com/pandas-dev/pandas/labels/ARM (is dedicated to aarm64 but at least we have arm context) - https://github.com/pandas-dev/pandas/labels/Build (dedicated to build for several platforms) - unfortunately there is not yocto label or embedded target label This might help to get more visible with our issue Anyway I come back …",,,,,,Anecdotal,comment,,,,,,,,2023-10-12,github/mweitner,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1759186901,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
by the way I am a bit worried for the future supporting arm 32bit target using pandas or even Data science python3 eco system as there seems to be a lot going on decreasing project effort by removing 32bit support in future see open issue: - https://github.com/pandas-dev/pandas/issues/44453,,,,,,Anecdotal,comment,,,,,,,,2023-10-12,github/mweitner,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1759193562,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"Back to technical issue.. I might have found a track while forced rebuilding of pandas by pip I see there might be no wheel support for arm 32 bit ""anymore"" ... as it downloads tar with egg-info. If you follow Issue #44453 discussion it mentions that numpy project has dropped 32bit wheel support... Yocto build of our numpy version has wheel support but 64 bit tag: ``` root@raspberrypi4:~# cat /usr/lib/python3.10/site-packages/numpy-1.22.3.dist-info/WHEEL Wheel-Version: 1.0 Generator: bdist_whee…",,,,,,Anecdotal,comment,,,,,,,,2023-10-12,github/mweitner,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1759284666,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"If you agree with my analysis, I am going to dive into fixing yocto recipes and hopefully coming up with patches and/or upstream pull request... Can you support here as well? Anyway I keep you informed...",,,,,,Anecdotal,comment,,,,,,,,2023-10-12,github/mweitner,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1759314742,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"Yes, definitely the solution is to adapt the Yocto recipes to compile the right version of these packages. And yes, I am willing to support the effort.",,,,,,Anecdotal,comment,,,,,,,,2023-10-12,github/scinteeb,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1759565317,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"When cross-compiled for arm 32-bit under Yocto, the WHEELs that are claimed to be compiled for X86_64 contain in fact the libraries with the correct signature ""ELF 32-bit LSB shared object, ARM, EABI5 version 1 (SYSV), dynamically linked"". So there are two issues to be addressed: 1. correct the setup.py for creating the wheel with the proper name/tag 2. find what is causing the actual problem, the mismatch of intp_t type between modules",,,,,,Anecdotal,comment,,,,,,,,2023-10-13,github/scinteeb,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1761167963,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"The item #1 can easily be addressed if we create a bbappend file with the content: SETUPTOOLS_BUILD_ARGS += ""--plat-name ${MACHINE}"" This will create the wheel using the machine name instead of x86_64 (i.e. Cython-0.29.28-cp310-cp310-**qemuarm**.whl). Also the WHEEL tag is properly set. Sure that the ${MACHINE} parameter can be replaced with TUNE_ARCH if we want to have the CPU type in there. I think that this should be part of setuptools3.bbclass as it will be a good practice to have the targe…",,,,,,Anecdotal,comment,,,,,,,,2023-10-13,github/scinteeb,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1761390072,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"yes also just did same with SETUPTOOLS_BUILD_ARGS set to ``` SETUPTOOLS_BUILD_ARGS:append = "" --plat-name linux-armv7l"" ``` which works nicely .. So think we got the solution for #1 great thx for your help So you think patching setuptool3.bbclass could be right fix for it ... Next is #2 The pandas problem, the mismatch of intp_t type between modules which I will check next week on raspberrypi",,,,,,Anecdotal,comment,,,,,,,,2023-10-13,github/mweitner,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1762047032,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"From my perspective, having the setuptool3.bbclass patched is the right approach. However, I cannot say that I can see all use cases / implications of this so it could be that the maintainer will disagree. I'll try to get in contact with them. On #2, I still don't know if the issue is coming from numpy/pandas or from cython.",,,,,,Anecdotal,comment,,,,,,,,2023-10-16,github/scinteeb,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1764392775,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"Hi Bogdan, yes agree our problem is not completely solved with setuptools3..bbclass and fix on wheel tag... I think the content of wheel file is not right meaning compiled numpy and pandas especially its C extensions... Unfortunately I do not have a lot of time rest of the week... what I started with is building numpy and pandas in context of Yocto extSDK trying to find the right build options for numpy and pandas ... and verify on my raspi 32bit ...",,,,,,Anecdotal,comment,,,,,,,,2023-10-17,github/mweitner,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1766114305,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"In our team, this changes to recipes fixed issue - https://lore.kernel.org/all/a2876316-9a43-45f7-890a-ac1b8d840cf1@gmail.com/T/#m6db8c349850bab51b66f5cf1579e45185b814ff0",,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/qarmin,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-2710418897,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
Hi there 👋 I'm trying to work around this issue as well. The recipe changes mentioned by @qarmin [above ](https://github.com/pandas-dev/pandas/issues/55305#issuecomment-2710418897 )refer to a small patch and link to [a gist](https://gist.github.com/OldManYellsAtCloud/9e935907807b6ddcd0900bfb1455e343) which is now a 404 and the wayback machine didn't archive it. Does anyone know what that small patch did as it would help me in understanding what a workaround looks like.,,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/rb1,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-3144159687,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"@rb1 In case no one has the patch, here is a description from the top of my head: 1. Remove ""import numpy"" from line 18 in setup.py 2. Also in setup.py search for `numpy.get_include()` calls, and change them to `os.getenv('NUMPY_INCLUDE_DIR')` If I remember well, that was the whole content of the referenced gist.",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/OldManYellsAtCloud,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-3144210813,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"Add axis argument to DataFrame.corr #### Location of the documentation https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html #### Documentation problem How should I get the correlation among the rows? I can obviously do `df.T.corr()` but this strikes me as a workaround rather than a nice way to do things. I wrote a `df.corr(axis=1)` assuming that would work. Curious what people feel is idiomatic, and would recommend some mention in the docs.",,,,,,Anecdotal,issue,,,,,,,,2020-06-26,github/alexlenail,https://github.com/pandas-dev/pandas/issues/35002,repo: pandas-dev/pandas | keyword: workaround | state: open
You should try asking on StackOverflow where the Q&A format is better served to answer this. This board is reserved for issues and enhancement requests,,,,,,Anecdotal,comment,,,,,,,,2020-06-26,github/WillAyd,https://github.com/pandas-dev/pandas/issues/35002#issuecomment-650246457,repo: pandas-dev/pandas | issue: Add axis argument to DataFrame.corr | keyword: workaround
"@WillAyd (I should have mentioned this in my original post:) StackOverflow has an answer recommending df.T.corr(), but although that works I don't think it's idiomatic, and if there's a better way, I think it should be mentioned in the docs. If not, I'm recommending that the .corr() function be updated to take an `axis` keyword argument.",,,,,,Anecdotal,comment,,,,,,,,2020-06-26,github/alexlenail,https://github.com/pandas-dev/pandas/issues/35002#issuecomment-650249726,repo: pandas-dev/pandas | issue: Add axis argument to DataFrame.corr | keyword: workaround
Since the OP we've been moving away from axis keywords in cases where under the covers it is just a transpose-operate-transpose.,,,,,,Anecdotal,comment,,,,,,,,2025-08-02,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/35002#issuecomment-3146127425,repo: pandas-dev/pandas | issue: Add axis argument to DataFrame.corr | keyword: workaround
"PERF: `DataFrame.copy(deep=True)` returns a view on the original pyarrow buffer ### Pandas version checks - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this issue exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [x] I have confirmed this issue exists on the main branch of pandas. ### Reproducible Example Over in https://github.com/dask/dask/issues/12022#issuecomment-3104950072, I'm debugging a test failu…",,,,,,Anecdotal,issue,,,,,,,,2025-07-23,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/61930,repo: pandas-dev/pandas | keyword: workaround | state: open
"Using `pyarrow.compute.take` with an empty array seems to work: ```python out = x.iloc[:0].copy(deep=True) for k, v in out.items(): if isinstance(v.array, pd.arrays.ArrowExtensionArray): values = pyarrow.compute.take(pyarrow.array(v.array), pyarrow.array([], type=""int32"")) out[k] = v._constructor(pd.array(values, dtype=v.array.dtype), index=v.index, name=v.name) ``` So I'd probably be fine with leaving the behavior as is (not having to copy the data is nice, in many cases).",,,,,,Anecdotal,comment,,,,,,,,2025-07-23,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/61930#issuecomment-3109001065,repo: pandas-dev/pandas | issue: PERF: `DataFrame.copy(deep=True)` returns a view on the original pyarrow buffer | keyword: workaround
"I would currently lean towards maintaining the current behavior of `copy` being a shallow copy because the underlying array being immutable, but open to other thoughts cc @jbrockmendel @jorisvandenbossche Another solution, if you don't care about preserving the original chunking layout like in `take` method, is to just `combine_chunks()` of the underlying array in the `pd.arrays.ArrowExtensionArray` i.e. `values = pa.array(v.array).combine_chunks()`",,,,,,Anecdotal,comment,,,,,,,,2025-07-23,github/mroeschke,https://github.com/pandas-dev/pandas/issues/61930#issuecomment-3109392476,repo: pandas-dev/pandas | issue: PERF: `DataFrame.copy(deep=True)` returns a view on the original pyarrow buffer | keyword: workaround
Do we expect end-users to have issues with this or just libraries like dask (who we can trust to handle this on their own)?,,,,,,Anecdotal,comment,,,,,,,,2025-07-23,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/61930#issuecomment-3109420172,repo: pandas-dev/pandas | issue: PERF: `DataFrame.copy(deep=True)` returns a view on the original pyarrow buffer | keyword: workaround
"> Another solution, if you don't care about preserving the original chunking layout like in take method, is to just combine_chunks() of the underlying array Thanks. > Do we expect end-users to have issues with this I'm not sure, but my guess is that this that the vast majority of users are better off with the current behavior (deep copy not actually copying): The only place this is really (negatively) observable is when you have a copy of a slice of a DataFrame that outlives the original DataFr…",,,,,,Anecdotal,comment,,,,,,,,2025-07-23,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/61930#issuecomment-3109496940,repo: pandas-dev/pandas | issue: PERF: `DataFrame.copy(deep=True)` returns a view on the original pyarrow buffer | keyword: workaround
"I think we should maybe reconsider this. Certainly now we try to make much less copies within pandas (with CoW), I do think that there is value in an expected / gauranteed ""deep copy"" behaviour.",,,,,,Anecdotal,comment,,,,,,,,2025-07-23,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/61930#issuecomment-3109590632,repo: pandas-dev/pandas | issue: PERF: `DataFrame.copy(deep=True)` returns a view on the original pyarrow buffer | keyword: workaround
"Either works for me. The [workaround in dask](https://github.com/dask/dask/pull/12025/files#diff-d805dac0ba6fd4e90a18a453851ab73e8257c52b88b20b1ae1fd4c0aa46a0fe3R201-R205) isn't free, but it ends up being relatively cheap since we're declining with size-0 arrays. This is pretty subtle. I'm not sure how many people are using `.copy()` to break references vs. `.copy()` to get a mutable copy of the data (and if we're being honest, the majority of the `.copy()` calls in the wild are probably attemp…",,,,,,Anecdotal,comment,,,,,,,,2025-07-25,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/61930#issuecomment-3116039565,repo: pandas-dev/pandas | issue: PERF: `DataFrame.copy(deep=True)` returns a view on the original pyarrow buffer | keyword: workaround
"> Another use case that comes to mind for wanting an actual copy is to de-chunk a highly fragmented ChunkedArray. Indeed, we just hit that [here](https://github.com/dask/dask/pull/12025). > Another solution [...] values = pa.array(v.array).combine_chunks() This ended up being ``` values = pa.chunked_array([v.array]).combine_chunks() v._constructor( pd.array(values, dtype=v.array.dtype), index=v.index, name=v.name ) ``` i.e. using the `pa.chunked_array` function with a (length-1) list of arrays.",,,,,,Anecdotal,comment,,,,,,,,2025-07-25,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/61930#issuecomment-3119908192,repo: pandas-dev/pandas | issue: PERF: `DataFrame.copy(deep=True)` returns a view on the original pyarrow buffer | keyword: workaround
"DOC: SQL-style join conditions ### Pandas version checks - [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/) ### Location of the documentation https://pandas.pydata.org/docs/getting_started/comparison/comparison_with_sql.html ### Documentation problem Not described how to do more complicated SQL-like joins on conditions, such as ```sql SELECT * FROM A LEFT JOIN B ON A.key = B.key AND A.a >= B.b ``` ### Suggest…",,,,,,Anecdotal,issue,,,,,,,,2024-03-04,github/jxu,https://github.com/pandas-dev/pandas/issues/57722,repo: pandas-dev/pandas | keyword: workaround | state: open
"I'm not familiar with SQL mechanics but does it actually only merge on rows that `A.a >= B.b` evaluates true or does it match on `A.key = B.key` first and then filter it further with `A.a >= B.b` (meaning there's some intermediate result)? If it's the latter, then I suppose an example showing the merge followed by a query could be done but I believe that's inferable from the current examples. To my knowledge there is no way to do the former with pandas",,,,,,Anecdotal,comment,,,,,,,,2024-03-05,github/Delengowski,https://github.com/pandas-dev/pandas/issues/57722#issuecomment-1977810718,repo: pandas-dev/pandas | issue: DOC: SQL-style join conditions | keyword: workaround
"I'm not actually sure how to do this. My understanding of LEFT JOIN in SQL is logically 1. Cartesian Product 2. Filter rows based on condition 3. Give rows not matched from left table nulls So I think the pandas merge is join but only on keys... you could do what I wrote step by step, but I think the product step would be expensive in implementation",,,,,,Anecdotal,comment,,,,,,,,2024-03-05,github/jxu,https://github.com/pandas-dev/pandas/issues/57722#issuecomment-1977815915,repo: pandas-dev/pandas | issue: DOC: SQL-style join conditions | keyword: workaround
"That's the logical result, but the SQL engine can run it in a way that's more efficient. But if I write it as two steps in pandas, the whole cross product is calculated before any filtering.",,,,,,Anecdotal,comment,,,,,,,,2024-03-06,github/jxu,https://github.com/pandas-dev/pandas/issues/57722#issuecomment-1981368774,repo: pandas-dev/pandas | issue: DOC: SQL-style join conditions | keyword: workaround
"> I'm not familiar with SQL mechanics but does it actually only merge on rows that `A.a >= B.b` evaluates true or does it match on `A.key = B.key` first and then filter it further with `A.a >= B.b` (meaning there's some intermediate result)? > > If it's the latter, then I suppose an example showing the merge followed by a query could be done but I believe that's inferable from the current examples. > > To my knowledge there is no way to do the former with pandas The SQL join condition is applie…",,,,,,Anecdotal,comment,,,,,,,,2024-03-06,github/jxu,https://github.com/pandas-dev/pandas/issues/57722#issuecomment-1981374166,repo: pandas-dev/pandas | issue: DOC: SQL-style join conditions | keyword: workaround
"@jxu if you are running an equi join with a non equi join in pandas, then there is no Cartesian join. The join is executed first on the equi join, followed by the non equi join(which is a filter on the results of the equi join). SQL does the same but in a transparent way - an EXPLAIN plan will likely show an equi join followed by a non equi join filter",,,,,,Anecdotal,comment,,,,,,,,2024-03-07,github/samukweku,https://github.com/pandas-dev/pandas/issues/57722#issuecomment-1982997982,repo: pandas-dev/pandas | issue: DOC: SQL-style join conditions | keyword: workaround
"ENH: speed up wide DataFrame.line plots by using a single LineCollection <!-- Thank you for your contribution to pandas! Please fill in the checklist below. Mark completed items with an “x”. --> ### What does this PR change? * **Speeds up `DataFrame.plot(kind=""line"")` when the frame is “wide”.** * If the DataFrame has **> 200 columns**, a **numeric index** (e.g. `RangeIndex` or integer/float values), is **not** a time-series plot, has **no stacking** and **no error bars**, we now draw everythin…",,,,,,Anecdotal,issue,,,,,,,,2025-07-03,github/EvMossan,https://github.com/pandas-dev/pandas/pull/61764,repo: pandas-dev/pandas | keyword: workaround | state: open
BUG: doing df.to_parquet and then reading it with pd.read_parquet causes KeyError ### Pandas version checks - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas. ### Reproducible…,,,,,,Anecdotal,issue,,,,,,,,2025-06-26,github/elbg,https://github.com/pandas-dev/pandas/issues/61714,repo: pandas-dev/pandas | keyword: workaround | state: open
"replace ```py df.to_parquet(""temp.parquet"") ``` with ```py df.reset_index().to_parquet(""temp.parquet"") ``` it worked for me, maybe it works for you too.",,,,,,Anecdotal,comment,,,,,,,,2025-06-26,github/Jopestpe,https://github.com/pandas-dev/pandas/issues/61714#issuecomment-3009088212,repo: pandas-dev/pandas | issue: BUG: doing df.to_parquet and then reading it with pd.read_parquet causes KeyError | keyword: workaround
"Yup that's another workaround. However, I would rather have pandas handling the index, rather than resetting it when saving to parquet and setting it back again when loading the parquet",,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/elbg,https://github.com/pandas-dev/pandas/issues/61714#issuecomment-3018166444,repo: pandas-dev/pandas | issue: BUG: doing df.to_parquet and then reading it with pd.read_parquet causes KeyError | keyword: workaround
"BUG: Index[Float64].insert(1, False) casts False to 0 ### Pandas version checks - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas. ### Reproducible Example ```python idx = pd.…",,,,,,Anecdotal,issue,,,,,,,,2025-06-26,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/61709,repo: pandas-dev/pandas | keyword: workaround | state: open
"On initial inspection, this seems to be because dtype compatibility checks were bypassed when dealing with `ExtensionArray`. This coerces False -> 0.0 without warning. A workaround is to do something like ``` pd.Index([1., 2., 3], dtype=""object"").insert(1, False) ``` If we need a fix for this, we can add a dtype compatibility check before inserting into `ExtensionArray`. Should I open a PR?",,,,,,Anecdotal,comment,,,,,,,,2025-06-26,github/kernelism,https://github.com/pandas-dev/pandas/issues/61709#issuecomment-3007161580,"repo: pandas-dev/pandas | issue: BUG: Index[Float64].insert(1, False) casts False to 0 | keyword: workaround"
[READY] perf improvements for strftime This PR is a new clean version of #46116 - [x] closes #44764 - [x] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature - [x] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit). - [x] Added [type annotations](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.…,,,,,,Anecdotal,issue,,,,,,,,2023-02-10,github/smarie,https://github.com/pandas-dev/pandas/pull/51298,repo: pandas-dev/pandas | keyword: workaround | state: open
"This PR is still pretty big. Any reason why you are introducing a new `fast_strftime` keyword instead of just trying to improve performance inplace? I think that would help to reduce the size, though still probably need to break up in smaller subsets. The bigger a PR is, the harder it is to review so ends up in a long review cycle",,,,,,Anecdotal,comment,,,,,,,,2023-02-10,github/WillAyd,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1426419179,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"Thanks @WillAyd for the comment ! I filled the what's new doc to help with the review process. I will also try to move part of the mods in a separate PR, I'll tell you when this is done",,,,,,Anecdotal,comment,,,,,,,,2023-02-17,github/smarie,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1434286980,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
This pull request is stale because it has been open for thirty days with no activity. Please [update](https://pandas.pydata.org/pandas-docs/stable/development/contributing.html#updating-your-pull-request) and respond to this comment if you're still interested in working on this.,,,,,,Anecdotal,comment,,,,,,,,2023-03-25,github/github-actions[bot],https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1483612952,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"> This pull request is stale because it has been open for thirty days with no activity. Please [update](https://pandas.pydata.org/pandas-docs/stable/development/contributing.html#updating-your-pull-request) and respond to this comment if you're still interested in working on this. Please leave this PR open, we are waiting for https://github.com/pandas-dev/pandas/pull/51459 to be merged to reduce the volume of changes, but otherwise this is ready",,,,,,Anecdotal,comment,,,,,,,,2023-03-26,github/smarie,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1484209969,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"This is now ready for review @MarcoGorelli or @mroeschke or @WillAyd . The PR is still pretty big, but it is a single feature. I moved everything I could to other PRs (#51459, #53003, #46405, #47570, #46759, #46361) since original PR #46116, ... 15 months ago :) The only items I can think of removing to make this (marginally) smaller would be : - these two test files : pandas/tests/arrays/categorical/test_repr.py and pandas/tests/io/formats/test_to_csv.py . Indeed the tests do not contain the f…",,,,,,Anecdotal,comment,,,,,,,,2023-05-14,github/smarie,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1546900733,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"thanks for updating I'll take a look when I get a chance - perf improvements generally aren't highest-priority (compared with regressions / bugs / API changes), but this would be nice to have, the improvement is quite impressive. Just saying this to manage expectations, it may take some time to get this in Generally speaking, do you know if the `strftime` performance is ever a bottleneck for anyone? It would be good improve it, but if I'm reading the results correctly, it doesn't look critical?…",,,,,,Anecdotal,comment,,,,,,,,2023-05-15,github/MarcoGorelli,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1547594631,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"> Generally speaking, do you know if the `strftime` performance is ever a bottleneck for anyone? It would be good improve it, but if I'm reading the results correctly, it doesn't look critical? Well, it is actually quite critical for industrial applications. Actually, the very reason why I opened this thread of PRs more than one year ago was that our cloud applications in production rely on pydantic + pandas to expose dataframes as payloads in web services, and we identified that our main bottl…",,,,,,Anecdotal,comment,,,,,,,,2023-05-15,github/smarie,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1548206348,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"> Is there any case when you can envisage this failing? No, I think that I found most edge cases already. The mechanism is based on two items: - the `convert_strftime_format` template converter, that moslty contains string ""is in"" and ""replace"" statements. https://github.com/pandas-dev/pandas/pull/51298/files#diff-fd90258af6c8946c90f0af064c63f1a5941102ba948e798c2c5b248308197219R133 - the python %-formatting mechanisms with keywords https://github.com/pandas-dev/pandas/pull/51298/files#diff-8102…",,,,,,Anecdotal,comment,,,,,,,,2023-05-15,github/smarie,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1548449784,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"Hello @MarcoGorelli , did you have a chance to start reviewing this ? Basically there is not much here : - new file pandas/_libs/tslibs/strftime.py providing `convert_strftime_format` and associated exception. - Timestamp arrays (`DatetimeArray`, `DatetimeLikeArrayMixin`): `format_array_from_datetime` has a new `fast_strftime: bool = True` argument, it is propagated to `strftime`. - Period arrays: `period_array_strftime` has a new `fast_strftime: bool = True` argument. It relies on `period_form…",,,,,,Anecdotal,comment,,,,,,,,2023-06-01,github/smarie,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1571743326,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"> If this is too much to review, maybe I can split this PR in three: > > * first only for timestamps (not periods). > > * then CSV formatting > > * then period and periodarrays > > Would that help you ? Gentle ping - @MarcoGorelli , just to get you opinion",,,,,,Anecdotal,comment,,,,,,,,2023-06-13,github/smarie,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1589333942,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"hey - sorry for the delay, I was away the past two weeks I have some priorities at the moment I'm afraid, and very limited maintenance time available - but yes, any effort to split it up would certainly help, thanks",,,,,,Anecdotal,comment,,,,,,,,2023-06-13,github/MarcoGorelli,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1589618463,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"Thanks @MarcoGorelli for taking the time to answer. Ok, I'll try to split this in 3 in the upcoming weeks. Also if other reviewers already familiar with the previous similar PRs can help, it can maybe be a solution to accelerate review time? (@mroeschke , @jreback or @jbrockmendel ) Thanks again, we'll see how this goes",,,,,,Anecdotal,comment,,,,,,,,2023-06-14,github/smarie,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1590969891,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"Just to disclose priorities - a couple of high-priority items I think really need addressing for the 2.x releases: - reviewing #52064 - addressing #34916 If you felt like helping to address either, then the sooner we can get those in, the sooner we can move forwards with this one Thanks for your patience and understanding generally, interactions with you have been very positive",,,,,,Anecdotal,comment,,,,,,,,2023-06-16,github/MarcoGorelli,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1594234919,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"Thanks @MarcoGorelli for the positive feedback ! If I can find some time one of these weeks I'll have a look, otherwise I'll wait until this is completed",,,,,,Anecdotal,comment,,,,,,,,2023-06-16,github/smarie,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1595212959,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"> ```python > idx.strftime('%-d') > ``` Thanks @MarcoGorelli ! Indeed this is not recognized as a valid pattern. I do not know this pattern, it is not indicated in [python docs](https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior) but still appears to be recognized by the C strftime. Still, I cannot find any explanation about it in the doc: https://man7.org/linux/man-pages/man3/strftime.3.html Is this a nominal behaviour or a side-effect ? If this is desirable (and I gues…",,,,,,Anecdotal,comment,,,,,,,,2023-06-30,github/smarie,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1615147038,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"ah maybe it's OK, thanks - I was looking at https://strftime.org/, but it says ""platform-specific"". still, would be good to preserve behaviour there if possible",,,,,,Anecdotal,comment,,,,,,,,2023-06-30,github/MarcoGorelli,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1615153593,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"> then I suppose that the safest fix is to replace [these lines](https://github.com/pandas-dev/pandas/pull/51298/files#diff-fd90258af6c8946c90f0af064c63f1a5941102ba948e798c2c5b248308197219R270-R271) by ""if there is a remaining %, raise an UnsupportedStrFmtDirective error"". This will trigger a fallback to the legacy strftime usage. sounds fine can we remove the `fast_strftime` option if we want to get this in for pandas 2.x please? if we're confident in it, let's just use it can we also add a hy…",,,,,,Anecdotal,comment,,,,,,,,2023-07-28,github/MarcoGorelli,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1655952627,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"Thanks for the pull request, but it appears to have gone stale. If interested in continuing, please merge in the main branch, address any review comments and/or failing tests, and we can reopen.",,,,,,Anecdotal,comment,,,,,,,,2023-09-18,github/mroeschke,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1724029632,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"Sorry @mroeschke and @MarcoGorelli I have had much trouble finding time to work on this lately, but I still aim at completing it in the upcoming weeks.",,,,,,Anecdotal,comment,,,,,,,,2023-09-25,github/smarie,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1733950832,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"@mroeschke @MarcoGorelli : please reopen, I have updated with the latest main branch. (https://github.com/pandas-dev/pandas/pull/55603 was a bit painful , note for myself. But I think I managed to find the ""new"" right destination for all tests) Once reopened, I will apply the two changes we discussed above (removing faststrftime arg everywhere and defaulting to legacy method in presence of an unresolved %) Thanks!",,,,,,Anecdotal,comment,,,,,,,,2023-11-11,github/smarie,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1806837450,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"@mroeschke @MarcoGorelli : now ready for review. All tests pass, the remaining CI failures are not relevant to this PR it seems, but to a warning in numpy related to encoding when scipy `sem` is used",,,,,,Anecdotal,comment,,,,,,,,2023-11-18,github/smarie,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1817639945,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"didn't manage to finish this, but I've put it in my tasks for next week quick comment - I wouldn't introduce `pd.convert_strftime_format`, I don't think it should be a top-level function. maybe in `pandas.tseries.api` (I think that's it)?",,,,,,Anecdotal,comment,,,,,,,,2024-01-12,github/MarcoGorelli,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1888945584,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"BUG: pd.Grouper cannot be reused in some cases - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of pandas. - [X] (optional) I have confirmed this bug exists on the master branch of pandas. --- #### Code Sample, a copy-pastable example ```python import numpy as np import pandas as pd np.random.seed(1) # Create one dataframe dr = pd.date_range(""2021-01-01"", ""2021-08-30"") ind = np.random.randint(0,5,len(dr)) something …",,,,,,Anecdotal,issue,,,,,,,,2021-06-10,github/mimakaev,https://github.com/pandas-dev/pandas/issues/41930,repo: pandas-dev/pandas | keyword: workaround | state: open
"Thanks for reporting this @mimakaev, I see this on current master as well. Agree that either of your alternatives would be much better - would be good to know if the lack of reusability is a bug or just implicit by design. Worth mentioning there has been discussion about deprecating this (#41297) in favor of alternatives mentioned in that issue (which may serve as a workaround for your use case).",,,,,,Anecdotal,comment,,,,,,,,2021-07-01,github/mzeitlin11,https://github.com/pandas-dev/pandas/issues/41930#issuecomment-872571665,repo: pandas-dev/pandas | issue: BUG: pd.Grouper cannot be reused in some cases  | keyword: workaround
"It turns out this bug was the root cause of a completely different issue, ""different shapes don't align"" within some internal codes when grouping by datetime. It seems to be caching these codes for performance reasons, not expecting to be reused in a later context. It's likely that this even silently caused improper data after grouping!",,,,,,Anecdotal,comment,,,,,,,,2021-12-08,github/NowanIlfideme,https://github.com/pandas-dev/pandas/issues/41930#issuecomment-989145323,repo: pandas-dev/pandas | issue: BUG: pd.Grouper cannot be reused in some cases  | keyword: workaround
"ENH: Inform on row & column of failed type conversion when parsing CSV files ### Feature Type - [ ] Adding new functionality to pandas - [X] Changing existing functionality in pandas - [ ] Removing existing functionality in pandas ### Problem Description I imagine this must have been raised before, but I can't find references. Apologies if it's a duplicate or a better fit for upstream projects. When parsing a CSV file, pandas may fail to convert data to (explicit or inferred) types, e.g., becau…",,,,,,Anecdotal,issue,,,,,,,,2023-07-01,github/DavidToneian,https://github.com/pandas-dev/pandas/issues/53966,repo: pandas-dev/pandas | keyword: workaround | state: open
"Not sure if this is possible. At least in the Python engine, at the point that conversion is done, the data is already read into a numpy array that is then processed. Mapping element indices back to the original line is challenging, especially if things such as skipped lines are involved. This is also likely to have performance (both speed+memory) implications I think. Usually when I get an error like this, having the failed value is usually enough since I can just Ctrl+F (or grep) for it. Does…",,,,,,Anecdotal,comment,,,,,,,,2023-07-11,github/lithomas1,https://github.com/pandas-dev/pandas/issues/53966#issuecomment-1631095749,repo: pandas-dev/pandas | issue: ENH: Inform on row & column of failed type conversion when parsing CSV files | keyword: workaround
"I understand the data has already been read in at the point the exception is raised; to give better error messages still, one would need to keep track of metadata, i.e., which chunk of data originates from where (similar to what the `python` engine seems to be doing.) Granted, this would have performance impacts as you say, though I would guess it wouldn't be significant. Regarding Ctrl+F or grepping: I have often encountered this issue with data that are processed in production systems I don't…",,,,,,Anecdotal,comment,,,,,,,,2023-07-14,github/DavidToneian,https://github.com/pandas-dev/pandas/issues/53966#issuecomment-1635860070,repo: pandas-dev/pandas | issue: ENH: Inform on row & column of failed type conversion when parsing CSV files | keyword: workaround
"> Regarding Ctrl+F or grepping: I have often encountered this issue with data that are processed in production systems I don't have access to, and that don't allow (for regulatory, security, and practical reasons) to log the files that are fed into the CSV parser. Troubleshooting these kinds of issues would be much easier if one knew where the bad data are present. I'm not sure I follow here. How does having the line number/column number help if you don't have access to the file afterwards? You…",,,,,,Anecdotal,comment,,,,,,,,2023-07-14,github/lithomas1,https://github.com/pandas-dev/pandas/issues/53966#issuecomment-1636270192,repo: pandas-dev/pandas | issue: ENH: Inform on row & column of failed type conversion when parsing CSV files | keyword: workaround
"ENH: preview_csv(***.csv) for Fast First-N-Line Preview on Large Plus Size (>100GB) ### Feature Type - [x] Adding new functionality to pandas - [ ] Changing existing functionality in pandas - [ ] Removing existing functionality in pandas ### Problem Description The current `pandas.read_csv()` implementation is designed for robust and complete CSV parsing. However, even when users request only a few lines using `nrows=X`, the function: - **Initializes the full parsing engine** - Performs **colum…",,,,,,Anecdotal,issue,,,,,,,,2025-04-13,github/visheshrwl,https://github.com/pandas-dev/pandas/issues/61281,repo: pandas-dev/pandas | keyword: workaround | state: closed
Thanks for the request. Having to maintain an entirely different code path that does very similar things to `read_csv` seems to me to be a non-starter. I would like to understand why `read_csv` could not be improved to fit this purpose.,,,,,,Anecdotal,comment,,,,,,,,2025-04-14,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/61281#issuecomment-2802245009,repo: pandas-dev/pandas | issue: ENH: preview_csv(***.csv) for Fast First-N-Line Preview on Large Plus Size (>100GB) | keyword: workaround
"**Thank you for the thoughtful feedback @rhshadrach !** I completely understand the reluctance to maintain a separate code path - especially in a core function like `read_csv()`, which already carries significant complexity. `read_csv()` is designed for full-fidelity, schema-validated and optionally type-inferred ingestion. Introducing conditional short circuits for preview-style use cases pollutes that logic and increases branching inside a hot, complex code path. On the other hand, a dedicate…",,,,,,Anecdotal,comment,,,,,,,,2025-04-14,github/visheshrwl,https://github.com/pandas-dev/pandas/issues/61281#issuecomment-2802335328,repo: pandas-dev/pandas | issue: ENH: preview_csv(***.csv) for Fast First-N-Line Preview on Large Plus Size (>100GB) | keyword: workaround
BUILD: Nightly wheel building failed for some platforms ### Installation check - [x] I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas). ### Platform Github Actions ubuntu-latest ### Installation Method pip install ### pandas Version nightly wheel (see below) ### Python Version 3.12 ### Installation Logs <details> ``` python -m pip install \ --index-url https://pypi.anaconda.org/scientific-python-nightly-wheels/simpl…,,,,,,Anecdotal,issue,,,,,,,,2025-08-12,github/djhoese,https://github.com/pandas-dev/pandas/issues/62093,repo: pandas-dev/pandas | keyword: workaround | state: closed
I'm rerunning the CI job I had that failed. It looks like (see the logs above) that the wheel was not available and the sdist tarball was used instead. Oh...nightly wheel builds failed: https://github.com/pandas-dev/pandas/actions/runs/16898335562,,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/djhoese,https://github.com/pandas-dev/pandas/issues/62093#issuecomment-3179457283,repo: pandas-dev/pandas | issue: BUILD: Nightly wheel building failed for some platforms | keyword: workaround
"Thanks for raising the issue. I see that indeed some of the linux builds are failing somewhat randomly -> https://github.com/pandas-dev/pandas/pull/62124 I also see that some of the windows wheels are failing, I suppose that's not that you ran into, but something to fix as well (-> https://github.com/pandas-dev/pandas/pull/62126)",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/62093#issuecomment-3191727874,repo: pandas-dev/pandas | issue: BUILD: Nightly wheel building failed for some platforms | keyword: workaround
DEPR: attrs Discussion broken off from https://github.com/pandas-dev/pandas/issues/51280 PR #52152 Propagation of `attrs` in `__finalize__` is a small-but-everywhere performance hit that we should deprecate.,,,,,,Anecdotal,issue,,,,,,,,2023-03-24,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/52166,repo: pandas-dev/pandas | keyword: workaround | state: closed
"For context, this feature was added in pandas 1.0 (https://github.com/pandas-dev/pandas/pull/29062, cc @TomAugspurger). I personally have no idea how much `attrs` specifically is being used since it was introduced, but in general the ability to store metadata is a topic that has come up a lot. From a quick browsing of our issues, some related ones: * https://github.com/pandas-dev/pandas/issues/2485 (one of our most upvoted issues, which was closed pointing to `attrs`) * https://github.com/panda…",,,,,,Anecdotal,comment,,,,,,,,2023-03-27,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1484690676,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"From Joris in https://github.com/pandas-dev/pandas/issues/51280#issuecomment-1484680456 > If performance is the main argument that we would want to deprecate the related features to metadata propagation (attrs/flags, https://github.com/pandas-dev/pandas/issues/52165 and https://github.com/pandas-dev/pandas/issues/52166, where it is currently the only argument), I think we need some more investigation / proof that this is actually a problem. That's been bugging me too. I haven't looked at the pe…",,,,,,Anecdotal,comment,,,,,,,,2023-03-27,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1484970502,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"> where it is currently the only argument The other argument is that attrs/_metadata is only half-implemented, with a bunch of the test_finalize tests xfailed and a bunch more just wrong. And there is no real prospect of getting these fully working. If we do decide this is worth keeping, we should have Only One way to do it. _metadata and attrs do effectively the same thing in slightly different ways.",,,,,,Anecdotal,comment,,,,,,,,2023-03-27,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1485715043,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"> If we do decide this is worth keeping, we should have Only One way to do it. _metadata and attrs do effectively the same thing in slightly different ways. That is not really true I think. `attrs` allows users to use this for standard DataFrames, `_metadata` allows subclasses to use custom metadata that is not directly exposed to users through `attrs`.",,,,,,Anecdotal,comment,,,,,,,,2023-03-27,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1485718716,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"> And there is no real prospect of getting these fully working. Personally, this is the argument I find most persuading I encountered this in the USC contract too, they said they couldn't use `attrs` as they'd tried to and it was too unreliable (having tried fixing up the finalize tests, I'm not surprised) One could make the argument that some feature not working completely isn't a reason to deprecate it, but I'm not sure that's valid if the feature isn't being worked on (by contrast, datetime …",,,,,,Anecdotal,comment,,,,,,,,2023-03-31,github/MarcoGorelli,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1491803340,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"> but I'm not sure that's valid if the feature isn't being worked on To be clear I am not working on this myself, so I don't know the details. But I am not sure that this is true that it is not being worked on: judging by the the activity and linked PRs in https://github.com/pandas-dev/pandas/issues/28283, there is some work going on to improve this? (it might have slowed down the last months, but for example generally speaking for the year 2022, quite some PRs have been merged related to this)…",,,,,,Anecdotal,comment,,,,,,,,2023-03-31,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1491911740,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"I can chip away at these as I have free time. > As for users wanting to store metadata - does any other DataFrame library support this? If not, we shouldn't be saying ""yes"" to everything, especially given how limited maintenance resources are. xarray does, and I think is a good analog here.",,,,,,Anecdotal,comment,,,,,,,,2023-04-01,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1492966329,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"> I can chip away at these as I have free time. Awesome! > I think the bigger problem is that there is no longer an active champion following up on this within the core team Yeah if someone's willing to step up and champion it (like it looks Tom might be doing?) then I have no objections to salvaging this, apologies for having made some too heavy-handed comments earlier on this",,,,,,Anecdotal,comment,,,,,,,,2023-04-01,github/MarcoGorelli,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1493080738,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"An example of attrs use is one of my little personal projects : https://github.com/chourmo/netpandas It uses it to keep track of the column name with ('from' 'to') to represent a graph structure in a standard dataframe. This is just an example, do not make a decision just based on my use case :)",,,,,,Anecdotal,comment,,,,,,,,2023-04-02,github/chourmo,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1493288189,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"Another project that subclasses pandas and uses _metadata is https://github.com/theOehrly/Fast-F1. I have previously worked on some of the missing `__finalize__` calls and tests. If you decide that you want to keep this, I can likely offer some time to work on this as well over the next few weeks if you want to get this to a fully working state then.",,,,,,Anecdotal,comment,,,,,,,,2023-04-06,github/theOehrly,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1498712972,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"@chourmo @theOehrly thanks a lot for chiming in! That's useful feedback, and it's good to see real-world examples so we can better evaluate this. > Another project that subclasses pandas and uses _metadata is https://github.com/theOehrly/Fast-F1. @theOehrly I know you are aware of it, but for the general reader, the issue about subclasses/`_metadata` is at https://github.com/pandas-dev/pandas/issues/51280",,,,,,Anecdotal,comment,,,,,,,,2023-04-07,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1500301972,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
> Yeah if someone's willing to step up and champion it (like it looks Tom might be doing?) Champion might be a bit strong :) It'll just be an hour or so on random weekend mornings.,,,,,,Anecdotal,comment,,,,,,,,2023-04-15,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1509747400,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"Another “using it!” chime. Our library just converted to using dataframes for ResultSets. attrs will store things like asc/desc sort order, if a inserted row is “virtual” (unsaved to db), etc.",,,,,,Anecdotal,comment,,,,,,,,2023-04-17,github/ssweber,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1511671704,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"I like the having a fixed location where users can store their own meta data. But at the same time I think that `__finalize__ ` is not a nice concept, it's brittle. If we can't make the propagation work, I'd be in favor of keeping `attrs` but then drop the propagation. There is IMO still value in having a location to put user data related to the dataframe.",,,,,,Anecdotal,comment,,,,,,,,2023-05-05,github/topper-123,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1536580743,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"Update: After implementing and using, we only had to reattach attrs once, and it makes sense: ```python attrs = self.rows.attrs.copy() row_series = pd.Series(row) self.rows = pd.concat([self.rows, row_series.to_frame().T], ignore_index=True) self.rows.attrs = attrs ```",,,,,,Anecdotal,comment,,,,,,,,2023-05-05,github/ssweber,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1536638429,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"can we say we agree that we deprecate giving `attrs` to `__finalize__` but keep the attribute otherwise, i.e. make this a much simpler implementation?",,,,,,Anecdotal,comment,,,,,,,,2023-06-06,github/topper-123,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1579549164,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"Another user here. We use `attrs` to carry along additional metadata e.g. for timeseries. --- ### Comments on above suggestions concerning removal: > As for what users should do - I'd suggest they define their own dataclass where one field is metadata and another is the dataframe, and then take care of how to propagate it themselves This is quite inconvenient. You loose a lot of API. For example I can currently do `(s1 + s2) / 2`. For a dataframe or custom class one would have to reimplement al…",,,,,,Anecdotal,comment,,,,,,,,2023-08-02,github/timhoffm,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1661675919,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"Many thanks @timhoffm for your comment! I'm gonna reverse my previous stance then, it's really not too big of a deal to keep it. Furthermore, since I made my original comment, there have been PRs merged to improve attrs propagation",,,,,,Anecdotal,comment,,,,,,,,2023-08-02,github/MarcoGorelli,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1661797934,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"I'd find it a pretty significant loss of functionality if attrs went away, especially Series-based attrs. Here are just a few ways that it is being used in several of my packages: 1. To store custom custom rendering options that are set in conjunction with the ```register_series_accessor``` decorator. For example, I register a series accessor called ```highlight``` that lets users highlight chemical substructures in DataFrames in Jupyter. For lack of any better place to put that, I'm using ```a…",,,,,,Anecdotal,comment,,,,,,,,2023-12-23,github/scott-arne,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1868367139,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"It seems that Copy-On-Write removes some of the utility of attrs. Is there a way to set attrs on a column of a DataFrame? ``` pd.set_option(""mode.copy_on_write"", True) df = pd.DataFrame({""a"": [1, 1, 2], ""b"": [3, 4, 5]}) df[""a""].attrs[""name""] = ""x"" print(df[""a""].attrs) # {} df.loc[:, ""a""].attrs[""name""] = ""x"" print(df[""a""].attrs) # {} ser = df[""a""] ser.attrs[""name""] = 'x' df[""c""] = ser print(df[""c""].attrs) # {} ``` I imagine the 3rd example can be made to work with CoW, but not the 1st and 2nd. c…",,,,,,Anecdotal,comment,,,,,,,,2024-02-06,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1928743957,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"IMHO the first two should error out. Setting attrs is a write operation, but we certainly don't want this to make a copy of the dataframe. Furthermore, attrs is a global property of the dataframe and modifying that through a partial view may be confusing. So the only reasonable behavior is to not allow setting attrs on views.",,,,,,Anecdotal,comment,,,,,,,,2024-02-06,github/timhoffm,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-1929246223,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"Am I correct in understanding that the consensus is now that ``attrs`` is here to stay? I also use it and would like to increase my reliance on it (provided it doesn't go away). If it's there to stay, would it be OK to remove the experimental warning in the doc and instead specify when it is not propagated?",,,,,,Anecdotal,comment,,,,,,,,2024-06-01,github/tfardet,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-2143442168,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"Not a pandas core dev, but my take on this is that it's aspirational to support `attrs`. However, there are still some rough edges in particular also in connection with copy-on-write, so that it's not a first-class feature yet. I would characterize it as: - Is here to stay. - Works reasonably well, but be prepared for some limitations / bugs. - Should gradually improve in the long run.",,,,,,Anecdotal,comment,,,,,,,,2024-06-04,github/timhoffm,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-2146716621,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"I want to add one item to the list of projects using `.attrs` 😊 In the [`spatialdata`](https://github.com/scverse/spatialdata/) library (a framework developed for spatial biology, kind of an extension of microscopy which looks at molecular content in tissues), we use `.attrs` to store metadata (simple objects that are JSON-serializable) that is crucial for our data representation. More precisely, the `SpatialData` class is a container for various objects: `pd.DataFrame`, `xarray.DataArray`, `da…",,,,,,Anecdotal,comment,,,,,,,,2024-06-19,github/LucaMarconato,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-2178473375,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"> Not a pandas core dev, but my take on this is that it's aspirational to support `attrs`. However, there are still some rough edges in particular also in connection with copy-on-write, so that it's not a first-class feature yet. > > I would characterize it as: > > * Is here to stay. > * Works reasonably well, but be prepared for some limitations / bugs. > * Should gradually improve in the long run. Could we have confirmation by a Pandas core dev that `attrs` are staying? For example by formall…",,,,,,Anecdotal,comment,,,,,,,,2024-11-07,github/akors,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-2462252318,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"I've also used `.attrs` in several projects, and find it very useful — for units, provenance, dirty-state etc. I don't mind the support being incomplete. Two small additions I'd suggest to make the incomplete support feel more... complete? are: 1. If there isn't already, put a terse list in the documentation of which operations support it, and how (by shallow copy, by reference if any). 2. Add a `.set_attrs()` method that allows one-liners like `return pd.concat(dfs).set_attrs(dfs[0].attrs)`",,,,,,Anecdotal,comment,,,,,,,,2024-11-18,github/jobh,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-2483031457,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"> 1. If there isn't already, put a terse list in the documentation of which operations support it, and how (by shallow copy, by reference if any). The Notes section in https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.attrs.html#pandas.DataFrame.attrs is the best we have. It's a user-facing paraphrasing of the implementation: attrs handling is done in `__finalize__`, which most operations use (but there may be exceptions, which is why it may be hard to make a definitive list). `pd.c…",,,,,,Anecdotal,comment,,,,,,,,2024-11-18,github/timhoffm,https://github.com/pandas-dev/pandas/issues/52166#issuecomment-2483316352,repo: pandas-dev/pandas | issue: DEPR: attrs | keyword: workaround
"QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? ### Research - [X] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions. - [X] I have asked my usage related question on [StackOverflow](https://stackoverflow.com). ### Link to question on StackOverflow https://stackoverflow.com/q/77995105/4865723 ### Question about pandas Hello, and please take my apologize for as…",,,,,,Anecdotal,issue,,,,,,,,2024-03-05,github/buhtz,https://github.com/pandas-dev/pandas/issues/57734,repo: pandas-dev/pandas | keyword: workaround | state: closed
"Just do ``` pandas.set_option(""future.no_silent_downcasting"", True) ``` as suggested on the stack overflow question The series will retain object dtype in pandas 3.0 instead of casting to int64",,,,,,Anecdotal,comment,,,,,,,,2024-03-06,github/phofl,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-1981871396,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"> ``` > pandas.set_option(""future.no_silent_downcasting"", True) > ``` But doesn't this just deactivate the message but doesn't modify the behavior. To my understanding the behavior is the problem and need to get solved. Or not? My intention is to extinguish the fire and not just turn off the fire alarm but let the house burn down.",,,,,,Anecdotal,comment,,,,,,,,2024-03-07,github/buhtz,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-1982810492,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"I'm having this problem as well. I have the feeling it's related to `.replace` changing the types of the values (as one Stack Overflow commenter [implied](https://stackoverflow.com/questions/77900971/pandas-futurewarning-downcasting-object-dtype-arrays-on-fillna-ffill-bfill#comment137615068_77941616)). Altering the original example slightly: ```python s = Series(['foo', 'bar']) replace_dict = {'foo': '1', 'bar': '2'} # replacements maintain original types s = s.replace(replace_dict) ``` makes t…",,,,,,Anecdotal,comment,,,,,,,,2024-03-13,github/jerome-white,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-1994209937,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
> s that we never thought of it as doing any casting This is exactly the thing we are trying to solve. replace was previously casting your dtypes and will stop doing so in pandas 3,,,,,,Anecdotal,comment,,,,,,,,2024-03-13,github/phofl,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-1994213353,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"> This is exactly the thing we are trying to solve. replace was previously casting your dtypes and will stop doing so in pandas 3 But it is unclear how to replace and cast. E.g. when I have `[0, 1]` integers they stand for female and male. ``` df.gender = df.gender.astype(str) df.gender = df.gender.replace({'0': 'male', '1': 'female'}) ``` Is that the solution you have in mind? From a users perspective it is a smelling workaround. The other way around is nearly not possible because I can not ca…",,,,,,Anecdotal,comment,,,,,,,,2024-03-13,github/buhtz,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-1994220779,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"> The other way around is nearly not possible because I can not cast a str word to an integer. One alternative (although I realise a non `.replace` supported ""alternative"" may not be what was actually desired) is to use categoricals with `.assign`: ```python import pandas as pd df = pd.DataFrame(['male', 'male', 'female'], columns=['gender']) # from the original example genders = pd.Categorical(df['gender']) df = df.assign(gender=genders.codes) ``` If semantically similar data is spread across …",,,,,,Anecdotal,comment,,,,,,,,2024-03-13,github/jerome-white,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-1994448905,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"I got here, trying to understand what `pd.set_option('future.no_silent_downcasting', True)` does. The message I get is from `.fillna()`, which is the same message for `.ffill()` and `.bfill()`. So I'm posting this here in case someone is looking for the same answer using the mentioned functions. This is the warning message I get: ``` FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) …",,,,,,Anecdotal,comment,,,,,,,,2024-05-07,github/caballerofelipe,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-2098984793,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"So... I did some digging and I think I have a better grasp of what's going on with this FutureWarning. So I wrote an article in Medium to explain what's happening. If you want to give it a read, here it is: [Deciphering the cryptic FutureWarning for .fillna in Pandas 2](https://medium.com/@felipecaballero/deciphering-the-cryptic-futurewarning-for-fillna-in-pandas-2-01deb4e411a1) Long story short, do: ```python with pd.option_context('future.no_silent_downcasting', True): # Do you thing with fil…",,,,,,Anecdotal,comment,,,,,,,,2024-05-07,github/caballerofelipe,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-2099489978,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"I feel like this thread is starting to become a resource. In that spirit: I just experienced another case where `.replace` would have been amazing, but I now need an alternative: a column of strings that are meant to be floats, where the only ""offending"" values are empty strings (meant to be NaN's). Consider: ```python records = [ {'a': ''}, {'a': 12.3}, ] df = pd.DataFrame.from_records(records) ``` I would have first reached for `.replace`. Now I consider `.filla`, but that doesn't work either…",,,,,,Anecdotal,comment,,,,,,,,2024-05-15,github/jerome-white,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-2111724906,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"From your code: > ```python > x = df.assign(a=lambda x: pd.to_numeric(x['a'])) > ``` I would do it like this, it feels a little cleaner and easier to read: ```python df['a'] = pd.to_numeric(df['a']) ``` --- You said you wanted to use `replace`, if you want to use it, you can do this: ```python with pd.option_context('future.no_silent_downcasting', True): df2 = (df .replace('', float('nan')) # Replace empty string for nans .infer_objects() # Allow pandas to try to ""infer better dtypes"" ) df2.dty…",,,,,,Anecdotal,comment,,,,,,,,2024-05-20,github/caballerofelipe,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-2120759239,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"explicitly do the conversion in two steps and the future warning will go away. In the first step, do the replace with the numbers as strings to match the original dtype replace_dict = {'foo': '2', 'bar': '4'} in the second step, convert the dtype to int s = s.replace(replace_dict).astype(int) This will run without the warning even when you have not suppressed warnings",,,,,,Anecdotal,comment,,,,,,,,2024-07-06,github/Data-Salad,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-2211545401,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"I got this because I was trying to filter a dataframe using the output from `Series.str.isnumeric()`. My dataframe contained NA values, so the resulting mask contained NA values. Normally I use `fillna(False)` to get rid of these. What I would normally do: ``` python df = pd.DataFrame({'A': ['1', '2', 'test', pd.NA]}) mask = df['A'].str.isnumeric().fillna(False) ``` What I need to do now: ``` python df = pd.DataFrame({'A': ['1', '2', 'test', pd.NA]}) with pd.option_context('future.no_silent_dow…",,,,,,Anecdotal,comment,,,,,,,,2024-08-08,github/daviewales,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-2274836151,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"Hello Folks, Thanks for your helpful previous answers. Especially > explicitly do the conversion in two steps and the future warning will go away. > > In the first step, do the replace with the numbers as strings to match the original dtype replace_dict = {'foo': '2', 'bar': '4'} > > in the second step, convert the dtype to int s = s.replace(replace_dict).astype(int) > > This will run without the warning even when you have not suppressed warnings This works well when going trom string to int; b…",,,,,,Anecdotal,comment,,,,,,,,2024-11-01,github/Arnaudno,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-2451901972,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"Arnaudno, You're doing more work than you need to. The boolean value of all strings except for empty strings is true (empty strings have a boolean value of false). So in your case, you don't need the replace at all. All you need is to convert the string to boolean and you will get the result you want. df=pd.DataFrame({'a':['','X','','X','X']}) s = df['a'].astype('bool') print(s) and you will get 0 False 1 True 2 False 3 True 4 True Name: a, dtype: bool",,,,,,Anecdotal,comment,,,,,,,,2024-11-01,github/Data-Salad,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-2452343177,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"Những chuyện này là sao. Nói hỗ trợ giúp đỡ. Nhưng làm văn bản. Công thức. Không có lời dẫn hay thuyết minh. Rồi cuối cùng những thông tin vừa xong lãi vỡ ra. Lại bị thay đổi 1 lần nữa. 3 cái diện thoại giờ nó dùng k khác gì thập niên 80. Thông tin cá nhân giờ thể xác thực. Đến số điện thoại. Hiện tại cũng không thể chứng minh nó là của mình. Nói không tin. Không hợp tác phối hợp. Vậy 1 tháng qua tôi trải qua những gì ai biết không??? Vào Th 7, 21 thg 12, 2024 lúc 17:08 herzphi ***@***.***> đã …",,,,,,Anecdotal,comment,,,,,,,,2024-12-21,github/ghost,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-2558170221,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"To sum up, to resolve this warning, values of the same type should be used and then they can be cast to e.g. `int`, like that: ```python replace_dict = {'foo': '2', 'bar': '4'} # keys are str and so must be values s = s.replace(replace_dict).astype(int) # cast values to int ```",,,,,,Anecdotal,comment,,,,,,,,2024-12-27,github/satk0,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-2563747799,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"This solution is not perfect for those of us what have empty rows in the column they are replacing (I have a column of occasional text labels which I was converting to int's for plotting purposes). In this case `.astype(int)` can't cope with empty or nan, so `.astype(float)` is required.",,,,,,Anecdotal,comment,,,,,,,,2025-01-06,github/DrNickBailey,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-2572931244,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"In my case, I had to convert strings to either bools or nan depending on the value of the string. Instead of suppressing the warning resulting from using replace(), I think it is generally better to use map() when you need to both change dtypes and deal with null values. Given: ``` print(df['has_attribute']) Name A Yes B No C Unknown Name: has_attribute, Length: 3, dtype: object ``` I do: ``` df = df.map(lambda x: True if x == 'Yes' else (False if x == 'No' else np.nan)) print(df['has_attribute…",,,,,,Anecdotal,comment,,,,,,,,2025-01-07,github/SamLovesHoneyWater,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-2574215022,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"> Just do > > ``` > pandas.set_option(""future.no_silent_downcasting"", True) > ``` > > as suggested on the stack overflow question > > The series will retain object dtype in pandas 3.0 instead of casting to int64 I've done it for now, just wonder when my pandas is upgraded to 3.0 next time, will there be any notification that I can remove this pd.set_option?",,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/randomseed42,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-3182838142,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
TST: run python-dev CI on 3.14-dev I'd like to see how widespread the test breakage is due to https://github.com/pandas-dev/pandas/issues/61368. Also 3.14rc1 came out earlier this week so Pandas should probably start thinking about 3.14 support soonish.,,,,,,Anecdotal,issue,,,,,,,,2025-07-25,github/ngoldbaum,https://github.com/pandas-dev/pandas/pull/61950,repo: pandas-dev/pandas | keyword: workaround | state: closed
"After turning off the warning the tests results look much more reasonable. Here's the summary on Linux CI: <details> ``` =========================== short test summary info ============================ FAILED pandas/tests/copy_view/test_chained_assignment_deprecation.py::test_series_setitem[0] - Failed: DID NOT WARN. No warnings of type (<class 'Warning'>,) were emitted. Emitted warnings: []. FAILED pandas/tests/copy_view/test_chained_assignment_deprecation.py::test_series_setitem[indexer1] - F…",,,,,,Anecdotal,comment,,,,,,,,2025-07-25,github/ngoldbaum,https://github.com/pandas-dev/pandas/pull/61950#issuecomment-3120264536,repo: pandas-dev/pandas | issue: TST: run python-dev CI on 3.14-dev | keyword: workaround
"> @jorisvandenbossche did you ever have time to look closer at generating the chained assignment warning on 3.14 since it was reported in April? Unfortunately we're probably past the time when we can get C API changes merged into CPython to support this use-case, so it may not be easily feasible to detect what you're looking for just based on refcounts in 3.14 and newer. I didn't get to it yet, but now installed python 3.14 to try myself and took a first look. I added some more context to the i…",,,,,,Anecdotal,comment,,,,,,,,2025-08-03,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/pull/61950#issuecomment-3148436016,repo: pandas-dev/pandas | issue: TST: run python-dev CI on 3.14-dev | keyword: workaround
"OK, I think I've gotten everything except for the two test failures in `pandas/tests/generic/test_generic.py`, which I don't understand. It looks like `pytest.raises` is broken somehow or it's broken as a side effect of something else? Because the exception should be getting caught as far as I can see but it's not. <details> ``` goldbaum at Nathans-MBP in ~/Documents/pandas on 3.14-ci ± pytest pandas/tests/generic/test_generic.py ============================= test session starts ===============…",,,,,,Anecdotal,comment,,,,,,,,2025-08-07,github/ngoldbaum,https://github.com/pandas-dev/pandas/pull/61950#issuecomment-3166015839,repo: pandas-dev/pandas | issue: TST: run python-dev CI on 3.14-dev | keyword: workaround
"> Because the exception should be getting caught as far as I can see but it's not. OK, here's a weird one. This script runs without error on Python 3.13 but dies with an uncaught `ValueError` on 3.14.0rc1: ```python import pandas as pd obj1 = pd.DataFrame({'0': [1, 1, 1, 1], '1': [1, 1, 1, 1]}) obj2 = pd.DataFrame({'0': [1, 1, 1, 1], '1': [1, 1, 1, 1]}) try: obj1 and obj2 except ValueError: pass ``` ``` goldbaum at Nathans-MBP in ~/Documents/test ○ python test.py Traceback (most recent call las…",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/ngoldbaum,https://github.com/pandas-dev/pandas/pull/61950#issuecomment-3166257178,repo: pandas-dev/pandas | issue: TST: run python-dev CI on 3.14-dev | keyword: workaround
"@ngoldbaum I've been tracking this PR and happened to see your comments tonight and thought ""no way, that can't be"", but yeah I don't understand how that's possible without it being a python. Even if pandas is doing something wrong somehow it shouldn't be magically getting around a try/except.",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/djhoese,https://github.com/pandas-dev/pandas/pull/61950#issuecomment-3166282851,repo: pandas-dev/pandas | issue: TST: run python-dev CI on 3.14-dev | keyword: workaround
"> Seems kinda like a Python bug to me? I think this is https://github.com/python/cpython/issues/137288, which I think should be fixed in 3.14.0rc2. It's a little tricky to ignore these test failures because I can't actually catch these particular exceptions... I guess I can just skip them for `sys.version_info == (3, 14, 0, 'candidate', 1)` and then we can reassess when rc2 comes out?",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/ngoldbaum,https://github.com/pandas-dev/pandas/pull/61950#issuecomment-3166283125,repo: pandas-dev/pandas | issue: TST: run python-dev CI on 3.14-dev | keyword: workaround
"@ngoldbaum thanks for finding that upstream issue, good to see it is already fixed. FWIW, numpy has the same issue (`obj1 and obj2` where those objects are numpy arrays also bypasses the `except ValueError`)",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/pull/61950#issuecomment-3167940331,repo: pandas-dev/pandas | issue: TST: run python-dev CI on 3.14-dev | keyword: workaround
"Yup! Not really surprising to me that the Pandas test suite caught the upstream bug but the NumPy tests missed it, Pandas has much more comprehensive tests... I think I might split off the changes for the new error messages and `codecs.open` into their own PR so they can be merged separately. If you decide we ultimately need to disable the warning and workarounds in C aren't possible, we can merge this and then work with @mpage to get a fix in for 3.14.1. But hopefully you figure out how to get…",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/ngoldbaum,https://github.com/pandas-dev/pandas/pull/61950#issuecomment-3168066235,repo: pandas-dev/pandas | issue: TST: run python-dev CI on 3.14-dev | keyword: workaround
"Actually on second thought I don't think it makes sense to PR the warnings changes without any 3.14 testing, so I'll leave that here. @jorisvandenbossche please feel free to cherry-pick fc51e5f6fa5a8573db4c7e00750f4d9499c029a7 if you end up coming up with a better approach. I'll go ahead and re-enable all the CI to make sure I didn't break anything on older Python versions.",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/ngoldbaum,https://github.com/pandas-dev/pandas/pull/61950#issuecomment-3168253031,repo: pandas-dev/pandas | issue: TST: run python-dev CI on 3.14-dev | keyword: workaround
Closing in favor of Joris' PRs. Please feel free to cherry-pick [180081b](https://github.com/pandas-dev/pandas/commit/180081b04fa9c18ebec787b1c40b321f93a0dce2),,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/ngoldbaum,https://github.com/pandas-dev/pandas/pull/61950#issuecomment-3168425171,repo: pandas-dev/pandas | issue: TST: run python-dev CI on 3.14-dev | keyword: workaround
"ENH: Restore the functionality of `.fillna` ### Feature Type - [ ] Adding new functionality to pandas - [X] Changing existing functionality in pandas - [ ] Removing existing functionality in pandas ### Problem Description The currently very useful behaviour of `.fillna` is being deprecated. ```python a = pd.Series([True, False, None]) a # 0 True # 1 False # 2 None # dtype: object ``` Using `a.fillna` raises a warning: ```python a.fillna(True) # [/var/folders/66/bjmfs8315pjdbztwxmyvd7x80000gn/T/…",,,,,,Anecdotal,issue,,,,,,,,2024-09-18,github/tomprimozic,https://github.com/pandas-dev/pandas/issues/59831,repo: pandas-dev/pandas | keyword: workaround | state: closed
"Thanks for report! You mention that `infer_objects` didn't work, I'm not sure what you meant by this. Is it that you still get the warning? You need to use `infer_objects` with the future behavior like so: ```python ser = pd.Series([True, False, None]) with pd.option_context(""future.no_silent_downcasting"", True): print(ser.fillna(True).infer_objects(copy=False)) ```",,,,,,Anecdotal,comment,,,,,,,,2024-09-18,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/59831#issuecomment-2359060509,repo: pandas-dev/pandas | issue: ENH: Restore the functionality of `.fillna` | keyword: workaround
"> This is overly verbose, but would be acceptable if `boolean` was inferred automatically for `[True, False, None]` (or `[True, False, np.nan]`), but currently it's not... This is another good area where PDEP-13 would probably help a lot https://github.com/pandas-dev/pandas/pull/58455 If it helps, you can avoid the object-dtype based ""boolean"" array without an astype if you use the `pd.BooleanDtype` dtype argument as part of the constructor: ```python a = pd.Series([True, False, None], dtype=pd…",,,,,,Anecdotal,comment,,,,,,,,2024-09-24,github/WillAyd,https://github.com/pandas-dev/pandas/issues/59831#issuecomment-2372019430,repo: pandas-dev/pandas | issue: ENH: Restore the functionality of `.fillna` | keyword: workaround
"Hi. I just hit this ""bug"" or feature. It's annoying. Here's my fix ``` def bool_fillna_inplace(series: pd.Series) -> pd.Series: """""" Workaround for https://github.com/pandas-dev/pandas/issues/59831 Replaces nan/None with False."""""" series[pd.isna(series)] = False return series ``` or an alternative `return series & ~pd.isna(series)` but that one does a bitwise and which would return False for even number, in case the series is not purely boolean with nans.",,,,,,Anecdotal,comment,,,,,,,,2024-11-26,github/joaoe,https://github.com/pandas-dev/pandas/issues/59831#issuecomment-2500704786,repo: pandas-dev/pandas | issue: ENH: Restore the functionality of `.fillna` | keyword: workaround
"https://github.com/pandas-dev/pandas/issues/59831#issuecomment-2359060509 is the correct solution here. Though the ""copy"" keyword is no longer needed in that snippet.",,,,,,Anecdotal,comment,,,,,,,,2025-07-02,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/59831#issuecomment-3028823210,repo: pandas-dev/pandas | issue: ENH: Restore the functionality of `.fillna` | keyword: workaround
"ENH: Add a clear option to interpret strings as Pandas dtypes specifically. ### Feature Type - [X] Adding new functionality to pandas - [ ] Changing existing functionality in pandas - [ ] Removing existing functionality in pandas ### Problem Description When the pd.read_excel function encounters an empty cell with a numeric dtype, it leads to a conversion error. The solution to this, is to use the Pandas specific nullable numeric types, which work extremely well. The problem is that these datat…",,,,,,Anecdotal,issue,,,,,,,,2024-09-11,github/vitkosbence,https://github.com/pandas-dev/pandas/issues/59777,repo: pandas-dev/pandas | keyword: workaround | state: closed
"Thanks for the report, does specifying `pd.options.future.infer_string = True` do the trick? See https://pandas.pydata.org/pdeps/0014-string-dtype.html for more details.",,,,,,Anecdotal,comment,,,,,,,,2024-09-12,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/59777#issuecomment-2347192463,repo: pandas-dev/pandas | issue: ENH: Add a clear option to interpret strings as Pandas dtypes specifically. | keyword: workaround
ENH: Make merge_asof preserve the index ### Pandas version checks - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas. ### Reproducible Example ```python import pandas as pd imp…,,,,,,Anecdotal,issue,,,,,,,,2025-02-12,github/wahsmail,https://github.com/pandas-dev/pandas/issues/60919,repo: pandas-dev/pandas | keyword: workaround | state: closed
"Thanks for the report! I do not see any indication in the documentation that this should preserve the index, and taking a look at the implementation it seems apparent to me that it wasn't intended to. Reworking this issue as an enhancement request. If there is some indication in the docs that it should preserve the index, please do let us know.",,,,,,Anecdotal,comment,,,,,,,,2025-02-12,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/60919#issuecomment-2654943777,repo: pandas-dev/pandas | issue: ENH: Make merge_asof preserve the index | keyword: workaround
"Ok agreed, its not in the documentation. I noticed that pd.merge has the same effect - the docs of which say ""If joining columns on columns, the DataFrame indexes will be ignored."" Philosophically I would think that left joins preserve the shape (and index) of the left dataframe. But obviously for inner/outer joins, this becomes more ambiguous. I'll just plan to workaround this going forward, thanks.",,,,,,Anecdotal,comment,,,,,,,,2025-02-13,github/wahsmail,https://github.com/pandas-dev/pandas/issues/60919#issuecomment-2656587645,repo: pandas-dev/pandas | issue: ENH: Make merge_asof preserve the index | keyword: workaround
"I do think if any change is to be made here, it should be made across merge operations to maintain consistency. This would be a rather big change, and I don't see the impact as being worth the behavior change. Marking as a closing candidate.",,,,,,Anecdotal,comment,,,,,,,,2025-02-13,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/60919#issuecomment-2657788232,repo: pandas-dev/pandas | issue: ENH: Make merge_asof preserve the index | keyword: workaround
"BUG: StringArray is a subclass of PandasArray ### Pandas version checks - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [X] I have confirmed this bug exists on the main branch of pandas. ### Reproducible Example ```python In [2]: A = pd.array([""a1"", ""a2""], dtype=""string"") In [3]: A Out[3]: <StringArray> ['a1', 'a2'] Length: 2, dtype: string In [4]: …",,,,,,Anecdotal,issue,,,,,,,,2022-09-19,github/ehsantn,https://github.com/pandas-dev/pandas/issues/48638,repo: pandas-dev/pandas | keyword: workaround | state: closed
Thanks for the report. I don't think the API for extension arrays is very clear. Did you try to simply remove the subclass [here](https://github.com/pandas-dev/pandas/blob/main/pandas/core/arrays/string_.py#L220) and see what fails? I guess there may be some generic functionality that lives in the parent classes meant for numpy backed data. It'd surely make sense to move things to the right place if that's the case.,,,,,,Anecdotal,comment,,,,,,,,2022-09-20,github/datapythonista,https://github.com/pandas-dev/pandas/issues/48638#issuecomment-1252900177,repo: pandas-dev/pandas | issue: BUG: StringArray is a subclass of PandasArray | keyword: workaround
"> > This inconsistency causes issues for unboxing the values for Bodo JIT > > Can you elaborate on this? The code paths for unboxing each array type is different. For PandasArray, Bodo just needs to unbox `_ndarray` since PandasArray is just a wrapper for Numpy arrays. StringArray is a different code path with slight differences such as handling pd.NA. This led to an error in Bodo so I noticed this class hierarchy issue.",,,,,,Anecdotal,comment,,,,,,,,2022-09-21,github/ehsantn,https://github.com/pandas-dev/pandas/issues/48638#issuecomment-1253059993,repo: pandas-dev/pandas | issue: BUG: StringArray is a subclass of PandasArray | keyword: workaround
"> Thanks for the report. I don't think the API for extension arrays is very clear. Did you try to simply remove the subclass [here](https://github.com/pandas-dev/pandas/blob/main/pandas/core/arrays/string_.py#L220) and see what fails? I guess there may be some generic functionality that lives in the parent classes meant for numpy backed data. It'd surely make sense to move things to the right place if that's the case. Yes, the APIs should be clarified to enable more robust integration with Pand…",,,,,,Anecdotal,comment,,,,,,,,2022-09-21,github/ehsantn,https://github.com/pandas-dev/pandas/issues/48638#issuecomment-1253066882,repo: pandas-dev/pandas | issue: BUG: StringArray is a subclass of PandasArray | keyword: workaround
"ENH: generic `save` and `read` methods for DataFrame ### Feature Type - [x] Adding new functionality to pandas - [ ] Changing existing functionality in pandas - [ ] Removing existing functionality in pandas ### Problem Description Currently, pandas has separate IO methods for each file format (to_csv, read_parquet, etc.). This requires users to: - Remember multiple method names - Change code when switching formats ### Feature Description A unified `save`/`read` API would simplify common IO oper…",,,,,,Anecdotal,issue,,,,,,,,2025-01-25,github/zkurtz,https://github.com/pandas-dev/pandas/issues/60786,repo: pandas-dev/pandas | keyword: workaround | state: closed
"My workaround for now is to use [dummio](https://pypi.org/project/dummio/), specifically [dummio.pandas.df_io](https://github.com/zkurtz/dummio/blob/main/dummio/pandas/df_io.py), which serves as a draft implementation if there's interest to include this type of thing directly in pandas. ``` from dummio.pandas import df_io df = df_io.load(""data.parquet"") df_io.save(df, filepath=""data.feather"") ... etc ```",,,,,,Anecdotal,comment,,,,,,,,2025-01-29,github/zkurtz,https://github.com/pandas-dev/pandas/issues/60786#issuecomment-2622167441,repo: pandas-dev/pandas | issue: ENH: generic `save` and `read` methods for DataFrame | keyword: workaround
"Thanks for the request. I'm negative on this feature, it adds more code to maintain and test without providing any new behavior users cannot already and easily access. In addition, it requires pandas to parse paths and determine the extension, inferring what format to use. Also using `kwargs` in a signature prevents tools (linters, IDEs, notebooks) from determining the proper arguments and docs for the user. Both of these (inference and kwargs) I would like to see less of in pandas, not more.",,,,,,Anecdotal,comment,,,,,,,,2025-05-07,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/60786#issuecomment-2858973500,repo: pandas-dev/pandas | issue: ENH: generic `save` and `read` methods for DataFrame | keyword: workaround
"Agreed that this probably doesn't belong in pandas, and since dummio implements this the functionality can live there so closing",,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/mroeschke,https://github.com/pandas-dev/pandas/issues/60786#issuecomment-3155798565,repo: pandas-dev/pandas | issue: ENH: generic `save` and `read` methods for DataFrame | keyword: workaround
QST: best way to extend/subclass pandas.DataFrame ### Research - [x] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions. - [x] I have asked my usage related question on [StackOverflow](https://stackoverflow.com). ### Link to question on StackOverflow https://stackoverflow.com/questions/79594258/best-way-to-extend-subclass-pandas-dataframe ### Question about pandas I've written a [package](https://www.github.com/rwijtvliet…,,,,,,Anecdotal,issue,,,,,,,,2025-04-26,github/rwijtvliet,https://github.com/pandas-dev/pandas/issues/61362,repo: pandas-dev/pandas | keyword: workaround | state: closed
"> Behaves like a DataFrame... Is immutable These two are in conflict, pandas is not designed to be immutable. At the very least, you'd have to workaround: - `__setitem__`, `.loc`, .`iloc`, `.at`, `.iat` - `.to_numpy()`, `.values` - Any method with an inplace argument - Any method which acts inplace (e.g. `update`, `insert`) But with these requirements, I believe the only feasible option would be subclass DataFrame / Series.",,,,,,Anecdotal,comment,,,,,,,,2025-04-27,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/61362#issuecomment-2833430861,repo: pandas-dev/pandas | issue: QST: best way to extend/subclass pandas.DataFrame | keyword: workaround
"Bump pypa/cibuildwheel from 3.1.1 to 3.1.3 Bumps [pypa/cibuildwheel](https://github.com/pypa/cibuildwheel) from 3.1.1 to 3.1.3. <details> <summary>Release notes</summary> <p><em>Sourced from <a href=""https://github.com/pypa/cibuildwheel/releases"">pypa/cibuildwheel's releases</a>.</em></p> <blockquote> <h2>v3.1.3</h2> <ul> <li>🐛 Fix bug where &quot;latest&quot; dependencies couldn't update to pip 25.2 on Windows (<a href=""https://redirect.github.com/pypa/cibuildwheel/issues/2537"">#2537</a>)</li>…",,,,,,Anecdotal,issue,,,,,,,,2025-08-04,github/dependabot[bot],https://github.com/pandas-dev/pandas/pull/62039,repo: pandas-dev/pandas | keyword: workaround | state: closed
DOC: Improve documentation for DataFrame.__setitem__ and .loc assignment from Series - [x] closes #61662 - [x] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature - [x] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit). - [ ] Added [type annotations](https://pandas.pydata.org/pandas-docs/dev/development/contributing_code…,,,,,,Anecdotal,issue,,,,,,,,2025-07-07,github/niruta25,https://github.com/pandas-dev/pandas/pull/61804,repo: pandas-dev/pandas | keyword: workaround | state: closed
"Thanks @niruta25 for the PR > * Expanding the user guide with a dedicated section on Series assignment and index alignment I see that ""align"" is found 16 times when searching ""Intro to data structures"" section of the docs. This chapter is only preceded by ""10 minutes to pandas"" so i'm not sure that the linked issue which states ""The current documentation is incomplete and vague about how Series alignment works in assignments."" is correct that this fundamental paradigm of pandas is not covered i…",,,,,,Anecdotal,comment,,,,,,,,2025-07-08,github/simonjayhawkins,https://github.com/pandas-dev/pandas/pull/61804#issuecomment-3048162049,repo: pandas-dev/pandas | issue: DOC: Improve documentation for DataFrame.__setitem__ and .loc assignment from Series | keyword: workaround
BUG: Writing UUIDs fail ### Pandas version checks - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas. ### Reproducible Example ```python >>> df = pd.DataFrame({'id': [uuid.uuid…,,,,,,Anecdotal,issue,,,,,,,,2025-06-07,github/torchss,https://github.com/pandas-dev/pandas/issues/61602,repo: pandas-dev/pandas | keyword: workaround | state: closed
"#After review, this seems to be an issue with PyArrow not Pandas Specifically the convert_columns in from_pandas function of PyArrow as it is not able to work with the UUID library's UUID datatype. ```python import uuid import pandas as pd import pyarrow as pa df = pd.DataFrame({'id': [uuid.uuid4(), uuid.uuid4(), uuid.uuid4()]}) new_df = pa.Table.from_pandas(df) ``` Even the above code gives the same error A temporary workaround is to use the bytes data type before saving to parquet ```python i…",,,,,,Anecdotal,comment,,,,,,,,2025-06-11,github/DevastatingRPG,https://github.com/pandas-dev/pandas/issues/61602#issuecomment-2961134130,repo: pandas-dev/pandas | issue: BUG: Writing UUIDs fail | keyword: workaround
BUG: DataFrame.mul() corrupts data by setting values to zero ### Pandas version checks - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas. ### Reproducible Example ```python im…,,,,,,Anecdotal,issue,,,,,,,,2025-06-22,github/fheisigx,https://github.com/pandas-dev/pandas/issues/61687,repo: pandas-dev/pandas | keyword: workaround | state: closed
This bug doesn't occurs on recent versions of pandas. Our test runs perfectly fine on the last version and was related with the numexrp that is activated when the elements is more than 1M for optimizations.,,,,,,Anecdotal,comment,,,,,,,,2025-06-22,github/MarceloVelludo,https://github.com/pandas-dev/pandas/issues/61687#issuecomment-2994348248,repo: pandas-dev/pandas | issue: BUG: DataFrame.mul() corrupts data by setting values to zero | keyword: workaround
"@MarceloVelludo: what does `recent versions of pandas` mean explicitly? OP is reporting on 2.2.3, I just want to make sure you're reporting not seeing this issue there. Also, what architecture are you on? Posting `pd.show_versions()` would be helpful.",,,,,,Anecdotal,comment,,,,,,,,2025-07-13,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/61687#issuecomment-3067051720,repo: pandas-dev/pandas | issue: BUG: DataFrame.mul() corrupts data by setting values to zero | keyword: workaround
"> [@MarceloVelludo](https://github.com/MarceloVelludo): what does `recent versions of pandas` mean explicitly? OP is reporting on 2.2.3, I just want to make sure you're reporting not seeing this issue there. Also, what architecture are you on? Posting `pd.show_versions()` would be helpful. <details> <summary>INSTALLED VERSIONS</summary> ``` INSTALLED VERSIONS ------------------ commit : c888af6d0bb674932007623c0867e1fbd4bdc2c6 python : 3.13.5 python-bits : 64 OS : Linux OS-release : 6.15.6-1-MA…",,,,,,Anecdotal,comment,,,,,,,,2025-07-14,github/MarceloVelludo,https://github.com/pandas-dev/pandas/issues/61687#issuecomment-3068386625,repo: pandas-dev/pandas | issue: BUG: DataFrame.mul() corrupts data by setting values to zero | keyword: workaround
Thanks @fheisigx for the report. It does not seem to have been reproduced. Closing this but feel free to post further evidence if you are still encountering an issue and we can reopen.,,,,,,Anecdotal,comment,,,,,,,,2025-07-23,github/simonjayhawkins,https://github.com/pandas-dev/pandas/issues/61687#issuecomment-3106654543,repo: pandas-dev/pandas | issue: BUG: DataFrame.mul() corrupts data by setting values to zero | keyword: workaround
PDEP-15: Reject PDEP-10 - [ ] closes #xxxx (Replace xxxx with the GitHub issue number) - [ ] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature - [ ] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit). - [ ] Added [type annotations](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#type-hin…,,,,,,Anecdotal,issue,,,,,,,,2024-05-07,github/lithomas1,https://github.com/pandas-dev/pandas/pull/58623,repo: pandas-dev/pandas | keyword: workaround | state: closed
"Did I miss an official vote on rejecting this? I am not sure yet that I would want to reject, and am still leaning towards keeping in spite of some negative feedback",,,,,,Anecdotal,comment,,,,,,,,2024-05-07,github/WillAyd,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2099462110,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
"Nope, just opening since I said I would in the discussion issue. We'll still need a formal vote - I'm just kicking off the discussion here.",,,,,,Anecdotal,comment,,,,,,,,2024-05-07,github/lithomas1,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2099465667,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
cc @pandas-dev/pandas-core @pandas-dev/pandas-triage xref https://github.com/pandas-dev/pandas/issues/57073#issuecomment-2080683080 for context,,,,,,Anecdotal,comment,,,,,,,,2024-05-08,github/lithomas1,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2100847601,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
"As discussed in dev meeting on 5/8/24, suggestion is to do a new PDEP that reverts PDEP-10, and keeps any parts we want to keep.",,,,,,Anecdotal,comment,,,,,,,,2024-05-08,github/Dr-Irv,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2101242054,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
"> I am not sure yet that I would want to reject, and am still leaning towards keeping in spite of some negative feedback I'm now leaning towards approving the rejection. My approval of the original PDEP was based solely on improvements to default inference for other dtypes. Despite some recent comments about this, no discussion/clarification has followed on this topic. I'd need to see some positive evidence that the original PDEP-10 authors still intend to support delivering the promised enhanc…",,,,,,Anecdotal,comment,,,,,,,,2024-05-08,github/simonjayhawkins,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2101250349,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
"> As discussed in dev meeting on 5/8/24, suggestion is to do a new PDEP that reverts PDEP-10, and keeps any parts we want to keep. Yep, I'm planning on updating this current PR to do that, so if anyone has any objections or whatever, we can still discuss here.",,,,,,Anecdotal,comment,,,,,,,,2024-05-08,github/lithomas1,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2101510041,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
Minor note - do we need to rename this PR? Right now PDEP-10 shows twice on the website ![image](https://github.com/pandas-dev/pandas/assets/609873/694d839f-3c8e-474b-87e2-b72968049403),,,,,,Anecdotal,comment,,,,,,,,2024-05-18,github/WillAyd,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2118841631,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
"Yeah, I'll probably change the name to PDEP-15 once I get around to moving this to a separate PDEP (probably tomorrow). I was travelling the past week, so didn't really have time then.",,,,,,Anecdotal,comment,,,,,,,,2024-05-18,github/lithomas1,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2119005378,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
This pull request is stale because it has been open for thirty days with no activity. Please [update](https://pandas.pydata.org/pandas-docs/stable/development/contributing.html#updating-your-pull-request) and respond to this comment if you're still interested in working on this.,,,,,,Anecdotal,comment,,,,,,,,2024-06-22,github/github-actions[bot],https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2183596117,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
@lithomas1 I think you should update the PDEP based on comments from @WillAyd so we can move this forward and have a vote.,,,,,,Anecdotal,comment,,,,,,,,2024-06-24,github/Dr-Irv,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2186701029,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
"Thanks for the reminder, this slipped off my radar. > My vote on this will heavily depend if we can come to an agreement on PDEP 14 Yeah, ideally, we'd vote on these at the same time. I haven't been involved in maintenance for a while, so no idea how far along that one is.",,,,,,Anecdotal,comment,,,,,,,,2024-06-25,github/lithomas1,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2187681741,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
"Marking as draft as although I've pushed up another commit, I haven't fully finished and refined my thoughts yet. I also need to fix the formatting issues in this PDEP.",,,,,,Anecdotal,comment,,,,,,,,2024-06-26,github/lithomas1,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2190560971,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
"@lithomas1 now that PDEP-14 is accepted, we don't need to require `pyarrow` for pandas 3.0, so we should resolve this PDEP and get a vote going.",,,,,,Anecdotal,comment,,,,,,,,2024-07-24,github/Dr-Irv,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2248642628,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
This pull request is stale because it has been open for thirty days with no activity. Please [update](https://pandas.pydata.org/pandas-docs/stable/development/contributing.html#updating-your-pull-request) and respond to this comment if you're still interested in working on this.,,,,,,Anecdotal,comment,,,,,,,,2024-11-09,github/github-actions[bot],https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2465922463,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
"If there are no objections I'll open a vote on this shortly. While in general we've been assuming that we don't want to make pyarrow a required dependency as previously agreed, no formal vote has happened (other than in other PDEPs that assumed this one was already approved). While the discussion here could continue for longer, I think it had enough iterations to be ready for a vote. CC @pandas-dev/pandas-core",,,,,,Anecdotal,comment,,,,,,,,2025-06-06,github/datapythonista,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2949384853,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
"> > I am not sure yet that I would want to reject, and am still leaning towards keeping in spite of some negative feedback > > I'm now leaning towards approving the rejection. My approval of the original PDEP was based solely on improvements to default inference for other dtypes. Despite some recent comments about this, no discussion/clarification has followed on this topic. I'd need to see some positive evidence that the original PDEP-10 authors still intend to support delivering the promised …",,,,,,Anecdotal,comment,,,,,,,,2025-06-07,github/simonjayhawkins,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2952154132,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
"This PDEP only rejects PDEP-10, and doesn't discuss how to move forward, which will have to be decided later (for example via [PDEP-16](https://github.com/pandas-dev/pandas/pull/58988) if this one is approved). So I think the sooner we vote on this, the sooner we can decide what exactly to do for pandas 3.0, and the sooner we can start working on it. I started the voting at #61596. The deadline for voting is 15 days, until June 22nd (I guess if everybody who can votes cast their vote before, we…",,,,,,Anecdotal,comment,,,,,,,,2025-06-07,github/datapythonista,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2952210595,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
"in https://github.com/pandas-dev/pandas/issues/61596#issuecomment-2952390414 @rhshadrach wrote > > A call for vote is in violation of PDEP-1. > > > * After 30 days, with a note that there is at most 30 days remaining for discussion, and that a vote will be called for if no discussion occurs in the next 15 days. > > * After 45 days, with a note that there is at most 15 days remaining for discussion, and that a vote will be called for in 15 days. > > ... > > After 30 discussion days, in case 15 d…",,,,,,Anecdotal,comment,,,,,,,,2025-06-07,github/simonjayhawkins,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2952416223,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
"I officially call the vote for this PDEP. No fully agree this is needed, as this PDEP exceeded the discussion period by almost a year, but that gives people 15 days to finish the discussion. Note that the PDEP author is not an active pandas maintainer, and I'm not aware of anyone who is planning to lead the discussion or update the PDEP with the feedback. So, while it feels just like a waste of time, I'll reopen the voting issue in 15 days. I'll also be sending an email as stated in PDEP-1, but…",,,,,,Anecdotal,comment,,,,,,,,2025-06-07,github/datapythonista,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2952428054,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
"@simonjayhawkins > I may be misunderstanding or our PDEP guidelines may need to be updated but these cases are for when we move to an early vote. The PDEP process states ""A PDEP discussion will remain open for up to 60 days."". we have far exceeded this requirement and so we are not calling an early vote? In all cases there needs to be a notice that there will be a vote, even in the case of early voting. > by sending an early reminder of 15 days remaining until the voting period starts. I do not…",,,,,,Anecdotal,comment,,,,,,,,2025-06-07,github/rhshadrach,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2952585764,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
"> I do not think this needs updating. We need to give proper notice that a vote will be taking place. Anyone with final thoughts on a PDEP needs time to voice them prior to the vote starting. I find it a paradox that we acted like PDEP-10 was already rejected immediately just because of some informal discussions, and now we need to wait one month to have confirmation if people really wanted it. Anyway, Marco brought some very good points, maybe we do benefit from some extra discussion time, we'…",,,,,,Anecdotal,comment,,,,,,,,2025-06-07,github/datapythonista,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2952595225,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
"This is a bit of a mess at this point. PDEP-10 states: > Starting in pandas 2.2, pandas raises a FutureWarning when PyArrow is not installed in the users environment when pandas is imported. This will ensure that only one warning is raised and users can easily silence it if necessary. This warning will point to the feedback issue. The feedback issue is here: https://github.com/pandas-dev/pandas/issues/54466 We then received a lot of complaints, and we held a vote on Feb. 20, 2024, at https://gi…",,,,,,Anecdotal,comment,,,,,,,,2025-06-07,github/Dr-Irv,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2952652584,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
"That vote took 6 days to be decided based on the issue log. It was won by just one vote, with no previous time for discussion, and with an ad-hoc voting not in the governance, and not discusssed in advance. Everybody seems ok with it (including myself). Can't understand the double standards compared to adding 15 extra days delay here for something that people has been discussing and thinking about for more than two years. And that it just needs a final decision so things can be unblock so devs …",,,,,,Anecdotal,comment,,,,,,,,2025-06-07,github/datapythonista,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2952695251,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
"Going through the feedback issue, the concerns voiced by users that still remain are: - Package size - Users on systems that need to compile their own wheels I too am concerned about making PyArrow a hard dependency. Some of that is based on not understanding how many users we will leave without an upgrade path, and some of it is based on what maintenance PyArrow itself will receive (I've heard the situation there is not great, but don't have a good understanding). > One option we have is to re…",,,,,,Anecdotal,comment,,,,,,,,2025-06-08,github/rhshadrach,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2954040181,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
Thanks @rhshadrach. In my mind this is not a binary issue. There are three options: 1. reject PDEP-10 and effectively remove the future intentions of improved Arrow interoperability and improvements to default inference of some new datatypes from the pandas roadmap. That's achieved by accepting this PDEP. 2. do not reject PDEP-10 and make pyArrow a required dependency in pandas 3.0. That's achieved by rejecting this PDEP. 3. postpone making pyArrow a required dependency until the next major rel…,,,,,,Anecdotal,comment,,,,,,,,2025-06-08,github/simonjayhawkins,https://github.com/pandas-dev/pandas/pull/58623#issuecomment-2954155351,repo: pandas-dev/pandas | issue: PDEP-15: Reject PDEP-10 | keyword: workaround
BUG: .duplicated() ignoring duplicates for MultiIndex ### Pandas version checks - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas. ### Reproducible Example ```python #!/usr/bi…,,,,,,Anecdotal,issue,,,,,,,,2024-11-08,github/mmatous,https://github.com/pandas-dev/pandas/issues/60236,repo: pandas-dev/pandas | keyword: workaround | state: closed
"Thanks for the report. The levels that you are setting are not compatible with the codes in the MultiIndex. The `set_levels` method only changes the levels, and they must be compatible with the codes. You can see this error by passing `verify_integrity=True`. As such you wind up with a MultiIndex that has an invalid state, it is going to give you wrong answers.",,,,,,Anecdotal,comment,,,,,,,,2024-11-08,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/60236#issuecomment-2465792872,repo: pandas-dev/pandas | issue: BUG: .duplicated() ignoring duplicates for MultiIndex | keyword: workaround
"If manipulating the index in a way that creates duplicates makes these methods return invalid results, then what is the purpose of `is_unique` or `.duplicated()`? In any case what are the recommended steps here? Basically I need to do something similar to the MRE for my df. Disable checks for it's index. Recalculate values. I know this will result in duplicates. Then I wanted to use `pd.Index.duplicated()` to drop those and bring the index back to valid state. I didn't come up with this, I took…",,,,,,Anecdotal,comment,,,,,,,,2024-11-08,github/mmatous,https://github.com/pandas-dev/pandas/issues/60236#issuecomment-2465914121,repo: pandas-dev/pandas | issue: BUG: .duplicated() ignoring duplicates for MultiIndex | keyword: workaround
"> If manipulating the index in a way that creates duplicates makes these methods return invalid results It's not that you are creating duplicates. You are disabling safety checks, and then passing invalid data. That allows the index to get into an invalid state. > In any case what are the recommended steps here? Always pass `verify_integrity=True`.",,,,,,Anecdotal,comment,,,,,,,,2024-11-09,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/60236#issuecomment-2466190304,repo: pandas-dev/pandas | issue: BUG: .duplicated() ignoring duplicates for MultiIndex | keyword: workaround
"> Always pass verify_integrity=True. That results in ``` ValueError: Level values must be unique: [0, 0, 0, 0] on level 0 ``` That's, of course, to be expected but like I said, I need to perform a calculation that temporarily results in duplicate values. Hence the `verify_integrity=False`.",,,,,,Anecdotal,comment,,,,,,,,2024-11-29,github/mmatous,https://github.com/pandas-dev/pandas/issues/60236#issuecomment-2507059253,repo: pandas-dev/pandas | issue: BUG: .duplicated() ignoring duplicates for MultiIndex | keyword: workaround
"You cannot use `set_levels` the way you are trying to. You must adhere to the requirements of a MultiIndex that pandas assumes, namely that the level values and codes must be consistent. You are making them inconsistent. When you make them inconsistent, pandas of course gives wrong results. Closing.",,,,,,Anecdotal,comment,,,,,,,,2024-11-29,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/60236#issuecomment-2507821458,repo: pandas-dev/pandas | issue: BUG: .duplicated() ignoring duplicates for MultiIndex | keyword: workaround
"OK, so for anyone who finds this in the future (and to clarify for myself) - `set_levels()` simply changes the labels, as in ""displayed names"", of the MIndex, not values in a level. - the ""real values"" of MIndex are, in fact, the mentioned `codes` which act as an indices or pointers to the array with labels called `levels`. Which is a bit counterintuitive if you ask me, I would expect `names` or `labels`, and `.set_levels()` to simply set values in a level, and label/name to be set by `.set_lab…",,,,,,Anecdotal,comment,,,,,,,,2025-07-18,github/mmatous,https://github.com/pandas-dev/pandas/issues/60236#issuecomment-3091226389,repo: pandas-dev/pandas | issue: BUG: .duplicated() ignoring duplicates for MultiIndex | keyword: workaround
"Timestamp incorrectly handles datetime64 with exotic units #### Code Sample, a copy-pastable example if possible ```python >>> pd.Timestamp(np.datetime64('2019-01-01', '6h')) Timestamp('1978-03-02 20:00:00') ``` #### Problem description The `pd.Timestamp` constructor gives the wrong result when converting a `np.datetime64` that uses multiples of a standard unit. In the example above, I create a datetime64 with units of `6h` but the conversion appears to assume units of `s`. This happens with un…",,,,,,Anecdotal,issue,,,,,,,,2019-03-08,github/cbarrick,https://github.com/pandas-dev/pandas/issues/25611,repo: pandas-dev/pandas | keyword: workaround | state: closed
"Not sure we want to do a lot to support these multiple units, but at minimum should raise an error message - thanks for the report!",,,,,,Anecdotal,comment,,,,,,,,2019-03-10,github/chris-b1,https://github.com/pandas-dev/pandas/issues/25611#issuecomment-471240557,repo: pandas-dev/pandas | issue: Timestamp incorrectly handles datetime64 with exotic units | keyword: workaround
"FWIW, I'll describe my use case that led to the bug. I deal with weather forecasts that are released every six hours. Originally, our code base used `np.datetime64` for timestamps, and the easiest way to truncate to the six hour mark was to use 6h units. When we switched to `pd.Timestamp` incrementally, we passed numpy datetimes to the constructor, and then discovered the bug. The two features provided by the numpy behavior are truncation and type safety. For both cases, the Pandas way is to ju…",,,,,,Anecdotal,comment,,,,,,,,2019-03-10,github/cbarrick,https://github.com/pandas-dev/pandas/issues/25611#issuecomment-471329897,repo: pandas-dev/pandas | issue: Timestamp incorrectly handles datetime64 with exotic units | keyword: workaround
Does this issue still need work? I would be happy to look into this further and work on the error message as previously mentioned.,,,,,,Anecdotal,comment,,,,,,,,2019-04-08,github/mukundm19,https://github.com/pandas-dev/pandas/issues/25611#issuecomment-480937903,repo: pandas-dev/pandas | issue: Timestamp incorrectly handles datetime64 with exotic units | keyword: workaround
"I'm running into a similar situation with 10-minute data from our buoys. Here is the input numpy arrary: ```python In [21]: atIndex Out[21]: array(['2018-03-23T15:10', '2018-03-23T15:20', '2018-03-23T15:30', ..., '2019-03-17T10:30', '2019-03-17T10:40', '2019-03-17T10:50'], dtype='datetime64[10m]') ``` And here is what happens when I attempt to make a pandas.DatetimeIndex with it: ```python In [23]: pandas.DatetimeIndex(atIndex) Out[23]: DatetimeIndex(['1974-10-28 08:43:00', '1974-10-28 08:44:00…",,,,,,Anecdotal,comment,,,,,,,,2019-05-23,github/darynwhite,https://github.com/pandas-dev/pandas/issues/25611#issuecomment-495359576,repo: pandas-dev/pandas | issue: Timestamp incorrectly handles datetime64 with exotic units | keyword: workaround
Casting seems fine if it's lossless. But if the values can't be represented correctly as datetime64[ns] then we should raise (I suspect we do).,,,,,,Anecdotal,comment,,,,,,,,2019-05-24,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/25611#issuecomment-495749427,repo: pandas-dev/pandas | issue: Timestamp incorrectly handles datetime64 with exotic units | keyword: workaround
"@seberg im trying to detect the exotic unit with ``` num = (<PyDatetimeScalarObject*>obj).obmeta.num if num != 1: raise ... ``` but finding in a bunch of cases, including `np.datetime64(1, ""500s"")` num comes back as 0. Is there a better way to check for this?",,,,,,Anecdotal,comment,,,,,,,,2025-07-16,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/25611#issuecomment-3079363946,repo: pandas-dev/pandas | issue: Timestamp incorrectly handles datetime64 with exotic units | keyword: workaround
"Weird, I don't think that can be true!? The 500 is indeed that num. I should try that cython code (looked at the generated code only)... There *is* a mistake in NumPy's `pxd` file here: ``` ctypedef struct PyArray_DatetimeMetaData: NPY_DATETIMEUNIT base int64_t num ``` when it should be: ``` ctypedef struct PyArray_DatetimeMetaData: NPY_DATETIMEUNIT base int num ``` but with the cython code generation spitting out C/C++, the compiler will use the NumPy definition either way and a cast to int64 …",,,,,,Anecdotal,comment,,,,,,,,2025-07-16,github/seberg,https://github.com/pandas-dev/pandas/issues/25611#issuecomment-3080267501,repo: pandas-dev/pandas | issue: Timestamp incorrectly handles datetime64 with exotic units | keyword: workaround
"ENH: support new-style float_format string in to_csv ### Feature Type - [X] Adding new functionality to pandas - [X] Changing existing functionality in pandas - [ ] Removing existing functionality in pandas ### Problem Description **The `float_format` parameter in `pd.DataFrame.to_csv` doesn't support modern (i.e. since Python 2.6) format strings yet.** The documentation says about the `float_format` parameter: > **float_format : float_formatstr, Callable, default None** Format string for float…",,,,,,Anecdotal,issue,,,,,,,,2022-11-08,github/joooeey,https://github.com/pandas-dev/pandas/issues/49580,repo: pandas-dev/pandas | keyword: workaround | state: closed
"`pandas.__version__` is `2.3.0+4.g1dfc98e16a` in pandas 2.3.0 and python 3.9, not `2.3.0` ## How to reproduce: ``` docker run --rm python:3.9 bash -c ""pip install pandas && python -c 'import pandas; print(pandas.__version__)'"" ``` ## Output: ```sh % docker run --rm python:3.9 bash -c ""pip install pandas && python -c 'import pandas; print(pandas.__version__)'"" ... # pip install logs 2.3.0+4.g1dfc98e16a ``` Seems related to https://github.com/pandas-dev/pandas/issues/61563#issuecomment-2947099734",,,,,,Anecdotal,issue,,,,,,,,2025-06-06,github/harupy,https://github.com/pandas-dev/pandas/issues/61579,repo: pandas-dev/pandas | keyword: workaround | state: closed
I can confirm this. This breaks our pandas version check function in our sdk https://github.com/shinnytech/tqsdk-ci/actions/runs/15481907073/job/43589180749 ![Image](https://github.com/user-attachments/assets/f981017f-65f0-4713-8e34-87aa2c2b62f8),,,,,,Anecdotal,comment,,,,,,,,2025-06-06,github/shinny-taojiachun,https://github.com/pandas-dev/pandas/issues/61579#issuecomment-2948194373,"repo: pandas-dev/pandas | issue: `pandas.__version__` is `2.3.0+4.g1dfc98e16a` in pandas 2.3.0 and python 3.9, not `2.3.0` | keyword: workaround"
"@harupy Thank you for your assistant. We are aware what we use in the past may not be the best practice. Newer release of our sdk should change to better version check. But releasing newer version takes a lot of time. Meanwhile, this issue is blocking our users from using our sdk, especially new installation and upgrading of our sdk.",,,,,,Anecdotal,comment,,,,,,,,2025-06-06,github/shinny-taojiachun,https://github.com/pandas-dev/pandas/issues/61579#issuecomment-2948238675,"repo: pandas-dev/pandas | issue: `pandas.__version__` is `2.3.0+4.g1dfc98e16a` in pandas 2.3.0 and python 3.9, not `2.3.0` | keyword: workaround"
"I can confirm this happens to me, only for 3.9 and pandas installed via pip (installed via conda-forge has the right version)",,,,,,Anecdotal,comment,,,,,,,,2025-06-06,github/datapythonista,https://github.com/pandas-dev/pandas/issues/61579#issuecomment-2948969583,"repo: pandas-dev/pandas | issue: `pandas.__version__` is `2.3.0+4.g1dfc98e16a` in pandas 2.3.0 and python 3.9, not `2.3.0` | keyword: workaround"
Broken `ydata-profiling` integration due to this error. ``` ../../../venv3.9/lib/python3.9/site-packages/ydata_profiling/__init__.py:10: in <module> from ydata_profiling.compare_reports import compare # isort:skip # noqa ../../../venv3.9/lib/python3.9/site-packages/ydata_profiling/compare_reports.py:12: in <module> from ydata_profiling.profile_report import ProfileReport ../../../venv3.9/lib/python3.9/site-packages/ydata_profiling/profile_report.py:26: in <module> from visions import VisionsTyp…,,,,,,Anecdotal,comment,,,,,,,,2025-06-06,github/yaricom,https://github.com/pandas-dev/pandas/issues/61579#issuecomment-2949583364,"repo: pandas-dev/pandas | issue: `pandas.__version__` is `2.3.0+4.g1dfc98e16a` in pandas 2.3.0 and python 3.9, not `2.3.0` | keyword: workaround"
"Apologies, yes this is unexpected. This was due to having to release 3.9 wheels in an unorthodox way https://github.com/pandas-dev/pandas/pull/61569 with context in https://github.com/pandas-dev/pandas/issues/61563#issuecomment-2945331441 (Removing the `good first issue` since the fix just involves releasing pandas though our normal mechanisms)",,,,,,Anecdotal,comment,,,,,,,,2025-06-06,github/mroeschke,https://github.com/pandas-dev/pandas/issues/61579#issuecomment-2949810635,"repo: pandas-dev/pandas | issue: `pandas.__version__` is `2.3.0+4.g1dfc98e16a` in pandas 2.3.0 and python 3.9, not `2.3.0` | keyword: workaround"
BUG: assert_frame_equal(check_dtype=False) fails when comparing two DFs containing pd.NA that only differ in dtype (object vs Int32) ### Pandas version checks - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-develop…,,,,,,Anecdotal,issue,,,,,,,,2025-05-21,github/michiel-de-muynck,https://github.com/pandas-dev/pandas/issues/61473,repo: pandas-dev/pandas | keyword: gotcha | state: open
"Thanks for the report, this would pass if when converting the EA to a NumPy array we cast to object dtype. I haven't looked to see if this might cause issues in other cases. Since this is aimed at tests, I'm wondering if changing to object dtype is okay here. cc @jbrockmendel @mroeschke for any thoughts.",,,,,,Anecdotal,comment,,,,,,,,2025-05-21,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/61473#issuecomment-2899238020,repo: pandas-dev/pandas | issue: BUG: assert_frame_equal(check_dtype=False) fails when comparing two DFs containing pd.NA that only differ in dtype (object vs Int32) | keyword: gotcha
> this would pass if when converting the EA to a NumPy array we cast to object dtype Yah I'm pretty sure that the behavior of `df1['x'].to_numpy()` casting to a float dtype was a much-discussed intentional decision. Changing that would be a can of worms. I'm inclined to just discourage the use of a) check_dtype=False and b) using pd.NA in an object dtype column (note that `df1 == df2` raises),,,,,,Anecdotal,comment,,,,,,,,2025-05-21,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/61473#issuecomment-2899384489,repo: pandas-dev/pandas | issue: BUG: assert_frame_equal(check_dtype=False) fails when comparing two DFs containing pd.NA that only differ in dtype (object vs Int32) | keyword: gotcha
"@jbrockmendel - sorry, I wasn't clear. I meant just inside `assert_frame_equal` to use `.to_numpy(dtype=""object"")` when `check_dtype=False` rather than just `.to_numpy()`. Agreed changing the behavior of `.to_numpy()` is off the table.",,,,,,Anecdotal,comment,,,,,,,,2025-05-23,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/61473#issuecomment-2903091710,repo: pandas-dev/pandas | issue: BUG: assert_frame_equal(check_dtype=False) fails when comparing two DFs containing pd.NA that only differ in dtype (object vs Int32) | keyword: gotcha
Hi @venturero I already raised a PR for this based on the above discussion. You can checkout other issues from the `Issues` tab and follow the [contribution guide](https://pandas.pydata.org/docs/development/contributing.html) to submit a clean fix for the issue you're tackling.,,,,,,Anecdotal,comment,,,,,,,,2025-06-01,github/iabhi4,https://github.com/pandas-dev/pandas/issues/61473#issuecomment-2927857076,repo: pandas-dev/pandas | issue: BUG: assert_frame_equal(check_dtype=False) fails when comparing two DFs containing pd.NA that only differ in dtype (object vs Int32) | keyword: gotcha
Hi everyone! I’m Arun and I’d love to contribute to this issue. I’m new here but eager to learn. Could anyone guide me on where to start or if you have suggestions for first steps? Thank you! 🐼✨,,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/Arunkumarravik,https://github.com/pandas-dev/pandas/issues/61473#issuecomment-3144112604,repo: pandas-dev/pandas | issue: BUG: assert_frame_equal(check_dtype=False) fails when comparing two DFs containing pd.NA that only differ in dtype (object vs Int32) | keyword: gotcha
> Hi everyone! I’m Arun and I’d love to contribute to this issue. I’m new here but eager to learn. Could anyone guide me on where to start or if you have suggestions for first steps? Thank you! 🐼✨ Fixed.,,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/hiyuk1,https://github.com/pandas-dev/pandas/issues/61473#issuecomment-3144817825,repo: pandas-dev/pandas | issue: BUG: assert_frame_equal(check_dtype=False) fails when comparing two DFs containing pd.NA that only differ in dtype (object vs Int32) | keyword: gotcha
"DOC: Enforce Numpy Docstring Validation (Parent Issue) Pandas has a script for validating docstrings: https://github.com/pandas-dev/pandas/blob/c468028f5c2398c04d355cef7a8b6a3952620de2/ci/code_checks.sh#L68-L1266 Currently, some methods fail some of these checks. The work will be broken up into several tickets, to address the work in issues of a more manageable size. **Issues:** [DOC: Enforce Numpy Docstring Validation | pandas.Categorical #58064](https://github.com/pandas-dev/pandas/issues/580…",,,,,,Anecdotal,issue,,,,,,,,2024-03-29,github/jordan-d-murphy,https://github.com/pandas-dev/pandas/issues/58063,repo: pandas-dev/pandas | keyword: gotcha | state: open
"@datapythonista can you take a look over this? We had discussed this a week or so ago after the refactoring in code_checks.sh ( #57879 & #57908 ) so I wanted to make sure it's in line with what you were thinking. if it looks good to you, I'll go ahead and create issues for the rest of the method categories. Also, I did try to add labels to the child issues, but It doesn't seem to let me for some reason.",,,,,,Anecdotal,comment,,,,,,,,2024-03-29,github/jordan-d-murphy,https://github.com/pandas-dev/pandas/issues/58063#issuecomment-2026802214,repo: pandas-dev/pandas | issue: DOC: Enforce Numpy Docstring Validation (Parent Issue) | keyword: gotcha
"`--error` was removed as an argument of `scripts/validate_docstrings.py` in #57879 so the example command won't work. People can exclude it, instead checking with: `scripts/validate_docstrings.py --format=actions <method-name>`",,,,,,Anecdotal,comment,,,,,,,,2024-03-29,github/ThomasBur,https://github.com/pandas-dev/pandas/issues/58063#issuecomment-2027003664,repo: pandas-dev/pandas | issue: DOC: Enforce Numpy Docstring Validation (Parent Issue) | keyword: gotcha
"@ThomasBur I updated the example above, can you confirm it looks correct? (I've updated the examples in the other issues with the same fix)",,,,,,Anecdotal,comment,,,,,,,,2024-03-29,github/jordan-d-murphy,https://github.com/pandas-dev/pandas/issues/58063#issuecomment-2027477384,repo: pandas-dev/pandas | issue: DOC: Enforce Numpy Docstring Validation (Parent Issue) | keyword: gotcha
"Looks good, but you'll need to start the command with `python` (followed by `scripts/validate_docstrings.py --format=actions <method-name>`), even though it might seem obvious it's better to be clear.",,,,,,Anecdotal,comment,,,,,,,,2024-03-29,github/Aloqeely,https://github.com/pandas-dev/pandas/issues/58063#issuecomment-2027607599,repo: pandas-dev/pandas | issue: DOC: Enforce Numpy Docstring Validation (Parent Issue) | keyword: gotcha
"I think `python` is not required if the development virtual environment is activated, which I believe we should be encouraging.",,,,,,Anecdotal,comment,,,,,,,,2024-03-29,github/jordan-d-murphy,https://github.com/pandas-dev/pandas/issues/58063#issuecomment-2027657810,repo: pandas-dev/pandas | issue: DOC: Enforce Numpy Docstring Validation (Parent Issue) | keyword: gotcha
@jordan-d-murphy Mr murphy can you please add https://github.com/pandas-dev/pandas/issues/58498 to the body of the parent issue?,,,,,,Anecdotal,comment,,,,,,,,2024-04-30,github/tuhinsharma121,https://github.com/pandas-dev/pandas/issues/58063#issuecomment-2086430636,repo: pandas-dev/pandas | issue: DOC: Enforce Numpy Docstring Validation (Parent Issue) | keyword: gotcha
**EDIT:** My original targets were already completed. Pivoting to a new set of tasks from this issue. I would like to work on validating the docstrings for the following `tseries.offsets.BusinessDay` methods/properties: * `pandas.tseries.offsets.BusinessDay` * `pandas.tseries.offsets.BusinessDay.calendar` * `pandas.tseries.offsets.BusinessDay.holidays` * `pandas.tseries.offsets.BusinessDay.weekmask`,,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/doughillyer,https://github.com/pandas-dev/pandas/issues/58063#issuecomment-3130151042,repo: pandas-dev/pandas | issue: DOC: Enforce Numpy Docstring Validation (Parent Issue) | keyword: gotcha
"BUG/API: round-tripping non-nano datetime64s with to_json/read_json ```python import pandas as pd import pandas._testing as tm from io import StringIO df = pd.DataFrame( { ""A"": pd.to_datetime([""2013-01-01"", ""2013-01-02""]).as_unit(""s""), ""B"": [3.5, 3.5], } ) written = df.to_json(orient=""split"") >>> written '{""A"":{""0"":1356,""1"":1357},""B"":{""0"":3.5,""1"":3.5}}' result = pd.read_json(StringIO(written), orient=""split"", convert_dates=[""A""]) >>> result A B 0 1356 3.5 1 1357 3.5 tm.assert_frame_equal(result…",,,,,,Anecdotal,issue,,,,,,,,2023-11-04,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/55827,repo: pandas-dev/pandas | keyword: gotcha | state: open
Hmm shouldn't to_json be writing this as a date to begin with? I would be hesitant to make any guarantees on lossless-ness if we are just writing out the integer in to_json,,,,,,Anecdotal,comment,,,,,,,,2023-11-04,github/WillAyd,https://github.com/pandas-dev/pandas/issues/55827#issuecomment-1793504411,repo: pandas-dev/pandas | issue: BUG/API: round-tripping non-nano datetime64s with to_json/read_json | keyword: gotcha
"> shouldn't to_json be writing this as a date to begin with? no idea about the ""should"", but it isn't. still integers if we don't do the .as_unit (though surprisingly, not quite just multiply-by-`10**9`)",,,,,,Anecdotal,comment,,,,,,,,2023-11-04,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/55827#issuecomment-1793532981,repo: pandas-dev/pandas | issue: BUG/API: round-tripping non-nano datetime64s with to_json/read_json | keyword: gotcha
"Gotcha. Well I think that's a bug we need to investigate more with to_json, though ultimately even when we write dates I think we'd have the question as to what precision to read in since that is not going to be embedded in the JSON. My initial thought is that we should just stick with `ns` as the default unless a user cares to specify. The alternative I think is to find the best precision to fit a date as it is read in, but I'd be happy to defer that until someone really requests it",,,,,,Anecdotal,comment,,,,,,,,2023-11-04,github/WillAyd,https://github.com/pandas-dev/pandas/issues/55827#issuecomment-1793536942,repo: pandas-dev/pandas | issue: BUG/API: round-tripping non-nano datetime64s with to_json/read_json | keyword: gotcha
"Coming back here after @jbrockmendel comment https://github.com/pandas-dev/pandas/pull/55901#issuecomment-1836490661 - thanks for bringing this full circle OK my current point of view with more context is when we write integral values with to_json we should stick with the ""historic precedent"" of nanosecond. Ideally a user writes ISO strings, but when not I think there are too many different ways to interpret this data on a roundtrip, none seemingly better or worse than the other. So in that cas…",,,,,,Anecdotal,comment,,,,,,,,2023-12-01,github/WillAyd,https://github.com/pandas-dev/pandas/issues/55827#issuecomment-1836505925,repo: pandas-dev/pandas | issue: BUG/API: round-tripping non-nano datetime64s with to_json/read_json | keyword: gotcha
"I was only thinking about the integer case outlined in the OP, but that's a good point on the string roundtripping. Looks like ISO 8601 allows decimal fractions that can be used to determine the unit",,,,,,Anecdotal,comment,,,,,,,,2023-12-03,github/WillAyd,https://github.com/pandas-dev/pandas/issues/55827#issuecomment-1837599470,repo: pandas-dev/pandas | issue: BUG/API: round-tripping non-nano datetime64s with to_json/read_json | keyword: gotcha
"method `df.merge` is resetting the index method `df.merge` will throw away / reset the index of `df`. I dont think this is desired result. here's an example: ```python df = pd.DataFrame({'key' : [1,2,3]}, index=['very','important','index']) df2 = pd.DataFrame({'key' : [3,2,1], 'data':['c','b','a']}, index=[100,200,300]) df.merge(df2, on='key') ``` the result will lose the `very important index` in `df` and instead will have a generic `0 1 2` index. ``` key data 0 1 a 1 2 b 2 3 c ``` here is a m…",,,,,,Anecdotal,issue,,,,,,,,2023-03-05,github/aviadr1,https://github.com/pandas-dev/pandas/issues/51796,repo: pandas-dev/pandas | keyword: gotcha | state: open
have a look at the [docs](https://pandas.pydata.org/docs/user_guide/merging.html#brief-primer-on-merge-methods-relational-algebra) regarding `merge` and caveats on index preservation,,,,,,Anecdotal,comment,,,,,,,,2023-03-05,github/samukweku,https://github.com/pandas-dev/pandas/issues/51796#issuecomment-1455052303,repo: pandas-dev/pandas | issue: method `df.merge` is resetting the index | keyword: gotcha
"I see, thanks for the quick reply! I usually read the reference API docs and havent read through the user guide. this appears rather as a footnote in the user guide, and I found this behavior to be unexpected. note, that in the example, the join is in practice a 1-to-1 join as the keys are unique. IMHO having this documented in the `merge` reference to having a parameter to control this behavior would be great. for instance a `reindexing` parameter could be handy. it could have the values: - `l…",,,,,,Anecdotal,comment,,,,,,,,2023-03-05,github/aviadr1,https://github.com/pandas-dev/pandas/issues/51796#issuecomment-1455075981,repo: pandas-dev/pandas | issue: method `df.merge` is resetting the index | keyword: gotcha
"The only way to preserve the left index seems to be to reset_index before the merge and then set_index after, which surely defeats the purpose of the index which is to make joins faster?",,,,,,Anecdotal,comment,,,,,,,,2025-04-20,github/quantoid,https://github.com/pandas-dev/pandas/issues/51796#issuecomment-2817155509,repo: pandas-dev/pandas | issue: method `df.merge` is resetting the index | keyword: gotcha
"Well, it seems that we should just set the index on the right DataFrame before merging with `right_index=True`. As long as one index is present and it's a many to one join, this will preserve the left DataFrame's index. So for example, in the OP's example, we don't need any funny `reset_index` or whatsoever: ```python df = pd.DataFrame({""key"": [1, 2, 3]}, index=[""very"", ""important"", ""index""]) df2 = pd.DataFrame({ ""key"": [3, 2, 1], ""data"": [""c"", ""b"", ""a""] }, index=[100, 200, 300]) print(df.merge…",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/limwz01,https://github.com/pandas-dev/pandas/issues/51796#issuecomment-3125729296,repo: pandas-dev/pandas | issue: method `df.merge` is resetting the index | keyword: gotcha
"DOC: section on caveats of storing lists inside DataFrame/Series xref to a lot of issues, for example #16864 I think we could use a doc section stating storing nested lists/arrays inside a pandas object is preferred to be avoided, showing the downsides (perf, memory use) and a worked out example of an alternative. This seems to be earned knowledge that many have, but not sure we do a good job stating it clearly. Closely related, might also benefit from a little section encouraging use of Python…",,,,,,Anecdotal,issue,,,,,,,,2017-07-19,github/chris-b1,https://github.com/pandas-dev/pandas/issues/17027,repo: pandas-dev/pandas | keyword: gotcha | state: open
"I'd be happy to take this, just not sure what ""a worked out example of an alternative"" would look like? I've found a few discussions around storing lists in Dataframe cells and none of them discouraged it. This discussion on Stack Overflow is the only one I've found with alternatives: https://stackoverflow.com/questions/39661198/optimal-way-to-add-small-lists-to-pandas-dataframe. Which is the best option? Or is there another, better option? Thanks.",,,,,,Anecdotal,comment,,,,,,,,2017-12-30,github/pdpark,https://github.com/pandas-dev/pandas/issues/17027#issuecomment-354528081,repo: pandas-dev/pandas | issue: DOC: section on caveats of storing lists inside DataFrame/Series | keyword: gotcha
"https://stackoverflow.com/questions/45587778/python-explode-rows-from-panda-dataframe https://stackoverflow.com/questions/44361160/explode-a-csv-in-python https://stackoverflow.com/questions/38428796/how-to-do-lateral-view-explode-in-pandas FYI, the timings are suspect of course, these examples don't use a large enough frame to actually matter. https://github.com/pandas-dev/pandas/issues/16538 We should make a small section on this. Also should prob just write ``.explode`` :< (note for strings …",,,,,,Anecdotal,comment,,,,,,,,2017-12-30,github/jreback,https://github.com/pandas-dev/pandas/issues/17027#issuecomment-354546265,repo: pandas-dev/pandas | issue: DOC: section on caveats of storing lists inside DataFrame/Series | keyword: gotcha
more refs https://github.com/pandas-dev/pandas/issues/8517 - http://www.markhneedham.com/blog/2015/03/23/python-equivalent-to-flatmap-for-flattening-an-array-of-arrays/ - https://stackoverflow.com/questions/31080258/pysparks-flatmap-in-pandas - https://stackoverflow.com/questions/32468402/how-to-explode-a-list-inside-a-dataframe-cell-into-separate-rows,,,,,,Anecdotal,comment,,,,,,,,2017-12-30,github/jreback,https://github.com/pandas-dev/pandas/issues/17027#issuecomment-354546399,repo: pandas-dev/pandas | issue: DOC: section on caveats of storing lists inside DataFrame/Series | keyword: gotcha
"This is pretty idiomatic / efficient. ``` (pd.melt(df.nearest_neighbors.apply(pd.Series).reset_index(), id_vars=['name', 'opponent'], value_name='nearest_neighbors') .set_index(['name', 'opponent']) .drop('variable', axis=1) .dropna() .sort_index() ) ```",,,,,,Anecdotal,comment,,,,,,,,2017-12-30,github/jreback,https://github.com/pandas-dev/pandas/issues/17027#issuecomment-354546508,repo: pandas-dev/pandas | issue: DOC: section on caveats of storing lists inside DataFrame/Series | keyword: gotcha
"Just want to clarify something: this issue was opened with the intent, as I understand it, to document the fact that storing lists in dataframes is not ideal. However, the examples above are all about how to explode lists stored in data frames. Is the recommended approach to create a temporary data frame with lists in order to create the preferred dataframe without lists?",,,,,,Anecdotal,comment,,,,,,,,2018-01-01,github/pdpark,https://github.com/pandas-dev/pandas/issues/17027#issuecomment-354634606,repo: pandas-dev/pandas | issue: DOC: section on caveats of storing lists inside DataFrame/Series | keyword: gotcha
no a long form dataframe is ideal from a performance and idiomatic perspective. those examples are illustrative of what to do if they already have lists point is that you shouldn’t have them in the first place; if you do then you invariable need to convert them anyways,,,,,,Anecdotal,comment,,,,,,,,2018-01-01,github/jreback,https://github.com/pandas-dev/pandas/issues/17027#issuecomment-354635763,repo: pandas-dev/pandas | issue: DOC: section on caveats of storing lists inside DataFrame/Series | keyword: gotcha
"This example, also from here: https://stackoverflow.com/a/46161733, seems simpler/easier to understand? (df.nearest_neighbors.apply(pd.Series) .stack() .reset_index(level=2, drop=True) .to_frame('nearest_neighbors')) Any reason not to prefer it as the canonical example?",,,,,,Anecdotal,comment,,,,,,,,2018-01-02,github/pdpark,https://github.com/pandas-dev/pandas/issues/17027#issuecomment-354691239,repo: pandas-dev/pandas | issue: DOC: section on caveats of storing lists inside DataFrame/Series | keyword: gotcha
"I want to include an example of doing an ""explosion"" without creating an intermediary df with lists in cells. Here's my example - what do you think? df = (pd.DataFrame(OrderedDict([('name', ['A.J. Price']*3), ('opponent', ['76ers', 'blazers', 'bobcats']), ('attribute x', ['A','B','C']) ]) )) nn = [['Zach LaVine', 'Jeremy Lin', 'Nate Robinson', 'Isaia']]*3 df2 = pd.concat([df[['name','opponent']], pd.DataFrame(nn)], axis=1) df3 = (df2.set_index(['name', 'opponent']) .stack() .reset_index(level=2…",,,,,,Anecdotal,comment,,,,,,,,2018-01-03,github/pdpark,https://github.com/pandas-dev/pandas/issues/17027#issuecomment-354923957,repo: pandas-dev/pandas | issue: DOC: section on caveats of storing lists inside DataFrame/Series | keyword: gotcha
"Hi! I’d like to help with this doc issue by adding a section to the gotchas guide. It would explain why storing lists in DataFrame or Series cells is discouraged, and show better approaches like using `explode()` or `apply(pd.Series)`. Let me know if there are examples or notes you’d like included!",,,,,,Anecdotal,comment,,,,,,,,2025-07-26,github/gumus-g,https://github.com/pandas-dev/pandas/issues/17027#issuecomment-3122269356,repo: pandas-dev/pandas | issue: DOC: section on caveats of storing lists inside DataFrame/Series | keyword: gotcha
"String dtype: backwards compatibility of selecting ""object"" vs ""str"" columns in `select_dtypes` We provide the `DataFrame.select_dtypes()` method to easily subset columns based on data types (groups). See https://pandas.pydata.org/pandas-docs/version/2.3/user_guide/basics.html#selecting-columns-based-on-dtype At the moment, as documented, the select string columns you must use the `object` dtype: ```python >>> pd.options.future.infer_string = False >>> df = pd.DataFrame( ... { ... ""string"": lis…",,,,,,Anecdotal,issue,,,,,,,,2025-07-21,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/61916,repo: pandas-dev/pandas | keyword: gotcha | state: open
"Since a lot of systems likely rely on `select_dtypes(include=[object])` returning string columns, I think we should maintain backwards compatibility in 3.0, but emit a `FutureWarning` when `str` columns are implicitly selected. That avoids silent breakage while giving users time to update. In future versions, we can deprecate this behavior cleanly.",,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/arthurlw,https://github.com/pandas-dev/pandas/issues/61916#issuecomment-3100637404,"repo: pandas-dev/pandas | issue: String dtype: backwards compatibility of selecting ""object"" vs ""str"" columns in `select_dtypes` | keyword: gotcha"
"Briefly discussed this at the dev meeting as well, and general agreement with your suggestion. Let's keep the behaviour of `object` selecting string columns, and warn if that happens",,,,,,Anecdotal,comment,,,,,,,,2025-07-26,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/61916#issuecomment-3121563697,"repo: pandas-dev/pandas | issue: String dtype: backwards compatibility of selecting ""object"" vs ""str"" columns in `select_dtypes` | keyword: gotcha"
"BUG: Fix Series.str.zfill for ArrowDtype string arrays #61485 Implemented `_str_zfill` for `ArrowExtensionArray` to support `Series.str.zfill` on Arrow-backed string arrays (`ArrowDtype(pa.string())`). This fixes an AttributeError due to the method relying on `_str_map`, which wasn't implemented. Used `_apply_elementwise` to match the approach of other string methods. Added tests under `test_string_array.py` and confirmed they pass. Also confirmed no other relevant test files are broken and the…",,,,,,Anecdotal,issue,,,,,,,,2025-06-01,github/iabhi4,https://github.com/pandas-dev/pandas/pull/61533,repo: pandas-dev/pandas | keyword: gotcha | state: open
This pull request is stale because it has been open for thirty days with no activity. Please [update](https://pandas.pydata.org/pandas-docs/stable/development/contributing.html#updating-your-pull-request) and respond to this comment if you're still interested in working on this.,,,,,,Anecdotal,comment,,,,,,,,2025-07-17,github/github-actions[bot],https://github.com/pandas-dev/pandas/pull/61533#issuecomment-3081876838,repo: pandas-dev/pandas | issue: BUG: Fix Series.str.zfill for ArrowDtype string arrays #61485 | keyword: gotcha
BUG: Constructing series with Timedelta object results in datetime series ### Pandas version checks - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas. ### Reproducible Example…,,,,,,Anecdotal,issue,,,,,,,,2025-04-27,github/Casper-Guo,https://github.com/pandas-dev/pandas/issues/61365,repo: pandas-dev/pandas | keyword: gotcha | state: open
"I did some investigating and found that for datetime-related types (datetime64, timedelta64, etc) with the value `pd.NaT`, pandas stores them all as `<class 'pandas._libs.tslibs.nattype.NaTType'>`. This makes it impossible to differentiate between a `Timestamp(""NaT"")` and a `Timedelta(""NaT"")` during Series construction.",,,,,,Anecdotal,comment,,,,,,,,2025-04-30,github/arthurlw,https://github.com/pandas-dev/pandas/issues/61365#issuecomment-2840952840,repo: pandas-dev/pandas | issue: BUG: Constructing series with Timedelta object results in datetime series | keyword: gotcha
"I also traced the source code and it looks like the `dtype_if_all_nat` is assigned by `dtype_if_all_nat` in this case. As @arthurlw mentioned, there's no reliable way to determine whether a `NaTType` originates from a `Timestamp` or a `Timedelta`. 😢 https://github.com/pandas-dev/pandas/blob/337d40e5d55f7787e48f029486f47fd5a053bc80/pandas/core/dtypes/cast.py#L1195-L1206 https://github.com/pandas-dev/pandas/blob/337d40e5d55f7787e48f029486f47fd5a053bc80/pandas/_libs/lib.pyx#L2808-L2831",,,,,,Anecdotal,comment,,,,,,,,2025-05-04,github/chilin0525,https://github.com/pandas-dev/pandas/issues/61365#issuecomment-2849284328,repo: pandas-dev/pandas | issue: BUG: Constructing series with Timedelta object results in datetime series | keyword: gotcha
"Add support for ""regex"" library #### Code Sample, a copy-pastable example if possible ```python import re import pandas as pd import regex df = pd.DataFrame({""a"": [1, 2, 3], ""b"": [""a"", ""1"", ""2""]}) pattern = r""\d"" df.b.str.match(pattern) df.b.str.match(re.compile(pattern)) df.b.str.match(regex.compile(pattern)) # throws typeError ``` ``` --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-65-eec2b9ae9613> in <modu…",,,,,,Anecdotal,issue,,,,,,,,2018-08-24,github/pmav99,https://github.com/pandas-dev/pandas/issues/22496,repo: pandas-dev/pandas | keyword: gotcha | state: open
"No they are not. ``` python import re import typing import regex re_pat = re.compile(r""\d"") regex_pat = regex.compile(r""\d"") re_pat.__class__.mro() # [_sre.SRE_Pattern, object] isinstance(re_pat, typing.Pattern) # True regex_pat.__class__.mro() # [_regex.Pattern, object] isinstance(regex_pat, typing.Pattern) # False ```",,,,,,Anecdotal,comment,,,,,,,,2018-08-24,github/pmav99,https://github.com/pandas-dev/pandas/issues/22496#issuecomment-415756741,"repo: pandas-dev/pandas | issue: Add support for ""regex"" library | keyword: gotcha"
"@madimov, I think I used vanila `re` for pandas, and regex for everything else. Not nice ,but there was no feedback and I needed to move on.",,,,,,Anecdotal,comment,,,,,,,,2019-07-26,github/pmav99,https://github.com/pandas-dev/pandas/issues/22496#issuecomment-515495933,"repo: pandas-dev/pandas | issue: Add support for ""regex"" library | keyword: gotcha"
"We could probably optionally import regex and append it’s type to the re types we handle. > On Jul 26, 2019, at 10:23, pmav99 <notifications@github.com> wrote: > > @madimov, I think I used vanila re for pandas, and regex for everything else. Not nice ,but there was no feedback and I needed to move on. > > — > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",,,,,,Anecdotal,comment,,,,,,,,2019-07-26,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/22496#issuecomment-515518510,"repo: pandas-dev/pandas | issue: Add support for ""regex"" library | keyword: gotcha"
"No, I won't have time. On Mon, Jul 29, 2019 at 3:21 AM Miko Dimov <notifications@github.com> wrote: > @TomAugspurger <https://github.com/TomAugspurger> that would be great! > Might you have the time to give it a go? > > — > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub > <https://github.com/pandas-dev/pandas/issues/22496?email_source=notifications&email_token=AAKAOITEIYARGCLBCFFOYVLQB2SB5A5CNFSM4FRLSLXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMV…",,,,,,Anecdotal,comment,,,,,,,,2019-07-29,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/22496#issuecomment-515945773,"repo: pandas-dev/pandas | issue: Add support for ""regex"" library | keyword: gotcha"
"@TomAugspurger > Are patterns compiled by regex instances of typing.re.Pattern? If the answer to this is ""no"", then that's an upstream bug IMO.",,,,,,Anecdotal,comment,,,,,,,,2019-09-23,github/gwerbin,https://github.com/pandas-dev/pandas/issues/22496#issuecomment-533942940,"repo: pandas-dev/pandas | issue: Add support for ""regex"" library | keyword: gotcha"
"That said, here is my attempt at a fix: https://github.com/pandas-dev/pandas/compare/master...gwerbin:patch-2 Just made the edits here on Github, so haven't actually run any tests yet.",,,,,,Anecdotal,comment,,,,,,,,2019-09-23,github/gwerbin,https://github.com/pandas-dev/pandas/issues/22496#issuecomment-533943962,"repo: pandas-dev/pandas | issue: Add support for ""regex"" library | keyword: gotcha"
@jbrockmendel did you take a look at my proposed patch? It will probably need a major rebase obviously. Just want to make sure what I did is an acceptable approach before I put more time into it.,,,,,,Anecdotal,comment,,,,,,,,2020-04-17,github/gwerbin,https://github.com/pandas-dev/pandas/issues/22496#issuecomment-615331326,"repo: pandas-dev/pandas | issue: Add support for ""regex"" library | keyword: gotcha"
That looks roughly correct. You'll need to update some of the CI envs in `ci/deps` to include `regex` and skip the test if it isn't present.,,,,,,Anecdotal,comment,,,,,,,,2020-04-17,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/22496#issuecomment-615383573,"repo: pandas-dev/pandas | issue: Add support for ""regex"" library | keyword: gotcha"
"@jreback I'm not an experienced Pythonst. But I see that someone else has already proposed an easy solution just a few comments above, so I assumed it would be just as easy to submit a PR containing that code for you experts.",,,,,,Anecdotal,comment,,,,,,,,2021-03-18,github/lucazav,https://github.com/pandas-dev/pandas/issues/22496#issuecomment-801893259,"repo: pandas-dev/pandas | issue: Add support for ""regex"" library | keyword: gotcha"
@lucazav and someone needs to make an actual pull request with testing and documentation core devs can provide code review,,,,,,Anecdotal,comment,,,,,,,,2021-03-18,github/jreback,https://github.com/pandas-dev/pandas/issues/22496#issuecomment-801902558,"repo: pandas-dev/pandas | issue: Add support for ""regex"" library | keyword: gotcha"
"I totally forgot about this. I am willing to take the lead on this, going through the effort to update the docs, run the test suite, etc. *However*, I think my patch is a hack around the fact that `regex` objects are not instances of `typing.Pattern`. I can think of of two solutions that are better than the one I originally proposed: 1. Use a [runtime-checkable `typing.Protocol`](https://docs.python.org/3/library/typing.html#typing.Protocol) that covers the relevant methods and attributes used …",,,,,,Anecdotal,comment,,,,,,,,2021-03-18,github/gwerbin,https://github.com/pandas-dev/pandas/issues/22496#issuecomment-801990458,"repo: pandas-dev/pandas | issue: Add support for ""regex"" library | keyword: gotcha"
"> The reason I believe a generic solution is better than a regex-specific solution is that there are yet other regex libraries that someone might want to use (e.g. RE2). @gwerbin Above is so true. I wish I could use `pythonnet`'s regex engine because I have a few modules in C# and a few in Python and I want to use a single regex engine for both. Basically, we need to be able to switch the internal regex engine used for pandas' string methods.",,,,,,Anecdotal,comment,,,,,,,,2023-03-31,github/alegend4u,https://github.com/pandas-dev/pandas/issues/22496#issuecomment-1491192631,"repo: pandas-dev/pandas | issue: Add support for ""regex"" library | keyword: gotcha"
> we need to be able to switch the internal regex engine used for pandas' string methods. Like `pcre2`. See a [comparison of language features](https://en.wikipedia.org/w/index.php?title=Comparison_of_regular_expression_engines#Language_features) for regular expression engines.,,,,,,Anecdotal,comment,,,,,,,,2024-01-14,github/rootsmusic,https://github.com/pandas-dev/pandas/issues/22496#issuecomment-1891016557,"repo: pandas-dev/pandas | issue: Add support for ""regex"" library | keyword: gotcha"
"Known differences between [`google-RE2`](https://pypi.org/project/google-re2/#description) and `re`: > various PCRE features (e.g. backreferences, look-around assertions) are not supported. See the [canonical reference](https://github.com/google/re2/wiki/Syntax), but known syntactic ""gotchas"" relative to Python are: > * PCRE supports \Z and \z; RE2 supports \z; Python supports \z, but calls it \Z. You must rewrite \Z to \z in pattern strings. > * The error class does not provide any error infor…",,,,,,Anecdotal,comment,,,,,,,,2024-01-14,github/rootsmusic,https://github.com/pandas-dev/pandas/issues/22496#issuecomment-1891019322,"repo: pandas-dev/pandas | issue: Add support for ""regex"" library | keyword: gotcha"
"I recently had an application where I needed much faster regular expressions in pandas and here are a few of the workarounds I found: # Using The apply Method apply is typically slow, but since the execution time of re2 is so much faster this comes out way ahead. ``` import re2 import pandas def _apply_re2(value, regex): return True if value is not pandas.NA and regex.fullmatch(value) else False regex = re2.compile(r'asdf') series = pandas.Series(['qwer', 'asdf']) regex_match = series.apply(_ap…",,,,,,Anecdotal,comment,,,,,,,,2025-03-04,github/ptth222,https://github.com/pandas-dev/pandas/issues/22496#issuecomment-2698608729,"repo: pandas-dev/pandas | issue: Add support for ""regex"" library | keyword: gotcha"
"DOC: Comparing .loc/.iloc to tuples and chained indexing ### Pandas version checks - [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/) ### Location of the documentation https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#indexing-view-versus-copy ### Documentation problem ``` import pandas as pd # Creating a DataFrame with some sample data data = { 'Name': ['Jason', 'Emma', 'Alex', 'Sarah'], …",,,,,,Anecdotal,issue,,,,,,,,2024-12-31,github/joansigh,https://github.com/pandas-dev/pandas/issues/60632,repo: pandas-dev/pandas | keyword: gotcha | state: open
"Thanks for the report! > The documentation mentions how .iloc/.loc is a better option. For example, something such as the following. > > `df.loc[df['Name'] == 'Jason', 'Age'] = 29` > > However it is not clear about best practices regarding tuples, such as the following. > > `df[('Age', df['Name'] == 'Jason')] = 29 ` The following two lines are equivalent: ```python df[('Age', df['Name'] == 'Jason')] = 29 df['Age', df['Name'] == 'Jason'] = 29 ``` That is, the argument in the 2nd line above being…",,,,,,Anecdotal,comment,,,,,,,,2024-12-31,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/60632#issuecomment-2566584368,repo: pandas-dev/pandas | issue: DOC: Comparing .loc/.iloc to tuples and chained indexing | keyword: gotcha
"Gotcha, so anything with a comma is a tuple? Even though my two examples both involve tuples, the example that included loc would still be preferred because it is faster than the other example that didn't use loc?",,,,,,Anecdotal,comment,,,,,,,,2025-01-01,github/joansigh,https://github.com/pandas-dev/pandas/issues/60632#issuecomment-2567140936,repo: pandas-dev/pandas | issue: DOC: Comparing .loc/.iloc to tuples and chained indexing | keyword: gotcha
"ENH: `DataFrame.struct.explode(column, *, separator=""."")` method to pull struct subfields into the parent DataFrame ### Feature Type - [X] Adding new functionality to pandas - [ ] Changing existing functionality in pandas - [ ] Removing existing functionality in pandas ### Problem Description Currently, I can use `Series.struct.explode()` to create a `DataFrame` out of the subfields of a `ArrowDtype(pa.struct(...))` column. Joining these back to the original `DataFrame` is a little awkward. It'…",,,,,,Anecdotal,issue,,,,,,,,2024-08-23,github/tswast,https://github.com/pandas-dev/pandas/issues/59585,repo: pandas-dev/pandas | keyword: gotcha | state: open
"Thanks for the request. From an API design perspective, having a dtype-specific accessor on `DataFrame` seems like a bad approach. For the feature itself, it seems to me you can use `Series.struct.explode` along with `pd.concat([df, ...,], axis=1)`. Does this not work in your use-case?",,,,,,Anecdotal,comment,,,,,,,,2024-08-25,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/59585#issuecomment-2308854520,"repo: pandas-dev/pandas | issue: ENH: `DataFrame.struct.explode(column, *, separator=""."")` method to pull struct subfields into the parent DataFrame | keyword: gotcha"
"> From an API design perspective, having a dtype-specific accessor on DataFrame seems like a bad approach. Gotcha. I did see we have some already. e.g. [SparseFrameAccessor](https://github.com/pandas-dev/pandas/blob/6fa4eb43fbf01d558c9e8cd0fdde6fa5359c9d19/pandas/core/arrays/sparse/accessor.py#L242), which does seem [SparseDtype](https://pandas.pydata.org/docs/user_guide/sparse.html#sparsedtype) specific. > For the feature itself, it seems to me you can use Series.struct.explode along with pd.c…",,,,,,Anecdotal,comment,,,,,,,,2024-08-26,github/tswast,https://github.com/pandas-dev/pandas/issues/59585#issuecomment-2310762325,"repo: pandas-dev/pandas | issue: ENH: `DataFrame.struct.explode(column, *, separator=""."")` method to pull struct subfields into the parent DataFrame | keyword: gotcha"
"DOC: Categorical ""Memory Usage"" uses nbytes instead of `memory_usage(deep=True)` ### Pandas version checks - [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/) ### Location of the documentation https://pandas.pydata.org/docs/dev/user_guide/categorical.html#memory-usage ### Documentation problem The ""Memory Usage"" section states that the memory usage of an `object` dtype is a constant times the length of the dat…",,,,,,Anecdotal,issue,,,,,,,,2022-09-07,github/tehunter,https://github.com/pandas-dev/pandas/issues/48438,repo: pandas-dev/pandas | keyword: gotcha | state: open
BUG: Roundtripping bytes through pandas dataframes leads to data loss ### Pandas version checks - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [x] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas. ### Reproducible Example ```…,,,,,,Anecdotal,issue,,,,,,,,2024-04-10,github/maxburke,https://github.com/pandas-dev/pandas/issues/58205,repo: pandas-dev/pandas | keyword: gotcha | state: open
"**Issue Explanation:** The reported issue involves a problem with round-tripping bytes through Pandas dataframes, which results in data loss. Specifically, when hashing a string and converting it to bytes within a Pandas dataframe, there's an unexpected behavior where the trailing zero byte is dropped. This behavior is not intended and leads to inconsistencies in the stored data. **Reproducible Example:** ```python import hashlib import pandas as pd def hash_id(dgu_id: str) -> bytes: h = hashli…",,,,,,Anecdotal,comment,,,,,,,,2024-04-10,github/aabhinavg,https://github.com/pandas-dev/pandas/issues/58205#issuecomment-2046308158,repo: pandas-dev/pandas | issue: BUG: Roundtripping bytes through pandas dataframes leads to data loss | keyword: gotcha
"Dear @maxburke, Thank you for your insightful feedback. I acknowledge your concerns regarding the proposed solution. While converting byte objects to hexadecimal strings does indeed mitigate the immediate issue of preserving the trailing zero byte, I recognize that it might not completely resolve the underlying problem of data loss. However, I believe that this approach can serve as a beneficial interim solution while we delve deeper into the root cause of the data loss. By preserving the integ…",,,,,,Anecdotal,comment,,,,,,,,2024-04-10,github/aabhinavg,https://github.com/pandas-dev/pandas/issues/58205#issuecomment-2047295037,repo: pandas-dev/pandas | issue: BUG: Roundtripping bytes through pandas dataframes leads to data loss | keyword: gotcha
"If Pandas doesn't allow `bytes`-type objects to be inserted in dataframes then it should error out. If it _does_ allow `bytes`-type objects to be inserted into dataframes, then the data should be returned without loss.",,,,,,Anecdotal,comment,,,,,,,,2024-04-10,github/maxburke,https://github.com/pandas-dev/pandas/issues/58205#issuecomment-2047752310,repo: pandas-dev/pandas | issue: BUG: Roundtripping bytes through pandas dataframes leads to data loss | keyword: gotcha
"Hi @maxburke While debugging what i observe that During the data type conversion process using the `astype` method, trailing null bytes (`\x00`) are being removed from the input data. Inside file astype.py at line 183 I have added print statement to see the output ```python if copy or arr.dtype == object or dtype == object: # Explicit copy, or required since NumPy can't view from / to object. print(""Value of the array is "",arr) print(""Value of the arr.astype(dtype, copy=True)"", arr.astype(dtype…",,,,,,Anecdotal,comment,,,,,,,,2024-04-13,github/aabhinavg,https://github.com/pandas-dev/pandas/issues/58205#issuecomment-2053364749,repo: pandas-dev/pandas | issue: BUG: Roundtripping bytes through pandas dataframes leads to data loss | keyword: gotcha
"> If Pandas doesn't allow `bytes`-type objects to be inserted in dataframes then it should error out. If it _does_ allow `bytes`-type objects to be inserted into dataframes, then the data should be returned without loss. @maxburke pandas essentially indeed does support a bytes _dtype_, but will store them in an object-dtype column. Generally we don't error about that in places where you can specify a `dtype`, but I fully agree this is confusing. What is happening here is that you run into a num…",,,,,,Anecdotal,comment,,,,,,,,2024-04-13,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/58205#issuecomment-2053671686,repo: pandas-dev/pandas | issue: BUG: Roundtripping bytes through pandas dataframes leads to data loss | keyword: gotcha
"This works fine if you use the pyarrow binary type: ``` import hashlib import pandas as pd import pyarrow as pa def hash_id(dgu_id: str) -> bytes: h = hashlib.sha256() h.update(dgu_id.encode()) id = h.digest()[0:16] assert(len(id) == 16) return id df = pd.DataFrame({""id"": [""2021A00051007067""]}) df[""hash""] = df[""id""].apply(hash_id).astype(pd.ArrowDtype(pa.binary())) assert len(df[""hash""][0]) == 16 ``` np.bytes_ is not something we have ever supported, and given upstream there doesn't appear to b…",,,,,,Anecdotal,comment,,,,,,,,2024-04-13,github/WillAyd,https://github.com/pandas-dev/pandas/issues/58205#issuecomment-2053674372,repo: pandas-dev/pandas | issue: BUG: Roundtripping bytes through pandas dataframes leads to data loss | keyword: gotcha
> there is nothing downstream we will do either We could raise an informative error when a user does `astype(bytes)` though,,,,,,Anecdotal,comment,,,,,,,,2024-04-13,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/58205#issuecomment-2053674815,repo: pandas-dev/pandas | issue: BUG: Roundtripping bytes through pandas dataframes leads to data loss | keyword: gotcha
Hmm what are you suggesting? Just make `astype(bytes)` fail? Or is this part of the larger conversation in https://github.com/pandas-dev/pandas/issues/58141 ? Maybe that should default to the pyarrow bytes type instead of numpy?,,,,,,Anecdotal,comment,,,,,,,,2024-04-13,github/WillAyd,https://github.com/pandas-dev/pandas/issues/58205#issuecomment-2053676181,repo: pandas-dev/pandas | issue: BUG: Roundtripping bytes through pandas dataframes leads to data loss | keyword: gotcha
"Inside file astype.py What about adding an error or a warning message as trailing byte will be losses or we don't offer first class support for the np.byte_ for this situation? ```python if copy or arr.dtype == object or dtype == object: if dtype == 'bytes': # If conversion to bytes is not fully supported or may lead to data loss raise ValueError(""Conversion to bytes is not fully supported or may lead to data loss."") # Explicit copy, or required since NumPy can't view from / to object. return a…",,,,,,Anecdotal,comment,,,,,,,,2024-04-13,github/aabhinavg,https://github.com/pandas-dev/pandas/issues/58205#issuecomment-2053692764,repo: pandas-dev/pandas | issue: BUG: Roundtripping bytes through pandas dataframes leads to data loss | keyword: gotcha
"> @maxburke pandas essentially indeed does support a bytes _dtype_, but will store them in an object-dtype column. Generally we don't error about that in places where you can specify a `dtype`, but I fully agree this is confusing. > > What is happening here is that you run into a numpy gotcha ([numpy/numpy#3878](https://github.com/numpy/numpy/issues/3878)) in that the fixed-width bytes dtype considers the stored data as null-terminated C strings, and thus converting to Python objects removes an…",,,,,,Anecdotal,comment,,,,,,,,2024-04-17,github/maxburke,https://github.com/pandas-dev/pandas/issues/58205#issuecomment-2060277885,repo: pandas-dev/pandas | issue: BUG: Roundtripping bytes through pandas dataframes leads to data loss | keyword: gotcha
"BUG: .loc operation cannot locate existing index when having single string as index for dataframe ('string',) ### Pandas version checks - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [ ] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas)…",,,,,,Anecdotal,issue,,,,,,,,2024-03-06,github/carlonlv,https://github.com/pandas-dev/pandas/issues/57750,repo: pandas-dev/pandas | keyword: gotcha | state: open
"Note that the same operation with tuples with length >= 2 works as expected. Also in the above example, if I do temp.loc[temp.index == ('h',), 'b'] = True, it also works fine",,,,,,Anecdotal,comment,,,,,,,,2024-03-06,github/carlonlv,https://github.com/pandas-dev/pandas/issues/57750#issuecomment-1981359039,"repo: pandas-dev/pandas | issue: BUG: .loc operation cannot locate existing index when having single string as index for dataframe ('string',) | keyword: gotcha"
"Please have a look at the following answer on StackOverflow; (it's an old one but still relevant) https://stackoverflow.com/questions/40186361/pandas-dataframe-with-tuple-of-strings-as-index It will work if you pass the tuple inside a list as argument to the loc function (explanation in the link) `temp.loc[('h',), 'b'] = True` # error to `temp.loc[[('h',)], 'b'] = True` # works fine Hope this helps !",,,,,,Anecdotal,comment,,,,,,,,2024-03-06,github/mbarki-mohamed,https://github.com/pandas-dev/pandas/issues/57750#issuecomment-1981758983,"repo: pandas-dev/pandas | issue: BUG: .loc operation cannot locate existing index when having single string as index for dataframe ('string',) | keyword: gotcha"
"Thanks for the clarification. I was simply using the dataframe as dictionary. I guess this is one of the ""gotcha"" moment.",,,,,,Anecdotal,comment,,,,,,,,2024-03-06,github/carlonlv,https://github.com/pandas-dev/pandas/issues/57750#issuecomment-1981772343,"repo: pandas-dev/pandas | issue: BUG: .loc operation cannot locate existing index when having single string as index for dataframe ('string',) | keyword: gotcha"
"Thanks for the report. pandas uses tuples to signify values in a MultiIndex, and this is the reason why your lookup fails. One idea is to treat non-MulitIndexes differently here, allowing for lookups with tuples, whereas supplying tuples when the index is a MultiIndex would interpret them as levels. Perhaps this has some bad implications though, further investigations are welcome!",,,,,,Anecdotal,comment,,,,,,,,2024-03-07,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/57750#issuecomment-1984535262,"repo: pandas-dev/pandas | issue: BUG: .loc operation cannot locate existing index when having single string as index for dataframe ('string',) | keyword: gotcha"
"BUG: Python operator 'in' behaves in two different ways on Series object The python operator 'in' acting on a pandas.Series object behaves in two different ways. When iterating over the series it acts on the list of values. But for membership testing it acts on the index. Is this the desired behavior? ``` import pandas as pd s = pd.Series([1, 2, 3]) for v in s: print v # prints 1 2 3, i.e. 'in' is acting on the values for v in [1, 2, 3]: print v in s # prints True True False, i.e. the latter 'i…",,,,,,Anecdotal,issue,,,,,,,,2016-09-16,github/griai,https://github.com/pandas-dev/pandas/issues/14235,repo: pandas-dev/pandas | keyword: gotcha | state: open
"I agree it is confusing, but as you said, it is the documented behaviour (for iteration: http://pandas.pydata.org/pandas-docs/stable/basics.html#iteration, membership test: http://pandas.pydata.org/pandas-docs/stable/gotchas.html#using-the-in-operator although the argument that series is dict-like so it checks the keys/index and not the values, is not applied in the iteration case ...). Changing this will break users code, so is probably not going to happen soon (we could consider it in 2.0, bu…",,,,,,Anecdotal,comment,,,,,,,,2016-09-16,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-247541815,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
"I agree with the OP. There are a few StackOverflow questions about this already (e.g., [here](https://stackoverflow.com/questions/49393053/using-in-operator-with-pandas-series/49393472#49393472), and [here](https://stackoverflow.com/questions/47189831/pandas-series-and-the-in-operator) from the opposite expectation perspective). Let me make a few points: - `x in y`, for objects `y` that are iterable, is generally expected to return `True` exactly if `for z in y: ...` sets `z` to an object equal…",,,,,,Anecdotal,comment,,,,,,,,2019-10-27,github/cobalamin,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-546679847,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
"Just got bit by this. It was miserable. agreed that it needs to be consistent, and imho `in` on the actual values seems much more sensible. iteration on series works on values, and no one really wants to check indices ""implicitly"".",,,,,,Anecdotal,comment,,,,,,,,2021-07-22,github/ParitoshSingh07,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-884716408,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
"I agree that this is super-counter-intuitive. Nobody expects: ```python In [79]: [value for value in pd.Series([1,2,3])] Out[79]: [1, 2, 3] In [80]: 3 in pd.Series([1,2,3]) Out[80]: False ``` I have a suggestion: deprecate `Series.__contains__`. Concretely: 2.x behaviour: ```python In [70]: 3 in pd.Series([1, 2, 3]) <ipython-input-70-1e925d619135>:1: FutureWarning: membership in Series is deprecated. Please use `value in series.index` to preserve the current behaviour, or `value in series.array…",,,,,,Anecdotal,comment,,,,,,,,2023-07-10,github/MarcoGorelli,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-1628619800,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
"AFAICT the Series behavior is incorrect bc it uses `IndexOpsMixin.__iter__` where it is ""supposed to"" use `NDFrame.__iter__`. That said, i agree with Marco that deprecating `__contains__` is changing either. (id be open to deprecating `__iter__` too)",,,,,,Anecdotal,comment,,,,,,,,2023-07-10,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-1629412772,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
"Both `__iter__` and `__contains__` for DataFrames uses the columns. It therefore seems most natural that they use the index on a Series. This would also agree with a dictionary-analogy of Series. Rather than deprecating, I would suggest changing the `__iter__` to instead iterate over the index. Both Series and DataFrames are data structures with a well-defined order, and as such I think users expect to be able to iterate over them. While there is ambiguity because there are multiple choices for…",,,,,,Anecdotal,comment,,,,,,,,2023-07-10,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-1629652870,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
"Thanks for reading > I would suggest changing the __iter__ to instead iterate over the index. are you sure this wouldn't be too disruptive? I think it would change `list(ser)` to ```python In [3]: list(pd.Series([1,2,3])) Out[3]: [0, 1, 2] ``` which seems like a really big change It's unfortunate that when dealing with magic dunder methods, it's very difficult to do deprecations. We can hardly issue a warning like ""the behaviour of `__iter__` is doing to change, please pass argument `x` to sile…",,,,,,Anecdotal,comment,,,,,,,,2023-07-11,github/MarcoGorelli,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-1630854624,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
"> are you sure this wouldn't be too disruptive? It would be no more disruptive than deprecating the method entirely, no? > It's unfortunate that when dealing with magic dunder methods, it's very difficult to do deprecations. This can be done using options like `future.series_iteraiton` that @jbrockmendel has proposed. There is still an inconsistency however. I think users would expect that `list(ser)` and `ser.tolist()` behave the same, but I do feel `ser.tolist()` should return the values for …",,,,,,Anecdotal,comment,,,,,,,,2023-07-11,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-1631455357,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
there have been lots of discussion about this over the years imho either leave this (and fix iteration) or deprecate entirely changing to mean values is a non starter,,,,,,Anecdotal,comment,,,,,,,,2023-07-11,github/jreback,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-1631474727,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
"> It would be no more disruptive than deprecating the method entirely, no? My thinking is that a loud error telling you what to use instead is more user-friendly than a sudden behaviour change (even if preceded by futurewarnings, which unfortunately people often either silence or don't read)",,,,,,Anecdotal,comment,,,,,,,,2023-07-11,github/MarcoGorelli,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-1631493959,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
"> My thinking is that a loud error telling you what to use instead is more user-friendly than a sudden behaviour change (even if preceded by futurewarnings, which unfortunately people often either silence or don't read) This is definitely true. But if we were to agree that changing `__iter__` is better for users (and I understand that's a big if), then I don't think we should impoverish the users who follow good practices to help those that don't.",,,,,,Anecdotal,comment,,,,,,,,2023-07-11,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-1631503346,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
"Sure, agree in general, it's the ""changing `__iter__` being better for users"" that I'm really questioning - I would also expect `ser.tolist()` and `list(ser)` to give the same result",,,,,,Anecdotal,comment,,,,,,,,2023-07-12,github/MarcoGorelli,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-1632093267,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
"> Sure, agree in general, it's the ""changing `__iter__` being better for users"" that I'm really questioning - I would also expect `ser.tolist()` and `list(ser)` to give the same result I really think it is important and agree with the above, and that we should *not* change the behavior of `Series.__iter__()` . It's just too natural to believe that `list(ser)` produces the list of values. Agree that deprecating and removing `__contains__()` makes sense. If we're concerned about the lack of asymm…",,,,,,Anecdotal,comment,,,,,,,,2023-07-12,github/Dr-Irv,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-1632444113,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
"I think whether these return the same thing depends on whether `Series` should be more-dict like. I think it would be hard to change the behavior of `__iter__` without a. Annoying users since they would need to change their code from `for val in ser` to be `for val in ser.array` anyway to silence the warning. b. Not creating surprising results from code that hansn't been updated. If this were an election, I'd vote for leaving `__contains__` and deprecating with eventual raise on `__iter__`.",,,,,,Anecdotal,comment,,,,,,,,2023-07-12,github/bashtage,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-1632445910,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
> We could also consider deprecating DataFrame.__contains__() as well I think this would be a mistake. I find code like `if col in df: do_somthing(df[col])` to be really natural and useful.,,,,,,Anecdotal,comment,,,,,,,,2023-07-12,github/bashtage,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-1632448465,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
"> If this were an election, I'd vote for leaving __contains__ and deprecating with eventual raise on __iter__. Isn't `__contains__` the more counter-intuitive one? Pandas developers may think of Series as dicts, but do users? Does the analogy even hold, given that Series can have duplicate row labels but dict keys must be unique? Looks like this may be a topic for PDEP-13, there's more variety in opinion here than I was expecting - thanks all for weighing in!",,,,,,Anecdotal,comment,,,,,,,,2023-07-12,github/MarcoGorelli,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-1632536660,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
"> Pandas developers may think of Series as dicts, but do users? For those who don't care about the index (likely always using a RangeIndex), most likely not. Those that do use meaningful indices I would assume so. But I have no idea what that split may look like. > Does the analogy even hold, given that Series can have duplicate row labels but dict keys must be unique? I don't think uniqueness destroys the dict analogy - especially regarding iteration and containment. But perhaps there are case…",,,,,,Anecdotal,comment,,,,,,,,2023-07-12,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-1633230440,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
"I am a scientist in astronomy and this just bitted me. I feel lucky I discovered this before I publish my journal paper so that I did not publish wrong results in the frontier of science. But, are others aware of this? I feel worried. I think this is definitely a nightmare for scientists, and even a hidden bomb in science that could explode at any moment. Pandas and Numpy are the most important tools for researchers. Since ```a in List``` and ```a in numpy.array``` behaves the same, and Pandas …",,,,,,Anecdotal,comment,,,,,,,,2023-12-18,github/kaiwu-astro,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-1859504842,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
"> **Considering the leading role of Pandas in all subjects worldwide, I urge the developers think about this carefully. A simple implementation of `a in series`=`a in series.values` would save loads of errors in the future.** This is a debate. Is a `Series` like a `dict` or like an array? It's always been thought of as a `dict`, so the behavior is like a `dict`: ```python >>> import pandas as pd >>> d={1:""a"", 2:""b"", 3:""c""} >>> 1 in d True >>> ""a"" in d False >>> s = pd.Series(d) >>> s 1 a 2 b 3 …",,,,,,Anecdotal,comment,,,,,,,,2023-12-18,github/Dr-Irv,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-1860931119,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
"Thanks for your input @kaiwu-astro Agree, the risk of unexpected behaviour far outweighs the slight convenience of having to typing `x in series` as opposed to `x in series.index` --- still very +1 for deprecating",,,,,,Anecdotal,comment,,,,,,,,2023-12-18,github/MarcoGorelli,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-1860933117,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
"Can there be bad behaviors that result from having a container that appears to be a sequence but raises on calls to `__contains__`? If this is okay, I can be on board with deprecating. I do think we should make a decision on `__iter__` as well before deprecating.",,,,,,Anecdotal,comment,,,,,,,,2023-12-18,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-1861053799,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
"read_html docs inconsistent with behaviour if lxml not installed #### Code Sample, a copy-pastable example if possible ```python >>> dfs = pd.read_html('<table><tr><td>1</td></tr></table>') ImportError: lxml not found, please install it >>> dfs = pd.read_html('<table><tr><td>1</td></tr></table>', flavor='bs4') [ 0 0 1] ``` #### Problem description The documentation explictly states, in the HTML-parsing-gotchas page and the argument docstring that the fall back is to 'bs4+html5lib' if 'lxml' fai…",,,,,,Anecdotal,issue,,,,,,,,2019-12-15,github/attack68,https://github.com/pandas-dev/pandas/issues/30281,repo: pandas-dev/pandas | keyword: gotcha | state: open
"Looks like in https://github.com/pandas-dev/pandas/blob/37dfcc1acf3b37a1ff5251fee3380a179da1f2ed/pandas/io/html.py#L885-L886 we unconditionally raise an exception, even if the flavor was implicit from None. Will need some logic to handle that case.",,,,,,Anecdotal,comment,,,,,,,,2019-12-16,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/30281#issuecomment-566126727,repo: pandas-dev/pandas | issue: read_html docs inconsistent with behaviour if lxml not installed | keyword: gotcha
"I've run into this very same problem, where flavor was set to `('lxml', 'bs4')` from None, and in the loop it never in fact got to `bs4`. Would changing the order of the tuple be controversial, or rewriting the loop? If large/complicated consequences are not foreseen I could open a PR for this.",,,,,,Anecdotal,comment,,,,,,,,2023-01-31,github/bsipocz,https://github.com/pandas-dev/pandas/issues/30281#issuecomment-1410889834,repo: pandas-dev/pandas | issue: read_html docs inconsistent with behaviour if lxml not installed | keyword: gotcha
"API: Consolidate groupby as_index and group_keys Everything in this issue also applies to `Series.groupby` and `SeriesGroupBy`; I will just be writing it for `DataFrame`. Currently `DataFrame.groupby` have two arguments that are essentially for the same thing: - `as_index`: Whether to include the group keys in the index or, when the groupby is done on column labels (see [#49519](https://github.com/pandas-dev/pandas/issues/49519)), in the columns. - `group_keys`: Whether to include the group key…",,,,,,Anecdotal,issue,,,,,,,,2022-11-05,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/49543,repo: pandas-dev/pandas | keyword: gotcha | state: open
"Would it be feasible instead to add the new `keys_axis` to all the `DataFrameGroupby` methods instead (with a sensible default & restrictions per function)? This would include deprecating `as_index`/`group_keys` I remember having a similar API decision when deciding `df.groupby(engine=""numba"").mean()` vs `df.groupby().mean(engine=""numba"")` (current), and some pros were 1. Easier switching between arguments. If a user wants to switch from `group_keys=""columns""` and `group_keys=""infer""` between 2…",,,,,,Anecdotal,comment,,,,,,,,2022-11-07,github/mroeschke,https://github.com/pandas-dev/pandas/issues/49543#issuecomment-1305979548,repo: pandas-dev/pandas | issue: API: Consolidate groupby as_index and group_keys | keyword: gotcha
"Overall plan seems reasonable. I'm not sure `keys_axis` is very intuitive. Would `axis_keys` or maybe `axis_labels` be clearer (AFAIK the generic word, at least in typing, for the values in an index are labels)?",,,,,,Anecdotal,comment,,,,,,,,2022-11-07,github/bashtage,https://github.com/pandas-dev/pandas/issues/49543#issuecomment-1306016077,repo: pandas-dev/pandas | issue: API: Consolidate groupby as_index and group_keys | keyword: gotcha
"Love the overall idea. An `axis` feels a little strange to me because in theory I guess you could toss an `apply` against any axis and then separately have a `key_axis` keyword. Not sure there is a need for those two to be separate? Stepping back we have the question of: 1. Which axis should the computation be performed across 2. How should we align the output of the computation I think 1. should just continue to use the `axis` keyword we have throughout the library, but maybe the keyword for 2…",,,,,,Anecdotal,comment,,,,,,,,2022-11-07,github/WillAyd,https://github.com/pandas-dev/pandas/issues/49543#issuecomment-1306062635,repo: pandas-dev/pandas | issue: API: Consolidate groupby as_index and group_keys | keyword: gotcha
"@mroeschke - I definitely see the benefits of your proposal. The downside would be having to specify `keys_axis` for each method. I would be happy either way. @bashtage - the argument would be specifying where the groupby keys go. I don't think this is conveyed by e.g. `axis_labels=""columns""`. Similarly, `axis_keys=1` seems odd to me - as if it's stating ""the axis keys are 1"". On the other hand, I think saying the ""keys axis is 1"" make sense. @WillAyd > An `axis` feels a little strange to me be…",,,,,,Anecdotal,comment,,,,,,,,2022-11-08,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/49543#issuecomment-1306524420,repo: pandas-dev/pandas | issue: API: Consolidate groupby as_index and group_keys | keyword: gotcha
"Thanks for thinking this through and the proposal, @rhshadrach ! I added some thoughts below, but to be clear, while those might seem ""critical"" of the proposal, I certainly agree that we should try to clean this part of the API (those different keywords that do very similar things but not exactly and for different methods is not ideal ..) To summarize the context for myself, the underlying behaviour (not current API) we want to control somehow is the following aspect of the `groupby` behaviour…",,,,,,Anecdotal,comment,,,,,,,,2022-11-08,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/49543#issuecomment-1307254839,repo: pandas-dev/pandas | issue: API: Consolidate groupby as_index and group_keys | keyword: gotcha
"> Thanks for thinking this through and the proposal, @rhshadrach ! I added some thoughts below, but to be clear, while those might seem ""critical"" of the proposal, I certainly agree that we should try to clean this part of the API (those different keywords that do very similar things but not exactly and for different methods is not ideal ..) Thanks @jorisvandenbossche! Any and all thoughts (critical or not) are much appreciated. `apply` is a particularly hard method to reason about. Agreed on y…",,,,,,Anecdotal,comment,,,,,,,,2022-11-12,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/49543#issuecomment-1312555613,repo: pandas-dev/pandas | issue: API: Consolidate groupby as_index and group_keys | keyword: gotcha
"@jorisvandenbossche > > Rereading my comments, maybe my current thoughts could be summarized as: do we actually need to keep the group_keys=False optional behavior? If it is a reset_index(drop=True) away? > > I'm not sure what this means; Reflecting on your comments more, I'm not sure this is what you're necessarily getting at, but it occurs to me that we have `agg` for aggregations, `transform` for transformers, so we shouldn't restrict `apply` to having to support either one of those. If user…",,,,,,Anecdotal,comment,,,,,,,,2022-11-12,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/49543#issuecomment-1312576913,repo: pandas-dev/pandas | issue: API: Consolidate groupby as_index and group_keys | keyword: gotcha
"You'll need to set up an account on Kaggle, and download a token https://github.com/Kaggle/kaggle-api#api-credentials, and then ``` for i in $(kaggle kernels list --page-size 100 --sort-by 'voteCount' -p 1 --language python | awk '{ print $1 }' | tail -n +3); do kaggle kernels pull $i; done; ``` should work Then you can just remove all outputs with `nbstripout` and `grep` for patterns you want",,,,,,Anecdotal,comment,,,,,,,,2022-11-14,github/MarcoGorelli,https://github.com/pandas-dev/pandas/issues/49543#issuecomment-1313457193,repo: pandas-dev/pandas | issue: API: Consolidate groupby as_index and group_keys | keyword: gotcha
"Does `keys_location` make things any clearer? Yes I was confused before but @jorisvandenbossche did a great write up. Not sold on that as a name either but axis is definitely confusing I think also would be easier to borrow the terminology for accepted argument values from pivot_table of `index` and `values` rather than `index` and `columns`, as `columns` to me infers axis=1. I guess `index` and `values` would still be confusing if you groupby axis=1 and the keys actually do get put in the colu…",,,,,,Anecdotal,comment,,,,,,,,2022-11-15,github/WillAyd,https://github.com/pandas-dev/pandas/issues/49543#issuecomment-1314778130,repo: pandas-dev/pandas | issue: API: Consolidate groupby as_index and group_keys | keyword: gotcha
"**On the transform case** > > What would a transform or filter function (and strict transform or filter, like transform() or head(), not inferred in apply) do with keys_axis=""columns""|""index""? Or would we disallow this combo? > > Those methods are currently documented to preserve the original index and order. > > Users would be able to get the keys in the index or column if they so desired; the documentation would be changed. The default behavior `keys_axis=""infer""` would remain unchanged. This…",,,,,,Anecdotal,comment,,,,,,,,2022-11-15,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/49543#issuecomment-1315313064,repo: pandas-dev/pandas | issue: API: Consolidate groupby as_index and group_keys | keyword: gotcha
"> > > Rereading my comments, maybe my current thoughts could be summarized as: do we actually need to keep the group_keys=False optional behavior? If it is a reset_index(drop=True) away? > > > > > > I'm not sure what this means; > > Reflecting on your comments more, I'm not sure this is what you're necessarily getting at, but it occurs to me that we have `agg` for aggregations, `transform` for transformers, so we shouldn't restrict `apply` to having to support either one of those. If users want…",,,,,,Anecdotal,comment,,,,,,,,2022-11-15,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/49543#issuecomment-1315350470,repo: pandas-dev/pandas | issue: API: Consolidate groupby as_index and group_keys | keyword: gotcha
"BUG: pd.DataFrame.select_dtypes() method defaults None to float64 if arg is list-like #### Code Sample ```python >>> import pandas as pd >>> df = pd.DataFrame({'a': [1, 2] * 3, ... 'b': [True, False] * 3, ... 'c': [1.0, 2.0] * 3}) >>> df.select_dtypes(include=None) ValueError: at least one of include or exclude must be nonempty >>> df.select_dtypes(include=[None]) c 0 1.0 1 2.0 2 1.0 3 2.0 4 1.0 5 2.0 ``` ```python >>> import numpy as np >>> np.dtype(None) dtype('float64') ``` #### Problem desc…",,,,,,Anecdotal,issue,,,,,,,,2019-10-12,github/kacricon,https://github.com/pandas-dev/pandas/issues/28943,repo: pandas-dev/pandas | keyword: gotcha | state: open
"In my case it was basically an issue of the code silently failing when the method's arguments were malformed due to a bug. Not a major issue either way, but I feel like this behavior unintuitive.",,,,,,Anecdotal,comment,,,,,,,,2019-10-15,github/kacricon,https://github.com/pandas-dev/pandas/issues/28943#issuecomment-542283880,repo: pandas-dev/pandas | issue: BUG: pd.DataFrame.select_dtypes() method defaults None to float64 if arg is list-like | keyword: gotcha
"Gotcha, thanks. I don't have a strong opinion here. I'm not sure what the original motivation was for `np.dtype(None)` to be float. I think the fact that ```python df.select_dtypes(include='int'), df.select_dtypes(include=['int']) ``` are the same while ```python df.select_dtypes(include=None), df.select_dtypes(include=[None]) ``` are different argues that we shouldn't allow `None` in the list of include and exclude. We'll want to deprecate the current behavior first though.",,,,,,Anecdotal,comment,,,,,,,,2019-10-15,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/28943#issuecomment-542296206,repo: pandas-dev/pandas | issue: BUG: pd.DataFrame.select_dtypes() method defaults None to float64 if arg is list-like | keyword: gotcha
"DOC/VIS: plotting on current figure/axis with Series/DataFrame.plot() From [SO](http://stackoverflow.com/questions/26845727/pandas-plot-multiple-columns-of-a-timeseries-with-labels-example-from-pandas-d/26845829#26845829), I noticed that `Series.plot()` plots on the current axis if no ax is specified, and `DataFrame.plot` not (so creates a new figure when no ax is specified). Some things about this: - Is this deliberate? I think this is possible: - [doc](http://pandas.pydata.org/pandas-docs/sta…",,,,,,Anecdotal,issue,,,,,,,,2014-11-10,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/8776,repo: pandas-dev/pandas | keyword: gotcha | state: open
"This is something that's always bothered me a bit. I don't think there's a good reason for Series and DataFrame to behave differently. Matplotlib seems to always plot on the currently active axis (there may be exceptions), so we should probably follow that.",,,,,,Anecdotal,comment,,,,,,,,2014-11-15,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/8776#issuecomment-63185798,repo: pandas-dev/pandas | issue: DOC/VIS: plotting on current figure/axis with Series/DataFrame.plot() | keyword: gotcha
"Personally I think both should follow DataFrame behaviour of creating a new figure unless ax is explicitly passed. Like tacaswell says, pyplot works based on the currently active axis but once you start dealing with multiple axes, it's much nicer to use the object-oriented approach to creating figures and axes, and pass them explicitly to plotting functions. As it is, `Series.plot()` modifies your existing axes without giving you much warning, e.g.: ``` python import pandas as pd import matplot…",,,,,,Anecdotal,comment,,,,,,,,2014-11-16,github/onesandzeroes,https://github.com/pandas-dev/pandas/issues/8776#issuecomment-63246139,repo: pandas-dev/pandas | issue: DOC/VIS: plotting on current figure/axis with Series/DataFrame.plot() | keyword: gotcha
"The advantage of targeting the 'current axes', rather than making a new figure is that it lets you easily plot multiple lines to the same axes (which something at least I frequently do) without forcing the users to start using the OO api.",,,,,,Anecdotal,comment,,,,,,,,2014-11-17,github/tacaswell,https://github.com/pandas-dev/pandas/issues/8776#issuecomment-63257333,repo: pandas-dev/pandas | issue: DOC/VIS: plotting on current figure/axis with Series/DataFrame.plot() | keyword: gotcha
"Agreed to let `Series` behave like a `DataFrame`. When we want to plot on the same axes, we can do: ``` # Assuming ""serieses"" contains a list of series ax = None: for s in serieses: s.plot(ax=None) ```",,,,,,Anecdotal,comment,,,,,,,,2014-11-22,github/sinhrks,https://github.com/pandas-dev/pandas/issues/8776#issuecomment-64069004,repo: pandas-dev/pandas | issue: DOC/VIS: plotting on current figure/axis with Series/DataFrame.plot() | keyword: gotcha
"Strongly disagree with the suggestions to make Series behave like the current DataFrame implementation and would much rather have it the other way around. As a frequent user of pandas/mpl in interactive work, I seldom want to have to deal with the OO interface, and not having a default behavior of using plt.gcf()/gca() in what's supposed to be a convenience method makes it much less useful. Also, for what it's worth, the current behavior is a change from the old behavior in pandas 0.13.x: http:…",,,,,,Anecdotal,comment,,,,,,,,2014-12-18,github/rcarneva,https://github.com/pandas-dev/pandas/issues/8776#issuecomment-67517748,repo: pandas-dev/pandas | issue: DOC/VIS: plotting on current figure/axis with Series/DataFrame.plot() | keyword: gotcha
"From my perspective, the Series behavior of plotting on gca is super useful for interactive work, and I really don't want to get rid of that. I see Series/DataFrame.plot as being mainly for quick and dirty plots, which are usually done interactively. When I need to refine somethingI turn to seaborn / matplotlib and use the OO interface. So I guess what I'm saying is - document that Series and DataFrame are different for now - put off unifying the two till our mythical 1.0 release (where I'd vot…",,,,,,Anecdotal,comment,,,,,,,,2015-01-19,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/8776#issuecomment-70530834,repo: pandas-dev/pandas | issue: DOC/VIS: plotting on current figure/axis with Series/DataFrame.plot() | keyword: gotcha
"This is speaking wearing my not-mpl-dev hat, this is not the settled plan of mpl. One of the things I have been thinking about (but have not fully fleshed out yet) is having mpl provide a decorator/registration function as part of `pyplot` that takes care of ensuring that the axes object exist (and maybe registering the wrapped function into the same space). The idea would be that you have something like ``` python def pandas_plotting_func(ax, df_or_ds, bunch_of_style): ax.foo ``` in pandas and…",,,,,,Anecdotal,comment,,,,,,,,2015-01-19,github/tacaswell,https://github.com/pandas-dev/pandas/issues/8776#issuecomment-70548299,repo: pandas-dev/pandas | issue: DOC/VIS: plotting on current figure/axis with Series/DataFrame.plot() | keyword: gotcha
"I agree with @TomAugspurger here that the Series behaviour is certainly usefull for interactive plotting, so I wouldn't change that either. Let's indeed start with documenting better the current situation.",,,,,,Anecdotal,comment,,,,,,,,2015-01-19,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/8776#issuecomment-70571599,repo: pandas-dev/pandas | issue: DOC/VIS: plotting on current figure/axis with Series/DataFrame.plot() | keyword: gotcha
"> Personally I think both should follow DataFrame behaviour of creating a new figure unless ax is explicitly passed. Like tacaswell says, pyplot works based on the currently active axis but once you start dealing with multiple axes, it's much nicer to use the object-oriented approach to creating figures and axes, and pass them explicitly to plotting functions. I completely agree with this. I find it so annoying when I do something like ``` series.plot() ``` and nothing appears from an interacti…",,,,,,Anecdotal,comment,,,,,,,,2015-02-24,github/bashtage,https://github.com/pandas-dev/pandas/issues/8776#issuecomment-75863397,repo: pandas-dev/pandas | issue: DOC/VIS: plotting on current figure/axis with Series/DataFrame.plot() | keyword: gotcha
Your plot in being dumped into your currently open figure. You may need a call to `plt.draw()` (pandas may not being doing this automatically under the hood). Another option is to make a call to `plt.figure()` before plotting a series.,,,,,,Anecdotal,comment,,,,,,,,2015-02-25,github/tacaswell,https://github.com/pandas-dev/pandas/issues/8776#issuecomment-75889545,repo: pandas-dev/pandas | issue: DOC/VIS: plotting on current figure/axis with Series/DataFrame.plot() | keyword: gotcha
"> You may need a call to plt.draw() (pandas may not being doing this automatically under the hood). Another option is to make a call to plt.figure() before plotting a series. The problem with this process is that `pyplot` hasn't been imported - and it isn't clear why someone should need to access`pyplot` directly to do exploratory, often throw-away work.",,,,,,Anecdotal,comment,,,,,,,,2015-02-25,github/bashtage,https://github.com/pandas-dev/pandas/issues/8776#issuecomment-75898892,repo: pandas-dev/pandas | issue: DOC/VIS: plotting on current figure/axis with Series/DataFrame.plot() | keyword: gotcha
"Pandas is importing pyplot under the hood no matter what. The pyplot interface is for exploratory work (as opposed to the OO interface). On Tue, Feb 24, 2015, 22:23 Kevin Sheppard notifications@github.com wrote: > You may need a call to plt.draw() (pandas may not being doing this > automatically under the hood). Another option is to make a call to > plt.figure() before plotting a series. > > The problem with this process is that pyplot hasn't been imported - and > it isn't clear why someone sho…",,,,,,Anecdotal,comment,,,,,,,,2015-02-25,github/tacaswell,https://github.com/pandas-dev/pandas/issues/8776#issuecomment-75954242,repo: pandas-dev/pandas | issue: DOC/VIS: plotting on current figure/axis with Series/DataFrame.plot() | keyword: gotcha
"> Pandas is importing pyplot under the hood no matter what. The pyplot interface is for exploratory work (as opposed to the OO interface). I understand - my only gripe here is that it should not be necessary to explicitly `import pyplot` just a pandas `plot()` method, on either a `DataFrame` or a `Series`. Right now, this is only true for a `DataFrame`",,,,,,Anecdotal,comment,,,,,,,,2015-02-25,github/bashtage,https://github.com/pandas-dev/pandas/issues/8776#issuecomment-75965549,repo: pandas-dev/pandas | issue: DOC/VIS: plotting on current figure/axis with Series/DataFrame.plot() | keyword: gotcha
"I too am in the camp of finding it annoying that a Series does not open a plot window on it's own. Pandas is extraordinary helpful for exploratory data analysis and I just don't see the logic behind having a DataFrame opening a new figure just fine and a Series, which in terms of the data analyst is just another column, would NOT do that. I feel that this throws me off all the time, it just does not make any sense to me, so I would to see this discussed more on both the MPL and Pandas point of …",,,,,,Anecdotal,comment,,,,,,,,2015-04-17,github/michaelaye,https://github.com/pandas-dev/pandas/issues/8776#issuecomment-94080420,repo: pandas-dev/pandas | issue: DOC/VIS: plotting on current figure/axis with Series/DataFrame.plot() | keyword: gotcha
"I agree that the inconsistent behavior for .plot() is a gotcha. Personally, I like the DataFrame behavior. Plotting on the same axes is straight forward and intuitive: ``` ax = pandas.DataFrame(...).plot() pandas.DataFrame(...).plot(ax=ax) ``` As opposed to Series, which, with the current behavior, requires to the non-intuitive `to_frame()` to get the DataFrame like behavior. While this works and is not a lot of extra typing, I don't think this is a trick users discover on their own. And it cer…",,,,,,Anecdotal,comment,,,,,,,,2017-01-25,github/spencerogden,https://github.com/pandas-dev/pandas/issues/8776#issuecomment-275270005,repo: pandas-dev/pandas | issue: DOC/VIS: plotting on current figure/axis with Series/DataFrame.plot() | keyword: gotcha
"Responding (very late) to @michaelaye it 'works' with inline because it does not keep live figures around, thus the next call to `gca` has to make a new figure. No other backend works this way, the behavior of 'nbagg' is the normal case.",,,,,,Anecdotal,comment,,,,,,,,2017-02-17,github/tacaswell,https://github.com/pandas-dev/pandas/issues/8776#issuecomment-280679691,repo: pandas-dev/pandas | issue: DOC/VIS: plotting on current figure/axis with Series/DataFrame.plot() | keyword: gotcha
"BUG: min on non-numeric data with nans related #4279 related #5967 Min doesn't seem to work as expected with NaNs and non-numeric data: ``` In [1]: s = pd.Series(['alpha', np.nan, 'charlie', 'delta']) In [2]: s Out[2]: 0 alpha 1 NaN 2 charlie 3 delta dtype: object In [3]: s.min() # by default docstring suggests this should skipnan Out[3]: inf ``` The hack/workaround is to exclude them, perhaps we should special case this in the code: ``` In [4]: s[s.notnull()].min() Out[4]: 'alpha' ``` _From di…",,,,,,Anecdotal,issue,,,,,,,,2013-07-06,github/hayd,https://github.com/pandas-dev/pandas/issues/4147,repo: pandas-dev/pandas | keyword: gotcha | state: open
"@jreback Think we should fix this on the pandas side, can you think of a cheaper/efficient way to do this. Sticking this at the top of min/max/more? ``` if s.dtype == object and np.nan in s.values: s = s[s.notnull()] ```",,,,,,Anecdotal,comment,,,,,,,,2013-07-06,github/hayd,https://github.com/pandas-dev/pandas/issues/4147#issuecomment-20562190,repo: pandas-dev/pandas | issue: BUG: min on non-numeric data with nans | keyword: gotcha
"from core/nanops.py easy change ``` @bottleneck_switch() def nanmin(values, axis=None, skipna=True): values, mask, dtype = _get_values(values, skipna, fill_value_typ = '+inf') # numpy 1.6.1 workaround in Python 3.x if values.dtype == np.object) _ if sys.version_info[0] >= 3): # pragma: no cover import __builtin__ if values.ndim > 1: apply_ax = axis if axis is not None else 0 result = np.apply_along_axis(__builtin__.min, apply_ax, values) else: result = __builtin__.min(values) ##### add somethin…",,,,,,Anecdotal,comment,,,,,,,,2013-07-06,github/jreback,https://github.com/pandas-dev/pandas/issues/4147#issuecomment-20562256,repo: pandas-dev/pandas | issue: BUG: min on non-numeric data with nans | keyword: gotcha
"@jreback fancy throwing two cents to the numpy issue/discussion? Will have another go making the ""easy change"" this evening :). Bit confusing as `_get_values` seems to return the opposite mask then you'd expect (i.e. the null values are True).",,,,,,Anecdotal,comment,,,,,,,,2013-07-08,github/hayd,https://github.com/pandas-dev/pandas/issues/4147#issuecomment-20604336,repo: pandas-dev/pandas | issue: BUG: min on non-numeric data with nans | keyword: gotcha
"def not an easy change :) i'll put up a pr, but this is a terrible hack around the insanity of ``` python 'a' > inf and 'a' > -inf == True # ??? ```",,,,,,Anecdotal,comment,,,,,,,,2014-06-24,github/cpcloud,https://github.com/pandas-dev/pandas/issues/4147#issuecomment-46971759,repo: pandas-dev/pandas | issue: BUG: min on non-numeric data with nans | keyword: gotcha
"I think you might need to do something like this, e.g. order strings/non-strings separately: https://github.com/pydata/pandas/blob/master/pandas/core/algorithms.py#L144 take the min of both then do some heuristc",,,,,,Anecdotal,comment,,,,,,,,2014-06-24,github/jreback,https://github.com/pandas-dev/pandas/issues/4147#issuecomment-46972115,repo: pandas-dev/pandas | issue: BUG: min on non-numeric data with nans | keyword: gotcha
"The bug behaves differently now. ``` Python In [4]: s = pd.Series(['alpha', np.nan, 'charlie', 'delta']) In [5]: s Out[5]: 0 alpha 1 NaN 2 charlie 3 delta dtype: object In [6]: s.min() --------------------------------------------------------------------------- TypeError Traceback (most recent call last) /Users/facaiyan/Workshop/pandas/pandas/core/nanops.py in f(values, axis, skipna, **kwds) 99 else: --> 100 result = alt(values, axis=axis, skipna=skipna, **kwds) 101 except Exception: /Users/faca…",,,,,,Anecdotal,comment,,,,,,,,2016-08-13,github/facaiy,https://github.com/pandas-dev/pandas/issues/4147#issuecomment-239611187,repo: pandas-dev/pandas | issue: BUG: min on non-numeric data with nans | keyword: gotcha
"@jreback What do you mean by mask here? I know for floats and strings right now the nulls get masked to INF for min and -INF for max. What would we do for the strings though, what kind of values did you have in mind? Are pull requests welcome?",,,,,,Anecdotal,comment,,,,,,,,2017-01-01,github/gregorylivschitz,https://github.com/pandas-dev/pandas/issues/4147#issuecomment-269922504,repo: pandas-dev/pandas | issue: BUG: min on non-numeric data with nans | keyword: gotcha
@mrpoor pull requests are certainly welcome to fix this. With masking is meant that those values are not used when calculating the min or max (like `s[s.notnull()].min()` in user API),,,,,,Anecdotal,comment,,,,,,,,2017-01-02,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/4147#issuecomment-269945926,repo: pandas-dev/pandas | issue: BUG: min on non-numeric data with nans | keyword: gotcha
"This is not timeseries related, so you can put somewhere in `pandas/tests/series` (possibly `test_analytics.py`, you should look where the existing tests for `min()` are located)",,,,,,Anecdotal,comment,,,,,,,,2017-01-04,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/4147#issuecomment-270375322,repo: pandas-dev/pandas | issue: BUG: min on non-numeric data with nans | keyword: gotcha
"With a Categorical Series, I see this bug for `min` but not for `max`. With DataFrames of Categoricals, on the other hand, it's visible for both `min` and `max`. ```python import pandas as pd from pandas.api.types import CategoricalDtype as CD x = pd.Series(list(""abcaa""), dtype=CD(ordered = True)) print(x.min(), x.max()) # => a c xn = x.copy() xn[1] = None print(xn.min(), xn.max()) # => NaN c y = pd.Series(list(""cabdd""), dtype=CD(ordered = True)) print(pd.DataFrame({""x"": x, ""y"": y}).min(axis = …",,,,,,Anecdotal,comment,,,,,,,,2018-09-15,github/Kodiologist,https://github.com/pandas-dev/pandas/issues/4147#issuecomment-421593443,repo: pandas-dev/pandas | issue: BUG: min on non-numeric data with nans | keyword: gotcha
"@Kodiologist could this be because the df isn't separated into multiple elements/indices? (New member, sorry if I'm behind the ball but I'm trying to catch up!)",,,,,,Anecdotal,comment,,,,,,,,2018-10-18,github/Remnan13,https://github.com/pandas-dev/pandas/issues/4147#issuecomment-431172886,repo: pandas-dev/pandas | issue: BUG: min on non-numeric data with nans | keyword: gotcha
"I'm afraid I don't understand your question. The DataFrames in my example do have multiple elements. But I don't know much about pandas internals, anyway. I've only submitted one PR, which was back in 2015.",,,,,,Anecdotal,comment,,,,,,,,2018-10-18,github/Kodiologist,https://github.com/pandas-dev/pandas/issues/4147#issuecomment-431176238,repo: pandas-dev/pandas | issue: BUG: min on non-numeric data with nans | keyword: gotcha
"Gotcha, the issue was still open and I was curious if it's been resolved! I'll poke around a little more and see if I can be more accurate and specific On Thu, Oct 18, 2018, 2:51 PM Kodi Arfer <notifications@github.com> wrote: > I'm afraid I don't understand your question. The DataFrames in my example > do have multiple elements. But I don't know much about pandas internals, > anyway. I've only submitted one PR, which was back in 2015. > > — > You are receiving this because you commented. > Rep…",,,,,,Anecdotal,comment,,,,,,,,2018-10-18,github/Remnan13,https://github.com/pandas-dev/pandas/issues/4147#issuecomment-431192999,repo: pandas-dev/pandas | issue: BUG: min on non-numeric data with nans | keyword: gotcha
BUG: QST: `pd.read_html` gives tables with duplicated columns ### Pandas version checks - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas. - [X] I have confirmed this bug exists on the [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas) of pandas. ### Reproducible Example ```python #…,,,,,,Anecdotal,issue,,,,,,,,2023-12-05,github/rozeappletree,https://github.com/pandas-dev/pandas/issues/56337,repo: pandas-dev/pandas | keyword: gotcha | state: closed
"Thanks for the report! Can you include the code to reproduce in the OP here directly (rather than a link), and try to make it as minimal as possible? Have you seen [the documentation on the table-parsing gotchas](https://pandas.pydata.org/docs/user_guide/io.html#html-table-parsing-gotchas)?",,,,,,Anecdotal,comment,,,,,,,,2023-12-05,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/56337#issuecomment-1839912285,repo: pandas-dev/pandas | issue: BUG: QST: `pd.read_html` gives tables with duplicated columns | keyword: gotcha
TST: Adding tests for validating DataFrame.__setitem__ and .loc behavior - [ ] closes #xxxx (Replace xxxx with the GitHub issue number) - [x] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature - [x] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit). - [ ] Added [type annotations](https://pandas.pydata.org/pandas-docs/de…,,,,,,Anecdotal,issue,,,,,,,,2025-07-10,github/niruta25,https://github.com/pandas-dev/pandas/pull/61822,repo: pandas-dev/pandas | keyword: gotcha | state: closed
"@WillAyd Since I have you, last follow up from the https://github.com/pandas-dev/pandas/pull/61804, Should we keep these tests? Last convo on this issue regarding tests was here: https://github.com/pandas-dev/pandas/pull/61804#discussion_r2193379353",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/niruta25,https://github.com/pandas-dev/pandas/pull/61822#issuecomment-3142047091,repo: pandas-dev/pandas | issue: TST: Adding tests for validating  DataFrame.__setitem__ and .loc behavior | keyword: gotcha
> I appreciate adding tests but I think this PR is superfluous - these cases are definitely covered in the existing code Gotcha! Thanks for the insight. Closing this ticket.,,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/niruta25,https://github.com/pandas-dev/pandas/pull/61822#issuecomment-3142234892,repo: pandas-dev/pandas | issue: TST: Adding tests for validating  DataFrame.__setitem__ and .loc behavior | keyword: gotcha
BLD: Split out tests into pandas_tests package - [ ] closes #30741 (Replace xxxx with the GitHub issue number) - [ ] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature - [ ] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit). - [ ] Added [type annotations](https://pandas.pydata.org/pandas-docs/dev/development/contributin…,,,,,,Anecdotal,issue,,,,,,,,2023-04-29,github/lithomas1,https://github.com/pandas-dev/pandas/pull/53007,repo: pandas-dev/pandas | keyword: gotcha | state: closed
"> So we wouldn’t need to make a separate repo? Would we need to set something up on pypi? If “no” to both then very neat No, we wouldn't need to make a separate repo, but we'd have to set up a ""pandas-tests"" package on PyPI (unless we are removing ability to test altogether). The idea is that ""pandas-tests"" would have a pinned pandas version as a dependency, and we would release pandas and pandas-tests at the same time. The only thing left to do in this PR is to turn all the absolute imports in…",,,,,,Anecdotal,comment,,,,,,,,2023-04-30,github/lithomas1,https://github.com/pandas-dev/pandas/pull/53007#issuecomment-1529156365,repo: pandas-dev/pandas | issue: BLD: Split out tests into pandas_tests package | keyword: gotcha
This pull request is stale because it has been open for thirty days with no activity. Please [update](https://pandas.pydata.org/pandas-docs/stable/development/contributing.html#updating-your-pull-request) and respond to this comment if you're still interested in working on this.,,,,,,Anecdotal,comment,,,,,,,,2023-06-01,github/github-actions[bot],https://github.com/pandas-dev/pandas/pull/53007#issuecomment-1571121624,repo: pandas-dev/pandas | issue: BLD: Split out tests into pandas_tests package | keyword: gotcha
"OK, this POC kinda works now. The way it works is by excluding the tests from the sdist(via .gitattributes), and then building the wheel from that sdist. Since the tests were not included with the sdist, the tests will also not be included in the wheel. Since whether the tests are included (or not) is determined at sdist build time, running the tests normally for pandas development should just work. (Minor note: you can't do ``pytest`` or ``pytest pandas`` anymore since the config file doesn't …",,,,,,Anecdotal,comment,,,,,,,,2024-02-12,github/lithomas1,https://github.com/pandas-dev/pandas/pull/53007#issuecomment-1938820045,repo: pandas-dev/pandas | issue: BLD: Split out tests into pandas_tests package | keyword: gotcha
"In terms of savings, for the Windows wheels uploaded, it's like 2MBs (the originial is like 12.2 MB, and the one without tests is like 10.2 MB). I didn't measure uncompressed sizes, but the savings should be similarish.",,,,,,Anecdotal,comment,,,,,,,,2024-02-12,github/lithomas1,https://github.com/pandas-dev/pandas/pull/53007#issuecomment-1938834288,repo: pandas-dev/pandas | issue: BLD: Split out tests into pandas_tests package | keyword: gotcha
"> Thoughts on whether this would be worth it? I like the idea. Here are some thoughts: 1. It is useful to sometimes look at the test code to see how a method/class is used within pandas. So the docs would need to be updated accordingly, at least to indicate that you have to run the tests locally in a different way, but ideally telling users that the `pandas_tests` package has all of the tests in it. 2. If the plan is to put `pandas_tests` on PyPi, then you'd have to create a conda feedstock to …",,,,,,Anecdotal,comment,,,,,,,,2024-02-14,github/Dr-Irv,https://github.com/pandas-dev/pandas/pull/53007#issuecomment-1943940375,repo: pandas-dev/pandas | issue: BLD: Split out tests into pandas_tests package | keyword: gotcha
> Once complication is that `pandas/tests/extension/base` should not be excluded since it's a documented way for 3rd party EA authors to test their implementations https://pandas.pydata.org/docs/development/extending.html#testing-extension-arrays Couldn't that be handled by updating the documentation and telling people to download the `pandas_tests` package?,,,,,,Anecdotal,comment,,,,,,,,2024-02-14,github/Dr-Irv,https://github.com/pandas-dev/pandas/pull/53007#issuecomment-1944299179,repo: pandas-dev/pandas | issue: BLD: Split out tests into pandas_tests package | keyword: gotcha
Finally green here 😌 Anyone want to take a look? (NOTE: The diff is only big because I split the doctest stuff out of conftest. Actual changes are only ~200 lines or so),,,,,,Anecdotal,comment,,,,,,,,2024-03-19,github/lithomas1,https://github.com/pandas-dev/pandas/pull/53007#issuecomment-2005699386,repo: pandas-dev/pandas | issue: BLD: Split out tests into pandas_tests package | keyword: gotcha
Hey! Thinks this is a decent idea. How much does this decrease the dist size by? Would be good to know that before merging this as that's the main benefit of this change right? Shout if I'm missing something. How will we ensure users don't have mismatching versions of pandas and pandas_test installed if these are to deployed to pypi as sep packages? I didn't see any logic for this. As will need to pin version of pandas used in pandas_test right?,,,,,,Anecdotal,comment,,,,,,,,2024-03-19,github/alimcmaster1,https://github.com/pandas-dev/pandas/pull/53007#issuecomment-2008156477,repo: pandas-dev/pandas | issue: BLD: Split out tests into pandas_tests package | keyword: gotcha
"> Hey! > > Thinks this is a decent idea. > > How much does this decrease the dist size by? Would be good to know that before merging this as that's the main benefit of this change right? Shout if I'm missing something. It's about 15 MBs. The whole of pandas is around 45 MB, so it shaves off a third off the install side. > How will we ensure users don't have mismatching versions of pandas and pandas_test installed if these are to deployed to pypi as sep packages? I didn't see any logic for this.…",,,,,,Anecdotal,comment,,,,,,,,2024-03-19,github/lithomas1,https://github.com/pandas-dev/pandas/pull/53007#issuecomment-2008363494,repo: pandas-dev/pandas | issue: BLD: Split out tests into pandas_tests package | keyword: gotcha
"> I think now it may be important to run the wheel build job on PRs again since more PRs will be updating `pandas` and `pandas_tests` and we need to make sure there's always compatibility between the two. I think it should be OK personally. On a normal pandas build ``pandas_tests`` is just ``pandas.tests``, we don't exclude the tests on a regular build, just on builds from sdist (which would be the case of a wheel build).",,,,,,Anecdotal,comment,,,,,,,,2024-03-20,github/lithomas1,https://github.com/pandas-dev/pandas/pull/53007#issuecomment-2010493200,repo: pandas-dev/pandas | issue: BLD: Split out tests into pandas_tests package | keyword: gotcha
I would still prefer to have every PR run 1 job (Ubuntu only) pandas wheel without tests + `pandas_tests` wheel to validate that the connection between the two is never broken. Our CI jobs are relatively streamlined now and I think this job is definitely more important than some of the checks we run on every PR,,,,,,Anecdotal,comment,,,,,,,,2024-03-27,github/mroeschke,https://github.com/pandas-dev/pandas/pull/53007#issuecomment-2023816820,repo: pandas-dev/pandas | issue: BLD: Split out tests into pandas_tests package | keyword: gotcha
"Nice work, but I'm pretty lukewarm to the implementation. Seems like there are a lot of potential break points and CI is not the easiest to debug. For me locally I see 20.7 MB / 33.6 MB in the test folder is from data files, with a good deal of that from SAS. I am unsure if all of the added complexity here is worth it versus a simpler solution to find a better home for data files ![image](https://github.com/pandas-dev/pandas/assets/609873/f5a5f3b0-89df-444d-9eae-6d89085b9192)",,,,,,Anecdotal,comment,,,,,,,,2024-03-27,github/WillAyd,https://github.com/pandas-dev/pandas/pull/53007#issuecomment-2024004999,repo: pandas-dev/pandas | issue: BLD: Split out tests into pandas_tests package | keyword: gotcha
"> Nice work, but I'm pretty lukewarm to the implementation. Seems like there are a lot of potential break points and CI is not the easiest to debug. > > For me locally I see 20.7 MB / 33.6 MB in the test folder is from data files, with a good deal of that from SAS. I am unsure if all of the added complexity here is worth it versus a simpler solution to find a better home for data files We already don't ship the data files in the test suite, so the 15 MBs of savings is from the actual Python pan…",,,,,,Anecdotal,comment,,,,,,,,2024-03-27,github/lithomas1,https://github.com/pandas-dev/pandas/pull/53007#issuecomment-2024029394,repo: pandas-dev/pandas | issue: BLD: Split out tests into pandas_tests package | keyword: gotcha
> I would still prefer to have every PR run 1 job (Ubuntu only) pandas wheel without tests + `pandas_tests` wheel to validate that the connection between the two is never broken. > > Our CI jobs are relatively streamlined now and I think this job is definitely more important than some of the checks we run on every PR Gotcha. I guess the easiest route would be to run a subset of the wheel builders on every PR. I'll do that in a followup.,,,,,,Anecdotal,comment,,,,,,,,2024-03-27,github/lithomas1,https://github.com/pandas-dev/pandas/pull/53007#issuecomment-2024034573,repo: pandas-dev/pandas | issue: BLD: Split out tests into pandas_tests package | keyword: gotcha
"This has been stale for more than a year, it's conflicting, and I don't think Thomas is planning to continue work here. I'll close, we can always reopen or continue work in a new PR based of this branch if there is interest.",,,,,,Anecdotal,comment,,,,,,,,2025-06-06,github/datapythonista,https://github.com/pandas-dev/pandas/pull/53007#issuecomment-2949363426,repo: pandas-dev/pandas | issue: BLD: Split out tests into pandas_tests package | keyword: gotcha
"DOC: Write user guide page on apply/map/transform methods There is some information in our documentation regarding how to use user defined functions in pandas. The API pages of the used methods, and these sections: - https://pandas.pydata.org/docs/user_guide/groupby.html#aggregation-with-user-defined-functions - https://pandas.pydata.org/docs/user_guide/gotchas.html#gotchas-udf-mutation My understanding is that we've been mostly discouraging the use of functions like apply, or at least the comm…",,,,,,Anecdotal,issue,,,,,,,,2025-03-15,github/datapythonista,https://github.com/pandas-dev/pandas/issues/61126,repo: pandas-dev/pandas | keyword: gotcha | state: closed
"I'm interested in working on this user guide. Just to clarify, this user guide should provide guidance on everything about udfs (when to use them, their differences, etc), rather than just document what they do, correct?",,,,,,Anecdotal,comment,,,,,,,,2025-03-23,github/arthurlw,https://github.com/pandas-dev/pandas/issues/61126#issuecomment-2745930065,repo: pandas-dev/pandas | issue: DOC: Write user guide page on apply/map/transform methods | keyword: gotcha
"> Also, the APIs of the different methods are quite inconsistent, and in some cases cumbersome. Related: https://github.com/pandas-dev/pandas/issues/40112",,,,,,Anecdotal,comment,,,,,,,,2025-03-29,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/61126#issuecomment-2763355217,repo: pandas-dev/pandas | issue: DOC: Write user guide page on apply/map/transform methods | keyword: gotcha
Bug: Save original index and remap after function completes - [x] closes #55767 - [x] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature - [x] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit). - [x] Added [type annotations](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#type-hints) to …,,,,,,Anecdotal,issue,,,,,,,,2025-03-13,github/Jeffrharr,https://github.com/pandas-dev/pandas/pull/61116,repo: pandas-dev/pandas | keyword: gotcha | state: closed
> Thanks for the PR! I'm seeing a performance regression when the index does not contain duplicates. Can we do this conditionally. I appreciate the review! Can you confirm the performance regression numbers that you are seeing? Note that the timings in the original ticket were from a different machine than the one I'm working on. ASV Benchmarks appear similar enough to be reasonable in the standard case. -- Original Function -- ``` [37.66%] ··· series_methods.NSort.time_nlargest ok [37.66%] ···…,,,,,,Anecdotal,comment,,,,,,,,2025-03-26,github/Jeffrharr,https://github.com/pandas-dev/pandas/pull/61116#issuecomment-2755312275,repo: pandas-dev/pandas | issue: Bug: Save original index and remap after function completes | keyword: gotcha
"> Thanks for the PR! I'm seeing a performance regression when the index does not contain duplicates. Can we do this conditionally. Hey, can I get a re-review on this?",,,,,,Anecdotal,comment,,,,,,,,2025-04-09,github/Jeffrharr,https://github.com/pandas-dev/pandas/pull/61116#issuecomment-2788131111,repo: pandas-dev/pandas | issue: Bug: Save original index and remap after function completes | keyword: gotcha
"BUG (?): dtype.value_counts() shows categorical multiple times - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of pandas. - [x] (optional) I have confirmed this bug exists on the master branch of pandas. --- **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. #### Code Sample, a copy-…",,,,,,Anecdotal,issue,,,,,,,,2021-04-01,github/MarcoGorelli,https://github.com/pandas-dev/pandas/issues/40735,repo: pandas-dev/pandas | keyword: gotcha | state: closed
"The explanation for the behaviour is that two categorical dtypes with different categories are not considered as equal: ``` In [26]: pd.CategoricalDtype(categories=['a']) == pd.CategoricalDtype(categories=['b']) Out[26]: False ``` But since the repr is the same, that of course gives a bit a surprising result. And typically when doing a value counts of the dtypes, you will probably want to regard the different categorical dtypes as equal ..",,,,,,Anecdotal,comment,,,,,,,,2021-04-02,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/40735#issuecomment-812360298,repo: pandas-dev/pandas | issue: BUG (?): dtype.value_counts() shows categorical multiple times | keyword: gotcha
"Thanks Joris - so , when you say > And typically when doing a value counts of the dtypes, you will probably want to regard the different categorical dtypes as equal .. do you suggest that ```python category 3 object 1 dtype: int64 ``` should be the expected output, or that the current output is correct but that users should mentally combine the different categorical dtypes?",,,,,,Anecdotal,comment,,,,,,,,2021-04-02,github/MarcoGorelli,https://github.com/pandas-dev/pandas/issues/40735#issuecomment-812362020,repo: pandas-dev/pandas | issue: BUG (?): dtype.value_counts() shows categorical multiple times | keyword: gotcha
"Yeah, so that the current output is ""technically"" correct, but I think users will typically want your expected output. And I am not sure what the easiest way is to get that (probably converting the dtypes to string first? (or to it's type) Eg `df.dtypes.astype(str).value_counts()`)",,,,,,Anecdotal,comment,,,,,,,,2021-04-02,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/40735#issuecomment-812366234,repo: pandas-dev/pandas | issue: BUG (?): dtype.value_counts() shows categorical multiple times | keyword: gotcha
"OK, thanks - so perhaps we can leave `value_counts` as is, and just put this as an extra example in the docs, something like ```python Note that the repr of the values is used to populate the index of the output - when working with different categorical dtypes, you might want to convert them to str first: >>> df = pd.DataFrame({'a': [1], 'b': ['2'], 'c': [3], 'd': [3]}).astype({'a': 'category', 'c': 'category', 'd': 'category'}) >>> df.dtypes.astype(str).value_counts() category 3 object 1 dtype…",,,,,,Anecdotal,comment,,,,,,,,2021-04-02,github/MarcoGorelli,https://github.com/pandas-dev/pandas/issues/40735#issuecomment-812465209,repo: pandas-dev/pandas | issue: BUG (?): dtype.value_counts() shows categorical multiple times | keyword: gotcha
"> OK, thanks - so perhaps we can leave `value_counts` as is, and just put this as an extra example in the docs, something like I like the idea of documenting this as well (if it isn't already). This tripped me up in the past and I had to use the same workaround.",,,,,,Anecdotal,comment,,,,,,,,2021-04-05,github/dsaxton,https://github.com/pandas-dev/pandas/issues/40735#issuecomment-813507922,repo: pandas-dev/pandas | issue: BUG (?): dtype.value_counts() shows categorical multiple times | keyword: gotcha
"Edit: Doh, I missed https://github.com/pandas-dev/pandas/issues/40735#issuecomment-812465209. +1 on that. > And typically when doing a value counts of the dtypes, you will probably want to regard the different categorical dtypes as equal I think that's a guess - some users might also want them to be not equal (because, after all, they aren't). I agree that users will most likely find the output in the OP confusing, but that is a general issue with trying to differentiate Python objects via thei…",,,,,,Anecdotal,comment,,,,,,,,2023-08-29,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/40735#issuecomment-1697159797,repo: pandas-dev/pandas | issue: BUG (?): dtype.value_counts() shows categorical multiple times | keyword: gotcha
@jorisvandenbossche - are you good with the resolution proposed in https://github.com/pandas-dev/pandas/issues/40735#issuecomment-812465209,,,,,,Anecdotal,comment,,,,,,,,2023-08-29,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/40735#issuecomment-1697164688,repo: pandas-dev/pandas | issue: BUG (?): dtype.value_counts() shows categorical multiple times | keyword: gotcha
"Yes, certainly, documenting this gotcha with counting data types and giving an example how to count the categorical dtypes as one group sound certainly useful.",,,,,,Anecdotal,comment,,,,,,,,2023-08-29,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/40735#issuecomment-1697332409,repo: pandas-dev/pandas | issue: BUG (?): dtype.value_counts() shows categorical multiple times | keyword: gotcha
"It seems like there is a discrepancy between the expected output and the actual output.This suggests that there might be a mistake in the data or in the expected output. If you're trying to set the data types of columns in a Pandas DataFrame, you can do it like this: `df = pd.DataFrame({'a': [1], 'b': ['2'], 'c': [3], 'd': [3]}).astype({'a': 'category', 'b': 'category', 'c': 'category', 'd': 'category'})` This will set all columns to have the 'category' data type. If there's a specific issue or…",,,,,,Anecdotal,comment,,,,,,,,2023-10-04,github/aniketDash7,https://github.com/pandas-dev/pandas/issues/40735#issuecomment-1746254923,repo: pandas-dev/pandas | issue: BUG (?): dtype.value_counts() shows categorical multiple times | keyword: gotcha
"@HoWeiChin are you currently working on this? If not, I would be happy to work on this issue. What's the expected behavior here though?",,,,,,Anecdotal,comment,,,,,,,,2024-07-18,github/jahn96,https://github.com/pandas-dev/pandas/issues/40735#issuecomment-2237419311,repo: pandas-dev/pandas | issue: BUG (?): dtype.value_counts() shows categorical multiple times | keyword: gotcha
"im trying to setup a venv environment and hence pulled pandas but i get stuck when building dependencies: Installing collected packages: setuptools, packaging, numpy, Cython, setuptools_scm ERROR: Cannot set --home and --prefix together [notice] A new release of pip is available: 24.0 -> 24.1.2 [notice] To update, run: python.exe -m pip install --upgrade pip [end of output] Any idea guys?",,,,,,Anecdotal,comment,,,,,,,,2024-07-25,github/Maverick1905,https://github.com/pandas-dev/pandas/issues/40735#issuecomment-2250110699,repo: pandas-dev/pandas | issue: BUG (?): dtype.value_counts() shows categorical multiple times | keyword: gotcha
"In-memory to_csv compression #### Code Sample, a copy-pastable example if possible ```python # Attempt 1 import pandas as pd df = pd.DataFrame({""A"": [1, 2, 3, 4], ""B"": [5, 6, 7, 8], ""C"": [9, 10, 11, 12]}) test = df.to_csv(compression=""gzip"") type(test) ``` ``` RuntimeWarning: compression has no effect when passing file-like object as input. Out: str ``` ```python # Attempt 2 from io import BytesIO b_buf = BytesIO() df.to_csv(b_buf, compression=""gzip"") ``` ``` Out: TypeError: a bytes-like object…",,,,,,Anecdotal,issue,,,,,,,,2018-08-31,github/ZaxR,https://github.com/pandas-dev/pandas/issues/22555,repo: pandas-dev/pandas | keyword: gotcha | state: closed
"Interesting proposal! As a (temporary) workaround, could you not save to disk and then read into memory by any chance? BTW, if you have ideas on how to implement in-memory compression, go for it!",,,,,,Anecdotal,comment,,,,,,,,2018-09-01,github/gfyoung,https://github.com/pandas-dev/pandas/issues/22555#issuecomment-417892733,repo: pandas-dev/pandas | issue: In-memory to_csv compression | keyword: gotcha
"Hey - thanks for the reply @gfyoung , and sorry for my delay in replying. The functions where I use this are part of a library, so temporarily saving to disk isn't ideal (can't be sure what the end-user's local environment will look like). My thought was something like this as a workaround: ``` import gzip from io import BytesIO import pandas as pd df = pd.DataFrame({""A"": [1, 2, 3, 4], ""B"": [5, 6, 7, 8], ""C"": [9, 10, 11, 12]}) b_buf = BytesIO() with gzip.open(b_buf, 'wb') as f: f.write(df.to_st…",,,,,,Anecdotal,comment,,,,,,,,2018-09-06,github/ZaxR,https://github.com/pandas-dev/pandas/issues/22555#issuecomment-418958420,repo: pandas-dev/pandas | issue: In-memory to_csv compression | keyword: gotcha
@ZaxR : Gotcha. Interesting...I think it would be a solid enhancement nonetheless. Would be open to proposals for implementation.,,,,,,Anecdotal,comment,,,,,,,,2018-09-06,github/gfyoung,https://github.com/pandas-dev/pandas/issues/22555#issuecomment-418960315,repo: pandas-dev/pandas | issue: In-memory to_csv compression | keyword: gotcha
"I agree that the compression argument should take effect when a file-like object is passed. This enhancement would likely include implementing zip writing support in _get_handle, which would address the frustration I had in https://github.com/pandas-dev/pandas/pull/22011#issuecomment-406831478.",,,,,,Anecdotal,comment,,,,,,,,2018-09-21,github/dhimmel,https://github.com/pandas-dev/pandas/issues/22555#issuecomment-423399265,repo: pandas-dev/pandas | issue: In-memory to_csv compression | keyword: gotcha
"For the 'gzip' compression, `_get_handle()` is not being called when `BytesIO()` is passed. This causes it to fail at `csv.writer(gzip.GzipFile(fileobj=BytesIO()))` in csvs.py If `_get_handle()` is called on `BytesIO()` then what happens is `csv.writer(TextIOWrapper(gzip.GzipFile(fileobj=BytesIO())))` which fails b/c GzipFile opens it as read only. Setting the mode will work `csv.writer(TextIOWrapper(gzip.GzipFile(fileobj=BytesIO(), mode=mode)))` The 'bz2' compression fix is the same. 'xz' will…",,,,,,Anecdotal,comment,,,,,,,,2018-10-13,github/silverdrake11,https://github.com/pandas-dev/pandas/issues/22555#issuecomment-429516271,repo: pandas-dev/pandas | issue: In-memory to_csv compression | keyword: gotcha
"> There is too much logic in _get_handle, and it is called many times for reading and for writing. I agree. Especially since there is no docstring to define what the function intends to support. > One idea is for it to call _get_read_handle and _get_write_handle to split the logic. Or _get_handle_python2 and _get_handle_python3 could be an option. I agree it may be helpful to split read/write and 2/3. However, with 2019 only a couple months away, the purging of 2 from pandas is just around the …",,,,,,Anecdotal,comment,,,,,,,,2018-10-15,github/dhimmel,https://github.com/pandas-dev/pandas/issues/22555#issuecomment-429912813,repo: pandas-dev/pandas | issue: In-memory to_csv compression | keyword: gotcha
"@dhimmel : I agree that the implementation should be 3-oriented. If it's 2-oriented as well, great! If not, I would still write it out but just hold off on the PR until the turn of the calendar year.",,,,,,Anecdotal,comment,,,,,,,,2018-10-15,github/gfyoung,https://github.com/pandas-dev/pandas/issues/22555#issuecomment-429922627,repo: pandas-dev/pandas | issue: In-memory to_csv compression | keyword: gotcha
"@dhimmel and @gfyoung - Now that we've reached the new year/pandas release, any update on this enhancement? Would really love this capability :)",,,,,,Anecdotal,comment,,,,,,,,2019-02-08,github/ZaxR,https://github.com/pandas-dev/pandas/issues/22555#issuecomment-461927715,repo: pandas-dev/pandas | issue: In-memory to_csv compression | keyword: gotcha
"@ZaxR : Thanks for the ping! We're still in the process of releasing versions that are Python-2-compatible, so we might want to hold on this a little longer. That being said, proposals are a pure Python-3-compatible implementation would be great 👍",,,,,,Anecdotal,comment,,,,,,,,2019-02-08,github/gfyoung,https://github.com/pandas-dev/pandas/issues/22555#issuecomment-461936029,repo: pandas-dev/pandas | issue: In-memory to_csv compression | keyword: gotcha
"I was also looking for the compress functionality in order to produce base64 encoded links. ```python @staticmethod def _to_base64_encoded_link(data: pd.DataFrame): csv = data.to_csv(index=False) b64 = base64.b64encode( csv.encode() ).decode() # some strings <-> bytes conversions necessary here link = f'<a href=""data:file/csv;base64,{b64}"" download=""data.csv"">Download</a>' return link ``` Currently my dataframes are to big. So I would like to compress them.",,,,,,Anecdotal,comment,,,,,,,,2020-02-06,github/MarcSkovMadsen,https://github.com/pandas-dev/pandas/issues/22555#issuecomment-582863938,repo: pandas-dev/pandas | issue: In-memory to_csv compression | keyword: gotcha
"I modified some code which seems to produce the bz2 for me and appears to avoid the intermediate on-disk csv file. Does this help you guys? I'm not sure how efficient the memory allocation is or how in-memory it all is, so if there are optimizations, please advise! I'm a py newbie (first day), and this stuff seemed to be poorly documented, so thanks for making this thread and apologies for my ignorance. #This code takes a pandas df and makes clickable link in your ipynb UI to download a bz2 com…",,,,,,Anecdotal,comment,,,,,,,,2020-05-20,github/davidkh1255,https://github.com/pandas-dev/pandas/issues/22555#issuecomment-631168485,repo: pandas-dev/pandas | issue: In-memory to_csv compression | keyword: gotcha
"MultiIndex row indexing with .loc fail with tuple but work with list of indices #### Code Sample, a copy-pastable example if possible ```python data = {""ID1"": [1, 1, 1, 2, 2], ""ID2"": [1001, 1001, 1002, 1001, 1002], ""ID3"": [1, 2, 1, 1, 2], ""Value"": [1, 2, 9, 3, 4]} df = pd.DataFrame(data).set_index([""ID1"", ""ID2"", ""ID3""]) desired_rows = ((1, 1001, 1), (1, 1001, 2), (2, 1002, 2)) # the rows to be extracted print(df) Out[3]: Value ID1 ID2 ID3 1 1001 1 1 2 2 1002 1 9 2 1001 1 3 1002 2 4 ``` #### Pro…",,,,,,Anecdotal,issue,,,,,,,,2017-07-15,github/mansenfranzen,https://github.com/pandas-dev/pandas/issues/16943,repo: pandas-dev/pandas | keyword: gotcha | state: closed
"> One solution would be to convert the tuple to a list I suspect this would break other tests, were tuples have different meanings that lists for slicing. I may be wrong though, if you want to give it a shot.",,,,,,Anecdotal,comment,,,,,,,,2017-07-15,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/16943#issuecomment-315532144,repo: pandas-dev/pandas | issue: MultiIndex row indexing with .loc fail with tuple but work with list of indices | keyword: gotcha
"this technically is not covered by the doc-string ``- A list or array of labels, e.g. ['a', 'b', 'c'].`` but we almost always accept array-like (which includes tuples). The reason this is confusing slightly is that a non-nested tuple is *also* valid as a single indexer.",,,,,,Anecdotal,comment,,,,,,,,2017-07-15,github/jreback,https://github.com/pandas-dev/pandas/issues/16943#issuecomment-315538158,repo: pandas-dev/pandas | issue: MultiIndex row indexing with .loc fail with tuple but work with list of indices | keyword: gotcha
"I came across a slightly related issue: using a multi-index dataframe, why can I only use a tuple as an indexer and not a list (i.e. why do they give different results)? Using the example data, if I want to pull out rows where `ID1=1` and `ID2=1001`, I can only use a tuple inside `loc`: df.loc[(1, 1001)] This returns the desired slice: ``` Value ID3 1 1 2 2 ``` I can't use a list: df.loc[[1, 1001]] This seems to imply that I want values `1` and `1001` for the first level of the index only: ``` …",,,,,,Anecdotal,comment,,,,,,,,2018-02-01,github/cbrnr,https://github.com/pandas-dev/pandas/issues/16943#issuecomment-362222682,repo: pandas-dev/pandas | issue: MultiIndex row indexing with .loc fail with tuple but work with list of indices | keyword: gotcha
"@cbrnr Yes, that is intended behaviour. For single ""labels"" of a MultiIndex (so one value for each level), we always use tuples and not a list, because it would otherwise be difficult to distinguish. I think for this case we are quite consistent within pandas. It is the other way around (in a case where we want list-like, do we accept tuple?) that there can be more discussion. Typically we allfow tuples as list-like, but exactly for the reason above (tuples are used to indicate labels of a MI) …",,,,,,Anecdotal,comment,,,,,,,,2018-02-01,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/16943#issuecomment-362248941,repo: pandas-dev/pandas | issue: MultiIndex row indexing with .loc fail with tuple but work with list of indices | keyword: gotcha
"For the original issue: given the possible confusion between the two, I think it might be better in this case to not interpret the tuple as a list-like. But, in that case, shouldn't it raise an error? As if we interpret `((1, 1001, 1), (1, 1001, 2), (2, 1002, 2))` as a single label, it should not find it? cc @toobaz interesting case :-)",,,,,,Anecdotal,comment,,,,,,,,2018-02-01,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/16943#issuecomment-362249791,repo: pandas-dev/pandas | issue: MultiIndex row indexing with .loc fail with tuple but work with list of indices | keyword: gotcha
"Thanks @jorisvandenbossche, this makes sense! I usually don't distinguish between lists and tuples in plain Python since they are both list-like objects. So this Pandas behavior tripped me up a bit - is this documented clearly somewhere?",,,,,,Anecdotal,comment,,,,,,,,2018-02-01,github/cbrnr,https://github.com/pandas-dev/pandas/issues/16943#issuecomment-362249835,repo: pandas-dev/pandas | issue: MultiIndex row indexing with .loc fail with tuple but work with list of indices | keyword: gotcha
"> But, in that case, shouldn't it raise an error? As if we interpret ((1, 1001, 1), (1, 1001, 2), (2, 1002, 2)) as a single label, it should not find it? Ah, no, I suppose this is wrong. It seems that it *does* interpret `((1, 1001, 1), (1, 1001, 2), (2, 1002, 2))` as a list, but not as a list of labels, but as a list of lists (a list of indexers into one level). So it is indexing as: ``` In [21]: df.loc[pd.IndexSlice[[1, 1001, 1], [1, 1001, 2], [2, 1002, 2]], :] Out[21]: Value ID1 ID2 ID3 1 10…",,,,,,Anecdotal,comment,,,,,,,,2018-02-01,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/16943#issuecomment-362250780,repo: pandas-dev/pandas | issue: MultiIndex row indexing with .loc fail with tuple but work with list of indices | keyword: gotcha
"> I usually don't distinguish between lists and tuples in plain Python since they are both list-like objects. So this Pandas behavior tripped me up a bit - is this documented clearly somewhere? Yes, this is one of the gotcha's due to the complexity of MultiIndexing that we somehow *need* to distinguish between both. And documentation can certainly better about those things. But in general this is also an area where we would need more extensive testing of the different cases, and then better doc…",,,,,,Anecdotal,comment,,,,,,,,2018-02-01,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/16943#issuecomment-362251299,repo: pandas-dev/pandas | issue: MultiIndex row indexing with .loc fail with tuple but work with list of indices | keyword: gotcha
"I could add a statement that tuples are needed in the case of multiple indexers on a multiindex: http://pandas.pydata.org/pandas-docs/stable/advanced.html#using-slicers. Not a warning box, but maybe there's a note or an info box? Or is there a better place to put such a note? Let me know and I can take care of that in a PR.",,,,,,Anecdotal,comment,,,,,,,,2018-02-01,github/cbrnr,https://github.com/pandas-dev/pandas/issues/16943#issuecomment-362252103,repo: pandas-dev/pandas | issue: MultiIndex row indexing with .loc fail with tuple but work with list of indices | keyword: gotcha
"Ah, so we are actually using tuples there in the docs :-) So I just might have had the wrong assumption that a list would work (regarding the last of my comment above https://github.com/pandas-dev/pandas/issues/16943#issuecomment-362250780). But yes, adding a note there that those multiple indexers need a be contained in a tuple is a good idea (and using the IndexSlice makes it even more explicit)",,,,,,Anecdotal,comment,,,,,,,,2018-02-01,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/16943#issuecomment-362256599,repo: pandas-dev/pandas | issue: MultiIndex row indexing with .loc fail with tuple but work with list of indices | keyword: gotcha
"> I could add a statement that tuples are needed in the case of multiple indexers on a multiindex: Great idea! I think such statement should actually go at the beginning of http://pandas.pydata.org/pandas-docs/stable/advanced.html#advanced-indexing-with-hierarchical-index You could maybe start by stating that ``MultiIndex`` keys take the form of tuples, then you could swap the first two examples currently provided (move the _complete indexing_ one first), then introduce partial indexing, and me…",,,,,,Anecdotal,comment,,,,,,,,2018-02-01,github/toobaz,https://github.com/pandas-dev/pandas/issues/16943#issuecomment-362263338,repo: pandas-dev/pandas | issue: MultiIndex row indexing with .loc fail with tuple but work with list of indices | keyword: gotcha
DEPR: Raise `FutureWarning` about raising an error in __array__ when copy=False cannot be honored - [x] closes #60340(Replace xxxx with the GitHub issue number) - [ ] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature - [ ] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit).,,,,,,Anecdotal,issue,,,,,,,,2024-11-22,github/KevsterAmp,https://github.com/pandas-dev/pandas/pull/60395,repo: pandas-dev/pandas | keyword: gotcha | state: closed
"Thanks for the review despite this PR being in draft. Working on the tests and changes based on the comments, will mark it as ready once done",,,,,,Anecdotal,comment,,,,,,,,2024-11-23,github/KevsterAmp,https://github.com/pandas-dev/pandas/pull/60395#issuecomment-2495467464,repo: pandas-dev/pandas | issue: DEPR: Raise `FutureWarning` about raising an error in __array__ when copy=False cannot be honored | keyword: gotcha
```bash =================================== FAILURES =================================== ___________________________ test_to_parquet_new_file ___________________________ [gw1] linux -- Python 3.12.8 /home/runner/micromamba/envs/test/bin/python3.12 [XPASS(strict)] TODO(infer_string) fastparquet ```,,,,,,Anecdotal,comment,,,,,,,,2025-01-03,github/mroeschke,https://github.com/pandas-dev/pandas/pull/60395#issuecomment-2569676180,repo: pandas-dev/pandas | issue: DEPR: Raise `FutureWarning` about raising an error in __array__ when copy=False cannot be honored | keyword: gotcha
BUG: Raise TypeError when subracting DateTimeArray and other date types - [x] closes #59571 (Replace xxxx with the GitHub issue number) - [x] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature - [x] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit). - [ ] Added [type annotations](https://pandas.pydata.org/pandas-docs/de…,,,,,,Anecdotal,issue,,,,,,,,2024-09-26,github/KevsterAmp,https://github.com/pandas-dev/pandas/pull/59901,repo: pandas-dev/pandas | keyword: gotcha | state: closed
@rhshadrach - can you verify if my implementation here is correct? I'm not sure of this. A bit of guidance could be useful. Thanks,,,,,,Anecdotal,comment,,,,,,,,2024-09-26,github/KevsterAmp,https://github.com/pandas-dev/pandas/pull/59901#issuecomment-2376703298,repo: pandas-dev/pandas | issue: BUG: Raise TypeError when subracting DateTimeArray and other date types | keyword: gotcha
"```python from pandas.core.arrays import DatetimeArray datetime_result = self - other if isinstance(datetime_result, DatetimeArray): raise TypeError( ""TypeError: unsupported operand type(s) for -: "" f""'{type(self).__name__}' and '{type(other).__name__}'"" ) return -(datetime_result) ``` I added an outside top-level import because I am experiencing `Unbound` errors with `Datetimearray` in this line exactly: ```python if isinstance(datetime_result, DatetimeArray): ``` <br> Also, I replaced the mes…",,,,,,Anecdotal,comment,,,,,,,,2024-11-12,github/KevsterAmp,https://github.com/pandas-dev/pandas/pull/59901#issuecomment-2470330096,repo: pandas-dev/pandas | issue: BUG: Raise TypeError when subracting DateTimeArray and other date types | keyword: gotcha
"My implementation ~raises~ re-raises new message in `TypeError` on `DateTimeArrays` only. Since there are already other error messages for other dtypes like ```python Cannot subtract tz-naive and tz-aware datetime-like objects. ``` If I implement it as ```python try: return -(self - other) except TypeError as e: raise TypeError( ""Unsupported operand type(s) for -: "" f""'{type(self).__name__}' and '{type(other).__name__}'"" ) from e ``` I would have to fix all tests related to raising `tz-naive/tz…",,,,,,Anecdotal,comment,,,,,,,,2024-11-16,github/KevsterAmp,https://github.com/pandas-dev/pandas/pull/59901#issuecomment-2480854966,repo: pandas-dev/pandas | issue: BUG: Raise TypeError when subracting DateTimeArray and other date types | keyword: gotcha
"Looks like my previous commit affects other datetime related operations. Could you give me ideas on catching the error? From my understanding, when a datetime objects go into this block ```python return -(self - other) ``` It will always raise a `TypeError`. So my previous implementation catches it before doing the operation. ```python # We get here with e.g. datetime objects from pandas.core.arrays import DatetimeArray datetime_result = self - other if isinstance(datetime_result, DatetimeArray…",,,,,,Anecdotal,comment,,,,,,,,2024-11-17,github/KevsterAmp,https://github.com/pandas-dev/pandas/pull/59901#issuecomment-2481155524,repo: pandas-dev/pandas | issue: BUG: Raise TypeError when subracting DateTimeArray and other date types | keyword: gotcha
This pull request is stale because it has been open for thirty days with no activity. Please [update](https://pandas.pydata.org/pandas-docs/stable/development/contributing.html#updating-your-pull-request) and respond to this comment if you're still interested in working on this.,,,,,,Anecdotal,comment,,,,,,,,2025-01-05,github/github-actions[bot],https://github.com/pandas-dev/pandas/pull/59901#issuecomment-2571446457,repo: pandas-dev/pandas | issue: BUG: Raise TypeError when subracting DateTimeArray and other date types | keyword: gotcha
"It appears this PR has gone stale. If you're interested in continuing @KevsterAmp, merge main, address comments, and the fixup the test failures above, and we'd be more than happy to reopen.",,,,,,Anecdotal,comment,,,,,,,,2025-01-11,github/rhshadrach,https://github.com/pandas-dev/pandas/pull/59901#issuecomment-2585228417,repo: pandas-dev/pandas | issue: BUG: Raise TypeError when subracting DateTimeArray and other date types | keyword: gotcha
BUG: Remove `read_json` datetime deprecation warning - [x] closes #59511 (Replace xxxx with the GitHub issue number) - [x] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature - [x] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit). - [x] Added [type annotations](https://pandas.pydata.org/pandas-docs/dev/development/contr…,,,,,,Anecdotal,issue,,,,,,,,2024-09-07,github/KevsterAmp,https://github.com/pandas-dev/pandas/pull/59743,repo: pandas-dev/pandas | keyword: gotcha | state: closed
"@lithomas1 - am I right in guessing that there is likely not to be any more releases from the `2.2.x` branch? If that's the case, this PR should target `2.3.x`. @KevsterAmp - CI issues are likely temporary.",,,,,,Anecdotal,comment,,,,,,,,2024-09-07,github/rhshadrach,https://github.com/pandas-dev/pandas/pull/59743#issuecomment-2335143402,repo: pandas-dev/pandas | issue: BUG: Remove `read_json` datetime deprecation warning | keyword: gotcha
Make pandas/io/sql.py work with sqlalchemy 2.0 - [x] closes #40686 - [ ] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature - [x] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit). - [ ] Added [type annotations](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#type-hints) to new arguments…,,,,,,Anecdotal,issue,,,,,,,,2022-09-15,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576,repo: pandas-dev/pandas | keyword: gotcha | state: closed
"> I'm sorry I'm not so familiar with the SQLAlchemy changes, but do you mind explaining why we should filter `RemovedIn20Warnings` instead of updating the code? I assume we want to be compatible with both 1.4 and 2.0 which are not compatible. But feels like it'd make more sense to for now use `if` statements to support both versions. My approach is to update pandas/io/sql.py so that it can support both versions. I also had to update pandas/tests/io/test_sql.py, because pandas users will need ru…",,,,,,Anecdotal,comment,,,,,,,,2022-09-16,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1249231454,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"Thanks a lot for all the information @cdcadman, that's useful. There is something I still don't fully understand. Let me explain in detail, and please correct me if I'm wrong, I can surely be missing something. Let me use this example: ```python # 1.4 session.query(User).get(42) # 2.0 session.get(User, 42) ``` I see two different cases. **Case A: 2.0 syntax also works in 1.4** This case is easy, we should simply replace one by the other, right? **Case B: 2.0 syntax doesn't work on 1.4** Then, w…",,,,,,Anecdotal,comment,,,,,,,,2022-09-17,github/datapythonista,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1250058091,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"@datapythonista I agree with everything you wrote. I don't think we should censor `RemovedIn20Warning`. The only warnings filter I put into any code is this code in pandas/tests/io/test_sql.py: ``` if SQLALCHEMY_INSTALLED: # This only matters if the environment variable SQLALCHEMY_WARN_20 is set to 1. pytestmark = pytest.mark.filterwarnings(""error::sqlalchemy.exc.RemovedIn20Warning"") ``` This causes tests to fail if they raise RemovedIn20Warning. Your comment about checking sqlalchemy versions …",,,,,,Anecdotal,comment,,,,,,,,2022-09-17,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1250079650,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"@datapythonista I modified this PR's code, title, and original comment, and it is again ready for review. Since I am now testing 2.0-style sqlalchemy connectables instead of checking for warnings, I think this can close #40686. Since I did make a lot of changes, would you prefer that I start a new PR instead?",,,,,,Anecdotal,comment,,,,,,,,2022-09-23,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1256456148,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"Is it known when SQLAlchemy 2.0 will be released? If it comes before pandas 2.0 (late 2022/early 2023), maybe we can just bump the minimum version of SQLAlchemy to 2.0 and adopt the new syntax. I'm not in love in duplicating all the testing & having to support 2 versions of an optional dependency with different syntax. xref: https://github.com/pandas-dev/pandas/issues/44823",,,,,,Anecdotal,comment,,,,,,,,2022-09-23,github/mroeschke,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1256475116,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"@mroeschke I haven't seen a release date for sqlalchemy 2.0. Instead of duplicating all the tests, I could make the `future` argument of `create_engine` depend on either a global constant in test_sql.py or an environment variable. The `future` argument will be supported in sqlalchemy 2.0 and required to be `True`. I could also put this PR on hold until the beta release of sqlalchemy 2.0 comes out.",,,,,,Anecdotal,comment,,,,,,,,2022-09-23,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1256713833,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"I'm planning to make some changes to this PR. Firstly, I noticed that pandas.io.sql.execute is documented, right above this line: https://pandas.pydata.org/docs/user_guide/io.html?highlight=sql%20execute#engine-connection-examples . As it stands, my PR would make this return a context manager instead of a Results Iterable, and I don't think I need to make this change, so I will change it back. I plan to make `SQLDatabase` accept only a SQLAlchemy `Connection` and not an `Engine`. I would change…",,,,,,Anecdotal,comment,,,,,,,,2022-10-28,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1294907916,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"@datapythonista @mroeschke Based on your feedback, I took out all the test duplication, and instead ran the tests with sqlalchemy 2.0.0b2 installed to ensure that this can close #40686. This PR will allow pandas to work with sqlalchemy 1.4.16 (the documented minimum version) and higher, even after sqlalchemy 2.0 is released. I found a note here (written 10/13/2022) on the timing of sqlalchemy 2.0: https://www.sqlalchemy.org/blog/2022/10/13/sqlalchemy-2.0.0b1-released/ >we will likely move from …",,,,,,Anecdotal,comment,,,,,,,,2022-10-31,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1297071602,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
Thanks for your work on this @cdcadman. IMO I would still be partial on just supporting sqlalchemy 2.0 syntax/tests/min version when it becomes available.,,,,,,Anecdotal,comment,,,,,,,,2022-10-31,github/mroeschke,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1297416747,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"@mroeschke As a pandas/sqlalchemy user, it would be really helpful to get these changes into pandas sooner, so that I can get my code ready for sqlalchemy 2.0 sooner. This PR is making the kind of changes envisioned in the first paragraph of the sqlalchemy 2.0 migration document: https://docs.sqlalchemy.org/en/14/changelog/migration_20.html#overview . >The SQLAlchemy 2.0 transition presents itself in the SQLAlchemy 1.4 release as a series of steps that allow an application of any size or comple…",,,,,,Anecdotal,comment,,,,,,,,2022-11-01,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1298783273,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"Thanks @cdcadman. I misunderstood that sqlalchemy 1.4 already has 2.0 functionality, so we can adopt 2.0 syntax/functionality while pandas min version is 1.4. > I made three types of changes to pandas/tests/io/test_sql.py: Could you split these into 3 separate PRs? It's difficult for me to determine which changes correspond with a certain objective. Generally, 1 PR targeting 1 type of change is easier for review.",,,,,,Anecdotal,comment,,,,,,,,2022-11-03,github/mroeschke,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1302463388,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"@mroeschke , I split this into two commits. The tests pass after the first commit with sqlalchemy 1.4.44, but I had to modify `test_sql.py` to make it work with sqlalchemy 2.0.0b3.",,,,,,Anecdotal,comment,,,,,,,,2022-11-19,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1320788745,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"Unfortunately, I introduced a bug in #49531 . Now iteration through the result of `read_sql_query` with a `chunksize` argument can fail, since the connection already closed. I will create a new PR to fix that.",,,,,,Anecdotal,comment,,,,,,,,2022-11-29,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1330157296,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"@mroeschke, I found another bug in addition to the one I intend to resolve with #49967. The second bug is with `pandas.io.sql.execute`. If the caller passes a query that returns a resultset, then the connection might close before the caller can fetch the results. I'm having trouble finding a good workaround. Would it be ok to stop allowing either a sqlalchemy `Engine` or `str` to be passed to this method? It is not a well-documented method. The only mention I found of it is above this line: htt…",,,,,,Anecdotal,comment,,,,,,,,2022-12-09,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1344756733,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"Another potential solution to the `pandas.io.sql.execute` bug is to use `freeze` to fetch all the results before cleaning up. This is inefficient, and I will update the usage docs to point that out, but it will allow downstream code to continue working if it does not pull large amounts of data using `pandas.io.sql.execute`. It will also require updating the minimum version of sqlalchemy to 1.4.45, which is not yet released: https://github.com/sqlalchemy/sqlalchemy/discussions/8962",,,,,,Anecdotal,comment,,,,,,,,2022-12-09,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1344843261,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"Status update: I think this is ready, but it has two PRs in front of it (#50177 and #49967) and I'm still not getting the CI checks to all pass.",,,,,,Anecdotal,comment,,,,,,,,2022-12-13,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1349184851,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"There is one failing test unrelated to the changes I am making. I welcome any suggestions: ``` pandas/tests/io/parser/common/test_file_buffer_url.py::test_file_descriptor_leak[c_high] - AssertionError: ([], [pconn(fd=17, family=<AddressFamily.AF_INET: 2>, type=<SocketKind.SOCK_STREAM: 1>, laddr=addr(ip='10.1.0.145', port=36288), raddr=addr(ip='140.82.112.4', port=443), status='ESTABLISHED')]) ```",,,,,,Anecdotal,comment,,,,,,,,2023-01-06,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1373907318,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"@phofl , the first commit is split off in a separate PR: #49967 . I could make a new PR for the second commit, as it is a separate item: Make PandasSQL.execute arguments more precise. Some of my changes were to get mypy to work with the sqlalchemy 2.0 beta version - for example removing the import of `to_instance` from `sqlalchemy.types`.",,,,,,Anecdotal,comment,,,,,,,,2023-01-06,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1374029941,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"Ah got you, thx. Not necessary. Could you activate the warning here though? To check that ci passes. Set the env variable as in my pr",,,,,,Anecdotal,comment,,,,,,,,2023-01-06,github/phofl,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1374034068,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"@moritzmeister , I was hoping it could be included in pandas 1.5.3, but the decision was not to include it: https://github.com/pandas-dev/pandas/issues/49857#issuecomment-1374889050 . For your issue, I am surrounding the query string with `sqlalchemy.text`.",,,,,,Anecdotal,comment,,,,,,,,2023-01-30,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1408935831,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"Thanks for the link @cdcadman, then I suppose it will end up in the 2.0 release. Yeah I also fell back to wrapping the text. I am suprised that the sqlalchemy version was not pinned to <2.0.0 on 1.5.3 in the meantime.",,,,,,Anecdotal,comment,,,,,,,,2023-01-30,github/moritzmeister,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1409385794,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
Intermittent error fetching value from multi-indexed dataframe - [x] I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas) on StackOverflow for similar questions. - [x] I have asked my usage related question on [StackOverflow](https://stackoverflow.com). - See [this question](https://stackoverflow.com/questions/65927344/accessing-dataframe-with-multiindex-with-date-intermittently-failing) and [this question](https://stackoverflow.com/questions/65937092/read-pickl…,,,,,,Anecdotal,issue,,,,,,,,2021-02-03,github/Stevinson,https://github.com/pandas-dev/pandas/issues/39585,repo: pandas-dev/pandas | keyword: gotcha | state: closed
"Yes, this was changed before 1.2 Beta is interpreted as column, which does not exist, hence the keyerror. You have to use ``breakdown.loc[(date(2020, 6, 3), ""beta"")]``",,,,,,Anecdotal,comment,,,,,,,,2021-02-03,github/phofl,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-772907889,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
"``` b 100 c 100 d 0 e 100 f 0 g 0 Name: (2020-06-03, beta), dtype: int64 ``` This is returned on master with the change",,,,,,Anecdotal,comment,,,,,,,,2021-02-03,github/phofl,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-772910194,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
"I'm confused. When I run that code snippet locally, on Heroku and on an online python editor I see the error occur roughly 1 in 10 runs when running as a new process every time. However, if I run the snippet in an infinite loop I do not see it error.",,,,,,Anecdotal,comment,,,,,,,,2021-02-04,github/Stevinson,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-772913072,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
"I am not sure why this should ever fail when indexed with a tuple. Started round about 30 times, did not fail. Could you provide the traceback of a failure?",,,,,,Anecdotal,comment,,,,,,,,2021-02-04,github/phofl,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-772913937,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
"``` Traceback (most recent call last): File ""/Users/edward/miniconda3/envs/playground/lib/python3.9/site-packages/pandas/core/indexes/base.py"", line 3080, in get_loc return self._engine.get_loc(casted_key) File ""pandas/_libs/index.pyx"", line 70, in pandas._libs.index.IndexEngine.get_loc File ""pandas/_libs/index.pyx"", line 101, in pandas._libs.index.IndexEngine.get_loc File ""pandas/_libs/hashtable_class_helper.pxi"", line 4554, in pandas._libs.hashtable.PyObjectHashTable.get_item File ""pandas/_li…",,,,,,Anecdotal,comment,,,,,,,,2021-02-04,github/Stevinson,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-772915690,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
"Yeh, `breakdown` is successfully created before failure. ```python b c d e f g date a 2020-06-03 15:59:59.999999+00:00 alpha 100 100 0 100 0 0 2020-06-03 alpha 100 100 0 100 0 0 beta 100 100 0 100 0 0 2020-06-04 alpha 100 100 0 100 0 0 beta 100 100 0 100 0 0 2020-06-05 alpha 100 100 0 100 0 0 beta 100 100 0 100 0 0 ```",,,,,,Anecdotal,comment,,,,,,,,2021-02-04,github/Stevinson,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-772918599,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
"``` Traceback (most recent call last): File ""/Users/edward/miniconda3/envs/playground/lib/python3.9/site-packages/pandas/core/generic.py"", line 3732, in xs loc, new_index = index._get_loc_level( File ""/Users/edward/miniconda3/envs/playground/lib/python3.9/site-packages/pandas/core/indexes/multi.py"", line 3034, in _get_loc_level return (self._engine.get_loc(key), None) File ""pandas/_libs/index.pyx"", line 705, in pandas._libs.index.BaseMultiIndexCodesEngine.get_loc TypeError: unsupported operand …",,,,,,Anecdotal,comment,,,,,,,,2021-02-04,github/Stevinson,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-772920904,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
"This is weird, I am getting ``` b 100 c 100 d 0 e 100 f 0 g 0 Name: (2020-06-03, beta), dtype: int64 ``` again ``done = breakdown.loc[(date(2020, 6, 3), ""beta""), :]`` You are on pandas 1.2.1 right? cc @jbrockmendel Thoughts on how to handle this?",,,,,,Anecdotal,comment,,,,,,,,2021-02-04,github/phofl,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-772928620,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
``` INSTALLED VERSIONS ------------------ commit : 9d598a5e1eee26df95b3910e3f2934890d062caa python : 3.9.0.final.0 python-bits : 64 OS : Darwin OS-release : 19.6.0 Version : Darwin Kernel Version 19.6.0: Mon Aug 31 22:12:52 PDT 2020; root:xnu-6153.141.2~1/RELEASE_X86_64 machine : x86_64 processor : i386 byteorder : little LC_ALL : None LANG : None LOCALE : None.UTF-8 pandas : 1.2.1 numpy : 1.19.5 pytz : 2020.5 dateutil : 2.8.1 pip : 21.0 setuptools : 49.6.0.post20210108 Cython : None pytest : N…,,,,,,Anecdotal,comment,,,,,,,,2021-02-04,github/Stevinson,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-772930010,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
"@phofl i gotta run, but tenatively this looks weird: ``` key = date(2020, 6, 3) mi = breakdown.index >>> mi.get_level_values(0).get_loc(key) slice(0, 3, None) >>> mi._get_level_indexer(key, 0) # <--i.e. mi.get_loc(key) slice(0, 2, None) ```",,,,,,Anecdotal,comment,,,,,,,,2021-02-04,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-773444658,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
"Yep, can see the changing result now too. This is a groupby issue. I am getting ``` b c d e f g date a 2020-06-03 15:59:59.999999+00:00 alpha 200 200 0 200 0 0 beta 100 100 0 100 0 0 2020-06-04 alpha 100 100 0 100 0 0 beta 100 100 0 100 0 0 2020-06-05 alpha 100 100 0 100 0 0 beta 100 100 0 100 0 0 ``` as breakdown in the slice(0,2) cases while ``` b c d e f g date a 2020-06-03 15:59:59.999999+00:00 alpha 100 100 0 100 0 0 2020-06-03 alpha 100 100 0 100 0 0 beta 100 100 0 100 0 0 2020-06-04 alph…",,,,,,Anecdotal,comment,,,,,,,,2021-02-04,github/phofl,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-773581012,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
"This is happening in https://github.com/pandas-dev/pandas/blob/ca4f20434c2abba3d3d421a1ee48b1421c8ebc25/pandas/_libs/hashtable_class_helper.pxi.in#L1227 The return value of ``kh_get_pymap`` is wrong in case of slice(0, 2). Would need a bit of help to debug this further",,,,,,Anecdotal,comment,,,,,,,,2021-02-04,github/phofl,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-773607046,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
"This is happening in groupby, not related to Indexing. See my second to last comment. This causes different groups which causes then the KeyError",,,,,,Anecdotal,comment,,,,,,,,2021-02-05,github/phofl,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-774324048,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
"Yes, sorry should have thought about this earlier. I added raise ValueError right before kh_get_pymap is called to show the stack ``` Traceback (most recent call last): File ""/home/developer/.config/JetBrains/PyCharm2020.3/scratches/scratch_4.py"", line 426, in <module> breakdown = df.groupby([""date"", ""a""]).sum() File ""/home/developer/PycharmProjects/pandas/pandas/core/groupby/groupby.py"", line 1670, in sum result = self._agg_general( File ""/home/developer/PycharmProjects/pandas/pandas/core/grou…",,,,,,Anecdotal,comment,,,,,,,,2021-02-05,github/phofl,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-774350555,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
maybe use breakpoint() instead of ValueError to track down the call args? i still think its really weird to get here with a slice,,,,,,Anecdotal,comment,,,,,,,,2021-02-09,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-775555273,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
"Not sure if I understand you correctly, but we are getting there with ``` breakdown = df.groupby([""date"", ""a""]).sum() ``` not with the indexing, so not related to the slice?",,,,,,Anecdotal,comment,,,,,,,,2021-02-13,github/phofl,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-778676468,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
"The error is raised in there yes, but the *cause* of the error is, that the groupby line returns ``` b c d e f g date a 2020-06-03 15:59:59.999999+00:00 alpha 100 100 0 100 0 0 2020-06-03 alpha 100 100 0 100 0 0 beta 100 100 0 100 0 0 2020-06-04 alpha 100 100 0 100 0 0 beta 100 100 0 100 0 0 2020-06-05 alpha 100 100 0 100 0 0 beta 100 100 0 100 0 0 ``` most of the time (14 out of 15 or so) but sometimes we get ``` b c d e f g date a 2020-06-03 15:59:59.999999+00:00 alpha 200 200 0 200 0 0 beta …",,,,,,Anecdotal,comment,,,,,,,,2021-02-14,github/phofl,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-778827036,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
"Of course 2 minutes later i get it on master. With `breakdown` as defined in the OP: ``` mi = breakdown.index key = (date(2020, 6, 3), ""beta"") indices = [0 if pd.isna(v) else lev.get_loc(v) + 1 for lev, v in zip(mi.levels, key)] inds = mi.levels[0].get_loc(key[0]) ``` The `indices =` line is part of BaseMultiIndexCodesEngine.get_loc, and the last line is just the relevant part of the `indices =` line. On runs that dont raise, `inds == 1`. On runs that raise, `inds == slice(0, 2)`, which raises …",,,,,,Anecdotal,comment,,,,,,,,2021-02-14,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-778827062,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
"Wasn't explaining it very well either I think, sorry for this. I am not familiar enough with ``` kh_get_pymap(self.table, <PyObject*>val) ``` to debug this on my own. The input values are the same but the output varies sometimes.",,,,,,Anecdotal,comment,,,,,,,,2021-02-14,github/phofl,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-778827810,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
"@phofl I think I can explain why `kh_get_pymap` returns different indexes for different runs. We use the `hash`-function of PyObject for the hash-map. The hash functions for `str` and `bytes` are salted, see [here](https://docs.python.org/3/reference/datamodel.html#object.__hash__) : > By default, the __hash__() values of str and bytes objects are “salted” with an unpredictable random value. Although they remain constant within an individual Python process, they are not predictable between repe…",,,,,,Anecdotal,comment,,,,,,,,2021-02-14,github/realead,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-778853813,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
"CSV files with header break with delim_whitespace and skiprows using the C-engine #### Code Sample, a copy-pastable example if possible ```python # Python 3 import pandas as pd from io import StringIO data = """"""Meta: x "" <- Space before quote char id value 1 10 2 20 """""" pd.read_csv(StringIO(data), delim_whitespace=True, skiprows=3) ``` #### Problem description When there is a quote char with a space before *and* using `delim_whitespace=True` and `skiprows` reading a CSV file breaks with EmptyDa…",,,,,,Anecdotal,issue,,,,,,,,2017-12-08,github/rgieseke,https://github.com/pandas-dev/pandas/issues/18692,repo: pandas-dev/pandas | keyword: gotcha | state: closed
"@rgieseke : Thanks for reporting this! Handling malformed rows is just not an easy question, and I agree here that indeed we should handle this case. #12900 was a design choice on our part to ensure that quoted lines got fully skipped, even if they had line-terminators within them. In your case, because your quoted line never properly terminates, `read_csv` will skip over everything. Surprised to see that the Python engine is okay with this. Line skipping behavior is hard to understand there be…",,,,,,Anecdotal,comment,,,,,,,,2017-12-11,github/gfyoung,https://github.com/pandas-dev/pandas/issues/18692#issuecomment-350601364,repo: pandas-dev/pandas | issue: CSV files with header break with delim_whitespace and skiprows using the C-engine | keyword: gotcha
"@gfyoung Thanks for the quick feedback! Here is a more real-world example from the data where I encountered the problem (simplified, this is Fortran namelist metadata plus whitespace separated columns), it's not really malformed, just having a space within the quotes. Again, with `engine=""python""` it works. And without the `delim_whitespace` and commas as separator there is also no problem. ```python import pandas as pd from io import StringIO data = """"""&THISFILE_SPECIFICATIONS THISFILE_UNITS=""…",,,,,,Anecdotal,comment,,,,,,,,2017-12-11,github/rgieseke,https://github.com/pandas-dev/pandas/issues/18692#issuecomment-350675308,repo: pandas-dev/pandas | issue: CSV files with header break with delim_whitespace and skiprows using the C-engine | keyword: gotcha
"Indeed, this example is harder to explain away, since it isn't particularly malformed in this case...have a look at the CParser code to see where the discrepancy is arising. > If there were newlines in the header to be skipped, wouldn't it be okay to treat them as newlines? If one has a weird header one wants to be skipped, one would need to check anyway if the first line is correctly identified. Not sure I fully understand your question here.",,,,,,Anecdotal,comment,,,,,,,,2017-12-11,github/gfyoung,https://github.com/pandas-dev/pandas/issues/18692#issuecomment-350768637,repo: pandas-dev/pandas | issue: CSV files with header break with delim_whitespace and skiprows using the C-engine | keyword: gotcha
"> Not sure I fully understand your question here. Sorry, that was hard to parse ... I hadn't thought of the usecase of actually wanting to skip a number of rows. I only ever use skiprows to get rid of meta information in header lines.",,,,,,Anecdotal,comment,,,,,,,,2017-12-11,github/rgieseke,https://github.com/pandas-dev/pandas/issues/18692#issuecomment-350770921,repo: pandas-dev/pandas | issue: CSV files with header break with delim_whitespace and skiprows using the C-engine | keyword: gotcha
"> Sorry, that was hard to parse ... I hadn't thought of the usecase of actually wanting to skip a number of rows. I only ever use skiprows to get rid of meta informationin header lines. Ah, gotcha. In any case, FWIW, if you remove the `skiprows` parameter, Python can still read the input, while the C engine still can't. Thus, that definitely points to some kind of parsing issue.",,,,,,Anecdotal,comment,,,,,,,,2017-12-11,github/gfyoung,https://github.com/pandas-dev/pandas/issues/18692#issuecomment-350772234,repo: pandas-dev/pandas | issue: CSV files with header break with delim_whitespace and skiprows using the C-engine | keyword: gotcha
"I've experienced a similar issue when using skiprows to skip ""corrupt"" lines in the csv files. In this test case no exception is thrown, but some rows are just missing. Tested with pandas 0.22.0 and 0.19.2. Perhaps adding an argument to disable parsing of quotes in skipped rows is an option to fix this? Personally I'd prefer if this would be the default behaviour, and the argument can be used to enable parsing of quotes in skipped rows. ## Example ```python import pandas as pd from io import St…",,,,,,Anecdotal,comment,,,,,,,,2018-05-03,github/cip,https://github.com/pandas-dev/pandas/issues/18692#issuecomment-386212964,repo: pandas-dev/pandas | issue: CSV files with header break with delim_whitespace and skiprows using the C-engine | keyword: gotcha
"DOC: fix SA05 errors in docstrings Pandas has a script for validating docstrings: https://github.com/pandas-dev/pandas/blob/1d7aedcd19470f689e4db625271802eb49531e80/ci/code_checks.sh#L145-L206 Currently, some methods fail the SA05 check. The task here is: - take 2-4 methods - run: scripts/validate_docstrings.py --format=actions --errors=SA05 `method-name` - check if validation docstrings passes for those methods, and if it’s necessary fix the docstrings according to whatever error is reported -…",,,,,,Anecdotal,issue,,,,,,,,2024-02-13,github/jordan-d-murphy,https://github.com/pandas-dev/pandas/issues/57392,repo: pandas-dev/pandas | keyword: gotcha | state: closed
I'll take ``` pandas.PeriodIndex.asfreq pandas.arrays.ArrowStringArray pandas.arrays.StringArray ```,,,,,,Anecdotal,comment,,,,,,,,2024-02-13,github/williamking1221,https://github.com/pandas-dev/pandas/issues/57392#issuecomment-1942260563,repo: pandas-dev/pandas | issue: DOC: fix SA05 errors in docstrings  | keyword: gotcha
"I found a small fix that resolved all of the ExponentialMovingWindow, Expanding, Rolling, and Window SA05 errors (and pandas.plotting.boxplot). This fix takes care of the following: ``` pandas.core.window.ewm.ExponentialMovingWindow.corr pandas.core.window.ewm.ExponentialMovingWindow.cov pandas.core.window.ewm.ExponentialMovingWindow.mean pandas.core.window.ewm.ExponentialMovingWindow.std pandas.core.window.ewm.ExponentialMovingWindow.sum pandas.core.window.ewm.ExponentialMovingWindow.var panda…",,,,,,Anecdotal,comment,,,,,,,,2024-02-14,github/jordan-d-murphy,https://github.com/pandas-dev/pandas/issues/57392#issuecomment-1943054366,repo: pandas-dev/pandas | issue: DOC: fix SA05 errors in docstrings  | keyword: gotcha
I think this is the updated list of what's left: ``` pandas.core.groupby.SeriesGroupBy.first pandas.core.groupby.SeriesGroupBy.last pandas.core.resample.Resampler.first pandas.core.resample.Resampler.last pandas.core.window.expanding.Expanding.aggregate pandas.core.window.rolling.Rolling.aggregate pandas.plotting.bootstrap_plot pandas.plotting.radviz ```,,,,,,Anecdotal,comment,,,,,,,,2024-02-14,github/jordan-d-murphy,https://github.com/pandas-dev/pandas/issues/57392#issuecomment-1943069872,repo: pandas-dev/pandas | issue: DOC: fix SA05 errors in docstrings  | keyword: gotcha
Hey @jordan-d-murphy Do we need to do something before our changes are merged even if all the checks pass? Somebody reviewed my changes for my above PR but neither merged them nor rejected them. Am I missing something ?,,,,,,Anecdotal,comment,,,,,,,,2024-02-14,github/Pistonamey,https://github.com/pandas-dev/pandas/issues/57392#issuecomment-1944202114,repo: pandas-dev/pandas | issue: DOC: fix SA05 errors in docstrings  | keyword: gotcha
"@Pistonamey Looks like your PR is merged now, can you check again? Sometimes there might by multiple reviewers on a PR and the first reviewer might add their approval review but leave it for another reviewer to merge. Looks like that was the case here ![image](https://github.com/pandas-dev/pandas/assets/35613487/a3f80b0f-d1bf-4b7f-ba92-060121c8b699) Great work! Thank you for helping out 🙂",,,,,,Anecdotal,comment,,,,,,,,2024-02-14,github/jordan-d-murphy,https://github.com/pandas-dev/pandas/issues/57392#issuecomment-1944257945,repo: pandas-dev/pandas | issue: DOC: fix SA05 errors in docstrings  | keyword: gotcha
"> I'll take > > ``` > pandas.core.resample.Resampler.first > pandas.core.resample.Resampler.last > ``` Seems like both are ok, even without changes. I'll just remove these two then.",,,,,,Anecdotal,comment,,,,,,,,2024-02-14,github/williamking1221,https://github.com/pandas-dev/pandas/issues/57392#issuecomment-1944499498,repo: pandas-dev/pandas | issue: DOC: fix SA05 errors in docstrings  | keyword: gotcha
"Hi @jordan-d-murphy, For ``` pandas.core.window.expanding.Expanding.aggregate pandas.core.window.rolling.Rolling.aggregate ``` There is a ES01 -- No extended summary found error. If there a suggested fix to this? Thanks!",,,,,,Anecdotal,comment,,,,,,,,2024-02-14,github/williamking1221,https://github.com/pandas-dev/pandas/issues/57392#issuecomment-1944502026,repo: pandas-dev/pandas | issue: DOC: fix SA05 errors in docstrings  | keyword: gotcha
"For the ES01 I have opened a PR to get these addressed. See https://github.com/pandas-dev/pandas/pull/57359 I have a merge conflict to resolve in order to get that merged, and then I'll open an Issue to fix all ES01 errors. (Similar to this issue) Hopefully that will happen later today when I get home from work. I need to do that from my personal laptop. But feel free to fix it if you want to! The ultimate goal of All these issues I'm opening is to have correct docstrings for all the functions.…",,,,,,Anecdotal,comment,,,,,,,,2024-02-14,github/jordan-d-murphy,https://github.com/pandas-dev/pandas/issues/57392#issuecomment-1944725818,repo: pandas-dev/pandas | issue: DOC: fix SA05 errors in docstrings  | keyword: gotcha
"Thanks @jordan-d-murphy! I will pull from main, which as the ignore ES01 checks, and then raise a PR for ``` pandas.core.window.expanding.Expanding.aggregate pandas.core.window.rolling.Rolling.aggregate ``` Do you have a link to a doc on resolving ES01 issues?",,,,,,Anecdotal,comment,,,,,,,,2024-02-15,github/williamking1221,https://github.com/pandas-dev/pandas/issues/57392#issuecomment-1947513017,repo: pandas-dev/pandas | issue: DOC: fix SA05 errors in docstrings  | keyword: gotcha
"@williamking1221 Yeah here's some info on the ES01s from what I understand. I'll show two examples to illustrate: Running the following `scripts/validate_docstrings.py --format=actions --errors=ES01 pandas.Categorical.map` outputs: ``` ################################################################################ ################################## Validation ################################## ################################################################################ Docstring for ""panda…",,,,,,Anecdotal,comment,,,,,,,,2024-02-16,github/jordan-d-murphy,https://github.com/pandas-dev/pandas/issues/57392#issuecomment-1947529238,repo: pandas-dev/pandas | issue: DOC: fix SA05 errors in docstrings  | keyword: gotcha
"@williamking1221 thanks for opening the issue to close this out, I added one comment as a suggestion to ensure the integrity of the SA05 checks",,,,,,Anecdotal,comment,,,,,,,,2024-02-16,github/jordan-d-murphy,https://github.com/pandas-dev/pandas/issues/57392#issuecomment-1947533748,repo: pandas-dev/pandas | issue: DOC: fix SA05 errors in docstrings  | keyword: gotcha
@williamking1221 What we were talking about was adding SA05 here https://github.com/pandas-dev/pandas/blob/43209e798dd86237a39b667932b4ec438a4ae059/ci/code_checks.sh#L68-L70 when we remove the block here https://github.com/pandas-dev/pandas/blob/43209e798dd86237a39b667932b4ec438a4ae059/ci/code_checks.sh#L3185-L3187,,,,,,Anecdotal,comment,,,,,,,,2024-02-16,github/jordan-d-murphy,https://github.com/pandas-dev/pandas/issues/57392#issuecomment-1947644377,repo: pandas-dev/pandas | issue: DOC: fix SA05 errors in docstrings  | keyword: gotcha
> @williamking1221 What we were talking about was adding SA05 here > > https://github.com/pandas-dev/pandas/blob/43209e798dd86237a39b667932b4ec438a4ae059/ci/code_checks.sh#L68-L70 > > > when we remove the block here > https://github.com/pandas-dev/pandas/blob/43209e798dd86237a39b667932b4ec438a4ae059/ci/code_checks.sh#L3185-L3187 Ah gotcha! Thanks! Fixed it in recent commit.,,,,,,Anecdotal,comment,,,,,,,,2024-02-16,github/williamking1221,https://github.com/pandas-dev/pandas/issues/57392#issuecomment-1947651433,repo: pandas-dev/pandas | issue: DOC: fix SA05 errors in docstrings  | keyword: gotcha
"for anyone interested, I've opened some similar issues: [DOC: fix RT03 errors in docstrings](https://github.com/pandas-dev/pandas/issues/57416) [DOC: fix SA01 errors in docstrings](https://github.com/pandas-dev/pandas/issues/57417) [DOC: fix PR07 errors in docstrings](https://github.com/pandas-dev/pandas/issues/57420) [DOC: fix PR01 errors in docstrings](https://github.com/pandas-dev/pandas/issues/57438) [DOC: fix ES01 errors in docstrings](https://github.com/pandas-dev/pandas/issues/57440) [DO…",,,,,,Anecdotal,comment,,,,,,,,2024-02-19,github/jordan-d-murphy,https://github.com/pandas-dev/pandas/issues/57392#issuecomment-1953146320,repo: pandas-dev/pandas | issue: DOC: fix SA05 errors in docstrings  | keyword: gotcha
"Translate more banned import config to ruff Translate some config in `unwanted-patterns-in-tests` to `ruff`. I think the main selling point of `ruff` here is just the ease of implementing new rules, like the currently unenabled `pytest.warns` one. The major difference is that `unwanted-patterns-in-tests` is currently only enabled in `pandas/tests` while with this implementation, `TID251` will be enabled globally. There are several approaches we can explore to reconcile the 2 behaviors. - Keep i…",,,,,,Anecdotal,issue,,,,,,,,2024-02-06,github/tqa236,https://github.com/pandas-dev/pandas/pull/57283,repo: pandas-dev/pandas | keyword: gotcha | state: closed
BUG: Fix near-minimum timestamp handling - [x] closes #57150 - [x] [Tests added and passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#writing-tests) if fixing a bug or adding a new feature - [x] All [code checks passed](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#pre-commit). - [ ] Added [type annotations](https://pandas.pydata.org/pandas-docs/dev/development/contributing_codebase.html#type-hints) to new arguments/metho…,,,,,,Anecdotal,issue,,,,,,,,2024-02-09,github/robert-schmidtke,https://github.com/pandas-dev/pandas/pull/57314,repo: pandas-dev/pandas | keyword: gotcha | state: closed
"@WillAyd where would I add tests for this? If I'm not mistaken, then the `npy_datetimestruct_to_datetime` function is never tested directly. Should I use the reproducible example in the original issue or come up with a narrower test?",,,,,,Anecdotal,comment,,,,,,,,2024-02-09,github/robert-schmidtke,https://github.com/pandas-dev/pandas/pull/57314#issuecomment-1935622524,repo: pandas-dev/pandas | issue: BUG: Fix near-minimum timestamp handling | keyword: gotcha
"> @WillAyd where would I add tests for this? If I'm not mistaken, then the `npy_datetimestruct_to_datetime` function is never tested directly. Should I use the reproducible example in the original issue or come up with a narrower test? I'll fix the failing Windows tests and found in the documentation some guidance on where and how to add the tests. Will mark ready for review once done.",,,,,,Anecdotal,comment,,,,,,,,2024-02-09,github/robert-schmidtke,https://github.com/pandas-dev/pandas/pull/57314#issuecomment-1936158564,repo: pandas-dev/pandas | issue: BUG: Fix near-minimum timestamp handling | keyword: gotcha
"I have started over with a failing test, see the results on https://github.com/pandas-dev/pandas/pull/57314/commits/b4c825da2f87131b379ee81119ccdc3ba8237fec. To resolve, I went with a minimally invasive approach that just checks for negative extreme before scaling to microseconds, and then doing the calculation backwards from the minimum valid timestamp. I didn't find any constant defined for this (`NPY_MIN_INT64 + 1`), so instead of polluting the global namespace in `np_datetime.h`, I went wit…",,,,,,Anecdotal,comment,,,,,,,,2024-02-16,github/robert-schmidtke,https://github.com/pandas-dev/pandas/pull/57314#issuecomment-1948430840,repo: pandas-dev/pandas | issue: BUG: Fix near-minimum timestamp handling | keyword: gotcha
BLD: remove support for 32 bit seems microsoft has removed support starting 3.10 (as is numpy) we should do the same,,,,,,Anecdotal,issue,,,,,,,,2021-11-14,github/jreback,https://github.com/pandas-dev/pandas/issues/44453,repo: pandas-dev/pandas | keyword: pro tip | state: closed
"Apologies if I'm misunderstanding here, but it looks like Numpy isn't dropping 32-bit support [entirely](https://github.com/numpy/numpy/issues/20277#issuecomment-957529088). It looks like they're only dropping wheel generation because of infra constraints but will continue to test. I'm interested because in Gentoo, we still support a number of 32-bit architectures (arm, x86 being the two main ones), and Pandas ends up in the depgraph often unavoidably if we want to run test suites on programs. …",,,,,,Anecdotal,comment,,,,,,,,2021-11-15,github/thesamesam,https://github.com/pandas-dev/pandas/issues/44453#issuecomment-968468140,repo: pandas-dev/pandas | issue: BLD: remove support for 32 bit | keyword: pro tip
"To be clear, @jreback only did a proposal to remove them, nothing has been decided yet, and your feedback on this idea is welcome. Since numpy is not removing them, and if we also switch to cibuildwheel (cfr https://github.com/pandas-dev/pandas/issues/44027, assuming this makes it easier to keep 32bit wheels), I personally think we can follow numpy on this.",,,,,,Anecdotal,comment,,,,,,,,2021-11-15,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/44453#issuecomment-968629729,repo: pandas-dev/pandas | issue: BLD: remove support for 32 bit | keyword: pro tip
"For the record, I don't have a problem with removing 32-bit wheels or 32-bit testing, if that is cumbersome. However, what I would like to avoid is removing the code to support 32-bit setups, or rejecting patches that fix them.",,,,,,Anecdotal,comment,,,,,,,,2021-11-15,github/mgorny,https://github.com/pandas-dev/pandas/issues/44453#issuecomment-968663636,repo: pandas-dev/pandas | issue: BLD: remove support for 32 bit | keyword: pro tip
"> For the record, I don't have a problem with removing 32-bit wheels or 32-bit testing, if that is cumbersome. However, what I would like to avoid is removing the code to support 32-bit setups, or rejecting patches that fix them. @mgorny these would always be welcome it's the testing / support that can be cumbersome (it's not so burdensome more that if others are dropping support then we should as well; note that numpy is usually the last to drop support for things)",,,,,,Anecdotal,comment,,,,,,,,2021-11-15,github/jreback,https://github.com/pandas-dev/pandas/issues/44453#issuecomment-968832580,repo: pandas-dev/pandas | issue: BLD: remove support for 32 bit | keyword: pro tip
"i agree with @jorisvandenbossche, following numpy's lead on this seems reasonable xref https://discuss.python.org/t/dropping-32-bit-packages/5476",,,,,,Anecdotal,comment,,,,,,,,2021-11-15,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/44453#issuecomment-969193671,repo: pandas-dev/pandas | issue: BLD: remove support for 32 bit | keyword: pro tip
"At the very least, I'd like to stop making 32-bit wheels by 2.0. Note that this issue of dropping 32 bit has already come up before. xref #15889",,,,,,Anecdotal,comment,,,,,,,,2021-11-15,github/lithomas1,https://github.com/pandas-dev/pandas/issues/44453#issuecomment-969339795,repo: pandas-dev/pandas | issue: BLD: remove support for 32 bit | keyword: pro tip
"FYI, we are dropping Linux 32 bit builds after numpy 1.22 comes out, where they also will be dropping 32 bit Linux builds. Windows 32 bit will not be affected, and we will still test/support 32 bit.",,,,,,Anecdotal,comment,,,,,,,,2021-11-22,github/lithomas1,https://github.com/pandas-dev/pandas/issues/44453#issuecomment-974937782,repo: pandas-dev/pandas | issue: BLD: remove support for 32 bit | keyword: pro tip
>why should we support 32 bit at all? >this is just more work - sure it's probably easy but we are all volunteer Pro tip: by not contributing into open source at all instead of contributing (and by not doing any other work you can avoid doing) you can keep even more work away of you.,,,,,,Anecdotal,comment,,,,,,,,2022-03-01,github/KOLANICH,https://github.com/pandas-dev/pandas/issues/44453#issuecomment-1055636670,repo: pandas-dev/pandas | issue: BLD: remove support for 32 bit | keyword: pro tip
"> I personally think we can follow numpy on this. I proposed to drop 32-bit windows wheels in numpy/numpy#23717. I do not recall seeing 32-bit windows issues or requests over the past few years, has pandas? FWIW, SciPy already dropped 32-bit windows wheels.",,,,,,Anecdotal,comment,,,,,,,,2023-05-21,github/mattip,https://github.com/pandas-dev/pandas/issues/44453#issuecomment-1556170092,repo: pandas-dev/pandas | issue: BLD: remove support for 32 bit | keyword: pro tip
"I am episodically using python on a Pentium III-based PC on Linux and until a very recent time I was also using a Core2Duo-based PC (clearly 64-bit capable, but it has only 2 GiB of RAM so I cannot afford wasting it on 64-bit pointers, also it has quite some other software installed, including some drivers for some expensive and legacy hardware that are 32-bit only. Now that PC is still used, including Python, but by other people (it is a company PC) ). >I do not recall seeing 32-bit windows is…",,,,,,Anecdotal,comment,,,,,,,,2023-05-21,github/KOLANICH,https://github.com/pandas-dev/pandas/issues/44453#issuecomment-1556175968,repo: pandas-dev/pandas | issue: BLD: remove support for 32 bit | keyword: pro tip
"IMO, the only thing worth keeping 32-bit support for is WASM, at least until the memory64(https://github.com/WebAssembly/memory64) proposal goes through, as people seem interested in using pandas with pyodide. > I am episodically using python on a Pentium III-based PC on Linux and until a very recent time I was also using a Core2Duo-based PC (clearly 64-bit capable, but it has only 2 GiB of RAM so I cannot afford wasting it on 64-bit pointers, also it has quite some other software installed, in…",,,,,,Anecdotal,comment,,,,,,,,2023-05-23,github/lithomas1,https://github.com/pandas-dev/pandas/issues/44453#issuecomment-1560151445,repo: pandas-dev/pandas | issue: BLD: remove support for 32 bit | keyword: pro tip
">Is there a reason that you are not OK using older versions of pandas? ""Just use an old version"" is a very poor argument. Of course I'm free to use old versions of everything (because a lot of things drop support with old versions of their dependencies, it is easy and pleasant to stop doing things you personally are not rewarded for, as Linus says, ""good riddance"", and as the Russian proverb says ""[taking] a woman off a cart [makes it] easier for the horse""), but when one has to use old version…",,,,,,Anecdotal,comment,,,,,,,,2023-05-23,github/KOLANICH,https://github.com/pandas-dev/pandas/issues/44453#issuecomment-1560193136,repo: pandas-dev/pandas | issue: BLD: remove support for 32 bit | keyword: pro tip
"You ask for simple answers, but our world is complex. The simple answer: right now I don't need bleeding edge version of Pandas on that machine. The complex answer: 1) I may need it in future 2) every drop of support creates chain reactions in multiple ways. a. forces users to replace their hardware, decreasing incentives to maintain support of older hardware for other software b. creates incentives for packages depending on the one that has dropped support to drop support. They are broken any …",,,,,,Anecdotal,comment,,,,,,,,2023-05-24,github/KOLANICH,https://github.com/pandas-dev/pandas/issues/44453#issuecomment-1560278778,repo: pandas-dev/pandas | issue: BLD: remove support for 32 bit | keyword: pro tip
"I'm a little confused here. Didn't we agree that general 32-bit support will be kept and patches will still be accepted? If that is the case, I'm not sure I don't understand what you are talking about now. Gentoo very much needs 32-bit support in the latest version. Supporting old versions for a limited time usually triples the maintenance cost for us (and we're just a few volunteers maintaining hundreds of packages), and permanently is simply a no-go. Old versions tend to collect bugs (and Pyt…",,,,,,Anecdotal,comment,,,,,,,,2023-05-24,github/mgorny,https://github.com/pandas-dev/pandas/issues/44453#issuecomment-1560452701,repo: pandas-dev/pandas | issue: BLD: remove support for 32 bit | keyword: pro tip
I think the decision here was that pandas no longer distributes wheels for 32 bit platforms (starting 2.1 IIRC) but pandas will continue to accept patches and test on 32 bit Linux so going to close,,,,,,Anecdotal,comment,,,,,,,,2023-11-21,github/mroeschke,https://github.com/pandas-dev/pandas/issues/44453#issuecomment-1821566190,repo: pandas-dev/pandas | issue: BLD: remove support for 32 bit | keyword: pro tip
"CLN/ENH/BLD: Remove need for 2to3 for Python 3. Fixes #4375 and #4372. Many changes to make codebase compatible in 2 and 3. For `range`, `zip`, `map`, etc. tried to favor iterators over needing to use lists. Changes: - No more 2to3 in setup.py - merges util/compat and util/py3compat into pandas/compat - incorporates useful parts of the six library into compat (+ adds SIX to LICENSES) - defaults to using iterators in both Python 2 and Python 3 for range, zip, map, and filter - Adds lrange, lzip,…",,,,,,Anecdotal,issue,,,,,,,,2013-07-27,github/jtratner,https://github.com/pandas-dev/pandas/pull/4384,repo: pandas-dev/pandas | keyword: pro tip | state: closed
"@cpcloud actually, only used in one place in library and it's used as an iterator anyways. still need to check for itertools changes.",,,,,,Anecdotal,comment,,,,,,,,2013-07-27,github/jtratner,https://github.com/pandas-dev/pandas/pull/4384#issuecomment-21673197,repo: pandas-dev/pandas | issue: CLN/ENH/BLD: Remove need for 2to3 for Python 3. | keyword: pro tip
"@cpcloud just put `from six.moves import filter` at the top of the file and you'll be fine. It just uses itertools.ifilter under the hood, so slap a list on it otherwise. Very simple",,,,,,Anecdotal,comment,,,,,,,,2013-07-27,github/jtratner,https://github.com/pandas-dev/pandas/pull/4384#issuecomment-21673525,repo: pandas-dev/pandas | issue: CLN/ENH/BLD: Remove need for 2to3 for Python 3. | keyword: pro tip
I believe all of those issues are taken care of during translation from the Cython to C. I don't see why any Cython would need to be changed. Are you getting compilation errors?,,,,,,Anecdotal,comment,,,,,,,,2013-07-27,github/cpcloud,https://github.com/pandas-dev/pandas/pull/4384#issuecomment-21674005,repo: pandas-dev/pandas | issue: CLN/ENH/BLD: Remove need for 2to3 for Python 3. | keyword: pro tip
@cpcloud guess I can empirically see that that's not true...just gone through so many changes (and I can't get py3 pandas to work on my mac).,,,,,,Anecdotal,comment,,,,,,,,2013-07-27,github/jtratner,https://github.com/pandas-dev/pandas/pull/4384#issuecomment-21674342,repo: pandas-dev/pandas | issue: CLN/ENH/BLD: Remove need for 2to3 for Python 3. | keyword: pro tip
"@cpcloud I've been having trouble getting pandas with Python 3 to install on my mac, which is why I haven't been able to step through.",,,,,,Anecdotal,comment,,,,,,,,2013-07-28,github/jtratner,https://github.com/pandas-dev/pandas/pull/4384#issuecomment-21674932,repo: pandas-dev/pandas | issue: CLN/ENH/BLD: Remove need for 2to3 for Python 3. | keyword: pro tip
ah ok... it's weird because i think you actually have to `setup.py install` it for `2to3` to run which is annoying whenever you make a change...you have to keep doing it,,,,,,Anecdotal,comment,,,,,,,,2013-07-28,github/cpcloud,https://github.com/pandas-dev/pandas/pull/4384#issuecomment-21674948,repo: pandas-dev/pandas | issue: CLN/ENH/BLD: Remove need for 2to3 for Python 3. | keyword: pro tip
"@cpcloud well, the current build has 2to3 disabled, so you don't actually need to do that. I'm trying a separate build with 2to3 enabled to see if it resolves the problem.",,,,,,Anecdotal,comment,,,,,,,,2013-07-28,github/jtratner,https://github.com/pandas-dev/pandas/pull/4384#issuecomment-21674987,repo: pandas-dev/pandas | issue: CLN/ENH/BLD: Remove need for 2to3 for Python 3. | keyword: pro tip
anyway my dev version works so just keep at it i think you'll get it ... i would offer to clone my env but i'm not using mac,,,,,,Anecdotal,comment,,,,,,,,2013-07-28,github/cpcloud,https://github.com/pandas-dev/pandas/pull/4384#issuecomment-21675006,repo: pandas-dev/pandas | issue: CLN/ENH/BLD: Remove need for 2to3 for Python 3. | keyword: pro tip
"@cpcloud okay, pretty sure I can figure it out. It doesn't fail with any of the stata errors with 2to3, so that means I just have to apply some of those fixes and it will work :).",,,,,,Anecdotal,comment,,,,,,,,2013-07-28,github/jtratner,https://github.com/pandas-dev/pandas/pull/4384#issuecomment-21675020,repo: pandas-dev/pandas | issue: CLN/ENH/BLD: Remove need for 2to3 for Python 3. | keyword: pro tip
"ahh so the problem is actually not in `_read_header` you need to convert the calls to `np.where(map(...))` to `np.where(list(map()))` then you're golden...i just tried it, it works",,,,,,Anecdotal,comment,,,,,,,,2013-07-28,github/cpcloud,https://github.com/pandas-dev/pandas/pull/4384#issuecomment-21675394,repo: pandas-dev/pandas | issue: CLN/ENH/BLD: Remove need for 2to3 for Python 3. | keyword: pro tip
"DRA: Fix flake in dra BindingConditions integration test <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is …",,,,,,Anecdotal,issue,,,,,,,,2025-08-18,github/KobayashiD27,https://github.com/kubernetes/kubernetes/pull/133585,repo: kubernetes/kubernetes | keyword: best practice | state: open
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.34` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.34.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133585#issuecomment-3194919418,repo: kubernetes/kubernetes | issue: DRA: Fix flake in dra BindingConditions integration test | keyword: best practice
"Hi @KobayashiD27. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once th…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133585#issuecomment-3194919529,repo: kubernetes/kubernetes | issue: DRA: Fix flake in dra BindingConditions integration test | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133585#"" title=""Author self-approved"">KobayashiD27</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [klueska](https://github.com/klueska) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted by th…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133585#issuecomment-3194920020,repo: kubernetes/kubernetes | issue: DRA: Fix flake in dra BindingConditions integration test | keyword: best practice
"@KobayashiD27: The following tests **failed**, say `/retest` to rerun all failed tests or `/retest-required` to rerun all mandatory failed tests: Test name | Commit | Details | Required | Rerun command --- | --- | --- | --- | --- pull-kubernetes-kind-dra-n-1 | 97a146d47dfb8c73696c256b71acdaa4424905a2 | [link](https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/133585/pull-kubernetes-kind-dra-n-1/1957598484685131776) | false | `/test pull-kubernetes-kind-dra-n-1` pull-kubernetes-kind-dra…",,,,,,Anecdotal,comment,,,,,,,,2025-08-19,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133585#issuecomment-3198846429,repo: kubernetes/kubernetes | issue: DRA: Fix flake in dra BindingConditions integration test | keyword: best practice
"added WithFlaky to the device plugin test case: supports extended resources together with ResourceClaim <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of i…",,,,,,Anecdotal,issue,,,,,,,,2025-07-29,github/yliaog,https://github.com/kubernetes/kubernetes/pull/133286,repo: kubernetes/kubernetes | keyword: best practice | state: open
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133286#issuecomment-3133905877,repo: kubernetes/kubernetes | issue: added WithFlaky to the device plugin test case: supports extended resources together with ResourceClaim | keyword: best practice
"In the PR description you claim that it conflicts, but that claim is not supported by an actual explanation why or how. Let's investigate and find the root cause before making further changes. /cc @johnbelamaric",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/pohly,https://github.com/kubernetes/kubernetes/pull/133286#issuecomment-3133977010,repo: kubernetes/kubernetes | issue: added WithFlaky to the device plugin test case: supports extended resources together with ResourceClaim | keyword: best practice
"18 minutes after the test finishes, kubelet still publishes 'example.com/resource: ""0""' in node.status.Capacity and node.status.Allocatable. I tried to remove it from node.status, but kubelet keeps putting it back. still need to figure out how to cleanly remove it. while true; do date; kubectl get no -o yaml|grep example; sleep 5; done Tue Jul 29 08:24:39 PM UTC 2025 example.com/resource: ""0"" example.com/resource: ""2"" ... ... ... Tue Jul 29 08:42:17 PM UTC 2025 example.com/resource: ""0"" example…",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/yliaog,https://github.com/kubernetes/kubernetes/pull/133286#issuecomment-3134022809,repo: kubernetes/kubernetes | issue: added WithFlaky to the device plugin test case: supports extended resources together with ResourceClaim | keyword: best practice
"@pohly @johnbelamaric found out the root cause. the feature ""DRADeviceBindingConditions: true"" does not work well with ""DRAExtendedResources: true"", i think DRADeviceBindingConditions was merged last night? I did not enable it when running the test locally. 360 2025-07-29T23:19:13.141476715Z stderr F panic: runtime error: invalid memory address or nil pointer dereference 361 2025-07-29T23:19:13.141506335Z stderr F [signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x193c851] k8s.io/ku…",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/yliaog,https://github.com/kubernetes/kubernetes/pull/133286#issuecomment-3134422654,repo: kubernetes/kubernetes | issue: added WithFlaky to the device plugin test case: supports extended resources together with ResourceClaim | keyword: best practice
"So this E2E failure was legitimate and https://github.com/kubernetes/kubernetes/pull/133290 fixes the problem, and was merged via https://github.com/kubernetes/kubernetes/pull/132522. There's still some learnings here: removing the extended resource from node status doesn't really work because of the kubelet. We should update the code accordingly and replace patching the node status with waiting for the count to go to zero or (in case of a future kubelet enhancement) to not be present. Let's us…",,,,,,Anecdotal,comment,,,,,,,,2025-07-30,github/pohly,https://github.com/kubernetes/kubernetes/pull/133286#issuecomment-3135245756,repo: kubernetes/kubernetes | issue: added WithFlaky to the device plugin test case: supports extended resources together with ResourceClaim | keyword: best practice
"we can keep using this PR since it has some context already, what needs to figure out is why kubelet keeping publishing the extended resource after device plugin has been removed.",,,,,,Anecdotal,comment,,,,,,,,2025-07-30,github/yliaog,https://github.com/kubernetes/kubernetes/pull/133286#issuecomment-3136970606,repo: kubernetes/kubernetes | issue: added WithFlaky to the device plugin test case: supports extended resources together with ResourceClaim | keyword: best practice
"@BenTheElder we should fix the leak, but before that, it is better to comment out the other conflicting test, which was added for testing the deployDevicePlugin, which has already been tested in ""must run pods with extended resource on dra nodes and device plugin nodes"" test case. they are already marked as Serial, but that would not help, since the leaked example.com/resouce in the cluster's nodes still interferes with the next test run in the same cluster.",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/yliaog,https://github.com/kubernetes/kubernetes/pull/133286#issuecomment-3180769933,repo: kubernetes/kubernetes | issue: added WithFlaky to the device plugin test case: supports extended resources together with ResourceClaim | keyword: best practice
"> they are already marked as Serial, but that would not help, since the leaked example.com/resouce in the cluster's nodes still interferes with the next test run in the same cluster. Ah ... that's pretty bad, would like to get that fixed. We usually don't comment out tests, we e.g. `[Flaky]` or `[Serial]` or `[Disruptive]` tag them or delete them outright as appropriate. If deleted they will still be in git. If meant to be only temporarily disabled, Flaky etc works. framework.WithFlaky() might …",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/133286#issuecomment-3181291214,repo: kubernetes/kubernetes | issue: added WithFlaky to the device plugin test case: supports extended resources together with ResourceClaim | keyword: best practice
@BenTheElder wdyt? shall we merge this? I think this flake will be fixed with it: https://testgrid.k8s.io/sig-node-dynamic-resource-allocation#ci-kind-dra-all,,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/johnbelamaric,https://github.com/kubernetes/kubernetes/pull/133286#issuecomment-3189222581,repo: kubernetes/kubernetes | issue: added WithFlaky to the device plugin test case: supports extended resources together with ResourceClaim | keyword: best practice
"> @BenTheElder wdyt? shall we merge this? I think this flake will be fixed with it: https://testgrid.k8s.io/sig-node-dynamic-resource-allocation#ci-kind-dra-all Yeah, I think withflaky is an OK stop-gap, but we also really need to address leaking things on the cluster. At least it's not leaking in the proposed conformance test. AIUI, this fix only ""works"" because in CI we're deleting the clusters between runs and we now only run one of the tests. But the tests should be safe to run repeatedly (…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/133286#issuecomment-3197924449,repo: kubernetes/kubernetes | issue: added WithFlaky to the device plugin test case: supports extended resources together with ResourceClaim | keyword: best practice
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133286#pullrequestreview-3129403686"" title=""Approved"">BenTheElder</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/133286#"" title=""Author self-approved"">yliaog</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https://…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133286#issuecomment-3197926910,repo: kubernetes/kubernetes | issue: added WithFlaky to the device plugin test case: supports extended resources together with ResourceClaim | keyword: best practice
"> Yeah, I think withflaky is an OK stop-gap, but we also really need to address leaking things on the cluster. Agreed - real fix is needed.",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/johnbelamaric,https://github.com/kubernetes/kubernetes/pull/133286#issuecomment-3197927973,repo: kubernetes/kubernetes | issue: added WithFlaky to the device plugin test case: supports extended resources together with ResourceClaim | keyword: best practice
"The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass. This bot retests PRs for certain kubernetes repos according to the following rules: - The PR does have any `do-not-merge/*` labels - The PR does not have the `needs-ok-to-test` label - The PR is mergeable (does not have a `needs-rebase` label) - The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels) - The PR is failing tests required for merge You can: - Review the [full test history]…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/133286#issuecomment-3198325397,repo: kubernetes/kubernetes | issue: added WithFlaky to the device plugin test case: supports extended resources together with ResourceClaim | keyword: best practice
"The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass. This bot retests PRs for certain kubernetes repos according to the following rules: - The PR does have any `do-not-merge/*` labels - The PR does not have the `needs-ok-to-test` label - The PR is mergeable (does not have a `needs-rebase` label) - The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels) - The PR is failing tests required for merge You can: - Review the [full test history]…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/133286#issuecomment-3198745291,repo: kubernetes/kubernetes | issue: added WithFlaky to the device plugin test case: supports extended resources together with ResourceClaim | keyword: best practice
"@yliaog: The following test **failed**, say `/retest` to rerun all failed tests or `/retest-required` to rerun all mandatory failed tests: Test name | Commit | Details | Required | Rerun command --- | --- | --- | --- | --- pull-kubernetes-kind-dra-n-1 | edfa9a5bd28876c860d5a16edf5cbc1c8b1cf817 | [link](https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/133286/pull-kubernetes-kind-dra-n-1/1957591353235869696) | false | `/test pull-kubernetes-kind-dra-n-1` [Full PR test history](https://…",,,,,,Anecdotal,comment,,,,,,,,2025-08-19,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133286#issuecomment-3198788111,repo: kubernetes/kubernetes | issue: added WithFlaky to the device plugin test case: supports extended resources together with ResourceClaim | keyword: best practice
"Fix the bug where StartedPodsErrorsTotal metrics implementation doesn't match its name <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addr…",,,,,,Anecdotal,issue,,,,,,,,2025-08-18,github/HirazawaUi,https://github.com/kubernetes/kubernetes/pull/133589,repo: kubernetes/kubernetes | keyword: best practice | state: open
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.34` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.34.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133589#issuecomment-3196877387,repo: kubernetes/kubernetes | issue: Fix the bug where StartedPodsErrorsTotal metrics implementation doesn't match its name | keyword: best practice
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133589#issuecomment-3196877776,repo: kubernetes/kubernetes | issue: Fix the bug where StartedPodsErrorsTotal metrics implementation doesn't match its name | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133589#"" title=""Author self-approved"">HirazawaUi</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [mrunalp](https://github.com/mrunalp) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted by this…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133589#issuecomment-3196892138,repo: kubernetes/kubernetes | issue: Fix the bug where StartedPodsErrorsTotal metrics implementation doesn't match its name | keyword: best practice
"@HirazawaUi: The following test **failed**, say `/retest` to rerun all failed tests or `/retest-required` to rerun all mandatory failed tests: Test name | Commit | Details | Required | Rerun command --- | --- | --- | --- | --- pull-kubernetes-unit-windows-master | c565525c43b4ffb2aafff097de16bd9bf0dd19a3 | [link](https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/133589/pull-kubernetes-unit-windows-master/1957433846228062208) | false | `/test pull-kubernetes-unit-windows-master` [Full …",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133589#issuecomment-3197049590,repo: kubernetes/kubernetes | issue: Fix the bug where StartedPodsErrorsTotal metrics implementation doesn't match its name | keyword: best practice
I'm honestly not sure if we want this. it could be useful to check sandbox creation errors independent of container creation error. and we can infer this error rate by combining container and sandbox errors.. I do see a reason to unify the metrics between userns and non but honestly maybe deprecating the old metric and renaming the error to have _sandbox could be more clear?,,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/haircommander,https://github.com/kubernetes/kubernetes/pull/133589#issuecomment-3197792341,repo: kubernetes/kubernetes | issue: Fix the bug where StartedPodsErrorsTotal metrics implementation doesn't match its name | keyword: best practice
"> maybe deprecating the old metric and renaming the error to have _sandbox could be more clear? If we deprecate the `StartedPodsErrorsTotal` metric, we won't have a replacement metric to represent the number of errors during pod startup. Perhaps it would be better to retain `StartedPodsErrorsTotal` while also adding a new metric like `CreatePodSandboxErrorsTotal` (`kubelet_create_pod_sandbox_errors_total`)?",,,,,,Anecdotal,comment,,,,,,,,2025-08-19,github/HirazawaUi,https://github.com/kubernetes/kubernetes/pull/133589#issuecomment-3198773237,repo: kubernetes/kubernetes | issue: Fix the bug where StartedPodsErrorsTotal metrics implementation doesn't match its name | keyword: best practice
"Add concurrent worker configuration for DisruptionController <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this…",,,,,,Anecdotal,issue,,,,,,,,2025-04-20,github/xigang,https://github.com/kubernetes/kubernetes/pull/131386,repo: kubernetes/kubernetes | keyword: best practice | state: open
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.33` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.33.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-04-20,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131386#issuecomment-2817032375,repo: kubernetes/kubernetes | issue: Add concurrent worker configuration for DisruptionController | keyword: best practice
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-04-20,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131386#issuecomment-2817032417,repo: kubernetes/kubernetes | issue: Add concurrent worker configuration for DisruptionController | keyword: best practice
"Hi @xigang. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once the patc…",,,,,,Anecdotal,comment,,,,,,,,2025-04-20,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131386#issuecomment-2817032418,repo: kubernetes/kubernetes | issue: Add concurrent worker configuration for DisruptionController | keyword: best practice
"This PR [may require API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#what-apis-need-to-be-reviewed). If so, when the changes are ready, [complete the pre-review checklist and request an API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#mechanics). Status of requested reviews is tracked in the [API Review project](https://github.com/orgs/kubernetes/projects/169).",,,,,,Anecdotal,comment,,,,,,,,2025-04-20,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/131386#issuecomment-2817036267,repo: kubernetes/kubernetes | issue: Add concurrent worker configuration for DisruptionController | keyword: best practice
"@xigang Thanks for the PR. For a new feature like this, you should probably follow the [KEP](https://github.com/kubernetes/enhancements/blob/master/keps/README.md) process to provide more context and discuss the alternatives. 1. discuss this in the sig-apps meeting 2. raise a KEP PR",,,,,,Anecdotal,comment,,,,,,,,2025-04-28,github/siyuanfoundation,https://github.com/kubernetes/kubernetes/pull/131386#issuecomment-2835889608,repo: kubernetes/kubernetes | issue: Add concurrent worker configuration for DisruptionController | keyword: best practice
"@atiratree I currently don’t have any load metrics related to PDB. I noticed that @ricky1993 in the community encountered a performance issue #https://github.com/kubernetes/kubernetes/issues/82930 due to single-threaded processing of PDBs, so a PR was submitted to add multi-worker support to the Disruption Controller to address this issue. In large-scale cluster scenarios (like a single cluster with 150,000 Pods), the Disruption Controller can run into performance issues during reconciliation b…",,,,,,Anecdotal,comment,,,,,,,,2025-05-05,github/xigang,https://github.com/kubernetes/kubernetes/pull/131386#issuecomment-2850634992,repo: kubernetes/kubernetes | issue: Add concurrent worker configuration for DisruptionController | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/131386#"" title=""Author self-approved"">xigang</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/131386#pullrequestreview-2831027835"" title=""Approved"">yashpawar6849</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [smarterclayton](https://github.com/smarterclayton) for approval. For more information see [the Code Re…",,,,,,Anecdotal,comment,,,,,,,,2025-05-10,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131386#issuecomment-2869076193,repo: kubernetes/kubernetes | issue: Add concurrent worker configuration for DisruptionController | keyword: best practice
I am afraid it is insufficient to merge a user facing code without analyzing the impact. This is especially true because the disruption controller worker can result in an increased number of API requests.,,,,,,Anecdotal,comment,,,,,,,,2025-05-14,github/atiratree,https://github.com/kubernetes/kubernetes/pull/131386#issuecomment-2881703153,repo: kubernetes/kubernetes | issue: Add concurrent worker configuration for DisruptionController | keyword: best practice
"@atiratree We can expose the `ConcurrentDisruptionSyncs` and `ConcurrentDisruptionStalePodSyncs` parameters and set their default values to 1. This approach helps avoid putting excessive pressure on the API server, while also allowing us to tune the parameters in scenarios where the PDB controller is processing slowly.",,,,,,Anecdotal,comment,,,,,,,,2025-05-19,github/xigang,https://github.com/kubernetes/kubernetes/pull/131386#issuecomment-2890044599,repo: kubernetes/kubernetes | issue: Add concurrent worker configuration for DisruptionController | keyword: best practice
"The Kubernetes project currently lacks enough contributors to adequately respond to all PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Mark this PR as fresh with `/remove-lifecycle stale` - Close this PR with `/close` - Offer to help out with [I…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/131386#issuecomment-3198414973,repo: kubernetes/kubernetes | issue: Add concurrent worker configuration for DisruptionController | keyword: best practice
"Add Kubelet stress test for pod cleanup when rejection due to `VolumeAttachmentLimitExceeded` <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you a…",,,,,,Anecdotal,issue,,,,,,,,2025-08-01,github/torredil,https://github.com/kubernetes/kubernetes/pull/133357,repo: kubernetes/kubernetes | keyword: best practice | state: open
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133357#issuecomment-3144688484,repo: kubernetes/kubernetes | issue: Add Kubelet stress test for pod cleanup when rejection due to `VolumeAttachmentLimitExceeded` | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133357#"" title=""Author self-approved"">torredil</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [random-liu](https://github.com/random-liu) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted by …",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133357#issuecomment-3144688828,repo: kubernetes/kubernetes | issue: Add Kubelet stress test for pod cleanup when rejection due to `VolumeAttachmentLimitExceeded` | keyword: best practice
/area test this test is useful. I would also urge to make some e2e test as e2e may catch some issues that are hard to repro with unit tests. I am not 100% sure we will need unit test stress if we can do e2e stress instead,,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/SergeyKanzhelev,https://github.com/kubernetes/kubernetes/pull/133357#issuecomment-3161361440,repo: kubernetes/kubernetes | issue: Add Kubelet stress test for pod cleanup when rejection due to `VolumeAttachmentLimitExceeded` | keyword: best practice
"@SergeyKanzhelev In the first iteration of this PR I was actually writing this stress test as an e2e test similar to https://github.com/kubernetes/kubernetes/blob/9d3fff50483cceafdca5047b303e5ec0c5b717bd/test/e2e/storage/csimock/mutable_csinode_allocatable.go#L175 but scaled up - then I realized we wouldn't have visibility into kubelet internals (podWorkers, statusManager, allocationManager) as we do in this test (to validate SyncTerminated pod completes successfully, allocated resources are cl…",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/torredil,https://github.com/kubernetes/kubernetes/pull/133357#issuecomment-3161427438,repo: kubernetes/kubernetes | issue: Add Kubelet stress test for pod cleanup when rejection due to `VolumeAttachmentLimitExceeded` | keyword: best practice
"> then I realized we wouldn't have visibility into kubelet internals (podWorkers, statusManager, allocationManager) as we do in this test (to validate SyncTerminated pod completes successfully, allocated resources are cleaned up, etc). The thing e2e will validate that the pod admission will keep breaking at the same stage and not on some new condition like IP exhaustion.",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/SergeyKanzhelev,https://github.com/kubernetes/kubernetes/pull/133357#issuecomment-3161604252,repo: kubernetes/kubernetes | issue: Add Kubelet stress test for pod cleanup when rejection due to `VolumeAttachmentLimitExceeded` | keyword: best practice
"> I think the code coverage gained from this test is valuable and worth merging. We will triage next week in SIG Node CI meeting and find a reviewer, thanks!",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/SergeyKanzhelev,https://github.com/kubernetes/kubernetes/pull/133357#issuecomment-3161607661,repo: kubernetes/kubernetes | issue: Add Kubelet stress test for pod cleanup when rejection due to `VolumeAttachmentLimitExceeded` | keyword: best practice
"@torredil: The following test **failed**, say `/retest` to rerun all failed tests or `/retest-required` to rerun all mandatory failed tests: Test name | Commit | Details | Required | Rerun command --- | --- | --- | --- | --- pull-kubernetes-unit-windows-master | e76da53ae3e3f1a14a3fcee66cd21595cfef0b9d | [link](https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/133357/pull-kubernetes-unit-windows-master/1957564667735838720) | unknown | `/test pull-kubernetes-unit-windows-master` [Full …",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133357#issuecomment-3198617466,repo: kubernetes/kubernetes | issue: Add Kubelet stress test for pod cleanup when rejection due to `VolumeAttachmentLimitExceeded` | keyword: best practice
"pull mounter from dl.k8s.io <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull reque…",,,,,,Anecdotal,issue,,,,,,,,2025-05-07,github/upodroid,https://github.com/kubernetes/kubernetes/pull/131639,repo: kubernetes/kubernetes | keyword: best practice | state: open
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-05-07,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131639#issuecomment-2857420281,repo: kubernetes/kubernetes | issue: pull mounter from dl.k8s.io | keyword: best practice
/test pull-kubernetes-integration-canary /test pull-kubernetes-integration-go-canary /test pull-kubernetes-e2e-kind-canary,,,,,,Anecdotal,comment,,,,,,,,2025-05-07,github/upodroid,https://github.com/kubernetes/kubernetes/pull/131639#issuecomment-2857425212,repo: kubernetes/kubernetes | issue: pull mounter from dl.k8s.io | keyword: best practice
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/131639#issuecomment-2858304615"" title=""LGTM"">dims</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/131639#"" title=""Author self-approved"">upodroid</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https://git.k8s.io/com…",,,,,,Anecdotal,comment,,,,,,,,2025-05-07,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131639#issuecomment-2858304872,repo: kubernetes/kubernetes | issue: pull mounter from dl.k8s.io | keyword: best practice
"The Kubernetes project currently lacks enough contributors to adequately respond to all PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Mark this PR as fresh with `/remove-lifecycle stale` - Close this PR with `/close` - Offer to help out with [I…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/131639#issuecomment-3198415003,repo: kubernetes/kubernetes | issue: pull mounter from dl.k8s.io | keyword: best practice
"WIP: eliminate md5 usage, block new usage <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targe…",,,,,,Anecdotal,issue,,,,,,,,2025-08-14,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/133511,repo: kubernetes/kubernetes | keyword: best practice | state: open
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.34` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.34.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133511#issuecomment-3186231031,"repo: kubernetes/kubernetes | issue: WIP: eliminate md5 usage, block new usage | keyword: best practice"
"[fixed verify spelling error in the linter config] > Once this PR has been reviewed and has the lgtm label, please assign [thockin](https://github.com/thockin) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). /assign thockin",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/133511#issuecomment-3186644050,"repo: kubernetes/kubernetes | issue: WIP: eliminate md5 usage, block new usage | keyword: best practice"
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133511#"" title=""Author self-approved"">BenTheElder</a>* **Once this PR has been reviewed and has the lgtm label**, please ask for approval from [thockin](https://github.com/thockin). For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted by t…",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133511#issuecomment-3189281684,"repo: kubernetes/kubernetes | issue: WIP: eliminate md5 usage, block new usage | keyword: best practice"
"`md5.New` is not enough, we also need at least `md5.Sum` I expanded the lint rule to all `md5.*` symbols, tagged the remainder of those and fixed up a storage e2e test caught in this.",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/133511#issuecomment-3189285970,"repo: kubernetes/kubernetes | issue: WIP: eliminate md5 usage, block new usage | keyword: best practice"
"@BenTheElder: The following tests **failed**, say `/retest` to rerun all failed tests or `/retest-required` to rerun all mandatory failed tests: Test name | Commit | Details | Required | Rerun command --- | --- | --- | --- | --- pull-kubernetes-linter-hints | 140c00d808bc775ff1588041f0740c4862dae25b | [link](https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/133511/pull-kubernetes-linter-hints/1956075111660392448) | false | `/test pull-kubernetes-linter-hints` pull-kubernetes-unit-wind…",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133511#issuecomment-3189710926,"repo: kubernetes/kubernetes | issue: WIP: eliminate md5 usage, block new usage | keyword: best practice"
"Note: I need to do some testing for endpoint, see discussion in https://github.com/kubernetes/kubernetes/issues/129652 This is NOT going to merge in 1.34 anyhow, it does not qualify: https://k8s.dev/release So I plan to iterate further this week on the last bits, but not at the highest priority. The rest of this PR should already be reviewable though.",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/133511#issuecomment-3198573372,"repo: kubernetes/kubernetes | issue: WIP: eliminate md5 usage, block new usage | keyword: best practice"
"[PodLevelResources] Event for pod-level resource manager incompatibility <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especi…",,,,,,Anecdotal,issue,,,,,,,,2025-06-30,github/KevinTMtz,https://github.com/kubernetes/kubernetes/pull/132634,repo: kubernetes/kubernetes | keyword: best practice | state: open
"Hi @KevinTMtz. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once the p…",,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/132634#issuecomment-3020760954,repo: kubernetes/kubernetes | issue: [PodLevelResources] Event for pod-level resource manager incompatibility | keyword: best practice
"> We also need changes for topology manager. For guaranteed pods, > > * If container-level resources are set with R=L (regardless or pod-level resources being set/unset) and scope=container, topology manager will still align the resources > * If only pod-level resources are set, or container-level are set but R!=L, skip alignment with the event being recorded Refer to https://github.com/kubernetes/kubernetes/pull/132634#discussion_r2193529219",,,,,,Anecdotal,comment,,,,,,,,2025-07-08,github/KevinTMtz,https://github.com/kubernetes/kubernetes/pull/132634#issuecomment-3050446467,repo: kubernetes/kubernetes | issue: [PodLevelResources] Event for pod-level resource manager incompatibility | keyword: best practice
"> Emitting events AND logs LGTM. I still believe the absolute best would be to have something permanent in pod status, because events are ephemeral, but I'll not push for that, let alone block this PR. Things like this have come up before, but a PodCondition doesn't seem quite right. Maybe we need to consider adding a new field/concept to the pod status for this. Something like `Warnings map[string]string`. Obviously out of scope for this PR. > I'm wondering if instead of passing down deep the …",,,,,,Anecdotal,comment,,,,,,,,2025-07-18,github/tallclair,https://github.com/kubernetes/kubernetes/pull/132634#issuecomment-3091239222,repo: kubernetes/kubernetes | issue: [PodLevelResources] Event for pod-level resource manager incompatibility | keyword: best practice
"@tallclair @ffromani > > I'm wondering if instead of passing down deep the recorder, we can't just bubble up a specific error or a new return value and centralize the logging and the event emission. > > Was this option evaluated? > +1 to this approach. If you bubble it up to the admission handler, you could just record the event there. Since I do not want to prevent pod admission, I would have to bubble it up, catch the specific warning and produce the event, but the pod would still have to be …",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/KevinTMtz,https://github.com/kubernetes/kubernetes/pull/132634#issuecomment-3095141396,repo: kubernetes/kubernetes | issue: [PodLevelResources] Event for pod-level resource manager incompatibility | keyword: best practice
"> @tallclair @ffromani > > > > I'm wondering if instead of passing down deep the recorder, we can't just bubble up a specific error or a new return value and centralize the logging and the event emission. > > > Was this option evaluated? > > > +1 to this approach. If you bubble it up to the admission handler, you could just record the event there. > > Since I do not want to prevent pod admission, I would have to bubble it up, catch the specific warning and produce the event, but the pod would s…",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/ffromani,https://github.com/kubernetes/kubernetes/pull/132634#issuecomment-3095344963,repo: kubernetes/kubernetes | issue: [PodLevelResources] Event for pod-level resource manager incompatibility | keyword: best practice
"> Thanks for your insights! > > From pratical perspective, I think you can address the issue at topology manager/allocation manager level. A concrete resource manager can opaquely return the error and can be handled in `admission.go`, which is used by topology manager, with another `errors.As` call: > > ``` > func GetPodAdmitResult(err error) lifecycle.PodAdmitResult { > if err == nil { > return lifecycle.PodAdmitResult{Admit: true} > } > > var admissionErr Error > if !errors.As(err, &admission…",,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/KevinTMtz,https://github.com/kubernetes/kubernetes/pull/132634#issuecomment-3100739591,repo: kubernetes/kubernetes | issue: [PodLevelResources] Event for pod-level resource manager incompatibility | keyword: best practice
/test pull-kubernetes-node-kubelet-serial-crio-cgroupv1 /test pull-kubernetes-node-kubelet-serial-crio-cgroupv2 /test pull-crio-cgroupv1-node-e2e-resource-managers /test pull-crio-cgroupv2-node-e2e-resource-managers,,,,,,Anecdotal,comment,,,,,,,,2025-07-23,github/ffromani,https://github.com/kubernetes/kubernetes/pull/132634#issuecomment-3106892241,repo: kubernetes/kubernetes | issue: [PodLevelResources] Event for pod-level resource manager incompatibility | keyword: best practice
/test pull-kubernetes-node-kubelet-serial-cpu-manager /test pull-kubernetes-node-kubelet-serial-hugepages /test pull-kubernetes-node-kubelet-serial-memory-manager /test pull-kubernetes-node-kubelet-serial-topology-manager,,,,,,Anecdotal,comment,,,,,,,,2025-07-23,github/ffromani,https://github.com/kubernetes/kubernetes/pull/132634#issuecomment-3106894454,repo: kubernetes/kubernetes | issue: [PodLevelResources] Event for pod-level resource manager incompatibility | keyword: best practice
@ndixita: The specified target(s) for `/test` were not found. The following commands are available to trigger required jobs: ``` /test pull-cos-containerd-e2e-ubuntu-gce ``` ``` /test pull-kubernetes-cmd ``` ``` /test pull-kubernetes-cmd-canary ``` ``` /test pull-kubernetes-cmd-go-canary ``` ``` /test pull-kubernetes-conformance-kind-ga-only-parallel ``` ``` /test pull-kubernetes-coverage-unit ``` ``` /test pull-kubernetes-dependencies ``` ``` /test pull-kubernetes-dependencies-go-canary ``` ``…,,,,,,Anecdotal,comment,,,,,,,,2025-07-25,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/132634#issuecomment-3119612167,repo: kubernetes/kubernetes | issue: [PodLevelResources] Event for pod-level resource manager incompatibility | keyword: best practice
"> I'm not too happy with this PR, but my comments were addressed, and I see no major problems. > > The most controversial point is IMO the change in `scope.go` about iterating over all the managers vs stopping at the first failure. In theory, managers are few (up to 3) and there is no even a remote plan to add more; managers should not depend on their ordering, so we expect no issues (e.g. resource leakage) if we continue iterating past the first allocation. We historically struggled with test …",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/ffromani,https://github.com/kubernetes/kubernetes/pull/132634#issuecomment-3128104633,repo: kubernetes/kubernetes | issue: [PodLevelResources] Event for pod-level resource manager incompatibility | keyword: best practice
"A closing though: I acknowledge the changes to `scope.go` are directly linked to the path I suggested about bubbling up errors vs pushing down the event recorder. Perhaps we wouldn't have had this concern if pushing down the event recorder to the managers. Or perhaps we would have ended up in the same spat, considering for good UX we do want to bubble up all the incompatibilities. But all in all, I think we need more time to figure out a better path forward. I'm confident a good solution for th…",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/ffromani,https://github.com/kubernetes/kubernetes/pull/132634#issuecomment-3128134692,repo: kubernetes/kubernetes | issue: [PodLevelResources] Event for pod-level resource manager incompatibility | keyword: best practice
/test pull-kubernetes-node-kubelet-serial-cpu-manager /test pull-kubernetes-node-kubelet-serial-hugepages /test pull-kubernetes-node-kubelet-serial-memory-manager /test pull-kubernetes-node-kubelet-serial-topology-manager,,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/ffromani,https://github.com/kubernetes/kubernetes/pull/132634#issuecomment-3132934380,repo: kubernetes/kubernetes | issue: [PodLevelResources] Event for pod-level resource manager incompatibility | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/132634#"" title=""Author self-approved"">KevinTMtz</a>* **Once this PR has been reviewed and has the lgtm label**, please ask for approval from [tallclair](https://github.com/tallclair). For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted by…",,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/132634#issuecomment-3152521498,repo: kubernetes/kubernetes | issue: [PodLevelResources] Event for pod-level resource manager incompatibility | keyword: best practice
"Require Storage Version Migrator to be Gated Behind RealFIFO <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this…",,,,,,Anecdotal,issue,,,,,,,,2025-08-18,github/michaelasp,https://github.com/kubernetes/kubernetes/pull/133596,repo: kubernetes/kubernetes | keyword: best practice | state: open
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.34` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.34.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133596#issuecomment-3197886889,repo: kubernetes/kubernetes | issue: Require Storage Version Migrator to be Gated Behind RealFIFO | keyword: best practice
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133596#issuecomment-3197887125,repo: kubernetes/kubernetes | issue: Require Storage Version Migrator to be Gated Behind RealFIFO | keyword: best practice
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133596#issuecomment-3198534512"" title=""Approved"">jpbetz</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/133596#"" title=""Author self-approved"">michaelasp</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https://git.k8…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133596#issuecomment-3198534995,repo: kubernetes/kubernetes | issue: Require Storage Version Migrator to be Gated Behind RealFIFO | keyword: best practice
"fix many incorrect deprecation warnings in godoc, enable deprecatedComment linter <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressin…",,,,,,Anecdotal,issue,,,,,,,,2025-08-15,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/133571,repo: kubernetes/kubernetes | keyword: best practice | state: open
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.34` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.34.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133571#issuecomment-3192317444,"repo: kubernetes/kubernetes | issue: fix many incorrect deprecation warnings in godoc, enable deprecatedComment linter | keyword: best practice"
"> Once this PR has been reviewed and has the lgtm label, please assign [deads2k](https://github.com/deads2k), [richabanker](https://github.com/richabanker), [soltysh](https://github.com/soltysh) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). /assign richabanker soltysh",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/133571#issuecomment-3198011822,"repo: kubernetes/kubernetes | issue: fix many incorrect deprecation warnings in godoc, enable deprecatedComment linter | keyword: best practice"
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133571#"" title=""Author self-approved"">BenTheElder</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/133571#issuecomment-3198003629"" title=""Approved"">jpbetz</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/133571#issuecomment-3198049904"" title=""Approved"">richabanker</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/133…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133571#issuecomment-3198050591,"repo: kubernetes/kubernetes | issue: fix many incorrect deprecation warnings in godoc, enable deprecatedComment linter | keyword: best practice"
> Kubernetes e2e suite: [It] [sig-node] Lifecycle Sleep Hook when create a pod with lifecycle hook using sleep action ignore terminated container https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/133571/pull-kubernetes-e2e-gce/1957508498203873280 /retest looks pretty rare: https://storage.googleapis.com/k8s-triage/index.html?pr=1&test=ignore%20terminated%20container,,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/133571#issuecomment-3198416017,"repo: kubernetes/kubernetes | issue: fix many incorrect deprecation warnings in godoc, enable deprecatedComment linter | keyword: best practice"
"DRA: wait for stats to converge in ""creates slices"" e2e test <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this…",,,,,,Anecdotal,issue,,,,,,,,2025-08-15,github/nojnhuh,https://github.com/kubernetes/kubernetes/pull/133562,repo: kubernetes/kubernetes | keyword: best practice | state: open
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.34` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.34.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133562#issuecomment-3190751836,"repo: kubernetes/kubernetes | issue: DRA: wait for stats to converge in ""creates slices"" e2e test | keyword: best practice"
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133562#issuecomment-3190751943,"repo: kubernetes/kubernetes | issue: DRA: wait for stats to converge in ""creates slices"" e2e test | keyword: best practice"
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133562#issuecomment-3192081741"" title=""Approved"">johnbelamaric</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/133562#issuecomment-3192254992"" title=""Approved"">klueska</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/133562#"" title=""Author self-approved"">nojnhuh</a>* The full list of commands accepted by this bot can be found [here…",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133562#issuecomment-3192255540,"repo: kubernetes/kubernetes | issue: DRA: wait for stats to converge in ""creates slices"" e2e test | keyword: best practice"
"The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass. This bot retests PRs for certain kubernetes repos according to the following rules: - The PR does have any `do-not-merge/*` labels - The PR does not have the `needs-ok-to-test` label - The PR is mergeable (does not have a `needs-rebase` label) - The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels) - The PR is failing tests required for merge You can: - Review the [full test history]…",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/133562#issuecomment-3192547466,"repo: kubernetes/kubernetes | issue: DRA: wait for stats to converge in ""creates slices"" e2e test | keyword: best practice"
/hold to keep from running the optional tests that will keep failing until the 1.34.0 release is cut. @Rajalakshmi-Girish This PR is otherwise ready to be added to the 1.34 milestone.,,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/nojnhuh,https://github.com/kubernetes/kubernetes/pull/133562#issuecomment-3192571509,"repo: kubernetes/kubernetes | issue: DRA: wait for stats to converge in ""creates slices"" e2e test | keyword: best practice"
"@nojnhuh: The following test **failed**, say `/retest` to rerun all failed tests or `/retest-required` to rerun all mandatory failed tests: Test name | Commit | Details | Required | Rerun command --- | --- | --- | --- | --- pull-kubernetes-kind-dra-n-1 | bf6c86b562d3328aa8badd17c7d3077e65e1d1e1 | [link](https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/133562/pull-kubernetes-kind-dra-n-1/1956441514133426176) | false | `/test pull-kubernetes-kind-dra-n-1` [Full PR test history](https:/…",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133562#issuecomment-3192593781,"repo: kubernetes/kubernetes | issue: DRA: wait for stats to converge in ""creates slices"" e2e test | keyword: best practice"
"add fake-registry-server command to agnhost <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a release tar…",,,,,,Anecdotal,issue,,,,,,,,2025-07-29,github/drjackild,https://github.com/kubernetes/kubernetes/pull/133272,repo: kubernetes/kubernetes | keyword: best practice | state: open
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133272#issuecomment-3131713612,repo: kubernetes/kubernetes | issue: add fake-registry-server command to agnhost | keyword: best practice
Welcome @drjackild! <br><br>It looks like this is your first PR to <a href='https://github.com/kubernetes/kubernetes'>kubernetes/kubernetes</a> 🎉. Please refer to our [pull request process documentation](https://www.kubernetes.dev/docs/guide/pull-requests/) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https…,,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133272#issuecomment-3131713652,repo: kubernetes/kubernetes | issue: add fake-registry-server command to agnhost | keyword: best practice
"Hi @drjackild. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once the p…",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133272#issuecomment-3131713706,repo: kubernetes/kubernetes | issue: add fake-registry-server command to agnhost | keyword: best practice
"@BenTheElder @dims do you guys okay with approach I choose for re-use the `preparer` image in the `agnhost/Makefile.bin`? I like how multistage build looks like (one file which contains every preprequisites to build agnhost image), but multi-arch build that completely disables caching and that means we re-download registry for every image we want to build. I would be happy to hear your thought, is there a better way to do this.",,,,,,Anecdotal,comment,,,,,,,,2025-07-31,github/drjackild,https://github.com/kubernetes/kubernetes/pull/133272#issuecomment-3141185452,repo: kubernetes/kubernetes | issue: add fake-registry-server command to agnhost | keyword: best practice
"Do I need to change `Dockerfile_windows` too, or we can limit this to linux images only? I see that windows image does not support everything that regular image does (like grpc server)",,,,,,Anecdotal,comment,,,,,,,,2025-07-31,github/drjackild,https://github.com/kubernetes/kubernetes/pull/133272#issuecomment-3141297562,repo: kubernetes/kubernetes | issue: add fake-registry-server command to agnhost | keyword: best practice
"> but multi-arch build that completely disables caching and that means we re-download registry for every image we want to build. I would be happy to hear your thought, is there a better way to do this. hmm? Image builds with buildx (which we use) should be cached for multi-arch, including with multi-stage builds.",,,,,,Anecdotal,comment,,,,,,,,2025-07-31,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/133272#issuecomment-3141371608,repo: kubernetes/kubernetes | issue: add fake-registry-server command to agnhost | keyword: best practice
IMHO just Linux. cc @SergeyKanzhelev @dims @aojea Sergey can you cc anyone from SIG node that should weigh in on what we need? I think this is looking good,,,,,,Anecdotal,comment,,,,,,,,2025-07-31,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/133272#issuecomment-3141374069,repo: kubernetes/kubernetes | issue: add fake-registry-server command to agnhost | keyword: best practice
"> > but multi-arch build that completely disables caching and that means we re-download registry for every image we want to build. I would be happy to hear your thought, is there a better way to do this. > > hmm? Image builds with buildx (which we use) should be cached for multi-arch, including with multi-stage builds. I'll try that again, but I believe that caching is disabled because of the top-level arg (I moved `BASEIMAGE` to the top of the file to be able to use it in the second image). Wi…",,,,,,Anecdotal,comment,,,,,,,,2025-07-31,github/drjackild,https://github.com/kubernetes/kubernetes/pull/133272#issuecomment-3141383869,repo: kubernetes/kubernetes | issue: add fake-registry-server command to agnhost | keyword: best practice
"@BenTheElder I've checked this again, same problem, with this image I've got first stage rebuild for every `BASEIMAGE` in the list: <details> <summary>Dockerfile</summary> ```dockerfile # Copyright 2019 The Kubernetes Authors. # # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed …",,,,,,Anecdotal,comment,,,,,,,,2025-07-31,github/drjackild,https://github.com/kubernetes/kubernetes/pull/133272#issuecomment-3141444572,repo: kubernetes/kubernetes | issue: add fake-registry-server command to agnhost | keyword: best practice
"@SergeyKanzhelev @BenTheElder I was able to pass node runtime conformance test with this code (I just took a code from deleted `runtime_conformance_test.go` and change it to use the latest `agnhost` image from my private registry with `DaemonSet` for the registry). <details> <summary>runtime_conformance_test.go</summary> ```go ...import and some setup here // NOTE: I actually have to change this admission level to LevelPriveleged // because of the hostport configuration. Not sure, is this impor…",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/drjackild,https://github.com/kubernetes/kubernetes/pull/133272#issuecomment-3166303229,repo: kubernetes/kubernetes | issue: add fake-registry-server command to agnhost | keyword: best practice
"> UPD: In the docs they also saying that ARG can invalidate cache: https://docs.docker.com/reference/dockerfile/#impact-on-build-caching Yes, but only if the inputs change. We shouldn't have variable args for fetching the pause image, except which version to fetch. The stage that fetches the pause image can be before everything else, and the files copied to another stage.",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/133272#issuecomment-3169443811,repo: kubernetes/kubernetes | issue: add fake-registry-server command to agnhost | keyword: best practice
"NOTE: As net-new tests, we're probably going to be trying to finish this early in 1.35, when it's working well we can consider a cherry-pick to 1.34, but I don't think this is eligible for post-code-freeze, it's not a simple bugfix.",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/133272#issuecomment-3169449856,repo: kubernetes/kubernetes | issue: add fake-registry-server command to agnhost | keyword: best practice
"> > UPD: In the docs they also saying that ARG can invalidate cache: https://docs.docker.com/reference/dockerfile/#impact-on-build-caching > > Yes, but only if the inputs change. > > We shouldn't have variable args for fetching the pause image, except which version to fetch. > > The stage that fetches the pause image can be before everything else, and the files copied to another stage. That was exactly how I did this initially with multistage. The problem I believe is in BASEIMAGE var, which we…",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/drjackild,https://github.com/kubernetes/kubernetes/pull/133272#issuecomment-3169476274,repo: kubernetes/kubernetes | issue: add fake-registry-server command to agnhost | keyword: best practice
"We can put the crane stage even before it, though IIRC for caching purposes stages can be independent depending on their inputs? If I'm forgetting that we can just put it first. but aside from the occasional local developer, the CI builds will be cold, and I think most local dev is usually just hacking on the go commands which is faster to do outside of the image.",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/133272#issuecomment-3169493565,repo: kubernetes/kubernetes | issue: add fake-registry-server command to agnhost | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133272#"" title=""Author self-approved"">drjackild</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [dims](https://github.com/dims) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted by this bot ca…",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133272#issuecomment-3169502827,repo: kubernetes/kubernetes | issue: add fake-registry-server command to agnhost | keyword: best practice
@SergeyKanzhelev @dims @BenTheElder @stlaz can I please get a review on this one? I want to move to the test implementation stage,,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/drjackild,https://github.com/kubernetes/kubernetes/pull/133272#issuecomment-3197909888,repo: kubernetes/kubernetes | issue: add fake-registry-server command to agnhost | keyword: best practice
"Mark API server errors as transient in csi raw block driver <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this …",,,,,,Anecdotal,issue,,,,,,,,2025-08-18,github/eltrufas,https://github.com/kubernetes/kubernetes/pull/133599,repo: kubernetes/kubernetes | keyword: best practice | state: open
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.34` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.34.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133599#issuecomment-3198244114,repo: kubernetes/kubernetes | issue: Mark API server errors as transient in csi raw block driver | keyword: best practice
"<a href=""https://easycla.lfx.linuxfoundation.org/#/?version=2""><img src=""https://s3.amazonaws.com/cla-project-logo-prod/cla-signed.svg?v=2"" alt=""CLA Signed"" align=""left"" height=""28"" width=""328"" ></a><br/><br />The committers listed above are authorized under a signed CLA.<ul><li>:white_check_mark: login: eltrufas / name: Rafael Castillo (ade9036374f56d5f3864f890e54cfd8e3702cfd6)</li></ul><!-- Date Modified: 2025-08-18 20:06:55.875190 -->",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/linux-foundation-easycla[bot],https://github.com/kubernetes/kubernetes/pull/133599#issuecomment-3198244182,repo: kubernetes/kubernetes | issue: Mark API server errors as transient in csi raw block driver | keyword: best practice
Welcome @eltrufas! <br><br>It looks like this is your first PR to <a href='https://github.com/kubernetes/kubernetes'>kubernetes/kubernetes</a> 🎉. Please refer to our [pull request process documentation](https://www.kubernetes.dev/docs/guide/pull-requests/) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:…,,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133599#issuecomment-3198244391,repo: kubernetes/kubernetes | issue: Mark API server errors as transient in csi raw block driver | keyword: best practice
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133599#issuecomment-3198244397,repo: kubernetes/kubernetes | issue: Mark API server errors as transient in csi raw block driver | keyword: best practice
"Hi @eltrufas. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once the pa…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133599#issuecomment-3198244426,repo: kubernetes/kubernetes | issue: Mark API server errors as transient in csi raw block driver | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133599#"" title=""Author self-approved"">eltrufas</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [msau42](https://github.com/msau42) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted by this bot…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133599#issuecomment-3198245085,repo: kubernetes/kubernetes | issue: Mark API server errors as transient in csi raw block driver | keyword: best practice
"Add gated /statusz diagnostics endpoint to kube-proxy Adds /livez, /readyz, /healthz, and /metrics to the listed paths in /statusz for kube-proxy. <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label th…",,,,,,Anecdotal,issue,,,,,,,,2025-08-17,github/ozalakshay,https://github.com/kubernetes/kubernetes/pull/133580,repo: kubernetes/kubernetes | keyword: best practice | state: open
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.34` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.34.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133580#issuecomment-3194295464,repo: kubernetes/kubernetes | issue: Add gated /statusz diagnostics endpoint to kube-proxy | keyword: best practice
"<a href=""https://easycla.lfx.linuxfoundation.org/#/?version=2""><img src=""https://s3.amazonaws.com/cla-project-logo-prod/cla-signed.svg?v=2"" alt=""CLA Signed"" align=""left"" height=""28"" width=""328"" ></a><br/><br />The committers listed above are authorized under a signed CLA.<ul><li>:white_check_mark: login: ozalakshay / name: Lakshay Oza (c1427af1ddc4b0f3eb10ef982bbe3be5b9586d88, 1ea4a119047dc6c9b025fdeb320a71b6f06057e3, 334d38178f2709cb034133fbfac70e0278c41299, 06e0ad3fd22f9c402a3c91d64edbd04ffd0…",,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/linux-foundation-easycla[bot],https://github.com/kubernetes/kubernetes/pull/133580#issuecomment-3194295493,repo: kubernetes/kubernetes | issue: Add gated /statusz diagnostics endpoint to kube-proxy | keyword: best practice
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133580#issuecomment-3194295503,repo: kubernetes/kubernetes | issue: Add gated /statusz diagnostics endpoint to kube-proxy | keyword: best practice
Welcome @ozalakshay! <br><br>It looks like this is your first PR to <a href='https://github.com/kubernetes/kubernetes'>kubernetes/kubernetes</a> 🎉. Please refer to our [pull request process documentation](https://www.kubernetes.dev/docs/guide/pull-requests/) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](http…,,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133580#issuecomment-3194295504,repo: kubernetes/kubernetes | issue: Add gated /statusz diagnostics endpoint to kube-proxy | keyword: best practice
"Hi @ozalakshay. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once the …",,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133580#issuecomment-3194295512,repo: kubernetes/kubernetes | issue: Add gated /statusz diagnostics endpoint to kube-proxy | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133580#"" title=""Author self-approved"">ozalakshay</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [robscott](https://github.com/robscott) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted by th…",,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133580#issuecomment-3194295714,repo: kubernetes/kubernetes | issue: Add gated /statusz diagnostics endpoint to kube-proxy | keyword: best practice
"This sounds like a feature rather than a bug fix Change log suggestion ```diff -kube-proxy: /statusz now lists available endpoints: /livez, /readyz, /healthz, and /metrics +kube-proxy: `/statusz` now lists available endpoints: `/livez`, `/healthz`, `/readyz`, and `/metrics`. ```",,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/lmktfy,https://github.com/kubernetes/kubernetes/pull/133580#issuecomment-3194457862,repo: kubernetes/kubernetes | issue: Add gated /statusz diagnostics endpoint to kube-proxy | keyword: best practice
"Hi @adrianmoisey , @lmktfy , and @brosner, Could you please take a look at this PR when you have a chance? I have addressed the latest feedback and updated the commit message to comply with the guidelines. This PR is now passing all tests and ready for review and approval. If everything looks good, could you please add the `lgtm` and `approved` labels and remove the `do-not-merge/invalid-commit-message` label so it can be merged? Thank you very much for your time and support!",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/ozalakshay,https://github.com/kubernetes/kubernetes/pull/133580#issuecomment-3197784885,repo: kubernetes/kubernetes | issue: Add gated /statusz diagnostics endpoint to kube-proxy | keyword: best practice
"> If everything looks good, could you please add the `lgtm` and `approved` labels and remove the `do-not-merge/invalid-commit-message` label so it can be merged? Hi, this still needs to be addressed: https://github.com/kubernetes/kubernetes/pull/133580#issuecomment-3197763489",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/adrianmoisey,https://github.com/kubernetes/kubernetes/pull/133580#issuecomment-3197787727,repo: kubernetes/kubernetes | issue: Add gated /statusz diagnostics endpoint to kube-proxy | keyword: best practice
[Keywords](https://help.github.com/articles/closing-issues-using-keywords) which can automatically close issues and at(@) or hashtag(#) mentions are not allowed in commit messages. **The list of commits with invalid commit messages**: - [c1427af](https://github.com/kubernetes/kubernetes/commits/c1427af1ddc4b0f3eb10ef982bbe3be5b9586d88) kube-proxy: list available endpoints in /statusz <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/communi…,,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133580#issuecomment-3197842675,repo: kubernetes/kubernetes | issue: Add gated /statusz diagnostics endpoint to kube-proxy | keyword: best practice
"Hi @adrianmoisey , @thockin , @tnqn and @lmktfy, I have made the changes accordingly, please do review and approve. I have addressed the latest feedback and updated the commit message to comply with the guidelines. This PR is now passing all tests and ready for review and approval. If everything looks good, could you please add the lgtm and approved labels and remove the do-not-merge/invalid-commit-message label so it can be merged? Thanks.",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/ozalakshay,https://github.com/kubernetes/kubernetes/pull/133580#issuecomment-3198081853,repo: kubernetes/kubernetes | issue: Add gated /statusz diagnostics endpoint to kube-proxy | keyword: best practice
"fix: Update unit test to catch actual nil Labels case and fix functionality to handle nil Labels … <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue …",,,,,,Anecdotal,issue,,,,,,,,2025-08-15,github/DerekFrank,https://github.com/kubernetes/kubernetes/pull/133573,repo: kubernetes/kubernetes | keyword: best practice | state: open
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.34` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.34.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133573#issuecomment-3192814825,repo: kubernetes/kubernetes | issue: fix: Update unit test to catch actual nil Labels case and fix functionality to handle nil Labels | keyword: best practice
"Hi @DerekFrank. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once the …",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133573#issuecomment-3192814974,repo: kubernetes/kubernetes | issue: fix: Update unit test to catch actual nil Labels case and fix functionality to handle nil Labels | keyword: best practice
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133573#"" title=""Author self-approved"">DerekFrank</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/133573#issuecomment-3198309599"" title=""Approved"">Jefftree</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https://git.…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133573#issuecomment-3198310207,repo: kubernetes/kubernetes | issue: fix: Update unit test to catch actual nil Labels case and fix functionality to handle nil Labels | keyword: best practice
"kubectl expose --help: change --selector help text for clarity <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if th…",,,,,,Anecdotal,issue,,,,,,,,2025-08-18,github/vishnumohanan404,https://github.com/kubernetes/kubernetes/pull/133598,repo: kubernetes/kubernetes | keyword: best practice | state: open
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.34` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.34.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133598#issuecomment-3198194353,repo: kubernetes/kubernetes | issue: kubectl expose --help: change --selector help text for clarity | keyword: best practice
"<a href=""https://easycla.lfx.linuxfoundation.org/#/?version=2""><img src=""https://s3.amazonaws.com/cla-project-logo-prod/cla-signed.svg?v=2"" alt=""CLA Signed"" align=""left"" height=""28"" width=""328"" ></a><br/><br />The committers listed above are authorized under a signed CLA.<ul><li>:white_check_mark: login: vishnumohanan404 / name: vishnumohanan (5c8f26f48032899031760e5b75ad259d23d312b2)</li></ul><!-- Date Modified: 2025-08-18 19:49:34.403220 -->",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/linux-foundation-easycla[bot],https://github.com/kubernetes/kubernetes/pull/133598#issuecomment-3198194384,repo: kubernetes/kubernetes | issue: kubectl expose --help: change --selector help text for clarity | keyword: best practice
Welcome @vishnumohanan404! <br><br>It looks like this is your first PR to <a href='https://github.com/kubernetes/kubernetes'>kubernetes/kubernetes</a> 🎉. Please refer to our [pull request process documentation](https://www.kubernetes.dev/docs/guide/pull-requests/) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation…,,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133598#issuecomment-3198194575,repo: kubernetes/kubernetes | issue: kubectl expose --help: change --selector help text for clarity | keyword: best practice
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133598#issuecomment-3198194580,repo: kubernetes/kubernetes | issue: kubectl expose --help: change --selector help text for clarity | keyword: best practice
"Hi @vishnumohanan404. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Onc…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133598#issuecomment-3198194640,repo: kubernetes/kubernetes | issue: kubectl expose --help: change --selector help text for clarity | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133598#"" title=""Author self-approved"">vishnumohanan404</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [eddiezane](https://github.com/eddiezane) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accept…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133598#issuecomment-3198194959,repo: kubernetes/kubernetes | issue: kubectl expose --help: change --selector help text for clarity | keyword: best practice
"publish an event when the container is restarted <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a releas…",,,,,,Anecdotal,issue,,,,,,,,2024-07-31,github/Ritikaa96,https://github.com/kubernetes/kubernetes/pull/126474,repo: kubernetes/kubernetes | keyword: best practice | state: open
This PR in first commit is to give an idea what is desired output for the resolution of issue: #123176 . The approach might change in future after more community discussion.,,,,,,Anecdotal,comment,,,,,,,,2024-07-31,github/Ritikaa96,https://github.com/kubernetes/kubernetes/pull/126474#issuecomment-2259585161,repo: kubernetes/kubernetes | issue: publish an event when the container is restarted | keyword: best practice
Will this be reviewed for approval before the v1.32 release or are we waiting for the release ? It will help a use case of ours so it will be nice if it can be reviewed /merged earlier than that. Although I understand there is a time and resource crunch . Thanks for your reviews in advance,,,,,,Anecdotal,comment,,,,,,,,2024-12-05,github/Ritikaa96,https://github.com/kubernetes/kubernetes/pull/126474#issuecomment-2519055095,repo: kubernetes/kubernetes | issue: publish an event when the container is restarted | keyword: best practice
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/126474#issuecomment-2634910719"" title=""Approved"">dchen1107</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/126474#"" title=""Author self-approved"">Ritikaa96</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https://git.…",,,,,,Anecdotal,comment,,,,,,,,2025-02-04,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/126474#issuecomment-2634911144,repo: kubernetes/kubernetes | issue: publish an event when the container is restarted | keyword: best practice
"> Can you expand on the use case for this event? That will help me understand how it should relate to the ContainerStarted event. I didn't read the whole history on the linked issue, but it seems like that issue is asking for a different event? (ContainerDied) Ahh, missed this. Thanks for spotting it.",,,,,,Anecdotal,comment,,,,,,,,2025-02-04,github/dchen1107,https://github.com/kubernetes/kubernetes/pull/126474#issuecomment-2634920132,repo: kubernetes/kubernetes | issue: publish an event when the container is restarted | keyword: best practice
"> Can you expand on the use case for this event? That will help me understand how it should relate to the ContainerStarted event. Hi @tallclair Thanks for taking a look at this PR. As per the PR use case, there is a mission critical application for which we need to know when the container stopped/restarted, as per in k8s code , container stopped/deleted event is recorded when pod gets deleted an otherwise `container started` is seen after the container is restarted successfully. Regarding the d…",,,,,,Anecdotal,comment,,,,,,,,2025-02-05,github/Ritikaa96,https://github.com/kubernetes/kubernetes/pull/126474#issuecomment-2635582151,repo: kubernetes/kubernetes | issue: publish an event when the container is restarted | keyword: best practice
"> As per the PR use case, there is a mission critical application for which we need to know when the container stopped/restarted It might help to elaborate on your requirements here. If this is mission critical, watching the pod would be a more reliable signal (events are best-effort). > The proposed change i.e. ""Restarting"" shows right after the container death for some resource based or reason which do not warrant pod deletion but just container restart until threshold. So it serves the purpo…",,,,,,Anecdotal,comment,,,,,,,,2025-02-05,github/tallclair,https://github.com/kubernetes/kubernetes/pull/126474#issuecomment-2635626672,repo: kubernetes/kubernetes | issue: publish an event when the container is restarted | keyword: best practice
"> > As per the PR use case, there is a mission critical application for which we need to know when the container stopped/restarted > > It might help to elaborate on your requirements here. If this is mission critical, watching the pod would be a more reliable signal (events are best-effort). > > > The proposed change i.e. ""Restarting"" shows right after the container death for some resource based or reason which do not warrant pod deletion but just container restart until threshold. So it serves…",,,,,,Anecdotal,comment,,,,,,,,2025-02-05,github/Ritikaa96,https://github.com/kubernetes/kubernetes/pull/126474#issuecomment-2635725204,repo: kubernetes/kubernetes | issue: publish an event when the container is restarted | keyword: best practice
"> A ContainerDied event seems more broadly useful, and has less overlap with ContainerStarted. Why the switch to ContainerRestarted? @tallclair I see why the confusing between line 243 addition of restarted event and then 291 stating successfully started container. How about we change the `event reason` from `containerRestarted` to `containerRestarting` i see that fits best to situation where restarting fails once or twice before a successful container starts. Rather than `containerDied` i'd pr…",,,,,,Anecdotal,comment,,,,,,,,2025-02-05,github/Ritikaa96,https://github.com/kubernetes/kubernetes/pull/126474#issuecomment-2635725354,repo: kubernetes/kubernetes | issue: publish an event when the container is restarted | keyword: best practice
"> however adding the event for the container restarting timestamp is a requirement for the use case Why? What's the use case? I'm not trying to be difficult here, but it will help me review (or suggest alternatives) if I have a better understanding of the use case. > In the issue it is mentioned as containerDied however the actual need for some internal work is for the time between container died & container successfully started. Sorry, I didn't understand this sentence. Are you saying you need…",,,,,,Anecdotal,comment,,,,,,,,2025-02-05,github/tallclair,https://github.com/kubernetes/kubernetes/pull/126474#issuecomment-2637955266,repo: kubernetes/kubernetes | issue: publish an event when the container is restarted | keyword: best practice
"> Why? What's the use case? > > I'm not trying to be difficult here, but it will help me review (or suggest alternatives) if I have a better understanding of the use case. > We are trying to utilise the moment the container died and restarted for research purposes. That's why need to tag the moment the container got exited or restarted. > This seems like a fairly specialized requirement, and I think ContainerDied (or maybe ContainerTerminated is better) would be more broadly useful. > > I'm sti…",,,,,,Anecdotal,comment,,,,,,,,2025-02-07,github/Ritikaa96,https://github.com/kubernetes/kubernetes/pull/126474#issuecomment-2642816642,repo: kubernetes/kubernetes | issue: publish an event when the container is restarted | keyword: best practice
/lgtm cancel /approve cancel to prevent accidental merge removing the approvals since conversation is still ongoing. We can restore these tags later. I am also very interested to understand the scenario. > We are trying to utilise the moment the container died and restarted for research purposes. Would the distributed tracing be a better choice for the research purposes? Events are not guarantee to be available and can be throttled.,,,,,,Anecdotal,comment,,,,,,,,2025-02-07,github/SergeyKanzhelev,https://github.com/kubernetes/kubernetes/pull/126474#issuecomment-2643623240,repo: kubernetes/kubernetes | issue: publish an event when the container is restarted | keyword: best practice
"Hi @SergeyKanzhelev @tallclair Our use case is based on utilising the events and users depends on it. I understand and agree events are not guaranteed to be available but they surely helps us in getting our results and observation. Adding `ContainerDied` or `ContainerTerminated` event is essential for our issue here. I'll be happy to work on long term solution for better container state clarity , however this event addition can solve our issue as per the use case.",,,,,,Anecdotal,comment,,,,,,,,2025-02-12,github/Ritikaa96,https://github.com/kubernetes/kubernetes/pull/126474#issuecomment-2652653032,repo: kubernetes/kubernetes | issue: publish an event when the container is restarted | keyword: best practice
"> Hi @SergeyKanzhelev @tallclair Our use case is based on utilising the events and users depends on it. I understand and agree events are not guaranteed to be available but they surely helps us in getting our results and observation. Adding `ContainerDied` or `ContainerTerminated` event is essential for our issue here. > > I'll be happy to work on long term solution for better container state clarity , however this event addition can solve our issue as per the use case. Any word for this? Pleas…",,,,,,Anecdotal,comment,,,,,,,,2025-02-24,github/Ritikaa96,https://github.com/kubernetes/kubernetes/pull/126474#issuecomment-2677520922,repo: kubernetes/kubernetes | issue: publish an event when the container is restarted | keyword: best practice
"Adding metrics for Maxunavailable feature in StatefulSet <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is …",,,,,,Anecdotal,issue,,,,,,,,2025-03-20,github/Edwinhr716,https://github.com/kubernetes/kubernetes/pull/130951,repo: kubernetes/kubernetes | keyword: best practice | state: open
This PR [may require stable metrics review](https://git.k8s.io/community/contributors/devel/sig-instrumentation/metric-stability.md). Stable metrics are guaranteed to **not change**. Please review the documentation for the requirements and lifecycle of stable metrics and ensure that your metrics meet these guidelines.,,,,,,Anecdotal,comment,,,,,,,,2025-03-21,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130951#issuecomment-2741995486,repo: kubernetes/kubernetes | issue: Adding metrics for Maxunavailable feature in StatefulSet | keyword: best practice
> /hold > for [#130951 (comment)](https://github.com/kubernetes/kubernetes/pull/130951#discussion_r2052372114) I believe the comment is being addressed in https://github.com/kubernetes/kubernetes/pull/130909. /hold cancel,,,,,,Anecdotal,comment,,,,,,,,2025-04-28,github/soltysh,https://github.com/kubernetes/kubernetes/pull/130951#issuecomment-2835304077,repo: kubernetes/kubernetes | issue: Adding metrics for Maxunavailable feature in StatefulSet | keyword: best practice
"> Added additional info: [#130951 (comment)](https://github.com/kubernetes/kubernetes/pull/130951#discussion_r2064385198) Filip's comment is on-point, this will need to be addressed.",,,,,,Anecdotal,comment,,,,,,,,2025-06-05,github/soltysh,https://github.com/kubernetes/kubernetes/pull/130951#issuecomment-2944999201,repo: kubernetes/kubernetes | issue: Adding metrics for Maxunavailable feature in StatefulSet | keyword: best practice
"> > /assign > > @richabanker any feedback from the sig-instrumentation pov? whoops, super sorry for the late reply, I think I was just curious, why is the new metric starting off at BETA stabilityLevel?",,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/richabanker,https://github.com/kubernetes/kubernetes/pull/130951#issuecomment-3020131572,repo: kubernetes/kubernetes | issue: Adding metrics for Maxunavailable feature in StatefulSet | keyword: best practice
"@helayoty: You must be a member of the [kubernetes/milestone-maintainers](https://github.com/orgs/kubernetes/teams/milestone-maintainers/members) GitHub team to set the milestone. If you believe you should be able to issue the /milestone command, please contact your Milestone Maintainers Team and have them propose you as an additional delegate for this responsibility. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/130951#issuecomment-3075981052): >/milestone 1.34 …",,,,,,Anecdotal,comment,,,,,,,,2025-07-15,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130951#issuecomment-3075981111,repo: kubernetes/kubernetes | issue: Adding metrics for Maxunavailable feature in StatefulSet | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/130951#"" title=""Author self-approved"">Edwinhr716</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/130951#pullrequestreview-2727736666"" title=""Approved"">hashim21223445</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/130951#pullrequestreview-2764909259"" title=""LGTM"">janetkuo</a>*, *<a href=""https://github.com/kubernetes/kubernete…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130951#issuecomment-3197739892,repo: kubernetes/kubernetes | issue: Adding metrics for Maxunavailable feature in StatefulSet | keyword: best practice
"@Edwinhr716: The following tests **failed**, say `/retest` to rerun all failed tests or `/retest-required` to rerun all mandatory failed tests: Test name | Commit | Details | Required | Rerun command --- | --- | --- | --- | --- pull-kubernetes-linter-hints | 4808d790de806f69e27a13e18c79b79e4234c714 | [link](https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/130951/pull-kubernetes-linter-hints/1957488485862477824) | false | `/test pull-kubernetes-linter-hints` pull-kubernetes-e2e-gce | …",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130951#issuecomment-3197934956,repo: kubernetes/kubernetes | issue: Adding metrics for Maxunavailable feature in StatefulSet | keyword: best practice
"Simplify Hint Generation in Memory Manager In the memory manager's static policy code that generates hints for the topology manager, unify the single-NUMA and cross-NUMA code paths that ensure that a group of NUMA nodes used for an allocation is treated as a single, indivisibile entity in future allocations. Before this PR the two paths are separate, but the single NUMA case is a special case of the cross-NUMA case, so this PR unifies them, yielding simpler code. <!-- Thanks for sending a pull …",,,,,,Anecdotal,issue,,,,,,,,2025-02-18,github/matte21,https://github.com/kubernetes/kubernetes/pull/130226,repo: kubernetes/kubernetes | keyword: best practice | state: open
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/130226#"" title=""Author self-approved"">matte21</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [random-liu](https://github.com/random-liu) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted by t…",,,,,,Anecdotal,comment,,,,,,,,2025-02-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130226#issuecomment-2664509455,repo: kubernetes/kubernetes | issue: Simplify Hint Generation in Memory Manager | keyword: best practice
"Hi @matte21. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once the pat…",,,,,,Anecdotal,comment,,,,,,,,2025-02-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130226#issuecomment-2664509529,repo: kubernetes/kubernetes | issue: Simplify Hint Generation in Memory Manager | keyword: best practice
"@ffromani code clean up is the only motivation. Also, I just noticed that function [isAffinityViolatingNUMAAllocations](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cm/memorymanager/policy_static.go#L1045-L1066) can be simplified pretty much in the same way. Should I push to this PR a new commit that simplifies that function as well?",,,,,,Anecdotal,comment,,,,,,,,2025-02-18,github/matte21,https://github.com/kubernetes/kubernetes/pull/130226#issuecomment-2666114322,repo: kubernetes/kubernetes | issue: Simplify Hint Generation in Memory Manager | keyword: best practice
"@matte21 > Should I push to this PR a new commit that simplifies that function as well? Please, do. It's easier to review one PR than two. /triage accepted /priority backlog",,,,,,Anecdotal,comment,,,,,,,,2025-02-27,github/bart0sh,https://github.com/kubernetes/kubernetes/pull/130226#issuecomment-2687156549,repo: kubernetes/kubernetes | issue: Simplify Hint Generation in Memory Manager | keyword: best practice
"@ffromani @bart0sh I'm sorry, I had completely forgotten about this. I just force-pushed to this branch. The force-push does the following things: 1. rebase on master. 2. address this review comment: https://github.com/kubernetes/kubernetes/pull/130226#issuecomment-2687156549 3. Given the change in point 2, it updates the code in function `calculateHints` that performs the check to use function `isAffinityViolatingNUMAAllocations` rather than doing the check ""manually"".",,,,,,Anecdotal,comment,,,,,,,,2025-05-13,github/matte21,https://github.com/kubernetes/kubernetes/pull/130226#issuecomment-2876823655,repo: kubernetes/kubernetes | issue: Simplify Hint Generation in Memory Manager | keyword: best practice
force-push to [d265cfa](https://github.com/kubernetes/kubernetes/commit/d265cfa0092b139f115e25f74438199892a49019) rebases on master,,,,,,Anecdotal,comment,,,,,,,,2025-05-15,github/matte21,https://github.com/kubernetes/kubernetes/pull/130226#issuecomment-2884189743,repo: kubernetes/kubernetes | issue: Simplify Hint Generation in Memory Manager | keyword: best practice
"force push to [f37bc85](https://github.com/kubernetes/kubernetes/pull/130226/commits/f37bc852cf27b509cc50e87c8794f1bfa8b110c5) rebases on master, which now includes the fix for the failing test (which was unrelated to this PR): https://github.com/kubernetes-sigs/windows-testing/pull/505",,,,,,Anecdotal,comment,,,,,,,,2025-05-18,github/matte21,https://github.com/kubernetes/kubernetes/pull/130226#issuecomment-2888693388,repo: kubernetes/kubernetes | issue: Simplify Hint Generation in Memory Manager | keyword: best practice
"The Kubernetes project currently lacks enough contributors to adequately respond to all PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Mark this PR as fresh with `/remove-lifecycle stale` - Close this PR with `/close` - Offer to help out with [I…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130226#issuecomment-3197909351,repo: kubernetes/kubernetes | issue: Simplify Hint Generation in Memory Manager | keyword: best practice
"Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especi…",,,,,,Anecdotal,issue,,,,,,,,2024-05-10,github/towca,https://github.com/kubernetes/kubernetes/pull/124800,repo: kubernetes/kubernetes | keyword: best practice | state: open
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2024-05-10,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2104814890,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"> `node-autoscaling.kubernetes.io/safe-to-evict=true` I feel like we are a caught between a rock and a hard place with this semantic. I can see why this semantic works well for CAS, but now this semantic causes awkwardness for Karpenter users since there's currently no scenario where `node-autoscaling.kubernetes.io/safe-to-evict=true` would apply, since all pods are safe to evict by default (and we layer blocking elements on top of it). Effectively, all Karpenter users would always be doing `no…",,,,,,Anecdotal,comment,,,,,,,,2024-05-10,github/jonathan-innis,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2104845595,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"@jonathan-innis > since all pods are safe to evict by default (and we layer blocking elements on top of it) So the only way for a pod to block consolidation of its node in Karpenter is for the user to explicitly opt that exact pod/workload into the blocking somehow? Or how does it work? Do you have/anticipate any such blocking config options that would span multiple workloads? If so, `safe-to-evict: true` is still useful for ""exceptions"", something like: * I want to configure X workloads togeth…",,,,,,Anecdotal,comment,,,,,,,,2024-05-13,github/towca,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2108397590,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"> The behavior I want for most of the workloads is to block consolidation on some conditions - e.g. if a pod uses local storage I think the core of the problem here is that the defaults for CAS and Karpenter are different when it comes to blocking evictions. Karpenter takes the opinion that you aren't ""blocking eviction"" of a pod unless you have a pod that tolerates the Karpenter drain/disruption taint. Even still -- with this in place -- we will still drain the node with this pod on there, we …",,,,,,Anecdotal,comment,,,,,,,,2024-06-12,github/jonathan-innis,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2163461167,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"> FWIW, I think that we should work to align on the behavior here _before_ we align on the taint. Ideally, we have one opinion about how this drain operation works in Kubernetes, this applies across all SIGs and is seen as ""the way to drain nodes gracefully"" and the `do-not-disrupt` or `safe-to-evict` annotations are just part of that story. Thoughts? I think we might be looking at this from slightly different perspectives, or confusing ""blocking eviction"" with ""blocking consolidation"". I fully…",,,,,,Anecdotal,comment,,,,,,,,2024-06-13,github/towca,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2165761877,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"> A new Pod annotation `node-autoscaling.kubernetes.io/safe-to-evict` is introduced. The annotation can be used to control Node autoscaler drain behavior. Value ""true"" means that a Pod is safe to evict, and Node autoscalers should not block consolidation of a Node because of it, when they normally would. Value ""false"" means that a Pod is not safe to evict, and Node autoscalers shouldn't consolidate a Node where such a pod is present. The annotation is supported by Cluster Autoscaler and Karpent…",,,,,,Anecdotal,comment,,,,,,,,2024-06-25,github/sftim,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2188268009,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"> Btw, this abstraction layer difference is also precisely why I see this annotation in the node-autoscaling prefix instead of the broader node-lifecycle one. My intention is for node-autoscaling.kubernetes.io/safe-to-evict=false to mean ""pod not safe to evict by autoscalers, so autoscalers can't consolidate its node"". In contrast, the way I'd understand node-lifecycle.kubernetes.io/safe-to-evict=false would be ""pod not safe to evict by anyone using the standard draining mechanism, so its node …",,,,,,Anecdotal,comment,,,,,,,,2024-07-09,github/MaciekPytel,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2218288529,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"The Kubernetes project currently lacks enough contributors to adequately respond to all PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Mark this PR as fresh with `/remove-lifecycle stale` - Close this PR with `/close` - Offer to help out with [I…",,,,,,Anecdotal,comment,,,,,,,,2024-10-07,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2397559216,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"> Fully blocking draining a node for everything in the cluster has serious implications, and would only be practical for very specific workflows So to be clear -- the way that we get around this problem today with `karpenter.sh/do-not-disrupt` annotation fully blocking the drain of pods for a node is that we have a separate concept known as the `terminationGracePeriod` of the node. When we choose to disrupt a node, we start draining all of the pods on it -- if there is a pod that has this annot…",,,,,,Anecdotal,comment,,,,,,,,2024-10-30,github/jonathan-innis,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2447981920,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"The Kubernetes project currently lacks enough active contributors to adequately respond to all PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Mark this PR as fresh with `/remove-lifecycle rotten` - Close this PR with `/close` - Offer to help out…",,,,,,Anecdotal,comment,,,,,,,,2024-11-29,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2508357566,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Reopen this PR with `/reopen` - Mark this PR as fresh with `/remove-lifecycle rotten` - Offe…",,,,,,Anecdotal,comment,,,,,,,,2024-12-29,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2564819065,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"@k8s-triage-robot: Closed this PR. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2564819065): >The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. > >This bot triages PRs according to the following rules: >- After 90d of inactivity, `lifecycle/stale` is applied >- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied >- After 30d of inactivity since…",,,,,,Anecdotal,comment,,,,,,,,2024-12-29,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2564819083,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"@towca: Reopened this PR. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2565375650): >/reopen Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository. </details>",,,,,,Anecdotal,comment,,,,,,,,2024-12-30,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2565375745,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Reopen this PR with `/reopen` - Mark this PR as fresh with `/remove-lifecycle rotten` - Offe…",,,,,,Anecdotal,comment,,,,,,,,2025-01-29,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2621477651,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"@k8s-triage-robot: Closed this PR. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2621477651): >The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. > >This bot triages PRs according to the following rules: >- After 90d of inactivity, `lifecycle/stale` is applied >- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied >- After 30d of inactivity since…",,,,,,Anecdotal,comment,,,,,,,,2025-01-29,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2621477875,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"@towca: Reopened this PR. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2621624734): >/reopen > Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository. </detai…",,,,,,Anecdotal,comment,,,,,,,,2025-01-29,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2621624925,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/124800#"" title=""Author self-approved"">towca</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [msau42](https://github.com/msau42) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted by this bot ca…",,,,,,Anecdotal,comment,,,,,,,,2025-01-29,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2621625629,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Reopen this PR with `/reopen` - Mark this PR as fresh with `/remove-lifecycle rotten` - Offe…",,,,,,Anecdotal,comment,,,,,,,,2025-02-28,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2690723998,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"@k8s-triage-robot: Closed this PR. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2690723998): >The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. > >This bot triages PRs according to the following rules: >- After 90d of inactivity, `lifecycle/stale` is applied >- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied >- After 30d of inactivity since…",,,,,,Anecdotal,comment,,,,,,,,2025-02-28,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2690724260,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"@towca: Reopened this PR. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2751970874): >/reopen Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository. </details>",,,,,,Anecdotal,comment,,,,,,,,2025-03-25,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2751971111,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Reopen this PR with `/reopen` - Mark this PR as fresh with `/remove-lifecycle rotten` - Offe…",,,,,,Anecdotal,comment,,,,,,,,2025-04-24,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2828613889,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"@k8s-triage-robot: Closed this PR. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2828613889): >The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. > >This bot triages PRs according to the following rules: >- After 90d of inactivity, `lifecycle/stale` is applied >- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied >- After 30d of inactivity since…",,,,,,Anecdotal,comment,,,,,,,,2025-04-24,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2828614092,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"[FG:InPlacePodVerticalScaling] kubelet: record container_resize_requests metric for all resize updates <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of is…",,,,,,Anecdotal,issue,,,,,,,,2025-07-18,github/natasha41575,https://github.com/kubernetes/kubernetes/pull/133060,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"@natasha41575: The label(s) `priority/important, priority/soon` cannot be applied, because the repository doesn't have them. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/133060): > > >#### What type of PR is this? > > > >/kind feature > >#### What this PR does / why we need it: > >Addresses the TODO in https://github.com/kubernetes/kubernetes/pull/132903#discussion_r2211029887. Fetch the old pod in the pod manager in `HandlePodUpdates` to better detect if this p…",,,,,,Anecdotal,comment,,,,,,,,2025-07-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133060#issuecomment-3089838398,repo: kubernetes/kubernetes | issue: [FG:InPlacePodVerticalScaling] kubelet: record container_resize_requests metric for all resize updates | keyword: best practice
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133060#"" title=""Author self-approved"">natasha41575</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/133060#issuecomment-3090668245"" title=""Approved"">tallclair</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https://g…",,,,,,Anecdotal,comment,,,,,,,,2025-07-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133060#issuecomment-3090668773,repo: kubernetes/kubernetes | issue: [FG:InPlacePodVerticalScaling] kubelet: record container_resize_requests metric for all resize updates | keyword: best practice
"@natasha41575: The following test **failed**, say `/retest` to rerun all failed tests or `/retest-required` to rerun all mandatory failed tests: Test name | Commit | Details | Required | Rerun command --- | --- | --- | --- | --- pull-kubernetes-unit-windows-master | 22d724969c96097ce2efdd01904f9b3226263adf | [link](https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/133060/pull-kubernetes-unit-windows-master/1946359156826968064) | false | `/test pull-kubernetes-unit-windows-master` [Ful…",,,,,,Anecdotal,comment,,,,,,,,2025-07-19,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133060#issuecomment-3091304761,repo: kubernetes/kubernetes | issue: [FG:InPlacePodVerticalScaling] kubelet: record container_resize_requests metric for all resize updates | keyword: best practice
"Clarify staging repository READMEs <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pul…",,,,,,Anecdotal,issue,,,,,,,,2025-08-15,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/133570,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.34` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.34.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133570#issuecomment-3192230769,repo: kubernetes/kubernetes | issue: Clarify staging repository READMEs | keyword: best practice
"/cc @dims I think we should actually ship this one in 1.34, it should be zero risk to the release but we have closed the issue trackers and lack more obviously contributors users to the correct location.",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/133570#issuecomment-3192233006,repo: kubernetes/kubernetes | issue: Clarify staging repository READMEs | keyword: best practice
"Additional diff in: https://github.com/kubernetes/kubernetes/pull/133570/commits/e49f6116f9eec5d48f2c8913e598fef496644d01 Based on @thockin's point that ""staging repository"" probably isn't the right term for the published version. Instead `[...] automatically puslished [staged repository](<link to staging docs>) [...]`",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/133570#issuecomment-3192818028,repo: kubernetes/kubernetes | issue: Clarify staging repository READMEs | keyword: best practice
/retest ... cluster bringup flake in https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/133570/pull-kubernetes-e2e-gce/1956469268799295488,,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/133570#issuecomment-3192863743,repo: kubernetes/kubernetes | issue: Clarify staging repository READMEs | keyword: best practice
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133570#"" title=""Author self-approved"">BenTheElder</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/133570#issuecomment-3192293873"" title=""LGTM"">dims</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/133570#pullrequestreview-3125265246"" title=""Approved"">thockin</a>* The full list of commands accepted by this bot can be found [here](ht…",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133570#issuecomment-3192870889,repo: kubernetes/kubernetes | issue: Clarify staging repository READMEs | keyword: best practice
"Deny pod admission for static pods referencing API objects <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this i…",,,,,,Anecdotal,issue,,,,,,,,2025-05-18,github/sreeram-venkitesh,https://github.com/kubernetes/kubernetes/pull/131837,repo: kubernetes/kubernetes | keyword: best practice | state: closed
CC @haircommander @liggitt Tagging from the discussion in the original issue. I'd love to hear what you folks think! Thanks!,,,,,,Anecdotal,comment,,,,,,,,2025-05-18,github/sreeram-venkitesh,https://github.com/kubernetes/kubernetes/pull/131837#issuecomment-2889052462,repo: kubernetes/kubernetes | issue: Deny pod admission for static pods referencing API objects | keyword: best practice
"> (We perhaps shouldn't assume that these claims will fail anyway). True, but if they don't make sense for static pods I'm okay with denying pods with `.spec.resources.claims` and / or `.spec.resourceClaims` too. I wonder if we'd want to let users create static pods with resource claims.",,,,,,Anecdotal,comment,,,,,,,,2025-05-19,github/sreeram-venkitesh,https://github.com/kubernetes/kubernetes/pull/131837#issuecomment-2891524396,repo: kubernetes/kubernetes | issue: Deny pod admission for static pods referencing API objects | keyword: best practice
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/131837#issuecomment-3008343257"" title=""Approved"">liggitt</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/131837#"" title=""Author self-approved"">sreeram-venkitesh</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https:…",,,,,,Anecdotal,comment,,,,,,,,2025-06-26,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131837#issuecomment-3008344056,repo: kubernetes/kubernetes | issue: Deny pod admission for static pods referencing API objects | keyword: best practice
"The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass. This bot retests PRs for certain kubernetes repos according to the following rules: - The PR does have any `do-not-merge/*` labels - The PR does not have the `needs-ok-to-test` label - The PR is mergeable (does not have a `needs-rebase` label) - The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels) - The PR is failing tests required for merge You can: - Review the [full test history]…",,,,,,Anecdotal,comment,,,,,,,,2025-06-26,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/131837#issuecomment-3008998447,repo: kubernetes/kubernetes | issue: Deny pod admission for static pods referencing API objects | keyword: best practice
"> As discussed in https://github.com/kubernetes/kubernetes/issues/103587#issuecomment-2433035095, we want to tighten the validation here so that pod would not be running at all in the node. Currently the static pod would be running in the node while the mirror pod would not be created (which is enforced in the API as per https://github.com/kubernetes/kubernetes/issues/103587#issuecomment-1456581669). This PR updates the validation logic so that the pod is denied admission for static pods refere…",,,,,,Anecdotal,comment,,,,,,,,2025-08-11,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/131837#issuecomment-3176584057,repo: kubernetes/kubernetes | issue: Deny pod admission for static pods referencing API objects | keyword: best practice
"This is also probably an ""action required"" and/or ""urgent upgrade notes"" release note @kubernetes/release-team-docs since those pods were silently permitted before and will now fail to admit? And you likely need to take action before upgrade, since these are static pods on disk that may e.g. be hosting your control plane.",,,,,,Anecdotal,comment,,,,,,,,2025-08-11,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/131837#issuecomment-3176614687,repo: kubernetes/kubernetes | issue: Deny pod admission for static pods referencing API objects | keyword: best practice
"> Updated the description to add an ""Action required"" line and label. Does that get synced into the 1.34 changelog automatically at this stage in the cycle? Should we mention that these would've been silently ignored previously and therefore are safe to drop?",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/131837#issuecomment-3180295947,repo: kubernetes/kubernetes | issue: Deny pod admission for static pods referencing API objects | keyword: best practice
x-ref: https://github.com/kubernetes/website/pull/51877#discussion_r2277577623 I think at least the release blog should make it clear that users can just remove these because they weren't doing anything before anyhow.,,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/131837#issuecomment-3189729954,repo: kubernetes/kubernetes | issue: Deny pod admission for static pods referencing API objects | keyword: best practice
"I filed https://github.com/kubernetes/kubernetes/issues/133535 because I think we should clear up the story about `.spec.priority` vs `.spec.priorityClassName` for static Pods. Doesn't need resolving for the release notes, but it is an open problem area (even if the answer is ""we've already decided what to do, now let's document that properly"").",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/lmktfy,https://github.com/kubernetes/kubernetes/pull/131837#issuecomment-3189739765,repo: kubernetes/kubernetes | issue: Deny pod admission for static pods referencing API objects | keyword: best practice
"(fwiw, this PR didn't modify any behavior related to priority or priorityClassName ... agree we should think through what *should* happen and make it clear in documentation / tested behavior, etc)",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/liggitt,https://github.com/kubernetes/kubernetes/pull/131837#issuecomment-3189757186,repo: kubernetes/kubernetes | issue: Deny pod admission for static pods referencing API objects | keyword: best practice
I still don't see this highlighted in action-required /urgent upgrade notice for: https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.34.md#changelog-since-v1330 cc @kubernetes/release-team-docs @kubernetes/release-team,,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/131837#issuecomment-3197653834,repo: kubernetes/kubernetes | issue: Deny pod admission for static pods referencing API objects | keyword: best practice
"Fix build error nfacct Tests on s390x by Generating Custom Test Data on s390x <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, e…",,,,,,Anecdotal,issue,,,,,,,,2025-04-30,github/saisindhuri91,https://github.com/kubernetes/kubernetes/pull/131552,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"<a href=""https://api.easycla.lfx.linuxfoundation.org/v2/repository-provider/github/sign/18706487/20580498/131552/#/?version=2""><img src=""https://s3.amazonaws.com/cla-project-logo-prod/cla-not-signed.svg"" alt=""CLA Not Signed"" align=""left"" height=""28"" width=""328""></a><br/><br /><ul><li><a href='https://api.easycla.lfx.linuxfoundation.org/v2/repository-provider/github/sign/18706487/20580498/131552/#/?version=2' target='_blank'>:x:</a> - login: @saisindhuri91 / name: saisindhuri91 . The commit (602…",,,,,,Anecdotal,comment,,,,,,,,2025-04-30,github/linux-foundation-easycla[bot],https://github.com/kubernetes/kubernetes/pull/131552#issuecomment-2841715392,repo: kubernetes/kubernetes | issue: Fix build error nfacct Tests on s390x by Generating Custom Test Data on s390x  | keyword: best practice
Welcome @saisindhuri91! <br><br>It looks like this is your first PR to <a href='https://github.com/kubernetes/kubernetes'>kubernetes/kubernetes</a> 🎉. Please refer to our [pull request process documentation](https://www.kubernetes.dev/docs/guide/pull-requests/) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](h…,,,,,,Anecdotal,comment,,,,,,,,2025-04-30,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131552#issuecomment-2841715583,repo: kubernetes/kubernetes | issue: Fix build error nfacct Tests on s390x by Generating Custom Test Data on s390x  | keyword: best practice
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-04-30,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131552#issuecomment-2841715590,repo: kubernetes/kubernetes | issue: Fix build error nfacct Tests on s390x by Generating Custom Test Data on s390x  | keyword: best practice
"Hi @saisindhuri91. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once t…",,,,,,Anecdotal,comment,,,,,,,,2025-04-30,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131552#issuecomment-2841715633,repo: kubernetes/kubernetes | issue: Fix build error nfacct Tests on s390x by Generating Custom Test Data on s390x  | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/131552#"" title=""Author self-approved"">saisindhuri91</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [andrewsykim](https://github.com/andrewsykim) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accep…",,,,,,Anecdotal,comment,,,,,,,,2025-04-30,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131552#issuecomment-2841716252,repo: kubernetes/kubernetes | issue: Fix build error nfacct Tests on s390x by Generating Custom Test Data on s390x  | keyword: best practice
"This is not strictly-speaking an s390x thing, it's a ""big-endian platform"" thing. (Though I guess we don't support any other big-endian platforms?) The simpler fix would be to just skip the unit tests on big-endian platforms.",,,,,,Anecdotal,comment,,,,,,,,2025-05-01,github/danwinship,https://github.com/kubernetes/kubernetes/pull/131552#issuecomment-2845116190,repo: kubernetes/kubernetes | issue: Fix build error nfacct Tests on s390x by Generating Custom Test Data on s390x  | keyword: best practice
"> The simpler fix would be to just skip the unit tests on big-endian platforms +1 , we can also add a skip based on the platform, we are doing that for windows per example ``` if runtime.GOARCH == ""s390x"" { t.Skipf(""Test is not supported on GOARCH=%s"", runtime.GOARCH) } ``` https://github.com/kubernetes/kubernetes/blob/e1cf24670f6f97063d42be0a494f4096a4ccb81e/pkg/volume/fc/fc_test.go#L434-L437",,,,,,Anecdotal,comment,,,,,,,,2025-05-02,github/aojea,https://github.com/kubernetes/kubernetes/pull/131552#issuecomment-2847237999,repo: kubernetes/kubernetes | issue: Fix build error nfacct Tests on s390x by Generating Custom Test Data on s390x  | keyword: best practice
"So we finally got a real s390x cluster and the unit tests are failing on this architecture, specifically this test. https://testgrid.k8s.io/ibm-k8s-s390x#ci-kubernetes-unit-s390x",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/upodroid,https://github.com/kubernetes/kubernetes/pull/131552#issuecomment-3196892392,repo: kubernetes/kubernetes | issue: Fix build error nfacct Tests on s390x by Generating Custom Test Data on s390x  | keyword: best practice
"list pods with resourceVersion when kubectl drain <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a relea…",,,,,,Anecdotal,issue,,,,,,,,2025-03-10,github/Sakuralbj,https://github.com/kubernetes/kubernetes/pull/130682,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"Adding the ""do-not-merge/release-note-label-needed"" label because no release-note block was detected, please follow our [release note process](https://git.k8s.io/community/contributors/guide/release-notes.md) to remove it. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https:…",,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130682#issuecomment-2710035258,repo: kubernetes/kubernetes | issue: list pods with resourceVersion when kubectl drain | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/130682#"" title=""Author self-approved"">Sakuralbj</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [ardaguclu](https://github.com/ardaguclu) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted by t…",,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130682#issuecomment-2710035421,repo: kubernetes/kubernetes | issue: list pods with resourceVersion when kubectl drain | keyword: best practice
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130682#issuecomment-2710035555,repo: kubernetes/kubernetes | issue: list pods with resourceVersion when kubectl drain | keyword: best practice
"Hi @Sakuralbj. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once the p…",,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130682#issuecomment-2710035618,repo: kubernetes/kubernetes | issue: list pods with resourceVersion when kubectl drain | keyword: best practice
"Changelog suggestion ```diff -In a large-scale cluster without the ConsistentReadFromCache feature enabled, draining nodes without resourceVersion can impose significant pressure on etcd, impacting the stability of cluster. +Changed `kubectl` to watch nodes with `resourceVersion` whilst draining. In a large cluster that does not have the _consistent reads from cache_ feature active, draining nodes without `resourceVersion` can impose significant pressure on etcd; this change provides a client-s…",,,,,,Anecdotal,comment,,,,,,,,2025-03-20,github/sftim,https://github.com/kubernetes/kubernetes/pull/130682#issuecomment-2741460265,repo: kubernetes/kubernetes | issue: list pods with resourceVersion when kubectl drain | keyword: best practice
"The Kubernetes project currently lacks enough contributors to adequately respond to all PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Mark this PR as fresh with `/remove-lifecycle stale` - Close this PR with `/close` - Offer to help out with [I…",,,,,,Anecdotal,comment,,,,,,,,2025-06-19,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130682#issuecomment-2986764251,repo: kubernetes/kubernetes | issue: list pods with resourceVersion when kubectl drain | keyword: best practice
"The Kubernetes project currently lacks enough active contributors to adequately respond to all PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Mark this PR as fresh with `/remove-lifecycle rotten` - Close this PR with `/close` - Offer to help out…",,,,,,Anecdotal,comment,,,,,,,,2025-07-19,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130682#issuecomment-3091936828,repo: kubernetes/kubernetes | issue: list pods with resourceVersion when kubectl drain | keyword: best practice
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Reopen this PR with `/reopen` - Mark this PR as fresh with `/remove-lifecycle rotten` - Offe…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130682#issuecomment-3195366945,repo: kubernetes/kubernetes | issue: list pods with resourceVersion when kubectl drain | keyword: best practice
"@k8s-triage-robot: Closed this PR. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/130682#issuecomment-3195366945): >The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. > >This bot triages PRs according to the following rules: >- After 90d of inactivity, `lifecycle/stale` is applied >- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied >- After 30d of inactivity since…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130682#issuecomment-3195367214,repo: kubernetes/kubernetes | issue: list pods with resourceVersion when kubectl drain | keyword: best practice
"kubelet: Fix crashloop backoff not applied on first container restart <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especiall…",,,,,,Anecdotal,issue,,,,,,,,2025-08-16,github/xigang,https://github.com/kubernetes/kubernetes/pull/133577,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.34` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.34.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-08-16,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133577#issuecomment-3193670070,repo: kubernetes/kubernetes | issue: kubelet: Fix crashloop backoff not applied on first container restart | keyword: best practice
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-08-16,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133577#issuecomment-3193670108,repo: kubernetes/kubernetes | issue: kubelet: Fix crashloop backoff not applied on first container restart | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133577#"" title=""Author self-approved"">xigang</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [sjenning](https://github.com/sjenning) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted by this b…",,,,,,Anecdotal,comment,,,,,,,,2025-08-16,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133577#issuecomment-3193670160,repo: kubernetes/kubernetes | issue: kubelet: Fix crashloop backoff not applied on first container restart | keyword: best practice
"@xigang: The following tests **failed**, say `/retest` to rerun all failed tests or `/retest-required` to rerun all mandatory failed tests: Test name | Commit | Details | Required | Rerun command --- | --- | --- | --- | --- pull-kubernetes-conformance-kind-ga-only-parallel | 2f9b2bcffe00c86c8c7216bb6024fe9779bc7236 | [link](https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/133577/pull-kubernetes-conformance-kind-ga-only-parallel/1956740847848394752) | true | `/test pull-kubernetes-con…",,,,,,Anecdotal,comment,,,,,,,,2025-08-16,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133577#issuecomment-3193772194,repo: kubernetes/kubernetes | issue: kubelet: Fix crashloop backoff not applied on first container restart | keyword: best practice
"@xigang: Closed this PR. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/133577#issuecomment-3194359584): >/close Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository. </details>",,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133577#issuecomment-3194359622,repo: kubernetes/kubernetes | issue: kubelet: Fix crashloop backoff not applied on first container restart | keyword: best practice
"The expected behavior of the Kubelet is that a container should be restarted immediately after the first failure, and only from the second failure onward should it enter exponential backoff. This change violates that behavioral contract, directly impacting the timing assumptions and state checks of a series of e2e/consistency tests.",,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/xigang,https://github.com/kubernetes/kubernetes/pull/133577#issuecomment-3194361569,repo: kubernetes/kubernetes | issue: kubelet: Fix crashloop backoff not applied on first container restart | keyword: best practice
"Clean(pkg/apis) use generic sets rather than deprecated sets struct and method <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, …",,,,,,Anecdotal,issue,,,,,,,,2025-02-10,github/ab1stone,https://github.com/kubernetes/kubernetes/pull/130065,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"Adding the ""do-not-merge/release-note-label-needed"" label because no release-note block was detected, please follow our [release note process](https://git.k8s.io/community/contributors/guide/release-notes.md) to remove it. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https:…",,,,,,Anecdotal,comment,,,,,,,,2025-02-10,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130065#issuecomment-2647620437,repo: kubernetes/kubernetes | issue: Clean(pkg/apis) use generic sets rather than deprecated sets struct and method | keyword: best practice
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-02-10,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130065#issuecomment-2647620683,repo: kubernetes/kubernetes | issue: Clean(pkg/apis) use generic sets rather than deprecated sets struct and method | keyword: best practice
"Hi @ab1stone. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once the pa…",,,,,,Anecdotal,comment,,,,,,,,2025-02-10,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130065#issuecomment-2647620710,repo: kubernetes/kubernetes | issue: Clean(pkg/apis) use generic sets rather than deprecated sets struct and method | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/130065#"" title=""Author self-approved"">ab1stone</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [deads2k](https://github.com/deads2k) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted by this b…",,,,,,Anecdotal,comment,,,,,,,,2025-02-10,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130065#issuecomment-2647621544,repo: kubernetes/kubernetes | issue: Clean(pkg/apis) use generic sets rather than deprecated sets struct and method | keyword: best practice
"@ab1stone: GitHub didn't allow me to request PR reviews from the following users: ab1stone. Note that only [kubernetes members](https://github.com/orgs/kubernetes/people) and repo collaborators can review this PR, and authors cannot review their own PRs. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/130065#issuecomment-2647629410): >/cc Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull…",,,,,,Anecdotal,comment,,,,,,,,2025-02-10,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130065#issuecomment-2647629522,repo: kubernetes/kubernetes | issue: Clean(pkg/apis) use generic sets rather than deprecated sets struct and method | keyword: best practice
"The Kubernetes project currently lacks enough contributors to adequately respond to all PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Mark this PR as fresh with `/remove-lifecycle stale` - Close this PR with `/close` - Offer to help out with [I…",,,,,,Anecdotal,comment,,,,,,,,2025-06-18,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130065#issuecomment-2982601185,repo: kubernetes/kubernetes | issue: Clean(pkg/apis) use generic sets rather than deprecated sets struct and method | keyword: best practice
"The Kubernetes project currently lacks enough active contributors to adequately respond to all PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Mark this PR as fresh with `/remove-lifecycle rotten` - Close this PR with `/close` - Offer to help out…",,,,,,Anecdotal,comment,,,,,,,,2025-07-18,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130065#issuecomment-3086652954,repo: kubernetes/kubernetes | issue: Clean(pkg/apis) use generic sets rather than deprecated sets struct and method | keyword: best practice
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Reopen this PR with `/reopen` - Mark this PR as fresh with `/remove-lifecycle rotten` - Offe…",,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130065#issuecomment-3194127596,repo: kubernetes/kubernetes | issue: Clean(pkg/apis) use generic sets rather than deprecated sets struct and method | keyword: best practice
"@k8s-triage-robot: Closed this PR. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/130065#issuecomment-3194127596): >The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. > >This bot triages PRs according to the following rules: >- After 90d of inactivity, `lifecycle/stale` is applied >- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied >- After 30d of inactivity since…",,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130065#issuecomment-3194127633,repo: kubernetes/kubernetes | issue: Clean(pkg/apis) use generic sets rather than deprecated sets struct and method | keyword: best practice
"cleanup: remove redundant cleanFlagSet nil check <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a releas…",,,,,,Anecdotal,issue,,,,,,,,2025-07-22,github/xigang,https://github.com/kubernetes/kubernetes/pull/133125,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133125#issuecomment-3103204314,repo: kubernetes/kubernetes | issue: cleanup: remove redundant cleanFlagSet nil check | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133125#"" title=""Author self-approved"">xigang</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [dchen1107](https://github.com/dchen1107) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted by this…",,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133125#issuecomment-3103205413,repo: kubernetes/kubernetes | issue: cleanup: remove redundant cleanFlagSet nil check | keyword: best practice
"Docs: Clarify that this is a staging repository and not for direct contributions <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing…",,,,,,Anecdotal,issue,,,,,,,,2025-04-17,github/sAchin-680,https://github.com/kubernetes/kubernetes/pull/131351,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.33` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.33.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-04-17,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131351#issuecomment-2812726950,repo: kubernetes/kubernetes | issue: Docs: Clarify that this is a staging repository and not for direct contributions | keyword: best practice
Welcome @sAchin-680! <br><br>It looks like this is your first PR to <a href='https://github.com/kubernetes/kubernetes'>kubernetes/kubernetes</a> 🎉. Please refer to our [pull request process documentation](https://git.k8s.io/community/contributors/guide/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documen…,,,,,,Anecdotal,comment,,,,,,,,2025-04-17,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131351#issuecomment-2812727120,repo: kubernetes/kubernetes | issue: Docs: Clarify that this is a staging repository and not for direct contributions | keyword: best practice
"Hi @sAchin-680. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once the …",,,,,,Anecdotal,comment,,,,,,,,2025-04-17,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131351#issuecomment-2812727165,repo: kubernetes/kubernetes | issue: Docs: Clarify that this is a staging repository and not for direct contributions | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/131351#"" title=""Author self-approved"">sAchin-680</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [deads2k](https://github.com/deads2k), [thockin](https://github.com/thockin) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The…",,,,,,Anecdotal,comment,,,,,,,,2025-04-17,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131351#issuecomment-2813316650,repo: kubernetes/kubernetes | issue: Docs: Clarify that this is a staging repository and not for direct contributions | keyword: best practice
"With ""upstream"" as the name of my git remote for this repo: ```console $ git remote -v origin ssh://git@github.com/BenTheElder/kubernetes.git (fetch) origin ssh://git@github.com/BenTheElder/kubernetes.git (push) upstream ssh://git@github.com/kubernetes/kubernetes.git (fetch) upstream no_push (push) ``` To fix the merge commit do: ```console git fetch upstream ``` And then: ```console git rebase upstream/master ``` Finally push the changes, we will force push because we are rewriting history for…",,,,,,Anecdotal,comment,,,,,,,,2025-04-28,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/131351#issuecomment-2836439272,repo: kubernetes/kubernetes | issue: Docs: Clarify that this is a staging repository and not for direct contributions | keyword: best practice
"The Kubernetes project currently lacks enough contributors to adequately respond to all PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Mark this PR as fresh with `/remove-lifecycle stale` - Close this PR with `/close` - Offer to help out with [I…",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/131351#issuecomment-3128170256,repo: kubernetes/kubernetes | issue: Docs: Clarify that this is a staging repository and not for direct contributions | keyword: best practice
"PR needs rebase. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository. </details>",,,,,,Anecdotal,comment,,,,,,,,2025-08-16,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131351#issuecomment-3193617495,repo: kubernetes/kubernetes | issue: Docs: Clarify that this is a staging repository and not for direct contributions | keyword: best practice
"Apply best practices to staging repos See: https://github.com/kubernetes/community/pull/8308 for the best practices. To understand more about staging, see the docs at https://github.com/kubernetes/kubernetes/tree/master/staging For every repo in `staging/src/k8s.io` we should: 1. Update the README for each these repos to clarify that contributions including issues and PRs should be made to this repo instead of the ""staged"" copy. 2. Request that issues be disabled in github.com/kubernetes/org fo…",,,,,,Anecdotal,issue,,,,,,,,2025-04-15,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/131315,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"Hi @sAchin-680, I've assigned but you don't need to be assigned to work on any issue. Please mention this issue in any pull requests or issues filed *but not the commit messages*, just the discussion on github, so we can track without excessive noise. Thanks!",,,,,,Anecdotal,comment,,,,,,,,2025-04-16,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/131315#issuecomment-2810227896,repo: kubernetes/kubernetes | issue: Apply best practices to staging repos | keyword: best practice
I think we could do these in bulk? We need the same repetitive action taken for each 1) PR the readmes in this repo 2) request issue disablement for remaining staging repos 3) update blockade config for remaining repos 1) should happen first And maybe before 2) and 3) we should check existing open issues / PRs and redirect those users,,,,,,Anecdotal,comment,,,,,,,,2025-04-17,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/131315#issuecomment-2813224449,repo: kubernetes/kubernetes | issue: Apply best practices to staging repos | keyword: best practice
"Thanks for the clarification! Will make changes as per your guidance, appreciate the direction! Let me know if there's anything you'd like prioritized or handled differently.",,,,,,Anecdotal,comment,,,,,,,,2025-04-17,github/sAchin-680,https://github.com/kubernetes/kubernetes/issues/131315#issuecomment-2813243906,repo: kubernetes/kubernetes | issue: Apply best practices to staging repos | keyword: best practice
"Thanks! We are actually in code freeze for the 1.33 release right now, so these won't merge until after april 23rd anyhow (https://k8s.dev/release), we have time to consolidate to one PR / issue for each of these tasks. I think it will be easier to get review and approve, for the READMEs we have approvers that can do all of those at once, and it will be easy to review the same language in each staging repo. The same for the blockade config and for disabling issues. I think that will be quicker …",,,,,,Anecdotal,comment,,,,,,,,2025-04-17,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/131315#issuecomment-2813968053,repo: kubernetes/kubernetes | issue: Apply best practices to staging repos | keyword: best practice
"Thanks @BenTheElder for the update and the helpful suggestion! I’ll go ahead and consolidate the changes into one PR per task (README updates, blockade config, and disabling issues) as you recommended. That definitely sounds more efficient and will make review easier post-freeze. I'll prep everything so it's ready to go after April 23rd. Appreciate your support and guidance!",,,,,,Anecdotal,comment,,,,,,,,2025-04-18,github/sAchin-680,https://github.com/kubernetes/kubernetes/issues/131315#issuecomment-2816111348,repo: kubernetes/kubernetes | issue: Apply best practices to staging repos | keyword: best practice
"Hi @BenTheElder !! I went through everything and noticed that I’ve already created consolidated PRs for each of the tasks you mentioned - one each for the README updates, disabling GitHub issues, and adding the blockades. Here are the links for reference: - README updates across all staging repos: [Docs: Clarify that this is a staging repository and not for direct contributions (#131351)](https://github.com/kubernetes/kubernetes/pull/131351) - Disable GitHub issues in staging mirror repos [Disa…",,,,,,Anecdotal,comment,,,,,,,,2025-04-18,github/sAchin-680,https://github.com/kubernetes/kubernetes/issues/131315#issuecomment-2816135098,repo: kubernetes/kubernetes | issue: Apply best practices to staging repos | keyword: best practice
"So we have the issues/wikis disabled now: https://github.com/kubernetes/org/issues/5545#issuecomment-3189316536 But we still need the blockade on PRs, and the README updates",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/131315#issuecomment-3190004459,repo: kubernetes/kubernetes | issue: Apply best practices to staging repos | keyword: best practice
https://github.com/kubernetes/test-infra/pull/35323 will apply the block on PRs with a message redirecting to read CONTRIBUTING.md Each staging repo already has a contributing.md explaining how to contribute. I would still like to get the README updates for these as well. https://github.com/kubernetes/kubernetes/pull/131351,,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/131315#issuecomment-3190043340,repo: kubernetes/kubernetes | issue: Apply best practices to staging repos | keyword: best practice
"> Did we transfer any existing issues to kubernetes? ... I think the answer is no, unfortunately ... OTOH, we didn't have appropriate issue templates there, and they can be filed again here if they are still active. Most repos had relatively few open issues (though that is also because of our closing inactive issues). GitHub offers ""disable issues"", but not ""disable new issues"". I would like to get https://github.com/kubernetes/kubernetes/pull/131351 or a similar PR to add another visible point…",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/131315#issuecomment-3190322060,repo: kubernetes/kubernetes | issue: Apply best practices to staging repos | keyword: best practice
"client-go and kubectl had quite a few, IIRC... pretty much all the ones in client-go were filed there by confused folks and got almost no attention kubectl on the other hand was actively used and encouraged by sig-cli for issue tracking, I think (active discussion as recently as yesterday in https://github.com/kubernetes/kubectl/issues/1769, referenced from https://github.com/kubernetes/kubernetes/pull/133498, I think) Can we turn issues back on for those two repos specifically, and do a sweep …",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/liggitt,https://github.com/kubernetes/kubernetes/issues/131315#issuecomment-3190369814,repo: kubernetes/kubernetes | issue: Apply best practices to staging repos | keyword: best practice
"Confirming: If we re-enable issues on the relevant repos, github should make the old issues available again untouched: https://github.com/kubernetes/org/issues/5545#issuecomment-3190518706 Open question which repos to re-enable ~permanently (kubectl?) and which to sweep and then re-disable (client-go ....) Let's thread that part in https://github.com/kubernetes/org/issues/5545 ---- For PR blockade, I think we're good, but we might consider adding auto-closing. At least now we'll send a clear me…",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/131315#issuecomment-3190520844,repo: kubernetes/kubernetes | issue: Apply best practices to staging repos | keyword: best practice
kubectl and client-go issues are re-enabled https://kubernetes.slack.com/archives/C01672LSZL0/p1755229845308349?thread_ts=1755229561.751699&cid=C01672LSZL0 I have moved all of the issues from client-go.,,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/131315#issuecomment-3190562927,repo: kubernetes/kubernetes | issue: Apply best practices to staging repos | keyword: best practice
"refactor(apis): refactor ComputePodQOS to make it more readable and maintainable <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing…",,,,,,Anecdotal,issue,,,,,,,,2025-01-15,github/googs1025,https://github.com/kubernetes/kubernetes/pull/129640,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-01-15,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/129640#issuecomment-2592666193,repo: kubernetes/kubernetes | issue: refactor(apis): refactor ComputePodQOS to make it more readable and maintainable | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/129640#"" title=""Author self-approved"">googs1025</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [liggitt](https://github.com/liggitt) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted by this …",,,,,,Anecdotal,comment,,,,,,,,2025-01-15,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/129640#issuecomment-2592667255,repo: kubernetes/kubernetes | issue: refactor(apis): refactor ComputePodQOS to make it more readable and maintainable | keyword: best practice
"The Kubernetes project currently lacks enough contributors to adequately respond to all PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Mark this PR as fresh with `/remove-lifecycle stale` - Close this PR with `/close` - Offer to help out with [I…",,,,,,Anecdotal,comment,,,,,,,,2025-04-18,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/129640#issuecomment-2814404376,repo: kubernetes/kubernetes | issue: refactor(apis): refactor ComputePodQOS to make it more readable and maintainable | keyword: best practice
"The Kubernetes project currently lacks enough contributors to adequately respond to all PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Mark this PR as fresh with `/remove-lifecycle stale` - Close this PR with `/close` - Offer to help out with [I…",,,,,,Anecdotal,comment,,,,,,,,2025-07-19,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/129640#issuecomment-3091459780,repo: kubernetes/kubernetes | issue: refactor(apis): refactor ComputePodQOS to make it more readable and maintainable | keyword: best practice
"do-not-merge: validate 133491 for DRA <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted …",,,,,,Anecdotal,issue,,,,,,,,2025-08-14,github/jackfrancis,https://github.com/kubernetes/kubernetes/pull/133530,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.34` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.34.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133530#issuecomment-3188937505,repo: kubernetes/kubernetes | issue: do-not-merge: validate 133491 for DRA | keyword: best practice
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133530#issuecomment-3188937850,repo: kubernetes/kubernetes | issue: do-not-merge: validate 133491 for DRA | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133530#"" title=""Author self-approved"">jackfrancis</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [liggitt](https://github.com/liggitt) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted by thi…",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133530#issuecomment-3188939144,repo: kubernetes/kubernetes | issue: do-not-merge: validate 133491 for DRA | keyword: best practice
> which particular test case flake this PR is trying to repro? After https://github.com/kubernetes/kubernetes/pull/133491 and https://github.com/kubernetes/kubernetes/pull/133321 I hope to repro none of them. :),,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/jackfrancis,https://github.com/kubernetes/kubernetes/pull/133530#issuecomment-3188986818,repo: kubernetes/kubernetes | issue: do-not-merge: validate 133491 for DRA | keyword: best practice
"btw, i'm monitoring https://testgrid.k8s.io/sig-release-1.34-blocking#ci-kubernetes-unit-1-34, there is a single flake on the dashboard from before the fix PR was merged, and the PR should fix that flake.",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/yliaog,https://github.com/kubernetes/kubernetes/pull/133530#issuecomment-3189011310,repo: kubernetes/kubernetes | issue: do-not-merge: validate 133491 for DRA | keyword: best practice
"@jackfrancis: The following test **failed**, say `/retest` to rerun all failed tests or `/retest-required` to rerun all mandatory failed tests: Test name | Commit | Details | Required | Rerun command --- | --- | --- | --- | --- pull-kubernetes-e2e-kind | 84ae2a729f2a2106b3963768293a97cdf75d6b5e | [link](https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/133530/pull-kubernetes-e2e-kind/1956019810475184128) | true | `/test pull-kubernetes-e2e-kind` [Full PR test history](https://prow.k8s…",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133530#issuecomment-3189645240,repo: kubernetes/kubernetes | issue: do-not-merge: validate 133491 for DRA | keyword: best practice
"bugfix: print log after get storage monitors failed <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a rel…",,,,,,Anecdotal,issue,,,,,,,,2025-03-07,github/sxllwx,https://github.com/kubernetes/kubernetes/pull/130629,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"Adding the ""do-not-merge/release-note-label-needed"" label because no release-note block was detected, please follow our [release note process](https://git.k8s.io/community/contributors/guide/release-notes.md) to remove it. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https:…",,,,,,Anecdotal,comment,,,,,,,,2025-03-07,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130629#issuecomment-2705827230,repo: kubernetes/kubernetes | issue: bugfix: print log after get storage monitors failed | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/130629#"" title=""Author self-approved"">sxllwx</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [logicalhan](https://github.com/logicalhan) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted by th…",,,,,,Anecdotal,comment,,,,,,,,2025-03-07,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130629#issuecomment-2705827702,repo: kubernetes/kubernetes | issue: bugfix: print log after get storage monitors failed | keyword: best practice
"The Kubernetes project currently lacks enough contributors to adequately respond to all PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Mark this PR as fresh with `/remove-lifecycle stale` - Close this PR with `/close` - Offer to help out with [I…",,,,,,Anecdotal,comment,,,,,,,,2025-06-15,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130629#issuecomment-2973980535,repo: kubernetes/kubernetes | issue: bugfix: print log after get storage monitors failed | keyword: best practice
"The Kubernetes project currently lacks enough active contributors to adequately respond to all PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Mark this PR as fresh with `/remove-lifecycle rotten` - Close this PR with `/close` - Offer to help out…",,,,,,Anecdotal,comment,,,,,,,,2025-07-15,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130629#issuecomment-3074178978,repo: kubernetes/kubernetes | issue: bugfix: print log after get storage monitors failed | keyword: best practice
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Reopen this PR with `/reopen` - Mark this PR as fresh with `/remove-lifecycle rotten` - Offe…",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130629#issuecomment-3189074391,repo: kubernetes/kubernetes | issue: bugfix: print log after get storage monitors failed | keyword: best practice
"@k8s-triage-robot: Closed this PR. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/130629#issuecomment-3189074391): >The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. > >This bot triages PRs according to the following rules: >- After 90d of inactivity, `lifecycle/stale` is applied >- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied >- After 30d of inactivity since…",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130629#issuecomment-3189074759,repo: kubernetes/kubernetes | issue: bugfix: print log after get storage monitors failed | keyword: best practice
"Add field error properties to blocking error for CEL validation <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if t…",,,,,,Anecdotal,issue,,,,,,,,2025-01-14,github/AlexanderYastrebov,https://github.com/kubernetes/kubernetes/pull/129612,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"Hi @AlexanderYastrebov. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. O…",,,,,,Anecdotal,comment,,,,,,,,2025-01-14,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/129612#issuecomment-2589652613,repo: kubernetes/kubernetes | issue: Add field error properties to blocking error for CEL validation | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/129612#"" title=""Author self-approved"">AlexanderYastrebov</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [jpbetz](https://github.com/jpbetz) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted b…",,,,,,Anecdotal,comment,,,,,,,,2025-01-14,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/129612#issuecomment-2589652716,repo: kubernetes/kubernetes | issue: Add field error properties to blocking error for CEL validation | keyword: best practice
"The Kubernetes project currently lacks enough contributors to adequately respond to all PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Mark this PR as fresh with `/remove-lifecycle stale` - Close this PR with `/close` - Offer to help out with [I…",,,,,,Anecdotal,comment,,,,,,,,2025-06-15,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/129612#issuecomment-2973692240,repo: kubernetes/kubernetes | issue: Add field error properties to blocking error for CEL validation | keyword: best practice
"The Kubernetes project currently lacks enough active contributors to adequately respond to all PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Mark this PR as fresh with `/remove-lifecycle rotten` - Close this PR with `/close` - Offer to help out…",,,,,,Anecdotal,comment,,,,,,,,2025-07-15,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/129612#issuecomment-3073426713,repo: kubernetes/kubernetes | issue: Add field error properties to blocking error for CEL validation | keyword: best practice
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Reopen this PR with `/reopen` - Mark this PR as fresh with `/remove-lifecycle rotten` - Offe…",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/129612#issuecomment-3188447550,repo: kubernetes/kubernetes | issue: Add field error properties to blocking error for CEL validation | keyword: best practice
"@k8s-triage-robot: Closed this PR. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/129612#issuecomment-3188447550): >The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. > >This bot triages PRs according to the following rules: >- After 90d of inactivity, `lifecycle/stale` is applied >- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied >- After 30d of inactivity since…",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/129612#issuecomment-3188447931,repo: kubernetes/kubernetes | issue: Add field error properties to blocking error for CEL validation | keyword: best practice
"fix: enhance websocket fallback to handle CRI incompatibility Previously, WebSocket fallback to SPDY only occurred for HTTP handshake errors (ErrBadHandshake). When CRI-O's WebSocket server terminates during initialization due to WebSocket v5 protocol incompatibility, kubectl debug would hang with ""websocket server finished before becoming ready"" errors instead of falling back to SPDY. <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read…",,,,,,Anecdotal,issue,,,,,,,,2025-08-08,github/sohankunkerkar,https://github.com/kubernetes/kubernetes/pull/133448,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.34` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.34.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133448#issuecomment-3168372907,repo: kubernetes/kubernetes | issue: fix: enhance websocket fallback to handle CRI incompatibility | keyword: best practice
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133448#issuecomment-3168373149,repo: kubernetes/kubernetes | issue: fix: enhance websocket fallback to handle CRI incompatibility | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133448#"" title=""Author self-approved"">sohankunkerkar</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [apelisse](https://github.com/apelisse), [yliaog](https://github.com/yliaog) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process).…",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133448#issuecomment-3168374155,repo: kubernetes/kubernetes | issue: fix: enhance websocket fallback to handle CRI incompatibility | keyword: best practice
It looks like the correct behavior. I have summarize my findings here: https://issues.redhat.com/browse/OCPBUGS-60036?focusedId=27766282&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-27766282 I'm going to close this PR but let me know if we want to handle this request.,,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/sohankunkerkar,https://github.com/kubernetes/kubernetes/pull/133448#issuecomment-3186434843,repo: kubernetes/kubernetes | issue: fix: enhance websocket fallback to handle CRI incompatibility | keyword: best practice
"fix: liveness probes failure does not log event <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a release…",,,,,,Anecdotal,issue,,,,,,,,2025-08-13,github/TP-O,https://github.com/kubernetes/kubernetes/pull/133497,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.34` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.34.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133497#issuecomment-3182139073,repo: kubernetes/kubernetes | issue: fix: liveness probes failure does not log event  | keyword: best practice
"<a href=""https://easycla.lfx.linuxfoundation.org/#/?version=2""><img src=""https://s3.amazonaws.com/cla-project-logo-prod/cla-signed.svg?v=2"" alt=""CLA Signed"" align=""left"" height=""28"" width=""328"" ></a><br/><br />The committers listed above are authorized under a signed CLA.<ul><li>:white_check_mark: login: TP-O / name: Tran Phong (1dd078a81ef74741f00febd7c252f3bee4a44b9d)</li></ul><!-- Date Modified: 2025-08-13 04:46:14.022495 -->",,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/linux-foundation-easycla[bot],https://github.com/kubernetes/kubernetes/pull/133497#issuecomment-3182139126,repo: kubernetes/kubernetes | issue: fix: liveness probes failure does not log event  | keyword: best practice
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133497#issuecomment-3182139192,repo: kubernetes/kubernetes | issue: fix: liveness probes failure does not log event  | keyword: best practice
Welcome @TP-O! <br><br>It looks like this is your first PR to <a href='https://github.com/kubernetes/kubernetes'>kubernetes/kubernetes</a> 🎉. Please refer to our [pull request process documentation](https://www.kubernetes.dev/docs/guide/pull-requests/) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https://go…,,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133497#issuecomment-3182139195,repo: kubernetes/kubernetes | issue: fix: liveness probes failure does not log event  | keyword: best practice
"Hi @TP-O. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once the patch …",,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133497#issuecomment-3182139214,repo: kubernetes/kubernetes | issue: fix: liveness probes failure does not log event  | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133497#"" title=""Author self-approved"">TP-O</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [derekwaynecarr](https://github.com/derekwaynecarr) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted…",,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133497#issuecomment-3182139536,repo: kubernetes/kubernetes | issue: fix: liveness probes failure does not log event  | keyword: best practice
Drastic increase of dimensionality like this requires an issue or even a KEP to discuss all possible imlpications. Scalability and performance needs to be accounted for. Closing for now /close,,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/SergeyKanzhelev,https://github.com/kubernetes/kubernetes/pull/133497#issuecomment-3186231646,repo: kubernetes/kubernetes | issue: fix: liveness probes failure does not log event  | keyword: best practice
@SergeyKanzhelev: Closed this PR. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/133497#issuecomment-3186231646): >Drastic increase of dimensionality like this requires an issue or even a KEP to discuss all possible imlpications. Scalability and performance needs to be accounted for. Closing for now > >/close Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have qu…,,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133497#issuecomment-3186231776,repo: kubernetes/kubernetes | issue: fix: liveness probes failure does not log event  | keyword: best practice
"Don't log irrelevant zone hints message on no endpoints <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a…",,,,,,Anecdotal,issue,,,,,,,,2025-07-02,github/milesbxf,https://github.com/kubernetes/kubernetes/pull/132680,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-07-02,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/132680#issuecomment-3027723566,repo: kubernetes/kubernetes | issue: Don't log irrelevant zone hints message on no endpoints | keyword: best practice
"Hi @milesbxf. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once the pa…",,,,,,Anecdotal,comment,,,,,,,,2025-07-02,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/132680#issuecomment-3027723602,repo: kubernetes/kubernetes | issue: Don't log irrelevant zone hints message on no endpoints | keyword: best practice
This is called from https://github.com/kubernetes/kubernetes/blob/95bff1b249e048b7e36ae857d7478dd2ac05346a/pkg/proxy/topology.go#L44 so I wonder if we want to be also defensive there and return faster if there are no endpoints /assign @danwinship,,,,,,Anecdotal,comment,,,,,,,,2025-07-02,github/aojea,https://github.com/kubernetes/kubernetes/pull/132680#issuecomment-3027990624,repo: kubernetes/kubernetes | issue: Don't log irrelevant zone hints message on no endpoints | keyword: best practice
"Yeah, it would probably be good to just bail out of CategorizeEndpoints early if there aren't any endpoints. But we should still fix `topologyModeFromHints`... Or at least, if it's going to give the wrong answer for 0 endpoints, we should document that, but it seems dumb to document it rather than fixing it.",,,,,,Anecdotal,comment,,,,,,,,2025-07-02,github/danwinship,https://github.com/kubernetes/kubernetes/pull/132680#issuecomment-3028980428,repo: kubernetes/kubernetes | issue: Don't log irrelevant zone hints message on no endpoints | keyword: best practice
"Unit test added (thanks also for the clarification suggestion on the comment) > Yeah, it would probably be good to just bail out of CategorizeEndpoints early if there aren't any endpoints. > > But we should still fix `topologyModeFromHints`... Or at least, if it's going to give the wrong answer for 0 endpoints, we should document that, but it seems dumb to document it rather than fixing it. Happy to add this too - what are the expected return values if there are no endpoints? Would `return nil,…",,,,,,Anecdotal,comment,,,,,,,,2025-07-03,github/milesbxf,https://github.com/kubernetes/kubernetes/pull/132680#issuecomment-3031663099,repo: kubernetes/kubernetes | issue: Don't log irrelevant zone hints message on no endpoints | keyword: best practice
"/ok-to-test please squash the commits , and you can add a separate commit with the `CategorizeEndpoints` changes > what are the expected return values if there are no endpoints? Would return nil, nil, nil, false do the trick (empty endpoints, hasAnyEndpoints=false)? those are named variables so you can just `return`",,,,,,Anecdotal,comment,,,,,,,,2025-07-03,github/aojea,https://github.com/kubernetes/kubernetes/pull/132680#issuecomment-3033400058,repo: kubernetes/kubernetes | issue: Don't log irrelevant zone hints message on no endpoints | keyword: best practice
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/132680#pullrequestreview-2995532453"" title=""Approved"">danwinship</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/132680#"" title=""Author self-approved"">milesbxf</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https:/…",,,,,,Anecdotal,comment,,,,,,,,2025-07-07,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/132680#issuecomment-3046798931,repo: kubernetes/kubernetes | issue: Don't log irrelevant zone hints message on no endpoints | keyword: best practice
It would be great to also have this included in the 1.33.x release because providers like AWS and other charges you for log messages. This issue will burn a lot of money for all customers... we got thousands of messages for this per minute even for small sized clusters...,,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/runningman84,https://github.com/kubernetes/kubernetes/pull/132680#issuecomment-3183457338,repo: kubernetes/kubernetes | issue: Don't log irrelevant zone hints message on no endpoints | keyword: best practice
"Fix crashloopbackoff #133472 <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull requ…",,,,,,Anecdotal,issue,,,,,,,,2025-08-12,github/RajaMuhammadAwais,https://github.com/kubernetes/kubernetes/pull/133483,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"Adding the ""do-not-merge/release-note-label-needed"" label because no release-note block was detected, please follow our [release note process](https://git.k8s.io/community/contributors/guide/release-notes.md) to remove it. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https:…",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133483#issuecomment-3179161401,repo: kubernetes/kubernetes | issue: Fix crashloopbackoff  #133472 | keyword: best practice
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.34` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.34.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133483#issuecomment-3179161453,repo: kubernetes/kubernetes | issue: Fix crashloopbackoff  #133472 | keyword: best practice
"<a href=""https://api.easycla.lfx.linuxfoundation.org/v2/repository-provider/github/sign/18706487/20580498/133483/#/?version=2""><img src=""https://s3.amazonaws.com/cla-project-logo-prod/cla-not-signed.svg?v=2"" alt=""CLA Not Signed"" align=""left"" height=""28"" width=""328"" ><br/><br ><ul><li>:x: login: @rajamuhammadawais1 / The commit (6e00471616ea7679c434eccd8ac75f6df35f537d, e898ee6aeef7c4e778f3beafc973fa2dde016365). This user is authorized, but they must confirm their affiliation with their company.…",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/linux-foundation-easycla[bot],https://github.com/kubernetes/kubernetes/pull/133483#issuecomment-3179161518,repo: kubernetes/kubernetes | issue: Fix crashloopbackoff  #133472 | keyword: best practice
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133483#issuecomment-3179161742,repo: kubernetes/kubernetes | issue: Fix crashloopbackoff  #133472 | keyword: best practice
"Hi @RajaMuhammadAwais. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. On…",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133483#issuecomment-3179161814,repo: kubernetes/kubernetes | issue: Fix crashloopbackoff  #133472 | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133483#"" title=""Author self-approved"">RajaMuhammadAwais</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [johnbelamaric](https://github.com/johnbelamaric) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of comman…",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133483#issuecomment-3179162689,repo: kubernetes/kubernetes | issue: Fix crashloopbackoff  #133472 | keyword: best practice
"there is a markdown file and 2 golang files with main in the root directory. i don't think we will ever land something like this. please work with sig-node, please use `draft` PRs if needed iterate. i'll close this out for now. you can check other PRs to sig-node code to see best practices. please review existing best practices for example in https://github.com/kubernetes/community/tree/master/contributors/devel",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/dims,https://github.com/kubernetes/kubernetes/pull/133483#issuecomment-3179470350,repo: kubernetes/kubernetes | issue: Fix crashloopbackoff  #133472 | keyword: best practice
"Update json-patch to v5.9.10 / v4.0.13 to remove pkg/errors indirect … …dependency <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressi…",,,,,,Anecdotal,issue,,,,,,,,2025-08-12,github/RajaMuhammadAwais,https://github.com/kubernetes/kubernetes/pull/133482,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"Adding the ""do-not-merge/release-note-label-needed"" label because no release-note block was detected, please follow our [release note process](https://git.k8s.io/community/contributors/guide/release-notes.md) to remove it. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https:…",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133482#issuecomment-3178932923,repo: kubernetes/kubernetes | issue: Update json-patch to v5.9.10 / v4.0.13 to remove pkg/errors indirect … | keyword: best practice
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.34` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.34.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133482#issuecomment-3178932977,repo: kubernetes/kubernetes | issue: Update json-patch to v5.9.10 / v4.0.13 to remove pkg/errors indirect … | keyword: best practice
"<a href=""https://api.easycla.lfx.linuxfoundation.org/v2/repository-provider/github/sign/18706487/20580498/133482/#/?version=2""><img src=""https://s3.amazonaws.com/cla-project-logo-prod/cla-not-signed.svg?v=2"" alt=""CLA Not Signed"" align=""left"" height=""28"" width=""328"" ><br/><br ><ul><li>:x: login: @rajamuhammadawais1 / The commit (6e00471616ea7679c434eccd8ac75f6df35f537d). This user is authorized, but they must confirm their affiliation with their company. Start the authorization process <a href='…",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/linux-foundation-easycla[bot],https://github.com/kubernetes/kubernetes/pull/133482#issuecomment-3178933010,repo: kubernetes/kubernetes | issue: Update json-patch to v5.9.10 / v4.0.13 to remove pkg/errors indirect … | keyword: best practice
Welcome @RajaMuhammadAwais! <br><br>It looks like this is your first PR to <a href='https://github.com/kubernetes/kubernetes'>kubernetes/kubernetes</a> 🎉. Please refer to our [pull request process documentation](https://www.kubernetes.dev/docs/guide/pull-requests/) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentatio…,,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133482#issuecomment-3178933233,repo: kubernetes/kubernetes | issue: Update json-patch to v5.9.10 / v4.0.13 to remove pkg/errors indirect … | keyword: best practice
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133482#issuecomment-3178933266,repo: kubernetes/kubernetes | issue: Update json-patch to v5.9.10 / v4.0.13 to remove pkg/errors indirect … | keyword: best practice
"Hi @RajaMuhammadAwais. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. On…",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133482#issuecomment-3178933274,repo: kubernetes/kubernetes | issue: Update json-patch to v5.9.10 / v4.0.13 to remove pkg/errors indirect … | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133482#"" title=""Author self-approved"">RajaMuhammadAwais</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [cblecker](https://github.com/cblecker) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepte…",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133482#issuecomment-3178933676,repo: kubernetes/kubernetes | issue: Update json-patch to v5.9.10 / v4.0.13 to remove pkg/errors indirect … | keyword: best practice
"fix typo <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference o…",,,,,,Anecdotal,issue,,,,,,,,2025-08-10,github/co63oc,https://github.com/kubernetes/kubernetes/pull/133454,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.34` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.34.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-08-10,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133454#issuecomment-3172430246,repo: kubernetes/kubernetes | issue: fix typo | keyword: best practice
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-08-10,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133454#issuecomment-3172430292,repo: kubernetes/kubernetes | issue: fix typo | keyword: best practice
"Hi @co63oc. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once the patc…",,,,,,Anecdotal,comment,,,,,,,,2025-08-10,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133454#issuecomment-3172430305,repo: kubernetes/kubernetes | issue: fix typo | keyword: best practice
"Greetings @co63oc , thanks for your PR! As fast as I know, PRs only related to typos shouldn't be opened, and instead found typos shall be fixed with actual code changes, that fixes bugs. But maybe things are different now? @BenTheElder WDYT?",,,,,,Anecdotal,comment,,,,,,,,2025-08-10,github/PatrickLaabs,https://github.com/kubernetes/kubernetes/pull/133454#issuecomment-3172833978,repo: kubernetes/kubernetes | issue: fix typo | keyword: best practice
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133454#"" title=""Author self-approved"">co63oc</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [dchen1107](https://github.com/dchen1107) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted by this…",,,,,,Anecdotal,comment,,,,,,,,2025-08-10,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133454#issuecomment-3172959114,repo: kubernetes/kubernetes | issue: fix typo | keyword: best practice
"> > As fast as I know, PRs only related to typos shouldn't be opened, and instead found typos shall be fixed with actual code changes, that fixes bugs. > I've updated the PR, modify file cmd/kubelet/app/server_windows.go. This is the information displayed in the command line. I think it needs to be fixed. https://www.kubernetes.dev/docs/guide/first-contribution/ After seeing the contribution guideline document, the content it includes - Help improve the Kubernetes documentation - Clarify code, …",,,,,,Anecdotal,comment,,,,,,,,2025-08-10,github/co63oc,https://github.com/kubernetes/kubernetes/pull/133454#issuecomment-3172960520,repo: kubernetes/kubernetes | issue: fix typo | keyword: best practice
"Allow requesting watch bookmarks for consistent read from informer ### What would you like to be added? It should be possible for a client of the apiserver that is watching a resource type to 'request' a watch bookmark be sent immediately, similar to how etcd allows requesting progress of a watch (https://github.com/etcd-io/etcd/issues/9855). Alternatively, having some kind of configurable 'higher frequency' (than the current default 1 minute) for bookmarks may be sufficient. This configuration…",,,,,,Anecdotal,issue,,,,,,,,2024-09-27,github/munnerz,https://github.com/kubernetes/kubernetes/issues/127693,repo: kubernetes/kubernetes | keyword: lesson learned | state: open
"@munnerz: The label(s) `sig/apimachinery` cannot be applied, because the repository doesn't have them. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/127693#issuecomment-2378470698): >/sig apimachinery Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](http…",,,,,,Anecdotal,comment,,,,,,,,2024-09-27,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/127693#issuecomment-2378470739,repo: kubernetes/kubernetes | issue: Allow requesting watch bookmarks for consistent read from informer | keyword: lesson learned
cc @serathius for SIG etcd visibility I'm also hesitant to expose this aspect of the apiserver/etcd interface to clients.,,,,,,Anecdotal,comment,,,,,,,,2024-09-27,github/jpbetz,https://github.com/kubernetes/kubernetes/issues/127693#issuecomment-2379438209,repo: kubernetes/kubernetes | issue: Allow requesting watch bookmarks for consistent read from informer | keyword: lesson learned
"> Hmm, I'm not sure I'd want to let a client force the API server to opt-into sub-second bookmark requests to etcd. I agree, this is certainly something that could cause an uptick in traffic per type. I think some form of distinct authorisation should be configured to make up for this. But with sufficient control around *who* can subscribe to this kind of progress notification, it doesn't seem too different to a client watching a particular busy API type (which fires sub-second watch events). C…",,,,,,Anecdotal,comment,,,,,,,,2024-09-27,github/munnerz,https://github.com/kubernetes/kubernetes/issues/127693#issuecomment-2379475201,repo: kubernetes/kubernetes | issue: Allow requesting watch bookmarks for consistent read from informer | keyword: lesson learned
"> Hmm, I'm not sure I'd want to let a client force the API server to opt-into sub-second bookmark requests to etcd. +1 to it. But to even consider that, we would need a mechanism that would limit the permission for it (i.e. only cluster admin can define who can request that). But I'm not sure if Jordan/Joe would consider even that enough protection.",,,,,,Anecdotal,comment,,,,,,,,2024-09-30,github/wojtek-t,https://github.com/kubernetes/kubernetes/issues/127693#issuecomment-2382449474,repo: kubernetes/kubernetes | issue: Allow requesting watch bookmarks for consistent read from informer | keyword: lesson learned
"Since I was already working on using watch progress notifications for keeping the watch cache up-to-date (https://github.com/kubernetes/enhancements/issues/2340), I figured it might be handy for the Kubernetes clients too, and wanted to see if there was interest here first. Consistent reads from cache utilized etcd progress notification which is the same idea as bookmarks just with different name. When working on the KEP we discovered many issues with etcd progress notifications, and I think th…",,,,,,Anecdotal,comment,,,,,,,,2024-10-03,github/serathius,https://github.com/kubernetes/kubernetes/issues/127693#issuecomment-2391469122,repo: kubernetes/kubernetes | issue: Allow requesting watch bookmarks for consistent read from informer | keyword: lesson learned
"Absolutely, I'd love to work on improving any issues identified in the current watch notification system. That said, I'm still keen to see how (or if) we can expose this to kube-apiserver clients as well - do some of the issues we've found in etcd's handling of these notifications also impact how we might want to expose this to k8s users (either direction of implementation, or scalability/deliverability guarantees?). Not being guaranteed is something that definitely needs to be investigated as …",,,,,,Anecdotal,comment,,,,,,,,2024-10-07,github/munnerz,https://github.com/kubernetes/kubernetes/issues/127693#issuecomment-2396467417,repo: kubernetes/kubernetes | issue: Allow requesting watch bookmarks for consistent read from informer | keyword: lesson learned
"> /retitle Allow requesting watch bookmarks for consistent read from informer How do you feel about adding this feature? If we did, exposing the bookmarks via the reflector should allow us to extend SharedIndexInformer to actually permit consistent reads, which could be a great way to ensure clients that today utilise live reads to ensure correctness, to reduce the apiserver burden down to that of just periodically sending a bookmark. I think the biggest blocker/question in all this is: * are t…",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/munnerz,https://github.com/kubernetes/kubernetes/issues/127693#issuecomment-3188140377,repo: kubernetes/kubernetes | issue: Allow requesting watch bookmarks for consistent read from informer | keyword: lesson learned
"I believe this would also help with use cases like Karmada's aggregated-apiserver: https://karmada.io/docs/reference/components/karmada-aggregated-apiserver This instantiates a watch cache (from k/apiserver) per remote store/endpoint, however they will presumably be unable to safely handle consistent reads due to the lack of regular bookmarks on the wire.",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/munnerz,https://github.com/kubernetes/kubernetes/issues/127693#issuecomment-3188146686,repo: kubernetes/kubernetes | issue: Allow requesting watch bookmarks for consistent read from informer | keyword: lesson learned
"Or am I getting something wrong in how bookmarking works in the watch cache, requiring the request to actually be instantiated explicitly by a client to function in order to maintain correctness? 👀",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/munnerz,https://github.com/kubernetes/kubernetes/issues/127693#issuecomment-3188150791,repo: kubernetes/kubernetes | issue: Allow requesting watch bookmarks for consistent read from informer | keyword: lesson learned
"> How do you feel about adding this feature? > ... > In my case, I am using a reflector to ultimately feed a proxy with a watch cache, so being able to request that a bookmark be sent every ~100ms The fan out effect of having the API server send bookmark events out to N watch clients in steady state many times per second in an ~idle cluster seems pretty bad to me.",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/liggitt,https://github.com/kubernetes/kubernetes/issues/127693#issuecomment-3188513484,repo: kubernetes/kubernetes | issue: Allow requesting watch bookmarks for consistent read from informer | keyword: lesson learned
"client-go: Add metrics into Informer ### What would you like to be added? The informer should expose metrics about queue/reflector/eventHandler. ### Why is this needed? 1. The informer lacks of metric, it is hard to known how many item in its queue/store. Add metrics for queue/store, it will help developers to find the number of pending deltas. 2. The informer create a RingGrowing pendingNotifications for every eventHandler. This RingGrowing will grow, but never shrink. An informer has some eve…",,,,,,Anecdotal,issue,,,,,,,,2023-11-27,github/chenk008,https://github.com/kubernetes/kubernetes/issues/122067,repo: kubernetes/kubernetes | keyword: lesson learned | state: open
"/sig instrumentation /sig scalability Hi all, adding monitoring metrics for RingGrowing pendingNotification for each eventHandler to track memory usage is very important. If the RingGrowing memory keeps increasing, it may lead to informer data latency issues. We should continue pushing this feature forward. https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/4346-informer-metrics https://github.com/kubernetes/enhancements/issues/4346 @wojtek-t @liggitt @aojea @chenk008…",,,,,,Anecdotal,comment,,,,,,,,2024-11-21,github/xigang,https://github.com/kubernetes/kubernetes/issues/122067#issuecomment-2490204199,repo: kubernetes/kubernetes | issue: client-go: Add metrics into Informer | keyword: lesson learned
"We recently saw this log coming from controller-runtime, which I believe is backed by the standard informer code: `""Unhandled Error"" err=""[sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:108](http://sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:108): Failed to watch *v1.Pod: failed to list *v1.Pod: Get \""https://<snip>/api/v1/namespaces/test-pods/pods?labelSelector=<snip>` I'd ask for informer to expose metrics so we can alert when the informers backing our contr…",,,,,,Anecdotal,comment,,,,,,,,2025-03-11,github/nathanperkins,https://github.com/kubernetes/kubernetes/issues/122067#issuecomment-2715618383,repo: kubernetes/kubernetes | issue: client-go: Add metrics into Informer | keyword: lesson learned
"The informers use client-go and other components like workqueue that already provide a lot of meaningful metrics https://github.com/kubernetes/client-go/blob/master/tools/metrics/metrics.go https://github.com/kubernetes/kubernetes/blob/7dc4af6c8a67271e2146dc9c60d9f4b6adfbab3b/staging/src/k8s.io/client-go/util/workqueue/metrics.go Specifically client-go will show error rate and responses already, have you checked that",,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/aojea,https://github.com/kubernetes/kubernetes/issues/122067#issuecomment-2716004927,repo: kubernetes/kubernetes | issue: client-go: Add metrics into Informer | keyword: lesson learned
"> The informers use client-go and other components like workqueue that already provide a lot of meaningful metrics > > https://github.com/kubernetes/client-go/blob/master/tools/metrics/metrics.go > > https://github.com/kubernetes/kubernetes/blob/7dc4af6c8a67271e2146dc9c60d9f4b6adfbab3b/staging/src/k8s.io/client-go/util/workqueue/metrics.go > > Specifically client-go will show error rate and responses already, have you checked that @aojea We also need metrics for the backlog of Informer `DeltaFI…",,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/xigang,https://github.com/kubernetes/kubernetes/issues/122067#issuecomment-2716096248,repo: kubernetes/kubernetes | issue: client-go: Add metrics into Informer | keyword: lesson learned
"We are using the workqueue metrics, they are helpful for tracking the latency for work items waiting or processing in the queue. My understanding is that when the informer has errors which prevent it from watching objects (like the one posted above), then events are not going to be added to the workqueue and won't be reflected in the workqueue metrics. I don't see the client-go metrics coming from controller-runtime by default. None of the metrics in [metrics/metrics.go](https://github.com/kube…",,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/nathanperkins,https://github.com/kubernetes/kubernetes/issues/122067#issuecomment-2716096934,repo: kubernetes/kubernetes | issue: client-go: Add metrics into Informer | keyword: lesson learned
"@xigang in that issue, the ""Discussion Link"" references this issue so that's why I discussed over here. Maybe we should update the description? I also saw the KEP doc says that discussing specific metrics is out of scope so I thought you didn't want to discuss them in your KEP.",,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/nathanperkins,https://github.com/kubernetes/kubernetes/issues/122067#issuecomment-2716105399,repo: kubernetes/kubernetes | issue: client-go: Add metrics into Informer | keyword: lesson learned
@nathanperkins I've implemented [ KEP 4346 ](https://github.com/kubernetes/kubernetes/pull/129160). Can you review the code?,,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/xigang,https://github.com/kubernetes/kubernetes/issues/122067#issuecomment-2716211553,repo: kubernetes/kubernetes | issue: client-go: Add metrics into Informer | keyword: lesson learned
"@xigang sorry, I'm not an owner or familiar with any of that code so I wouldn't be able to give a great review, other than standard golang readability advice.",,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/nathanperkins,https://github.com/kubernetes/kubernetes/issues/122067#issuecomment-2716217520,repo: kubernetes/kubernetes | issue: client-go: Add metrics into Informer | keyword: lesson learned
"We are conflating multiple things in this issue: - informers are builing on top of client-go and workqueue and tools/cache, both client-go and workqueue has metrics, @xigang has a KEP to expand the informer only metrics https://github.com/kubernetes/kubernetes/pull/129160 - client-go already has metrics for detecting issues with network connections , other thing is people that use it through controller-runtime https://github.com/kubernetes/kubernetes/issues/127739 Metrics are registerd when you…",,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/aojea,https://github.com/kubernetes/kubernetes/issues/122067#issuecomment-2716869316,repo: kubernetes/kubernetes | issue: client-go: Add metrics into Informer | keyword: lesson learned
">client-go already has metrics for detecting issues with network connections My goal is to fire an alert when controller caches haven't synced in a significant amount of time so they are operating on stale data and not enqueuing events. It doesn't look like any of the client-go metrics measure that in a satisfying way. Even the rest client metrics which could expose failed requests are limited to `host`, `verb`, and `code` labels which makes it difficult for me to target my alert on informer st…",,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/nathanperkins,https://github.com/kubernetes/kubernetes/issues/122067#issuecomment-2718602520,repo: kubernetes/kubernetes | issue: client-go: Add metrics into Informer | keyword: lesson learned
"> My goal is to fire an alert when controller caches haven't synced in a significant amount of time so they are operating on stale data and not enqueuing events your controller must not start until the caches are synced, you can signal that externally with a ready flag ...",,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/aojea,https://github.com/kubernetes/kubernetes/issues/122067#issuecomment-2719303555,repo: kubernetes/kubernetes | issue: client-go: Add metrics into Informer | keyword: lesson learned
">your controller must not start until the caches are synced, you can signal that externally with a ready flag ... Yes, but this failure happened after the caches were already synced and the controller was already started. We saw ERROR logs but no metric. The controller continued running and did not crash. We had to provide our own [DefaultWatchErrorHandler](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/cache#Options) so that we can create and increment a metric for it. I suppose log-bas…",,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/nathanperkins,https://github.com/kubernetes/kubernetes/issues/122067#issuecomment-2719319823,repo: kubernetes/kubernetes | issue: client-go: Add metrics into Informer | keyword: lesson learned
"> Yes, but this failure happened after the caches were already synced and the controller was already started. We saw ERROR logs but no metric. T @nathanperkins you are explaining the symptom and coming to a conclusion but you are not explaining the root cause, what do you mean by proper syncing? the informer syncs once and then watches ... if the connection breaks it reconnect ... I'm not able to understand the problem hence I do not understand what metric do you need > caches were already sync…",,,,,,Anecdotal,comment,,,,,,,,2025-03-14,github/aojea,https://github.com/kubernetes/kubernetes/issues/122067#issuecomment-2725113485,repo: kubernetes/kubernetes | issue: client-go: Add metrics into Informer | keyword: lesson learned
">you are explaining the symptom and coming to a conclusion but you are not explaining the root cause In our case, the root cause was a transient issue in either gke-metadata-server or the control plane causing authentication issues and i/o timeout. The root cause doesn't matter though, the part that matters is that the informer can be in a state where the watch is failed and not syncing properly. It is better to alert on the symptom (informer not synced or not watching) than the root cause. The…",,,,,,Anecdotal,comment,,,,,,,,2025-03-14,github/nathanperkins,https://github.com/kubernetes/kubernetes/issues/122067#issuecomment-2725331060,repo: kubernetes/kubernetes | issue: client-go: Add metrics into Informer | keyword: lesson learned
"> The root cause doesn't matter though, the part that matters is that the informer can be in a state where the watch is failed and not syncing properly that is the part we disagree, the root cause is what give us the lesson learned so we don't repeat the same mistake, that is why we do RCAs. so we can say ""oh, if we just had this metric we could have noticed it"" , and then we implement the metric and add alerting and monitoring so we don't repeat the same mistake > Yes, but there are conditions…",,,,,,Anecdotal,comment,,,,,,,,2025-03-14,github/aojea,https://github.com/kubernetes/kubernetes/issues/122067#issuecomment-2725364059,repo: kubernetes/kubernetes | issue: client-go: Add metrics into Informer | keyword: lesson learned
"RCA is definitely useful, that's not what I'm saying. I mean that alerts should not focus on root causes, they should focus on exposing service degradation that needs to be addressed by an engineer. While/after addressing the alert, the engineer will do RCA and that's when having other, more detailed logs and metrics is helpful. If we use alerts that focus on root causes, it's not ideal: - We have to configure a lot more alerts to cover each of the causes. - Causes are often lower level, and ar…",,,,,,Anecdotal,comment,,,,,,,,2025-03-14,github/nathanperkins,https://github.com/kubernetes/kubernetes/issues/122067#issuecomment-2725664544,repo: kubernetes/kubernetes | issue: client-go: Add metrics into Informer | keyword: lesson learned
"kubelet should track tcp_mem stats also along with cpu/ram/disk /kind feature /sig node **What happened**: A program started leaking TCP memory, which filled up the node's TCP stack memory. The network performance on the node degraded and connections to pods running on the node either times out or will hang for a long time. Node's `dmesg` had lines mentioning `TCP: out of memory -- consider tuning tcp_mem` Further reading and investigation reveals that this could happen when TCP stack runs out …",,,,,,Anecdotal,issue,,,,,,,,2018-04-10,github/shahidhk,https://github.com/kubernetes/kubernetes/issues/62334,repo: kubernetes/kubernetes | keyword: lesson learned | state: open
"/cc @thockin in continuation to our discussion at https://twitter.com/thockin/status/973965476173725696, took me some time to fixup the repro steps :smile:",,,,,,Anecdotal,comment,,,,,,,,2018-04-10,github/shahidhk,https://github.com/kubernetes/kubernetes/issues/62334#issuecomment-380071239,repo: kubernetes/kubernetes | issue: kubelet should track tcp_mem stats also along with cpu/ram/disk | keyword: lesson learned
"@cizixs I think it should. Do you know any other parameters which can cause a network failure/degradation, but easy to detect?",,,,,,Anecdotal,comment,,,,,,,,2018-04-16,github/shahidhk,https://github.com/kubernetes/kubernetes/issues/62334#issuecomment-381620877,repo: kubernetes/kubernetes | issue: kubelet should track tcp_mem stats also along with cpu/ram/disk | keyword: lesson learned
"Issues go stale after 90d of inactivity. Mark the issue as fresh with `/remove-lifecycle stale`. Stale issues rot after an additional 30d of inactivity and eventually close. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2018-07-15,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/62334#issuecomment-405097512,repo: kubernetes/kubernetes | issue: kubelet should track tcp_mem stats also along with cpu/ram/disk | keyword: lesson learned
"Stale issues rot after 30d of inactivity. Mark the issue as fresh with `/remove-lifecycle rotten`. Rotten issues close after an additional 30d of inactivity. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /lifecycle rotten",,,,,,Anecdotal,comment,,,,,,,,2018-08-14,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/62334#issuecomment-412918434,repo: kubernetes/kubernetes | issue: kubelet should track tcp_mem stats also along with cpu/ram/disk | keyword: lesson learned
"Issues go stale after 90d of inactivity. Mark the issue as fresh with `/remove-lifecycle stale`. Stale issues rot after an additional 30d of inactivity and eventually close. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2018-12-04,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/62334#issuecomment-444298481,repo: kubernetes/kubernetes | issue: kubelet should track tcp_mem stats also along with cpu/ram/disk | keyword: lesson learned
"Somewhat tangential to this, but more of an informational thing. From the network perspective, what sort of things are namespaced and what isn't? I am currently trying to debug a ""performance"" issue and was starting to focus on the network. From my research it appears settings like tcp_rmem and tpc_wmem (read and right buffers) are namespaced. Meaning you can set those values within a container and they don't affect the host settings. But a setting like tcp_mem (which list the max page allocati…",,,,,,Anecdotal,comment,,,,,,,,2019-03-13,github/cyrus-mc,https://github.com/kubernetes/kubernetes/issues/62334#issuecomment-472577470,repo: kubernetes/kubernetes | issue: kubelet should track tcp_mem stats also along with cpu/ram/disk | keyword: lesson learned
Having same issue on the master node. The resources are not getting deleted as the API server is unable to take new request. Kubelet is showing healthy though. ``` [1667891.052298] TCP: out of memory -- consider tuning tcp_mem [1668316.259318] TCP: out of memory -- consider tuning tcp_mem [1668316.997397] TCP: out of memory -- consider tuning tcp_mem [admin@xx~]$ cat /proc/sys/net/ipv4/tcp_mem 4096 4096 4096 [admin@xx~]$ cat /proc/net/sockstat sockets: used 582 TCP: inuse 259 orphan 0 tw 18 all…,,,,,,Anecdotal,comment,,,,,,,,2019-03-29,github/anjuls,https://github.com/kubernetes/kubernetes/issues/62334#issuecomment-477882117,repo: kubernetes/kubernetes | issue: kubelet should track tcp_mem stats also along with cpu/ram/disk | keyword: lesson learned
"This caught me today, had several `socket hang up` in running applications resulting in HTTP timeouts. Went and did a drain/rolling restart of all nodes to get us back to a happy place. Azure AKS, Kubernetes v1.13.5",,,,,,Anecdotal,comment,,,,,,,,2019-05-21,github/asc-adean,https://github.com/kubernetes/kubernetes/issues/62334#issuecomment-494512227,repo: kubernetes/kubernetes | issue: kubelet should track tcp_mem stats also along with cpu/ram/disk | keyword: lesson learned
"We have run into this issue as well, we had a was leaking open connections which lead to a whole node being unusable and introduced a noisy neighbor problem.",,,,,,Anecdotal,comment,,,,,,,,2019-08-29,github/tanya-borisova,https://github.com/kubernetes/kubernetes/issues/62334#issuecomment-526183952,repo: kubernetes/kubernetes | issue: kubelet should track tcp_mem stats also along with cpu/ram/disk | keyword: lesson learned
"Recently we experienced an interesting production problem. This application was running on multiple AWS EC2 instances behind Elastic Load Balancer. The application was running on GNU/Linux OS, Java 8, Tomcat 8 application server. All of sudden one of the application instances became unresponsive. All other application instances were handling the traffic properly. Whenever the HTTP request was sent to this application instance from the browser, we were getting following response to be printed on…",,,,,,Anecdotal,comment,,,,,,,,2019-09-03,github/suryababy,https://github.com/kubernetes/kubernetes/issues/62334#issuecomment-527410368,repo: kubernetes/kubernetes | issue: kubelet should track tcp_mem stats also along with cpu/ram/disk | keyword: lesson learned
"We've had this problem as well on GKE nodes (Container-Optimized OS). It would be great to see Kubernetes handle this as it can effectively break the network stack of an entire node. Slightly off topic, does anyone have any tips for determining which container/process is leaking the TCP memory? As a quick workaround we have increased the TCP memory but that can't work forever.",,,,,,Anecdotal,comment,,,,,,,,2020-10-08,github/linjmeyer,https://github.com/kubernetes/kubernetes/issues/62334#issuecomment-705587766,repo: kubernetes/kubernetes | issue: kubelet should track tcp_mem stats also along with cpu/ram/disk | keyword: lesson learned
"> CPU/RAM/disk with network is hard to know exactly what parameters you have to monitor, file descriptors, bw, packet drops, .... it can be a long list Also, its bursty nature will create a lot of false positives that will create system unstability To be clear, I acknowledge the problem, saying that I can't find a general solution that is stable and works for all ... this seems only a very specific problem",,,,,,Anecdotal,comment,,,,,,,,2022-08-26,github/aojea,https://github.com/kubernetes/kubernetes/issues/62334#issuecomment-1228175756,repo: kubernetes/kubernetes | issue: kubelet should track tcp_mem stats also along with cpu/ram/disk | keyword: lesson learned
"@linjmeyer > Slightly off topic, does anyone have any tips for determining which container/process is leaking the TCP memory? Have you found a way to identify the guilty pod ?",,,,,,Anecdotal,comment,,,,,,,,2022-09-15,github/utix,https://github.com/kubernetes/kubernetes/issues/62334#issuecomment-1248325897,repo: kubernetes/kubernetes | issue: kubelet should track tcp_mem stats also along with cpu/ram/disk | keyword: lesson learned
"Process of elimination, for us it was Jaeger collectors that were receiving/forwarding tons of trace data. GKE also allows you to increase the TCP memory in node pools, so a quick fix would be to increase tcp_mem if your provider allows for it. There is no good answer to determining the guilty container we just guessed unfortunately :(",,,,,,Anecdotal,comment,,,,,,,,2022-12-30,github/linjmeyer,https://github.com/kubernetes/kubernetes/issues/62334#issuecomment-1367957506,repo: kubernetes/kubernetes | issue: kubelet should track tcp_mem stats also along with cpu/ram/disk | keyword: lesson learned
"@s0uky I use this script on a node with su privileges: ``` find /proc/ -mindepth 1 -maxdepth 1 -name '[1-9]*' | while read -r procpid; do count=`cat $procpid/net/tcp |grep "" 08 ""|wc -l`; echo ""$procpid $count"" ; done 2>/dev/null|grep -v ""^0$"" -B1 |grep -v 0$ ``` This scan all the process on the node and for each search for close wait connections (status 08)",,,,,,Anecdotal,comment,,,,,,,,2022-12-30,github/utix,https://github.com/kubernetes/kubernetes/issues/62334#issuecomment-1367993288,repo: kubernetes/kubernetes | issue: kubelet should track tcp_mem stats also along with cpu/ram/disk | keyword: lesson learned
"Thank you @utix and @linjmeyer, I did workaround with prometheus node exporter. I enabled sysctl collector: ``` --collector.sysctl --collector.sysctl.include=net.ipv4.tcp_mem:min,pressure,max ``` Now I have prometheus metrics and I can monitor rate and of course alert on it. Is impossible to find leaking anything without tracing tools, still better than nothing...",,,,,,Anecdotal,comment,,,,,,,,2023-01-10,github/s0uky,https://github.com/kubernetes/kubernetes/issues/62334#issuecomment-1377208666,repo: kubernetes/kubernetes | issue: kubelet should track tcp_mem stats also along with cpu/ram/disk | keyword: lesson learned
"This issue has not been updated in over 1 year, and should be re-triaged. You can: - Confirm that this issue is still relevant with `/triage accepted` (org members only) - Close this issue with `/close` For more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/ /remove-triage accepted",,,,,,Anecdotal,comment,,,,,,,,2024-01-20,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/62334#issuecomment-1901757504,repo: kubernetes/kubernetes | issue: kubelet should track tcp_mem stats also along with cpu/ram/disk | keyword: lesson learned
"> Workarounds are good, but it's not clear to me if we should be doing more here - anyone who has direct context? If a pod is leaking connections the pod will kill the node, without any alert or monitoring. A pod should not be able to kill the node, or at least we need to monitor it. Happy to give more context if you need it.",,,,,,Anecdotal,comment,,,,,,,,2024-03-14,github/utix,https://github.com/kubernetes/kubernetes/issues/62334#issuecomment-1997884924,repo: kubernetes/kubernetes | issue: kubelet should track tcp_mem stats also along with cpu/ram/disk | keyword: lesson learned
"This is an old issue, which I won't have time to tackle in the near future - any context you can add here, to make it more approachable by some volunteer (could be you!) would help.",,,,,,Anecdotal,comment,,,,,,,,2024-03-14,github/thockin,https://github.com/kubernetes/kubernetes/issues/62334#issuecomment-1998307136,repo: kubernetes/kubernetes | issue: kubelet should track tcp_mem stats also along with cpu/ram/disk | keyword: lesson learned
``` [root@pub-k8stx-mgt-prd-004037-cvm ~]# dmesg -T [Wed May 22 19:08:35 2024] TCP: out of memory -- consider tuning tcp_mem [Wed May 22 19:08:47 2024] TCP: out of memory -- consider tuning tcp_mem [root@pub-k8stx-mgt-prd-004037-cvm ~]# sysctl -a 2>&1 | grep tcp_mem net.ipv4.tcp_mem = 1501206 2001609 3002412 [root@pub-k8stx-mgt-prd-004037-cvm ~]# cat /proc/net/sockstat sockets: used 8165 TCP: inuse 64 orphan 0 tw 1157 alloc 6881 mem 3003353 UDP: inuse 6 mem 2 UDPLITE: inuse 0 RAW: inuse 0 FRAG:…,,,,,,Anecdotal,comment,,,,,,,,2024-05-23,github/kong62,https://github.com/kubernetes/kubernetes/issues/62334#issuecomment-2126048438,repo: kubernetes/kubernetes | issue: kubelet should track tcp_mem stats also along with cpu/ram/disk | keyword: lesson learned
DRA E2E: label tests which need a certain minimum kubelet #### What type of PR is this? /kind cleanup #### What this PR does / why we need it: This is used by n-1 and n-2 version skew jobs to exclude current tests which cannot run with a certain older kubelet release because the tested functionality wasn't present yet. #### Which issue(s) this PR is related to: Related-to: https://github.com/kubernetes/test-infra/pull/34981 #### Special notes for your reviewer: /hold That job update must be mer…,,,,,,Anecdotal,issue,,,,,,,,2025-06-12,github/pohly,https://github.com/kubernetes/kubernetes/pull/132270,repo: kubernetes/kubernetes | keyword: lesson learned | state: closed
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-06-12,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/132270#issuecomment-2967463291,repo: kubernetes/kubernetes | issue: DRA E2E: label tests which need a certain minimum kubelet | keyword: lesson learned
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/132270#"" title=""Author self-approved"">pohly</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process) <details > Needs approval from an approver in each of …",,,,,,Anecdotal,comment,,,,,,,,2025-06-12,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/132270#issuecomment-2967463658,repo: kubernetes/kubernetes | issue: DRA E2E: label tests which need a certain minimum kubelet | keyword: lesson learned
"/test pull-kubernetes-kind-dra-n-1-canary pull-kubernetes-kind-dra-n-2-canary Too soon, the job update had not been rolled out.",,,,,,Anecdotal,comment,,,,,,,,2025-06-12,github/pohly,https://github.com/kubernetes/kubernetes/pull/132270#issuecomment-2967760584,repo: kubernetes/kubernetes | issue: DRA E2E: label tests which need a certain minimum kubelet | keyword: lesson learned
"/hold cancel Worked as intended: in the n-2 scenario, 40 tests can run. In n-1, 42 (now including seamless upgrade). Normal would be 43.",,,,,,Anecdotal,comment,,,,,,,,2025-06-13,github/pohly,https://github.com/kubernetes/kubernetes/pull/132270#issuecomment-2969682730,repo: kubernetes/kubernetes | issue: DRA E2E: label tests which need a certain minimum kubelet | keyword: lesson learned
etcd 3.6 client update #### What type of PR is this? /kind cleanup /hold POC until etcd 3.6 clients are released #### What this PR does / why we need it: Prove etcd 3.6 dependency updates work properly and drop cloud library dependencies #### Which issue(s) this PR fixes: Fixes https://github.com/kubernetes/kubernetes/issues/113366 #### Special notes for your reviewer: ```diff -Direct Dependencies: 184 -Transitive Dependencies: 327 -Total Dependencies: 365 +Direct Dependencies: 182 +Transitive …,,,,,,Anecdotal,issue,,,,,,,,2024-10-29,github/liggitt,https://github.com/kubernetes/kubernetes/pull/128419,repo: kubernetes/kubernetes | keyword: lesson learned | state: closed
"@liggitt: GitHub didn't allow me to request PR reviews from the following users: princepereira. Note that only [kubernetes members](https://github.com/orgs/kubernetes/people) and repo collaborators can review this PR, and authors cannot review their own PRs. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/128419): >#### What type of PR is this? > >/kind cleanup >/hold POC until etcd 3.6 clients are released > >#### What this PR does / why we need it: > >Prove etcd …",,,,,,Anecdotal,comment,,,,,,,,2024-10-29,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2444833469,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2024-10-29,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2444833680,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"The Kubernetes project currently lacks enough contributors to adequately respond to all PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Mark this PR as fresh with `/remove-lifecycle stale` - Close this PR with `/close` - Offer to help out with [I…",,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2717978337,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"unit test failures: * cel compat test is due to golang.org/x/text bump, picked in test fixup commit from https://github.com/kubernetes/kubernetes/pull/130913 to address * k8s.io/apiserver/pkg/storage/cacher ``` {Failed panic: test timed out after 3m0s running tests: TestWatchStreamSeparation (10s) ``` * k8s.io/apiserver/pkg/storage/etcd3 ``` {Failed === RUN TestProgressNotify utils.go:171: time out after waiting 30s on ResultChan logger.go:146: 2025-03-26T15:36:21.056Z ERROR etcd-server setting…",,,,,,Anecdotal,comment,,,,,,,,2025-03-27,github/liggitt,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2758265168,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"Hmm... if I *just* run TestWatchStreamSeparation locally, the test passes: ``` go test -race k8s.io/apiserver/pkg/storage/cacher -run TestWatchStreamSeparation ok k8s.io/apiserver/pkg/storage/cacher 13.683s ``` if I run the whole package locally, it takes just over 3 minutes: ``` go test -race k8s.io/apiserver/pkg/storage/cacher ok k8s.io/apiserver/pkg/storage/cacher 188.628s ``` if I do the same thing on master (with etcd 3.5.x), it runs about 10% faster: ``` go test -race k8s.io/apiserver/pkg…",,,,,,Anecdotal,comment,,,,,,,,2025-03-27,github/liggitt,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2758457402,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"> Did something in the etcd/embed server slow down between 3.5 and 3.6? We fixed a race condition when shutting down the embed etcdserver. It may only slow down the shutdown process a bit. Refer to https://github.com/etcd-io/etcd/pull/19221 and https://github.com/etcd-io/etcd/pull/19139. The fixes were also backported to 3.5.18. I believe K8s master branch is on an old etcd version, so you see the difference.",,,,,,Anecdotal,comment,,,,,,,,2025-03-27,github/ahrtr,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2758515738,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"> Hmm... if I _just_ run TestWatchStreamSeparation locally, the test passes: Do you mean running it alone passes, but running it in workflow fails? Not sure if it's related to https://github.com/etcd-io/etcd/pull/19195#issuecomment-2627486682",,,,,,Anecdotal,comment,,,,,,,,2025-03-27,github/ahrtr,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2758523365,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"for TestProgressNotify: on master: ``` go test -race k8s.io/apiserver/pkg/storage/etcd3 -count 1 -run TestProgressNotify ok k8s.io/apiserver/pkg/storage/etcd3 3.448s ``` on this branch with etcd ~v3.6.0-rc.2: ``` go test -race k8s.io/apiserver/pkg/storage/etcd3 -count 1 -run TestProgressNotify --- FAIL: TestProgressNotify (37.52s) utils.go:171: time out after waiting 30s on ResultChan logger.go:146: 2025-03-27T11:24:54.660-0400 ERROR etcd-server setting up serving from embedded etcd failed. {""e…",,,,,,Anecdotal,comment,,,,,,,,2025-03-27,github/liggitt,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2758548663,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"> Do you mean running it alone passes, but running it in workflow fails? That callout in the CI failure was a red herring... the issue was that the package timed out because it ran slightly slower. I don't think that's an etcd issue, that package hovers at the timeout boundary anyway.",,,,,,Anecdotal,comment,,,,,,,,2025-03-27,github/liggitt,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2758552678,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"> Looks like that test set clusterConfig.ExperimentalWatchProgressNotifyInterval and that is no longer effective... setting clusterConfig.WatchProgressNotifyInterval as well makes the test pass. Was it expected that ExperimentalWatchProgressNotifyInterval is non-functional in 3.6? If so, that's fine, it's easy to also set the non-experimental field. Oh, it's a known issue:) It's a breaking change for embedded use case, sorry. We are aware of it. Refer to https://github.com/etcd-io/etcd/issues/1…",,,,,,Anecdotal,comment,,,,,,,,2025-03-27,github/ahrtr,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2758558060,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"> Was it expected that ExperimentalWatchProgressNotifyInterval is non-functional in 3.6? To be clearer, it only affects the embedded use cases. If you run etcd binnary or image, they are not affected.",,,,,,Anecdotal,comment,,,,,,,,2025-03-27,github/ahrtr,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2758570122,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"Looks like the only failing test is timeout of k8s.io/apiserver/pkg/storage/cacher, due to slightly longer run times changing only the etcd 3.6.0-rc.3 dependency The unit tests in this package have hovered around problematic timeout levels for a while (https://github.com/kubernetes/kubernetes/issues/123685, https://github.com/kubernetes/kubernetes/issues/123850, https://github.com/kubernetes/kubernetes/issues/125688). @serathius, looks like the runtime of that package crept up slightly from 1.3…",,,,,,Anecdotal,comment,,,,,,,,2025-03-27,github/liggitt,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2759141859,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"> @serathius, looks like the runtime of that package crept up slightly from 1.32 → 1.33 Yea, that's my fault. I added a bunch of tests hoping we will fit in timeout. Just confirming, should we fix this after thaw? or we can slip in during test freeze?",,,,,,Anecdotal,comment,,,,,,,,2025-03-28,github/serathius,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2760573006,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"After thaw is fine, just wanted to get it on your radar. If we have a change ready to go that resolves this by then, that would be super helpful to get the 3.6 bump landed first thing in 1.34",,,,,,Anecdotal,comment,,,,,,,,2025-03-28,github/liggitt,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2760802863,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"> that would be super helpful to get the 3.6 bump landed first thing in 1.34 I think we need to land etcd 3.5.20 (or higher patch, 3.5.21 is the latest one for now) into 1.33.1 first, then 3.6.0 to 1.34 later, otherwise the upgrade from 3.5 to 3.6 may fail. cc @neolit123 EDIT: check this out: https://etcd.io/blog/2025/upgrade_from_3.5_to_3.6_issue/",,,,,,Anecdotal,comment,,,,,,,,2025-03-28,github/ahrtr,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2760812060,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"> That shouldn't block merging the 3.6 client into 1.34 though, right? Right, only the etcd image/binary version matters regarding the upgrade issue. Client sdk and image doesn't necessarily to be bound together. Ideally, I'd see landing both 3.5.20+ client & image into 1.33.1 first, and 3.6.0 client & image into 1.34 later.",,,,,,Anecdotal,comment,,,,,,,,2025-03-28,github/ahrtr,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2760844509,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"> Ideally, I'd see landing both 3.5.20+ client & image into 1.33.1 first, and 3.6.0 client & image into 1.34 later. Uh... if there are correctness bugs we want to be *sure* are fixed in the versions used in 1.33.x, we should be trying to land that in 1.33.0, not waiting for 1.33.1. Otherwise, upgrades using kubeadm 1.33.0 to 1.34.x will be risky, right?",,,,,,Anecdotal,comment,,,,,,,,2025-03-28,github/liggitt,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2761246585,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"> if there are correctness bugs we want to be _sure_ are fixed in the versions used in 1.33.x, As mentioned in https://etcd.io/blog/2025/upgrade_from_3.5_to_3.6_issue/, we fixed an upgrade issue (3.5 -> 3.6) in 3.5.20, and the root cause is the membership data maybe inconsistent between v2store (deprecated, but remain source of truth for membership data in 3.5) and v3store (source of truth in 3.6). We also fixed a [Missing delete event on watch](https://github.com/etcd-io/etcd/issues/19179) in …",,,,,,Anecdotal,comment,,,,,,,,2025-03-28,github/ahrtr,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2761301223,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"> Yes, it's correct. I was trying to catch up the code freeze to land 3.5.20 in 1.33, but still late one day as mentioned in [etcd-io/etcd#19557 (comment)](https://github.com/etcd-io/etcd/issues/19557#issuecomment-2739815296). So we should apply for an exception? Yes, I would think so. A bugfix we would consider valid for backport to release-1.33 is also valid to merge during code freeze before release, if we can do so confidently. Especially if it has implications for 1.33 → 1.34 upgrades with…",,,,,,Anecdotal,comment,,,,,,,,2025-03-28,github/liggitt,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2761307809,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"rebased on master, cacher unit tests are passing now with the speedups in https://github.com/kubernetes/kubernetes/pull/131103",,,,,,Anecdotal,comment,,,,,,,,2025-04-01,github/liggitt,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2770323088,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"@liggitt is this PR still WIP (I see ""WIP"" is still in the title)? I think it's ready to merge, since release-1.33 branch has already cut.",,,,,,Anecdotal,comment,,,,,,,,2025-04-09,github/ahrtr,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2789105744,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"@liggitt etcd v3.6.0-rc.5 is just out, could you please bump rc.5 in this PR? FYI. we will also bump rc.5 image as well, if both PRs are gree, then we will go ahead to release the final v3.6.0",,,,,,Anecdotal,comment,,,,,,,,2025-05-08,github/ahrtr,https://github.com/kubernetes/kubernetes/pull/128419#issuecomment-2864101066,repo: kubernetes/kubernetes | issue: etcd 3.6 client update | keyword: lesson learned
"Failure cluster [245f6915...]: Networking Granular Checks: Services should update endpoints: http ### Failure cluster [245f6915451c9a0c4f27](https://go.k8s.io/triage#245f6915451c9a0c4f27) ##### Error text: ``` [FAILED] failed dialing endpoint (recovery), did not find expected responses... Tries 39 Command curl -g -q -s 'http://10.64.3.188:9080/dial?request=hostname&protocol=udp&host=10.0.56.190&port=90&tries=1' retrieved map[netserver-1:{} netserver-2:{}] expected map[netserver-1:{} netserver-2…",,,,,,Anecdotal,issue,,,,,,,,2024-11-07,github/pohly,https://github.com/kubernetes/kubernetes/issues/128655,repo: kubernetes/kubernetes | keyword: lesson learned | state: closed
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2024-11-07,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/128655#issuecomment-2461851479,repo: kubernetes/kubernetes | issue: Failure cluster [245f6915...]: Networking Granular Checks: Services should update endpoints: http | keyword: lesson learned
"ok, these are one my favorites ``` retrieved map[netserver-1:{} netserver-2:{}] expected map[netserver-1:{} netserver-2:{}] ```",,,,,,Anecdotal,comment,,,,,,,,2024-11-07,github/aojea,https://github.com/kubernetes/kubernetes/issues/128655#issuecomment-2462287540,repo: kubernetes/kubernetes | issue: Failure cluster [245f6915...]: Networking Granular Checks: Services should update endpoints: http | keyword: lesson learned
"hmm > I1106 20:18:17.104471 10717 resource.go:231] Unable to fetch nettest-3464/netserver-1/webserver logs: an error on the server (""unknown"") has prevented the request from succeeding (get pods netserver-1) I1106 20:18:17.232755 10717 resource.go:231] Unable to fetch nettest-3464/test-container-pod/webserver logs: an error on the server (""unknown"") has prevented the request from succeeding (get pods test-container-pod) This test uses heavily the `exec` endpoint",,,,,,Anecdotal,comment,,,,,,,,2024-11-07,github/aojea,https://github.com/kubernetes/kubernetes/issues/128655#issuecomment-2462300700,repo: kubernetes/kubernetes | issue: Failure cluster [245f6915...]: Networking Granular Checks: Services should update endpoints: http | keyword: lesson learned
"Checking https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-cos-containerd-e2e-ubuntu-gce/1854253904900919296 API server logs show the error that causes ``` E1106 20:18:17.081598 9 status.go:71] ""Unhandled Error"" err=""apiserver received an error that is not an metav1.Status: &url.Error{Op:\""Get\"", URL:\""https://10.40.0.4:10250/containerLogs/nettest-3464/netserver-1/webserver?tailLines=42\"", Err:(*client.dialFailure)(0xc00e0db900)}: Get \""https://10.40.0.4:10250/containerLogs/nettest-3464/ne…",,,,,,Anecdotal,comment,,,,,,,,2024-11-07,github/aojea,https://github.com/kubernetes/kubernetes/issues/128655#issuecomment-2462381276,repo: kubernetes/kubernetes | issue: Failure cluster [245f6915...]: Networking Granular Checks: Services should update endpoints: http | keyword: lesson learned
"I got it， you are right. I didn't realize error is so important even though the error occurs during collecting debug info. Thanks, I learned. It is a good lesson for me.",,,,,,Anecdotal,comment,,,,,,,,2024-11-07,github/carlory,https://github.com/kubernetes/kubernetes/issues/128655#issuecomment-2462455644,repo: kubernetes/kubernetes | issue: Failure cluster [245f6915...]: Networking Granular Checks: Services should update endpoints: http | keyword: lesson learned
"> I got it， you are right. I didn't realize error is so important even though the error occurs during collecting debug info. Thanks, I learned. It is a good lesson for me. @carlory no worries, this happens when you got hit for this multiple times, something only you learn because you suffered it ... now you know 😄",,,,,,Anecdotal,comment,,,,,,,,2024-11-07,github/aojea,https://github.com/kubernetes/kubernetes/issues/128655#issuecomment-2462702572,repo: kubernetes/kubernetes | issue: Failure cluster [245f6915...]: Networking Granular Checks: Services should update endpoints: http | keyword: lesson learned
"Hi @liggitt @aojea @carlory Appreciate all of your efforts with this! Is the plan still to resolve this for v1.32 ? If so, a gentle reminder that the code freeze has started 02:00 UTC Friday November 8th 2024 . Please make sure any PRs have both lgtm and approved labels ASAP, and file an [Exception](https://github.com/kubernetes/sig-release/blob/master/releases/EXCEPTIONS.md).",,,,,,Anecdotal,comment,,,,,,,,2024-11-08,github/wendy-ha18,https://github.com/kubernetes/kubernetes/issues/128655#issuecomment-2463648914,repo: kubernetes/kubernetes | issue: Failure cluster [245f6915...]: Networking Granular Checks: Services should update endpoints: http | keyword: lesson learned
"@aojea: Closing this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/128655#issuecomment-2466114839): >> I see the #128657 is in the merging queue already. Thank @carlory > >/close > >merged Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://g…",,,,,,Anecdotal,comment,,,,,,,,2024-11-09,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/128655#issuecomment-2466114856,repo: kubernetes/kubernetes | issue: Failure cluster [245f6915...]: Networking Granular Checks: Services should update endpoints: http | keyword: lesson learned
"When a Pod with a PV is moved to another node stuck in ContainerCreating a long time <!-- This form is for bug reports and feature requests ONLY! If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/). --> **Is this a BUG REPORT or FEATURE REQUEST?**: > Uncomment only one, leave it on its own line: > /kind bug > /kind feature **What happe…",,,,,,Anecdotal,issue,,,,,,,,2017-09-26,github/diogo-reis,https://github.com/kubernetes/kubernetes/issues/53059,repo: kubernetes/kubernetes | keyword: lesson learned | state: closed
"@diogo-reis: Reiterating the mentions to trigger a notification: @kubernetes/sig-storage-bugs <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/53059#issuecomment-332257244): >/sig storage >cc @kubernetes/sig-storage-bugs Instructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue…",,,,,,Anecdotal,comment,,,,,,,,2017-09-26,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/53059#issuecomment-332257283,repo: kubernetes/kubernetes | issue: When a Pod with a PV is moved to another node stuck in ContainerCreating a long time | keyword: lesson learned
"k8s version 1.8 .. on AWS: ``` Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 3m (x2 over 3m) default-scheduler PersistentVolumeClaim is not bound: ""www-web-0"" (repeated 9 times) Normal Scheduled 2m default-scheduler Successfully assigned web-0 to ip-10-222-38-161.eu-west-1.compute.internal Normal SuccessfulMountVolume 2m kubelet, ip-10-222-38-161.eu-west-1.compute.internal MountVolume.SetUp succeeded for volume ""default-token-td6qx"" Warning FailedMo…",,,,,,Anecdotal,comment,,,,,,,,2017-10-10,github/airstand,https://github.com/kubernetes/kubernetes/issues/53059#issuecomment-335417276,repo: kubernetes/kubernetes | issue: When a Pod with a PV is moved to another node stuck in ContainerCreating a long time | keyword: lesson learned
I have the same problem on k8s 1.8.2 (OpenStack) how to reproduce is described here https://github.com/kubernetes/kubernetes/issues/50004,,,,,,Anecdotal,comment,,,,,,,,2017-11-08,github/stamak,https://github.com/kubernetes/kubernetes/issues/53059#issuecomment-342823454,repo: kubernetes/kubernetes | issue: When a Pod with a PV is moved to another node stuck in ContainerCreating a long time | keyword: lesson learned
"Got the same error. We modified the resource request/limits for one statefulset with 3 replicas. K8s moved one of the replicas to another node, which has enough resources, but the volume was still attached to the old node. K8s version: v1.8.1+coreos.0 Running on AWS Warning FailedAttachVolume 7m (x2987 over 12m) attachdetach Multi-Attach error for volume ""pvc-4fe430e8-db4d-11e7-9931-02138f142c30"" Volume is already exclusively attached to one node and can't be attached to another",,,,,,Anecdotal,comment,,,,,,,,2017-12-07,github/nexeck,https://github.com/kubernetes/kubernetes/issues/53059#issuecomment-349968113,repo: kubernetes/kubernetes | issue: When a Pod with a PV is moved to another node stuck in ContainerCreating a long time | keyword: lesson learned
"cinder detach is fixed here https://github.com/kubernetes/kubernetes/pull/56846 However, there is new ticket https://github.com/kubernetes/kubernetes/issues/58079 about pod failover times. This pod with pv failover times is same problem in all cloud providers not just openstack. @nexeck AWS detach things are fixed in https://github.com/kubernetes/kubernetes/pull/55893 looks like its coming to kube 1.9",,,,,,Anecdotal,comment,,,,,,,,2018-01-11,github/zetaab,https://github.com/kubernetes/kubernetes/issues/53059#issuecomment-356925084,repo: kubernetes/kubernetes | issue: When a Pod with a PV is moved to another node stuck in ContainerCreating a long time | keyword: lesson learned
"@diogo-reis, sorry for the late reply. You mentioned you ""When i move a Pod with the expression ""nodeSelector:"" to another Node "". Could you please confirm that the pod is first killed from node 1 and then started on node 2. Is node 1 still running?",,,,,,Anecdotal,comment,,,,,,,,2018-01-12,github/jingxu97,https://github.com/kubernetes/kubernetes/issues/53059#issuecomment-357375713,repo: kubernetes/kubernetes | issue: When a Pod with a PV is moved to another node stuck in ContainerCreating a long time | keyword: lesson learned
"We got this ""Multi-Attach error"" also on Azure, in v1.9.6, we found volume in node.volumesInUse is not removed even after pod with that volume has already been moved from the node for a very long time, I filed another issue here: https://github.com/kubernetes/kubernetes/issues/62282",,,,,,Anecdotal,comment,,,,,,,,2018-04-11,github/andyzhangx,https://github.com/kubernetes/kubernetes/issues/53059#issuecomment-380417679,repo: kubernetes/kubernetes | issue: When a Pod with a PV is moved to another node stuck in ContainerCreating a long time | keyword: lesson learned
"hi @SuperMarioo, this issue `""Multi-Attach error"" on Azure` is fixed in v1.9.7, PR: https://github.com/kubernetes/kubernetes/pull/62467 Pls follow below link to mitigate: https://github.com/andyzhangx/demo/blob/master/issues/azuredisk-issues.md#5-azure-disk-pvc-multi-attach-error-makes-disk-mount-very-slow-or-mount-failure-forever",,,,,,Anecdotal,comment,,,,,,,,2018-04-25,github/andyzhangx,https://github.com/kubernetes/kubernetes/issues/53059#issuecomment-384258767,repo: kubernetes/kubernetes | issue: When a Pod with a PV is moved to another node stuck in ContainerCreating a long time | keyword: lesson learned
"imo this ""bug"" exist in all volumetypes. If you have pod with pvc(any type, RWX types excluded) running in node1. You will shutdown that node1 -> the pod will start again in some another node but failovering(it will return that multi-attach error) volumes takes 6-10minutes because it will wait force detach. Options: 1) I am thinking could we make this force detach time faster, it is currently 6 minutes. 2) Allow force detach for volume if node has shutdown taint (which was added in #60009, no c…",,,,,,Anecdotal,comment,,,,,,,,2018-04-26,github/zetaab,https://github.com/kubernetes/kubernetes/issues/53059#issuecomment-384691488,repo: kubernetes/kubernetes | issue: When a Pod with a PV is moved to another node stuck in ContainerCreating a long time | keyword: lesson learned
"@zetaab that's correct, on Azure, time cost of disk detach and attach to another node would be around 1 min, so `Multi-Attach error` within that 1 min is expected, while we found an issue specific in containerized kubelet that UnmountDevice process always fail which lead to disk detach on one node never succeeded on one node, in that case, we hit `Multi-Attach error` for hours...",,,,,,Anecdotal,comment,,,,,,,,2018-04-27,github/andyzhangx,https://github.com/kubernetes/kubernetes/issues/53059#issuecomment-384883417,repo: kubernetes/kubernetes | issue: When a Pod with a PV is moved to another node stuck in ContainerCreating a long time | keyword: lesson learned
"This's not a bug, this's expected behavior. You must not double mount with ReadWriteOnce policy; this's what Kubernetes is trying to avoid. However if the node which's down does not respond within 6 minutes (default), the volumes will be forced associated to the replacement pod. [Reference](https://github.com/kubernetes/kubernetes/issues/50004) to this 'knowledge'. Unfortunately in case of iscsi, the replacement pod always remains in ContainerCreating state which's problematic.",,,,,,Anecdotal,comment,,,,,,,,2018-05-04,github/dElogics,https://github.com/kubernetes/kubernetes/issues/53059#issuecomment-386567757,repo: kubernetes/kubernetes | issue: When a Pod with a PV is moved to another node stuck in ContainerCreating a long time | keyword: lesson learned
@dElogics I think that there is a subproblem in AWS about provisiong EBS volumes and switching nodes. If it is provisioned as /dev/ and then reattached to be exposed on /dev/nvme nodes like M5. I think it will fail also if it is attached correctly to the node.,,,,,,Anecdotal,comment,,,,,,,,2018-05-04,github/bhack,https://github.com/kubernetes/kubernetes/issues/53059#issuecomment-386569599,repo: kubernetes/kubernetes | issue: When a Pod with a PV is moved to another node stuck in ContainerCreating a long time | keyword: lesson learned
"@AmazingTurtle can you describe in more detail what happened to the old node to cause your pod to be rescheduled? Did the node become NotReady, or was it upgraded, terminated or repaired?",,,,,,Anecdotal,comment,,,,,,,,2018-06-20,github/msau42,https://github.com/kubernetes/kubernetes/issues/53059#issuecomment-398815744,repo: kubernetes/kubernetes | issue: When a Pod with a PV is moved to another node stuck in ContainerCreating a long time | keyword: lesson learned
"In my case it became NotReady when I stopped the Docker service. I'm using Rancher 2.0 so everything (including kubelet) is containerized. It runs on 3 bare-metal Ubuntu 16.04 nodes, latest Docker CE and Kubernetes 1.11.1. I have a Deployment with 1 replica that mounts a Ceph RBD PVC in RWO mode. As long as the node is NotReady, the volume is not detached from it, so it cannot be mounted by another pod on different node.",,,,,,Anecdotal,comment,,,,,,,,2018-07-25,github/adampl,https://github.com/kubernetes/kubernetes/issues/53059#issuecomment-407592260,repo: kubernetes/kubernetes | issue: When a Pod with a PV is moved to another node stuck in ContainerCreating a long time | keyword: lesson learned
I've got the same issue with k8s 1.11.0 and ceph using dynamic provisioning. This issue also occures when I do a `kubectl apply -f deployment.yml` As such it's not possible to modify something without redeploying using delete/apply... :( (For me it took much longer than 6min),,,,,,Anecdotal,comment,,,,,,,,2018-08-07,github/eBeyond,https://github.com/kubernetes/kubernetes/issues/53059#issuecomment-411134499,repo: kubernetes/kubernetes | issue: When a Pod with a PV is moved to another node stuck in ContainerCreating a long time | keyword: lesson learned
We are facing the same issue with k8s 1.9.8 and rbd volumes. But in our case the pod was just redeployed on another node due to changes via`kubectl edit deployment ...`,,,,,,Anecdotal,comment,,,,,,,,2018-09-28,github/dakleine,https://github.com/kubernetes/kubernetes/issues/53059#issuecomment-425376627,repo: kubernetes/kubernetes | issue: When a Pod with a PV is moved to another node stuck in ContainerCreating a long time | keyword: lesson learned
@dakleine could you please provide what changes you made when edit deployment? What is the status of the old node? Thanks!,,,,,,Anecdotal,comment,,,,,,,,2018-09-28,github/jingxu97,https://github.com/kubernetes/kubernetes/issues/53059#issuecomment-425570127,repo: kubernetes/kubernetes | issue: When a Pod with a PV is moved to another node stuck in ContainerCreating a long time | keyword: lesson learned
"This happening to me on AWS as well. I have a pod right now that has been stuck in ""ContainerCreating"" for 15 minutes. Any ideas what to do?",,,,,,Anecdotal,comment,,,,,,,,2018-10-17,github/christensen143,https://github.com/kubernetes/kubernetes/issues/53059#issuecomment-430753003,repo: kubernetes/kubernetes | issue: When a Pod with a PV is moved to another node stuck in ContainerCreating a long time | keyword: lesson learned
"`Warning FailedMount 7s (x7 over 13m) kubelet, ip-x-x-x-x.x-x-x.compute.internal Unable to mount volumes for pod ""gitlab02-runner-678ffc74f4-m2w8m_build(7a473b7f-d23e-11e8-8cfe-0688ae24c2fe)"": timeout expired waiting for volumes to attach/mount for pod ""build""/""gitlab02-runner-678ffc74f4-m2w8m"". list of unattached/unmounted volumes=[data-volume]`",,,,,,Anecdotal,comment,,,,,,,,2018-10-17,github/christensen143,https://github.com/kubernetes/kubernetes/issues/53059#issuecomment-430753751,repo: kubernetes/kubernetes | issue: When a Pod with a PV is moved to another node stuck in ContainerCreating a long time | keyword: lesson learned
DRA: remove support for v1alpha2 kubelet gRPC API #### What type of PR is this? /kind cleanup /deprecation #### What this PR does / why we need it: Code simplification. #### Special notes for your reviewer: The v1alpha2 API is several releases old. No current drivers should still depend on it. #### Does this PR introduce a user-facing change? ```release-note ACTION-REQUIRED: DRA drivers using the v1alpha2 kubelet gRPC API are no longer supported and need to be updated. ``` #### Additional docum…,,,,,,Anecdotal,issue,,,,,,,,2024-04-15,github/pohly,https://github.com/kubernetes/kubernetes/pull/124316,repo: kubernetes/kubernetes | keyword: lesson learned | state: closed
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.30` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.30.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2024-04-15,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/124316#issuecomment-2056493194,repo: kubernetes/kubernetes | issue: DRA: remove support for v1alpha2 kubelet gRPC API | keyword: lesson learned
"@kannon92: You must be a member of the [kubernetes/milestone-maintainers](https://github.com/orgs/kubernetes/teams/milestone-maintainers/members) GitHub team to set the milestone. If you believe you should be able to issue the /milestone command, please contact your Milestone Maintainers Team and have them propose you as an additional delegate for this responsibility. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/124316#issuecomment-2056743662): >/milestone 1.31 …",,,,,,Anecdotal,comment,,,,,,,,2024-04-15,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/124316#issuecomment-2056743736,repo: kubernetes/kubernetes | issue: DRA: remove support for v1alpha2 kubelet gRPC API | keyword: lesson learned
> The v1alpha2 API is several releases old. No current drivers should still depend on it. Open source claims like this seem to be very difficult to prove. Can you share how you found this out? Did we mark v1alpha2 as deprecated?,,,,,,Anecdotal,comment,,,,,,,,2024-04-15,github/kannon92,https://github.com/kubernetes/kubernetes/pull/124316#issuecomment-2056747054,repo: kubernetes/kubernetes | issue: DRA: remove support for v1alpha2 kubelet gRPC API | keyword: lesson learned
"> Open source claims like this seem to be very difficult to prove. This is ""should"" as in ""must not"", not as in ""we believe it doesn't"".... > Did we mark v1alpha2 as deprecated? There is no need to mark alpha APIs as deprecated. They can get removed or modified from one release to the next without prior warning.",,,,,,Anecdotal,comment,,,,,,,,2024-04-15,github/pohly,https://github.com/kubernetes/kubernetes/pull/124316#issuecomment-2056764741,repo: kubernetes/kubernetes | issue: DRA: remove support for v1alpha2 kubelet gRPC API | keyword: lesson learned
"> Open source claims like this seem to be very difficult to prove. Can you share how you found this out? Did we mark v1alpha2 as deprecated? Traditionally, k8s has required one to ensure both forward-and-backwards compatibility even with `alpha` APIs. When designing the `v1alpha2` API for DRA we pointed out that this requirement is highly restrictive given that the whole point of `alpha` is to be able to iterate quickly and remove code that turned out to be a bad idea across API versions. For t…",,,,,,Anecdotal,comment,,,,,,,,2024-04-15,github/klueska,https://github.com/kubernetes/kubernetes/pull/124316#issuecomment-2056766956,repo: kubernetes/kubernetes | issue: DRA: remove support for v1alpha2 kubelet gRPC API | keyword: lesson learned
"A couple more comments for context, referencing https://kubernetes.io/docs/reference/using-api/deprecation-policy/ _If_ an alpha is kept in place, encoding/decoding of that version and conversion to/from the next alpha or beta must work. > API objects must be able to round-trip between API versions in a given release without information loss We wouldn't make a change even to an alpha type that would break decoding of data returned in that version from a previous server, we'd rev the alpha to al…",,,,,,Anecdotal,comment,,,,,,,,2024-04-15,github/liggitt,https://github.com/kubernetes/kubernetes/pull/124316#issuecomment-2056942757,repo: kubernetes/kubernetes | issue: DRA: remove support for v1alpha2 kubelet gRPC API | keyword: lesson learned
"> If an alpha is kept in place, encoding/decoding of that version and conversion to/from the next alpha or beta must work. Note that this is a gRPC API, not an API served by the apiserver. Therefore this part isn't relevant here. > When the lessons learned from one alpha result in significant changes to the next alpha that make conversion complicated / expensive / difficult / impossible, then I'm a big +1 on dropping the first alpha entirely. We decided to keep v1alpha2 when adding v1alpha3 bec…",,,,,,Anecdotal,comment,,,,,,,,2024-04-15,github/pohly,https://github.com/kubernetes/kubernetes/pull/124316#issuecomment-2057128675,repo: kubernetes/kubernetes | issue: DRA: remove support for v1alpha2 kubelet gRPC API | keyword: lesson learned
"retesting as the job has a ""Context retired without replacement."" status. Should be ""Job succeeded"" /test pull-kubernetes-node-e2e-crio-dra",,,,,,Anecdotal,comment,,,,,,,,2024-04-17,github/bart0sh,https://github.com/kubernetes/kubernetes/pull/124316#issuecomment-2060702813,repo: kubernetes/kubernetes | issue: DRA: remove support for v1alpha2 kubelet gRPC API | keyword: lesson learned
@bart0sh: The specified target(s) for `/test` were not found. The following commands are available to trigger required jobs: * `/test pull-cadvisor-e2e-kubernetes` * `/test pull-cos-containerd-e2e-ubuntu-gce` * `/test pull-kubernetes-conformance-kind-ga-only-parallel` * `/test pull-kubernetes-coverage-unit` * `/test pull-kubernetes-dependencies` * `/test pull-kubernetes-dependencies-go-canary` * `/test pull-kubernetes-e2e-gce` * `/test pull-kubernetes-e2e-gce-100-performance` * `/test pull-kube…,,,,,,Anecdotal,comment,,,,,,,,2024-04-17,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/124316#issuecomment-2060702895,repo: kubernetes/kubernetes | issue: DRA: remove support for v1alpha2 kubelet gRPC API | keyword: lesson learned
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/124316#issuecomment-2066512222"" title=""Approved"">klueska</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/124316#"" title=""Author self-approved"">pohly</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https://git.k8s.io…",,,,,,Anecdotal,comment,,,,,,,,2024-04-19,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/124316#issuecomment-2066512903,repo: kubernetes/kubernetes | issue: DRA: remove support for v1alpha2 kubelet gRPC API | keyword: lesson learned
"Field `status.hostIPs` added for Pod <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted p…",,,,,,Anecdotal,issue,,,,,,,,2022-04-22,github/wzshiming,https://github.com/kubernetes/kubernetes/pull/109616,repo: kubernetes/kubernetes | keyword: lesson learned | state: closed
Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.24` branch. This means every merged PR has to be cherry-picked into the release branch to be part of the upcoming v1.24.0 release.,,,,,,Anecdotal,comment,,,,,,,,2022-04-22,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/109616#issuecomment-1106365000,repo: kubernetes/kubernetes | issue: Field `status.hostIPs` added for Pod | keyword: lesson learned
"Types conversion https://github.com/kubernetes/kubernetes/pull/101566/commits/8e347718e3b91c9f1cd2bcb83ed70c8c6ea434a3 Add dropDisabledStatusFields https://github.com/kubernetes/kubernetes/pull/101566/commits/b3a82c6a5ea2c3297b5d84ed215c967be6f0b7b3 Add HostIPs by Kubelet https://github.com/kubernetes/kubernetes/pull/101566/commits/ac9d7203da982bbf257e2131cbcf07479445185a I think the reason why the original load is too large is that, it should be that when Feature is disabled, the Conversion se…",,,,,,Anecdotal,comment,,,,,,,,2022-04-22,github/wzshiming,https://github.com/kubernetes/kubernetes/pull/109616#issuecomment-1106441758,repo: kubernetes/kubernetes | issue: Field `status.hostIPs` added for Pod | keyword: lesson learned
is there a specific delta that shows the differences between this PR and the original? are there added tests that would have caught the issues with the previous PR?,,,,,,Anecdotal,comment,,,,,,,,2022-04-25,github/liggitt,https://github.com/kubernetes/kubernetes/pull/109616#issuecomment-1108564992,repo: kubernetes/kubernetes | issue: Field `status.hostIPs` added for Pod | keyword: lesson learned
"> is there a specific delta that shows the differences between this PR and the original? are there added tests that would have caught the issues with the previous PR? The difference is that the commit https://github.com/kubernetes/kubernetes/commit/8e347718e3b91c9f1cd2bcb83ed70c8c6ea434a3 is removed, and added some tests when the Feature is disabled to ensure that the HostIPs field does not work.",,,,,,Anecdotal,comment,,,,,,,,2022-04-27,github/wzshiming,https://github.com/kubernetes/kubernetes/pull/109616#issuecomment-1110705504,repo: kubernetes/kubernetes | issue: Field `status.hostIPs` added for Pod | keyword: lesson learned
"https://github.com/kubernetes/kubernetes/blob/d2cea9475bb7d2c2a1fa516ddce4af4f0cf3a4ef/pkg/kubelet/container/helpers.go#L334-L371 @aojea Should kubelet support the hostPort for two HostIPs? This is not mentioned in the KEP. Of course, this is isolated and can be supported in a new PR after this is merged.",,,,,,Anecdotal,comment,,,,,,,,2022-07-01,github/pacoxu,https://github.com/kubernetes/kubernetes/pull/109616#issuecomment-1171890099,repo: kubernetes/kubernetes | issue: Field `status.hostIPs` added for Pod | keyword: lesson learned
pull-kubernetes-e2e-gce-cos-alpha-features failing all runs on this PR on mostly network tests when the job is generally green looks suspicious... worth digging into why that is failing,,,,,,Anecdotal,comment,,,,,,,,2022-12-23,github/liggitt,https://github.com/kubernetes/kubernetes/pull/109616#issuecomment-1364357898,repo: kubernetes/kubernetes | issue: Field `status.hostIPs` added for Pod | keyword: lesson learned
"> pull-kubernetes-e2e-gce-cos-alpha-features failing all runs on this PR on mostly network tests when the job is generally green looks suspicious... worth digging into why that is failing https://storage.googleapis.com/kubernetes-jenkins/pr-logs/pull/109616/pull-kubernetes-e2e-gce-cos-alpha-features/1607060890820022272/artifacts/e2e-81843c4066-92f64-master/kube-apiserver.log ``` console E1225 17:47:50.611259 9 fieldmanager.go:210] ""[SHOULD NOT HAPPEN] failed to update managedFields"" err=""failed…",,,,,,Anecdotal,comment,,,,,,,,2022-12-26,github/wzshiming,https://github.com/kubernetes/kubernetes/pull/109616#issuecomment-1364892630,repo: kubernetes/kubernetes | issue: Field `status.hostIPs` added for Pod | keyword: lesson learned
"> It seems to be caused by `// +listType=set`, I removed it and added `API rule violation` and the test passed. please don't add new violations to the allowlist... `listType=atomic` would work as well (and matches the default if listType is unspecified) and not require adding that that would mean two different actors could not own individual items in the hostIPs list via server-side-apply, which seems correct... I would expect a single actor to be controlling all values in that field, right?",,,,,,Anecdotal,comment,,,,,,,,2023-01-03,github/liggitt,https://github.com/kubernetes/kubernetes/pull/109616#issuecomment-1369812564,repo: kubernetes/kubernetes | issue: Field `status.hostIPs` added for Pod | keyword: lesson learned
"> E1225 17:47:50.611259 9 fieldmanager.go:210] ""[SHOULD NOT HAPPEN] failed to update managedFields"" err=""failed to convert new object (non-graceful-shutdown-6830/sts-pod-gcepd-d969d9679-5jkzs; /v1, Kind=Pod) to smd typed: .status.hostIPs: element 0: associative list without keys has an element that's a map type"" VersionKind=""/, Kind="" namespace=""non-graceful-shutdown-6830"" name=""sts-pod-gcepd-d969d9679-5jkzs"" @apelisse - shouldn't a linter have caught this at openapi-generate-time, not at runti…",,,,,,Anecdotal,comment,,,,,,,,2023-01-03,github/liggitt,https://github.com/kubernetes/kubernetes/pull/109616#issuecomment-1369814909,repo: kubernetes/kubernetes | issue: Field `status.hostIPs` added for Pod | keyword: lesson learned
"Yes, it's exactly the same class of error that we had last month, tracked by https://github.com/kubernetes/kube-openapi/issues/343",,,,,,Anecdotal,comment,,,,,,,,2023-01-03,github/apelisse,https://github.com/kubernetes/kubernetes/pull/109616#issuecomment-1369991369,repo: kubernetes/kubernetes | issue: Field `status.hostIPs` added for Pod | keyword: lesson learned
"The advantage is that they ""feel"" similar and should probably be implemented the same way. Disagree? The disadvantage is that it's a little more complicated - attaching to the conversion logic On Sat, Jan 14, 2023 at 6:39 AM Shiming Zhang ***@***.***> wrote: > ***@***.**** commented on this pull request. > ------------------------------ > > In pkg/apis/core/validation/validation.go > <https://github.com/kubernetes/kubernetes/pull/109616#discussion_r1070291050> > : > > > @@ -3736,6 +3752,58 @@ f…",,,,,,Anecdotal,comment,,,,,,,,2023-01-15,github/thockin,https://github.com/kubernetes/kubernetes/pull/109616#issuecomment-1383290113,repo: kubernetes/kubernetes | issue: Field `status.hostIPs` added for Pod | keyword: lesson learned
"Aggregated discovery types #### What type of PR is this? /kind feature #### What this PR does / why we need it: Add types for aggregated discovery #### Which issue(s) this PR fixes: <!-- *Automatically closes linked issue when PR is merged. Usage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`. _If PR is about `failing-tests or flakes`, please post the related issues/tests in a comment and do not use `Fixes`_* --> Fixes # #### Special notes for your reviewer: #### Does this PR introd…",,,,,,Anecdotal,issue,,,,,,,,2022-08-23,github/Jefftree,https://github.com/kubernetes/kubernetes/pull/111978,repo: kubernetes/kubernetes | keyword: lesson learned | state: closed
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.25` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.25.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2022-08-23,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/111978#issuecomment-1224754965,repo: kubernetes/kubernetes | issue: Aggregated discovery types | keyword: lesson learned
"This PR [may require API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#what-apis-need-to-be-reviewed). If so, when the changes are ready, [complete the pre-review checklist and request an API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#mechanics). Status of requested reviews is tracked in the [API Review project](https://github.com/orgs/kubernetes/projects/13).",,,,,,Anecdotal,comment,,,,,,,,2022-08-23,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/111978#issuecomment-1224876482,repo: kubernetes/kubernetes | issue: Aggregated discovery types | keyword: lesson learned
"/hold Brief hold, i want to review for https://github.com/kubernetes/enhancements/pull/3364#discussion_r968650702 before this lands. Reviewing now.",,,,,,Anecdotal,comment,,,,,,,,2022-09-12,github/smarterclayton,https://github.com/kubernetes/kubernetes/pull/111978#issuecomment-1244019638,repo: kubernetes/kubernetes | issue: Aggregated discovery types | keyword: lesson learned
There was a drift between the internal/external version godocs earlier which may have caused some comments to not be addressed in one version. That has been fixed.,,,,,,Anecdotal,comment,,,,,,,,2022-10-18,github/Jefftree,https://github.com/kubernetes/kubernetes/pull/111978#issuecomment-1283009782,repo: kubernetes/kubernetes | issue: Aggregated discovery types | keyword: lesson learned
"/test pull-kubernetes-e2e-kind-ipv6 ``` Oct 18 22:35:13.426: INFO: Unexpected error: <*errors.errorString | 0xc000293d00>: { s: ""timed out waiting for the condition"", } Oct 18 22:35:13.426: FAIL: timed out waiting for the condition ```",,,,,,Anecdotal,comment,,,,,,,,2022-10-18,github/Jefftree,https://github.com/kubernetes/kubernetes/pull/111978#issuecomment-1283095811,repo: kubernetes/kubernetes | issue: Aggregated discovery types | keyword: lesson learned
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/111978#"" title=""Author self-approved"">Jefftree</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/111978#issuecomment-1284440025"" title=""Approved"">liggitt</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/111978#issuecomment-1284414234"" title=""Approved"">smarterclayton</a>* The full list of commands accepted by this bot can be found [he…",,,,,,Anecdotal,comment,,,,,,,,2022-10-19,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/111978#issuecomment-1284440642,repo: kubernetes/kubernetes | issue: Aggregated discovery types | keyword: lesson learned
"Aggregated Discovery Endpoint #### What type of PR is this? /kind feature #### What this PR does / why we need it: Details in KEP: https://github.com/kubernetes/enhancements/issues/3352 #### Which issue(s) this PR fixes: <!-- *Automatically closes linked issue when PR is merged. Usage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`. _If PR is about `failing-tests or flakes`, please post the related issues/tests in a comment and do not use `Fixes`_* --> Fixes # #### Special notes for …",,,,,,Anecdotal,issue,,,,,,,,2022-07-25,github/alexzielenski,https://github.com/kubernetes/kubernetes/pull/111409,repo: kubernetes/kubernetes | keyword: lesson learned | state: closed
"This PR [may require API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#what-apis-need-to-be-reviewed). If so, when the changes are ready, [complete the pre-review checklist and request an API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#mechanics). Status of requested reviews is tracked in the [API Review project](https://github.com/orgs/kubernetes/projects/13).",,,,,,Anecdotal,comment,,,,,,,,2022-07-27,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/111409#issuecomment-1197400818,repo: kubernetes/kubernetes | issue: Aggregated Discovery Endpoint | keyword: lesson learned
"I think we should refactor the API to reflect the lessons learned from our first discovery document. Details like separation of GVR from GVK and the handling of subresources should be first class concepts this time. Rather than try to determine that new API in the last two days before freeze, I think we should continue shaping the API with the plan of merging it early in 1.26 instead. Given that the PR was only opened three days ago, I think the timing on this was always very tight. @lavalamp s…",,,,,,Anecdotal,comment,,,,,,,,2022-07-28,github/deads2k,https://github.com/kubernetes/kubernetes/pull/111409#issuecomment-1198536637,repo: kubernetes/kubernetes | issue: Aggregated Discovery Endpoint | keyword: lesson learned
"> I think we should refactor the API to reflect the lessons learned from our first discovery document. Details like separation of GVR from GVK and the handling of subresources should be first class concepts this time. Rather than try to determine that new API in the last two days before freeze, I think we should continue shaping the API with the plan of merging it early in 1.26 instead. Given that the PR was only opened three days ago, I think the timing on this was always very tight. > > Danie…",,,,,,Anecdotal,comment,,,,,,,,2022-07-28,github/seans3,https://github.com/kubernetes/kubernetes/pull/111409#issuecomment-1198609892,repo: kubernetes/kubernetes | issue: Aggregated Discovery Endpoint | keyword: lesson learned
"> Rather than rely on client knowledge of defaults for fields like GVK, I'd like to see it always specified agree > subresource should be nested under the resource and always list GVK Unsure about the former, but adding GVK sounds good. > need some means of describing endpoints like /logs, /exec, /attach I think those are already in the open api, and we can keep this discovery doc only about things that need restmapping? > I like the idea of shaping the list like other lists and having an objec…",,,,,,Anecdotal,comment,,,,,,,,2022-07-28,github/lavalamp,https://github.com/kubernetes/kubernetes/pull/111409#issuecomment-1198630212,repo: kubernetes/kubernetes | issue: Aggregated Discovery Endpoint | keyword: lesson learned
"@deads2k I talked with the team, the intent here was actually not to change the API at all, just do the absolute minimum to get everything into a single document and make it safe given that it's coming out of a cache now. @seans3 believes that it's easy to adjust/update clients if we avoid making semantic changes. I still advise making a v2 for mechanical reasons (to reuse conversion generators etc), but not making semantic changes. Will that work for you? (I don't expect this to make the code …",,,,,,Anecdotal,comment,,,,,,,,2022-07-28,github/lavalamp,https://github.com/kubernetes/kubernetes/pull/111409#issuecomment-1198729209,repo: kubernetes/kubernetes | issue: Aggregated Discovery Endpoint | keyword: lesson learned
"Actually, it looks like we can re-use the `metav1.APIResource` type directly. That also removes any of the conversion code that currently existed.",,,,,,Anecdotal,comment,,,,,,,,2022-07-29,github/apelisse,https://github.com/kubernetes/kubernetes/pull/111409#issuecomment-1198746614,repo: kubernetes/kubernetes | issue: Aggregated Discovery Endpoint | keyword: lesson learned
"> I talked with the team, the intent here was actually not to change the API at all, This came up in the KEP review here: https://github.com/kubernetes/enhancements/pull/3364#discussion_r897282810 with a specific comment expanded here for convenience: > [deads2k](https://github.com/deads2k) [on Jun 14](https://github.com/kubernetes/enhancements/pull/3364#discussion_r897282810) I think we should make our API serialization the type we want. I can understand if internally you have a RESTMapper con…",,,,,,Anecdotal,comment,,,,,,,,2022-07-29,github/deads2k,https://github.com/kubernetes/kubernetes/pull/111409#issuecomment-1199818396,repo: kubernetes/kubernetes | issue: Aggregated Discovery Endpoint | keyword: lesson learned
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/111409#"" title=""Author self-approved"">alexzielenski</a>* **Once this PR has been reviewed and has the lgtm label**, please assign lavalamp for approval by writing `/assign @lavalamp` in a comment. For more information see:[The Kubernetes Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list…",,,,,,Anecdotal,comment,,,,,,,,2022-08-01,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/111409#issuecomment-1201352734,repo: kubernetes/kubernetes | issue: Aggregated Discovery Endpoint | keyword: lesson learned
"> Once a new API is released, it will be pinned. Creating a new API with the known flaws of the previous API is a mistake. The consuming code has to adapt either way. An example of the modifications that would provide an API that solves these items: https://github.com/kubernetes/kubernetes/pull/111409#issuecomment-1198536637 https://github.com/kubernetes/kubernetes/pull/111792",,,,,,Anecdotal,comment,,,,,,,,2022-08-10,github/deads2k,https://github.com/kubernetes/kubernetes/pull/111409#issuecomment-1211273145,repo: kubernetes/kubernetes | issue: Aggregated Discovery Endpoint | keyword: lesson learned
"@alexzielenski: The following tests **failed**, say `/retest` to rerun all failed tests or `/retest-required` to rerun all mandatory failed tests: Test name | Commit | Details | Required | Rerun command --- | --- | --- | --- | --- pull-kubernetes-conformance-kind-ipv6-parallel | fe0badc7633ec89f7cd4392eca3db7ea0dcbc6aa | [link](https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/111409/pull-kubernetes-conformance-kind-ipv6-parallel/1564388741965942784) | false | `/test pull-kubernetes-c…",,,,,,Anecdotal,comment,,,,,,,,2022-08-29,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/111409#issuecomment-1230964037,repo: kubernetes/kubernetes | issue: Aggregated Discovery Endpoint | keyword: lesson learned
"Panic on job controller Since the last days there is an unusual rate of flakiness on kind jobs, but couldn't find a pattern ![image](https://user-images.githubusercontent.com/6450081/201342503-14bccf80-0550-4d6a-8065-1ffa1d56e0eb.png) https://testgrid.k8s.io/sig-release-master-blocking#kind-master-parallel&width=20 https://storage.googleapis.com/k8s-triage/index.html?ci=0&pr=1 While looking at one test failure https://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-kind-e2e-parallel/1…",,,,,,Anecdotal,issue,,,,,,,,2022-11-11,github/aojea,https://github.com/kubernetes/kubernetes/issues/113852,repo: kubernetes/kubernetes | keyword: lesson learned | state: closed
"looks like the exit code matching doesn't handle a mix of terminated and unterminated containers, needs something like this: ```diff diff --git a/pkg/controller/job/pod_failure_policy.go b/pkg/controller/job/pod_failure_policy.go index a49030a4c3a..5a8f888118e 100644 --- a/pkg/controller/job/pod_failure_policy.go +++ b/pkg/controller/job/pod_failure_policy.go @@ -68,6 +68,8 @@ func matchPodFailurePolicy(podFailurePolicy *batch.PodFailurePolicy, failedPod * return nil, true, nil } +// matchOnExi…",,,,,,Anecdotal,comment,,,,,,,,2022-11-11,github/liggitt,https://github.com/kubernetes/kubernetes/issues/113852#issuecomment-1311690912,repo: kubernetes/kubernetes | issue: Panic on job controller | keyword: lesson learned
"> It will be great to have a tool that detect panics on the components big +1, a restart or panic detector on control plane components would be excellent",,,,,,Anecdotal,comment,,,,,,,,2022-11-11,github/liggitt,https://github.com/kubernetes/kubernetes/issues/113852#issuecomment-1311695987,repo: kubernetes/kubernetes | issue: Panic on job controller | keyword: lesson learned
"> > It will be great to have a tool that detect panics on the components > > big +1, a restart or panic detector on control plane components would be excellent @MadhavJivrajani this sounds like a nice project ^^^ but is better to check first with @BenTheElder and @spiffxp , I see there are some scripts that parse and dump the logs here https://github.com/kubernetes/test-infra/blob/f89c3d9701ef9fb70caeadebab2865069651863e/logexporter/cluster/log-dump.sh#L871 maybe we can add some `grep panic` th…",,,,,,Anecdotal,comment,,,,,,,,2022-11-11,github/aojea,https://github.com/kubernetes/kubernetes/issues/113852#issuecomment-1311702122,repo: kubernetes/kubernetes | issue: Panic on job controller | keyword: lesson learned
"I can add the fix proposed by @liggitt, but the underlying problem is #113855. I'll prepare a PR for each and see if we are ok merging both or just the hotfix.",,,,,,Anecdotal,comment,,,,,,,,2022-11-11,github/alculquicondor,https://github.com/kubernetes/kubernetes/issues/113852#issuecomment-1311776500,repo: kubernetes/kubernetes | issue: Panic on job controller | keyword: lesson learned
commented on https://github.com/kubernetes/kubernetes/issues/113855 we'll merge the panic fix right away in either case,,,,,,Anecdotal,comment,,,,,,,,2022-11-11,github/liggitt,https://github.com/kubernetes/kubernetes/issues/113852#issuecomment-1311784737,repo: kubernetes/kubernetes | issue: Panic on job controller | keyword: lesson learned
"Avoid putting new controllers into kube-controller-manager We should not put new controllers into kube-controller-manager, unless a compelling reasons exists why it can not or should not be a separate controller. We've had this as an implicit consequence of a lot of things we are working on: splitting up the code into separate repos, making kubernetes easier to install, and splitting out controllers or cloudproviders. Many new features have been written following this approach (ingress, network…",,,,,,Anecdotal,issue,,,,,,,,2016-08-10,github/justinsb,https://github.com/kubernetes/kubernetes/issues/30329,repo: kubernetes/kubernetes | keyword: lesson learned | state: closed
"I sort of disagree. For places where the controller needs to be pluggable, such as ingress, it makes sense to be out-of-core. For cases where the controller will never really be replaced (e.g. service, volumes, etc) the only effect of putting it outside is more moving parts for people to comprehend.",,,,,,Anecdotal,comment,,,,,,,,2016-08-10,github/thockin,https://github.com/kubernetes/kubernetes/issues/30329#issuecomment-238767211,repo: kubernetes/kubernetes | issue: Avoid putting new controllers into kube-controller-manager | keyword: lesson learned
"Before we do this, we need to ensure there is a generic way to get all clusters to run some addons by default. If not, we'll break all clusters every time we add a new required component.",,,,,,Anecdotal,comment,,,,,,,,2016-08-10,github/bgrant0607,https://github.com/kubernetes/kubernetes/issues/30329#issuecomment-238772228,repo: kubernetes/kubernetes | issue: Avoid putting new controllers into kube-controller-manager | keyword: lesson learned
"> the only effect of putting it outside is more moving parts for people to comprehend. Debugging an issue of a single controller from the assortment of cross-cutting behaviors has become obtuse at best now-a-days. It's compounded by the fact that we are sharing data, notifications, and locks now.",,,,,,Anecdotal,comment,,,,,,,,2016-08-12,github/timothysc,https://github.com/kubernetes/kubernetes/issues/30329#issuecomment-239553829,repo: kubernetes/kubernetes | issue: Avoid putting new controllers into kube-controller-manager | keyword: lesson learned
"I think the rule of thumb here is somewhere in the middle ground of ""all split"" or ""all shared"". It should probably be grouped by: - Common data set (multiple controllers dealing with pod lifecycle exclusively) - Common objectives (things that deal with volumes) - Things that are bulk ""replaced"" should be split out (scheduler) - Split because they depend on external tools that change at different rates or are not coupled to the core system. This exactly parallels a similar debate on API groups …",,,,,,Anecdotal,comment,,,,,,,,2016-08-12,github/smarterclayton,https://github.com/kubernetes/kubernetes/issues/30329#issuecomment-239557953,repo: kubernetes/kubernetes | issue: Avoid putting new controllers into kube-controller-manager | keyword: lesson learned
Layers doc (join kubernetes-dev to access): https://docs.google.com/document/d/1XkjVm4bOeiVkj-Xt1LgoGiqWsBfNozJ51dyI-ljzt1o/edit#,,,,,,Anecdotal,comment,,,,,,,,2017-03-10,github/bgrant0607,https://github.com/kubernetes/kubernetes/issues/30329#issuecomment-285585274,repo: kubernetes/kubernetes | issue: Avoid putting new controllers into kube-controller-manager | keyword: lesson learned
"We shouldn't need resource-specific support in the add-on manager, and we need to get away from round-tripping the resources in kubectl. We're planning to put apply support in apiserver: #17333, #12143 And we need to add strategic merge info to the OpenAPI schema: #25716",,,,,,Anecdotal,comment,,,,,,,,2017-03-10,github/bgrant0607,https://github.com/kubernetes/kubernetes/issues/30329#issuecomment-285586679,repo: kubernetes/kubernetes | issue: Avoid putting new controllers into kube-controller-manager | keyword: lesson learned
"Issues go stale after 90d of inactivity. Mark the issue as fresh with `/remove-lifecycle stale`. Stale issues rot after an additional 30d of inactivity and eventually close. Prevent issues from auto-closing with an `/lifecycle frozen` comment. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or `@fejta`. /lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2017-12-23,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/30329#issuecomment-353730761,repo: kubernetes/kubernetes | issue: Avoid putting new controllers into kube-controller-manager | keyword: lesson learned
set parallelism in integration tests using GOMAXPROCS xref https://github.com/kubernetes/kubernetes/issues/109038#issuecomment-1085950690 and https://github.com/kubernetes/kubernetes/issues/109038#issuecomment-1086055164 There is evidence integration test runs are overloading the test machine with concurrent build/link/test runs ```release-note NONE ```,,,,,,Anecdotal,issue,,,,,,,,2022-04-01,github/liggitt,https://github.com/kubernetes/kubernetes/pull/109239,repo: kubernetes/kubernetes | keyword: lesson learned | state: closed
"passed on the first try! - https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/109239/pull-kubernetes-integration/1509919036450803712 run took ~47 minutes, which is longer than the 36-38 minute success times when the job passes, but not nearly the longest presubmit",,,,,,Anecdotal,comment,,,,,,,,2022-04-01,github/liggitt,https://github.com/kubernetes/kubernetes/pull/109239#issuecomment-1086117688,repo: kubernetes/kubernetes | issue: set parallelism in integration tests using GOMAXPROCS | keyword: lesson learned
"we were already setting KUBE_INTEGRATION_TEST_MAX_CONCURRENCY to 4 in hack/jenkins/test-dockerized.sh someone in the past learned this lesson, and some test refactors in the distant past stopped honoring this",,,,,,Anecdotal,comment,,,,,,,,2022-04-01,github/liggitt,https://github.com/kubernetes/kubernetes/pull/109239#issuecomment-1086129268,repo: kubernetes/kubernetes | issue: set parallelism in integration tests using GOMAXPROCS | keyword: lesson learned
"/kind flake /prioritiy critical-urgent /lgtm I'm going to be AFK , in case you can get reviews I leave the lgtm and hold until you are satisfied with the tests /hold maybe is not only the `-p` and GOMAXPROCS is affecting something else ? https://github.com/kubernetes/kubernetes/pull/109240",,,,,,Anecdotal,comment,,,,,,,,2022-04-01,github/aojea,https://github.com/kubernetes/kubernetes/pull/109239#issuecomment-1086136147,repo: kubernetes/kubernetes | issue: set parallelism in integration tests using GOMAXPROCS | keyword: lesson learned
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/109239#pullrequestreview-929255208"" title=""LGTM"">BenTheElder</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/109239#"" title=""Author self-approved"">liggitt</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https://git.…",,,,,,Anecdotal,comment,,,,,,,,2022-04-01,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/109239#issuecomment-1086183208,repo: kubernetes/kubernetes | issue: set parallelism in integration tests using GOMAXPROCS | keyword: lesson learned
"/triage accepted e2e flakes up too? :( we used to have 90%+ passing, not so much today https://prow.k8s.io/?job=pull-kubernetes-e2e-kind",,,,,,Anecdotal,comment,,,,,,,,2022-04-01,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/109239#issuecomment-1086227721,repo: kubernetes/kubernetes | issue: set parallelism in integration tests using GOMAXPROCS | keyword: lesson learned
"batch with this in it hit an e2e failure and is retrying - https://prow.k8s.io/?repo=kubernetes%2Fkubernetes&type=batch&pull=109239 if the batch fails, this should go in solo",,,,,,Anecdotal,comment,,,,,,,,2022-04-01,github/liggitt,https://github.com/kubernetes/kubernetes/pull/109239#issuecomment-1086298855,repo: kubernetes/kubernetes | issue: set parallelism in integration tests using GOMAXPROCS | keyword: lesson learned
@aojea: ![goose image](https://images.unsplash.com/photo-1612812360660-1a81ac3f5e99?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=Mnw5NDI1N3wwfDF8cmFuZG9tfHx8fHx8fHx8MTY0ODg0NjI3OQ&ixlib=rb-1.2.1&q=80&w=400) <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/109239#issuecomment-1086312396): >/honk Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or sugges…,,,,,,Anecdotal,comment,,,,,,,,2022-04-01,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/109239#issuecomment-1086312411,repo: kubernetes/kubernetes | issue: set parallelism in integration tests using GOMAXPROCS | keyword: lesson learned
"add test cases for scheduler/PriorityQueue.Activate #### What type of PR is this? /kind cleanup #### What this PR does / why we need it: add unit test for `PriorityQueue.Activate` method under `pkg/scheduler` #### Which issue(s) this PR fixes: None #### Special notes for your reviewer: 1. `github.com/stretchr/testify/assert` is not used before in the current test file. The `assert` way: ``` assert.Equal(t, a, b, ""fail msg"") ``` I think it's more clear. But should we change it to: ``` if a != b …",,,,,,Anecdotal,issue,,,,,,,,2021-11-06,github/jiekun,https://github.com/kubernetes/kubernetes/pull/106203,repo: kubernetes/kubernetes | keyword: lesson learned | state: closed
"Thanks for your pull request. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please follow instructions at <https://git.k8s.io/community/CLA.md#the-contributor-license-agreement> to sign the CLA.** It may take a couple minutes for the CLA signature to be fully registered; after that, please reply here with a new comment and we'll verify. Thanks. --- - If you've already signed a CLA, it's possible we don't have your GitHub username or…",,,,,,Anecdotal,comment,,,,,,,,2021-11-06,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/106203#issuecomment-962435425,repo: kubernetes/kubernetes | issue: add test cases for scheduler/PriorityQueue.Activate | keyword: lesson learned
"@2014BDuck: This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions relate…",,,,,,Anecdotal,comment,,,,,,,,2021-11-06,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/106203#issuecomment-962435433,repo: kubernetes/kubernetes | issue: add test cases for scheduler/PriorityQueue.Activate | keyword: lesson learned
Welcome @2014BDuck! <br><br>It looks like this is your first PR to <a href='https://github.com/kubernetes/kubernetes'>kubernetes/kubernetes</a> 🎉. Please refer to our [pull request process documentation](https://git.k8s.io/community/contributors/guide/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands document…,,,,,,Anecdotal,comment,,,,,,,,2021-11-06,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/106203#issuecomment-962435434,repo: kubernetes/kubernetes | issue: add test cases for scheduler/PriorityQueue.Activate | keyword: lesson learned
"Hi @2014BDuck. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once the p…",,,,,,Anecdotal,comment,,,,,,,,2021-11-06,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/106203#issuecomment-962435435,repo: kubernetes/kubernetes | issue: add test cases for scheduler/PriorityQueue.Activate | keyword: lesson learned
"/ok-to-test 1. I think `assert.Equal` is simple and common used, no need to replace anymore. 2. It's a safe check. I checked all the related codes, it's safe to remove the snippet up to now, but I think it's somehow necessary since we should not trust the producer, design for robustness, WDYT @Huang-Wei",,,,,,Anecdotal,comment,,,,,,,,2021-11-08,github/kerthcet,https://github.com/kubernetes/kubernetes/pull/106203#issuecomment-962783751,repo: kubernetes/kubernetes | issue: add test cases for scheduler/PriorityQueue.Activate | keyword: lesson learned
Thank @alculquicondor for the guide. 1. Test cases are changed into a test table now. 2. `Queue` is created within the `for` loop so redundant assertion is removed. 3. `assert` lib is abandoned now. PTAL And what about the `pInfo ` check in: https://github.com/kubernetes/kubernetes/blob/135630d1db64658a571c27fa7a46a6964239e8e0/pkg/scheduler/internal/queue/scheduling_queue.go#L341,,,,,,Anecdotal,comment,,,,,,,,2021-12-09,github/jiekun,https://github.com/kubernetes/kubernetes/pull/106203#issuecomment-989439819,repo: kubernetes/kubernetes | issue: add test cases for scheduler/PriorityQueue.Activate | keyword: lesson learned
> Thank @alculquicondor for the guide. > > 1. Test cases are changed into a test table now. > 2. `Queue` is created within the `for` loop so redundant assertion is removed. > 3. `assert` lib is abandoned now. > > PTAL > > And what about the `pInfo ` check in: > > https://github.com/kubernetes/kubernetes/blob/135630d1db64658a571c27fa7a46a6964239e8e0/pkg/scheduler/internal/queue/scheduling_queue.go#L341 ----- I double-checked the code. We won't actually get here. And Code comments are also very c…,,,,,,Anecdotal,comment,,,,,,,,2021-12-16,github/denkensk,https://github.com/kubernetes/kubernetes/pull/106203#issuecomment-995538087,repo: kubernetes/kubernetes | issue: add test cases for scheduler/PriorityQueue.Activate | keyword: lesson learned
"/retest Hmm, I see there are a few tests that failed. The failed cases are not relevant to changes in this PR. I would like to run the test again and seek for suggestion",,,,,,Anecdotal,comment,,,,,,,,2021-12-18,github/jiekun,https://github.com/kubernetes/kubernetes/pull/106203#issuecomment-997135536,repo: kubernetes/kubernetes | issue: add test cases for scheduler/PriorityQueue.Activate | keyword: lesson learned
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/106203#"" title=""Author self-approved"">2014BDuck</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/106203#pullrequestreview-843969451"" title=""Approved"">alculquicondor</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](htt…",,,,,,Anecdotal,comment,,,,,,,,2022-01-04,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/106203#issuecomment-1005141809,repo: kubernetes/kubernetes | issue: add test cases for scheduler/PriorityQueue.Activate | keyword: lesson learned
"Thank @pacoxu and @alculquicondor . This is my first PR @ Kubernetes, please PING me if I missed the label, etc. And happy new year for you guys.",,,,,,Anecdotal,comment,,,,,,,,2022-01-05,github/jiekun,https://github.com/kubernetes/kubernetes/pull/106203#issuecomment-1005334169,repo: kubernetes/kubernetes | issue: add test cases for scheduler/PriorityQueue.Activate | keyword: lesson learned
"Adding EndpointSlice support for kube-proxy ipvs and iptables proxiers **What type of PR is this?** /kind feature **What this PR does / why we need it**: This adds EndpointSlice integration to kube-proxy for ipvs and iptables proxiers, building on the earlier API and Controller PRs here: https://github.com/kubernetes/kubernetes/pull/80766, https://github.com/kubernetes/kubernetes/pull/81048. **Special notes for your reviewer**: Until the PR this is built on gets merged in, the diff specific to …",,,,,,Anecdotal,issue,,,,,,,,2019-08-14,github/robscott,https://github.com/kubernetes/kubernetes/pull/81430,repo: kubernetes/kubernetes | keyword: lesson learned | state: closed
"Hi @robscott. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once the pa…",,,,,,Anecdotal,comment,,,,,,,,2019-08-14,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/81430#issuecomment-521371763,repo: kubernetes/kubernetes | issue: Adding EndpointSlice support for kube-proxy ipvs and iptables proxiers | keyword: lesson learned
"This PR [may require API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#what-apis-need-to-be-reviewed). If so, when the changes are ready, [complete the pre-review checklist and request an API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#mechanics). Status of requested reviews is tracked in the [API Review project](https://github.com/orgs/kubernetes/projects/13).",,,,,,Anecdotal,comment,,,,,,,,2019-08-15,github/fejta-bot,https://github.com/kubernetes/kubernetes/pull/81430#issuecomment-521471340,repo: kubernetes/kubernetes | issue: Adding EndpointSlice support for kube-proxy ipvs and iptables proxiers | keyword: lesson learned
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/81430#"" title=""Author self-approved"">robscott</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/81430#pullrequestreview-279917139"" title=""Approved"">thockin</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https://git.k…",,,,,,Anecdotal,comment,,,,,,,,2019-08-28,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/81430#issuecomment-525943216,repo: kubernetes/kubernetes | issue: Adding EndpointSlice support for kube-proxy ipvs and iptables proxiers | keyword: lesson learned
"/retest On Wed, Aug 28, 2019, 8:34 PM Kubernetes Prow Robot < notifications@github.com> wrote: > @robscott <https://github.com/robscott>: The following test *failed*, say > /retest to rerun them all: > Test name Commit Details Rerun command > pull-kubernetes-e2e-gce 86e2705 > <https://github.com/kubernetes/kubernetes/commit/86e2705b57f75d2b44fec3859dd8c7b8bd7c6a55> > link > <https://prow.k8s.io/view/gcs/kubernetes-jenkins/pr-logs/pull/81430/pull-kubernetes-e2e-gce/1166902142510829570/> /test > …",,,,,,Anecdotal,comment,,,,,,,,2019-08-29,github/thockin,https://github.com/kubernetes/kubernetes/pull/81430#issuecomment-526007572,repo: kubernetes/kubernetes | issue: Adding EndpointSlice support for kube-proxy ipvs and iptables proxiers | keyword: lesson learned
In my earlier rebase it missed like I missed this line somehow which would likely cause those tests to fail: https://github.com/kubernetes/kubernetes/pull/81612/files#diff-cf93bbed37202b7d58b5841f07ddd89fR623.,,,,,,Anecdotal,comment,,,,,,,,2019-08-29,github/robscott,https://github.com/kubernetes/kubernetes/pull/81430#issuecomment-526076147,repo: kubernetes/kubernetes | issue: Adding EndpointSlice support for kube-proxy ipvs and iptables proxiers | keyword: lesson learned
"AWS ELB healtcheck SYN Flood in pod Networking to service/pod when draining config are applied to existent AWS ELB. **Versions**: ``` Client Version: version.Info{Major:""1"", Minor:""19"", GitVersion:""v1.19.3"", GitCommit:""1e11e4a2108024935ecfcb2912226cedeafd99df"", GitTreeState:""clean"", BuildDate:""2020-10-14T12:50:19Z"", GoVersion:""go1.15.2"", Compiler:""gc"", Platform:""darwin/amd64""} Server Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.9"", GitCommit:""94f372e501c973a7fa9eb40ec9ebd2fe7c…",,,,,,Anecdotal,issue,,,,,,,,2021-02-23,github/ltagliamonte-dd,https://github.com/kubernetes/kubernetes/issues/99373,repo: kubernetes/kubernetes | keyword: lesson learned | state: closed
"@ltagliamonte-dd: This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions …",,,,,,Anecdotal,comment,,,,,,,,2021-02-23,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/99373#issuecomment-784480487,repo: kubernetes/kubernetes | issue: AWS ELB healtcheck SYN Flood in pod | keyword: lesson learned
- update: it breaks also if you start directly with connection draining on the ELB: ``` apiVersion: apps/v1 kind: Deployment metadata: name: lt-hello-world-web namespace: lt-test-ns spec: replicas: 1 selector: matchLabels: app: web service: lt-hello-world strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 0 type: RollingUpdate template: metadata: labels: app: web service: lt-hello-world spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labe…,,,,,,Anecdotal,comment,,,,,,,,2021-02-23,github/ltagliamonte-dd,https://github.com/kubernetes/kubernetes/issues/99373#issuecomment-784497575,repo: kubernetes/kubernetes | issue: AWS ELB healtcheck SYN Flood in pod | keyword: lesson learned
"When kube-proxy is in iptables mode, kube-proxy is not on the data path. By default the AWS ELB HC is configured: - TCP mode on the serviceNodePort. - every 10s with 5s timeout When the LB performs the node HC the TCP handshake reaches the pod doesn't stop at node level, so if you have a single pod in a big cluster, it likely get DDoSed by all this HCs, your pod needs to be able to deal with all this TCP connections. Or you should change the HC to stop at the node level, instead of reaching the…",,,,,,Anecdotal,comment,,,,,,,,2021-02-25,github/ltagliamonte-dd,https://github.com/kubernetes/kubernetes/issues/99373#issuecomment-785483463,repo: kubernetes/kubernetes | issue: AWS ELB healtcheck SYN Flood in pod | keyword: lesson learned
"Closing this issue. Lessons learned: - ELB HC reach the pods when kube-proxy is in iptable or ipvs mode. Your pod should be able to handle this additional TCP load. - python simpleHTTP server shouldn’t be used for anything else than playing around with python I switched the image to (hashicorp/http-echo:0.2.3) and a single pod is able to handle all the HC connections. On the healthcheck config: You may want to consider to move the HC to kube-proxy health endpoint, this doesn’t look conceptually…",,,,,,Anecdotal,comment,,,,,,,,2021-02-25,github/ltagliamonte-dd,https://github.com/kubernetes/kubernetes/issues/99373#issuecomment-786131088,repo: kubernetes/kubernetes | issue: AWS ELB healtcheck SYN Flood in pod | keyword: lesson learned
"Sidecar kubelet implementation **What type of PR is this?** /kind feature **What this PR does / why we need it**: This PR adds the functionality for the Sidecars KEP https://github.com/kubernetes/enhancements/issues/753 NOTE: This PR contains two commits from https://github.com/kubernetes/kubernetes/pull/79649 which implements the API making it seem a lot larger, please ignore the first two commits for this PR. I've separated the commits to each implement a different piece of functionality whic…",,,,,,Anecdotal,issue,,,,,,,,2019-07-30,github/Joseph-Irving,https://github.com/kubernetes/kubernetes/pull/80744,repo: kubernetes/kubernetes | keyword: lesson learned | state: closed
"Hi @Joseph-Irving. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once t…",,,,,,Anecdotal,comment,,,,,,,,2019-07-30,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-516396246,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
"@mattjmcnaughton no the hold is due to the fact it has a dependent PR so this should not be merged before that, api PR (https://github.com/kubernetes/kubernetes/pull/79649) I would like people to start reviewing this.",,,,,,Anecdotal,comment,,,,,,,,2019-07-30,github/Joseph-Irving,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-516437157,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
Is anyone from @kubernetes/sig-node-pr-reviews available to have a look at this? I would appreciate some feedback /test pull-kubernetes-integration /test pull-kubernetes-bazel-build,,,,,,Anecdotal,comment,,,,,,,,2019-08-19,github/Joseph-Irving,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-522575955,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
@Joseph-Irving: Reiterating the mentions to trigger a notification: @kubernetes/sig-node-pr-reviews <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-522575955): >Is anyone from @kubernetes/sig-node-pr-reviews available to have a look at this? >I would appreciate some feedback > >/test pull-kubernetes-integration >/test pull-kubernetes-bazel-build Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/communi…,,,,,,Anecdotal,comment,,,,,,,,2019-08-19,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-522576026,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
"Thanks @tomwans and @zhan849 for all your comments so far, it's great to have some insight from people who've already done something similar, very much appreciated. There are a few unresolved questions around grace period, pod status and ordering. Also definitely need more unit tests. I'm going away on a much needed holiday for two weeks so I will look at resolving all of this once I'm back.",,,,,,Anecdotal,comment,,,,,,,,2019-09-27,github/Joseph-Irving,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-535986393,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
"thanks @Joseph-Irving ! @tomwans and I synced up and put up some google doc about lessons we both learned supporting sidecar in production. Lyft for long running workloads and Pinterest for run-to-finish batch workloads, for your and other implementers / reviewers' reference https://docs.google.com/document/d/1bgrC2MhoJ2cWD62EOVXUJheWQQd6g4pCrwK3MXKzc7g/edit?usp=sharing (current the permission is anyone with the link can comment, pls lemme know if the doc cannot be accessed) for addressing ques…",,,,,,Anecdotal,comment,,,,,,,,2019-10-15,github/zhan849,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-542299233,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
thanks for doing that @zhan849 but I don't appear to be able to access that doc. it would be good to get sig-node's opinion on some of these questions. Such as whether we should be sharing the grace period between containers/sidecars or not. If anyone from @kubernetes/sig-node-pr-reviews or @derekwaynecarr wants to comment that would be appreciated. I will try and get to a sig-node meeting if I can but currently my tuesday evenings are quite busy so that could be tricky.,,,,,,Anecdotal,comment,,,,,,,,2019-10-16,github/Joseph-Irving,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-542712241,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
@Joseph-Irving: Reiterating the mentions to trigger a notification: @kubernetes/sig-node-pr-reviews <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-542712241): >thanks for doing that @zhan849 but I don't appear to be able to access that doc. > >it would be good to get sig-node's opinion on some of these questions. Such as whether we should be sharing the grace period between containers/sidecars or not. If anyone from @kubernetes/sig-node-pr-revie…,,,,,,Anecdotal,comment,,,,,,,,2019-10-16,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-542712293,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
"> @Joseph-Irving thanks for doing that @zhan849 but I don't appear to be able to access that doc. @Joseph-Irving sry about the confusion, updated the link, can you try again?",,,,,,Anecdotal,comment,,,,,,,,2019-10-16,github/zhan849,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-542749972,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
"@zhan849: Reiterating the mentions to trigger a notification: @kubernetes/sig-node-pr-reviews <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-542749972): >> @Joseph-Irving: Reiterating the mentions to trigger a notification: >> @kubernetes/sig-node-pr-reviews > >@Joseph-Irving sry about the confusion, updated the link Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-r…",,,,,,Anecdotal,comment,,,,,,,,2019-10-16,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-542750037,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
"From today's Sig Node meeting, I will raise a PR to the KEP so that we can debate the suggested changes there in regards to grace period and pod phase.",,,,,,Anecdotal,comment,,,,,,,,2019-10-29,github/Joseph-Irving,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-547552139,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
"the kep changes (kubernetes/enhancements#1344) were agreed with sig-node. So I will be keeping termination period as is. I will change Pod phase so that we no longer include the sidecars, to avoid them causing pods to be failed etc.",,,,,,Anecdotal,comment,,,,,,,,2019-11-05,github/Joseph-Irving,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-549966669,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
"@tedyu I'm sorry but I erased your review comments when I pushed new commits, I've replied to the places where you left comments.",,,,,,Anecdotal,comment,,,,,,,,2020-01-16,github/Joseph-Irving,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-575167936,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
"For my own book keeping, tracking the different components to review here. I'll try and keep this up to date. - [x] Shutdown triggering for sidecars when RestartPolicy != Always - [x] Pre-stop hooks sent to sidecars before non sidecar containers - [x] Sidecars terminated after normal containers - [x] Sidecars start before normal containers - [X] Sidecars not included in pod phase calculations - [x] General design, etc... @Joseph-Irving any big components I'm missing?",,,,,,Anecdotal,comment,,,,,,,,2020-02-13,github/mattjmcnaughton,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-585829293,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
"> For my own book keeping, tracking the different components to review here. I'll try and keep this up to date. > > * [ ] Shutdown triggering for sidecars when RestartPolicy != Always > > * [x] Pre-stop hooks sent to sidecars before non sidecar containers > > * [ ] Sidecars terminated after normal containers > > * [ ] Sidecars start before normal containers > > * [x] Sidecars not included in pod phase calculations > > * [ ] General design, etc... > > > @Joseph-Irving any big components I'm miss…",,,,,,Anecdotal,comment,,,,,,,,2020-02-14,github/mattjmcnaughton,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-586419246,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
"I've added a new commit addressing some of the more minor comments, I will keep things in new commits for now to avoid my rebasing accidentally destroying your review comments.",,,,,,Anecdotal,comment,,,,,,,,2020-02-17,github/Joseph-Irving,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-586978343,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
"> For my own book keeping, tracking the different components to review here. I'll try and keep this up to date. > > * [x] Shutdown triggering for sidecars when RestartPolicy != Always > > * [x] Pre-stop hooks sent to sidecars before non sidecar containers > > * [x] Sidecars terminated after normal containers > > * [x] Sidecars start before normal containers > > * [x] Sidecars not included in pod phase calculations > > * [x] General design, etc... > > > @Joseph-Irving any big components I'm miss…",,,,,,Anecdotal,comment,,,,,,,,2020-02-18,github/mattjmcnaughton,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-587468976,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
"Thanks @Joseph-Irving for working on this very complicated but important feature, and thanks @mattjmcnaughton for the detailed review from sig-node. currently from my end I'm seeing the following major issues (based on our experience in running in-house sidecar at Pinterest scale, not exactly same as this KEP though): 1. computePodAction test coverage 2. kill container order test coverage 3. a suspicious logic that if there are init container, sidecar containers might not be restarted 4. if sid…",,,,,,Anecdotal,comment,,,,,,,,2020-02-18,github/zhan849,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-587574703,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
"@zhan849 1) For compute pod actions I've looked at your suggested list of scenarios and some of them don't make sense to me or don't seem to be make sense in this implementation, I'm happy to add more cases as people see fit, I'm sure there will always be more scenarios we can add 2) If you have any suggestions how to better test this I'm very open to that 3) Can you please elaborate? I'm not sure what ""suspicious logic"" you're referring to 4 and 5 could do with some discussion but I agree that…",,,,,,Anecdotal,comment,,,,,,,,2020-02-18,github/Joseph-Irving,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-587585260,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
"> @zhan849 > > 1. For compute pod actions I've looked at your suggested list of scenarios and some of them don't make sense to me or don't seem to be make sense in this implementation, I'm happy to add more cases as people see fit, I'm sure there will always be more scenarios we can add I revised a bit to make them more explicit, and I'm happy to explain more if there is confusion. as these are cases we really see in production > 2. If you have any suggestions how to better test this I'm very o…",,,,,,Anecdotal,comment,,,,,,,,2020-02-18,github/zhan849,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-587689019,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
@zhan849 Ok so I pushed some new commits: Added a test case for verifying that the sidecar gets killed last Added a load of new test cases for compute pod actions Also I've responded to your comment and pointed out that there are tests that show the behaviour of the sidecars starting after the init containers.,,,,,,Anecdotal,comment,,,,,,,,2020-02-20,github/Joseph-Irving,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-589113221,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
"The commit hashes in #79649 do not match this. I don't want to re-review that - can you rebase that, then rebase this onto that one, and re-push both?",,,,,,Anecdotal,comment,,,,,,,,2020-02-21,github/thockin,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-589860906,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
> @zhan849 Ok so I pushed some new commits: > Added a test case for verifying that the sidecar gets killed last > Added a load of new test cases for compute pod actions > > Also I've responded to your comment and pointed out that there are tests that show the behaviour of the sidecars starting after the init containers. thanks for the work @Joseph-Irving ! will leave it to the community for further actions,,,,,,Anecdotal,comment,,,,,,,,2020-02-22,github/zhan849,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-589972398,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
/assign @derekwaynecarr @dchen1107 Can either of you look over this for sig-node approval? @thockin has been looking at the API and @mattjmcnaughton has done a review of the kubelet code. Please let me know what you think or if anything else needs doing to get this into a mergeable state.,,,,,,Anecdotal,comment,,,,,,,,2020-02-25,github/Joseph-Irving,https://github.com/kubernetes/kubernetes/pull/80744#issuecomment-591069281,repo: kubernetes/kubernetes | issue: Sidecar kubelet implementation | keyword: lesson learned
"Add mTLS encription between etcd and kube-apiserver in GCE <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/guide#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull requ…",,,,,,Anecdotal,issue,,,,,,,,2018-10-23,github/wenjiaswe,https://github.com/kubernetes/kubernetes/pull/70144,repo: kubernetes/kubernetes | keyword: lesson learned | state: closed
/cc @yguo0905 @cjcullen @jpbetz add in here early so I get the necessary tests going. I will let you know when it's ready for review. Thanks!,,,,,,Anecdotal,comment,,,,,,,,2018-10-23,github/wenjiaswe,https://github.com/kubernetes/kubernetes/pull/70144#issuecomment-432412843,repo: kubernetes/kubernetes | issue: Add mTLS encription between etcd and kube-apiserver in GCE | keyword: lesson learned
@wenjiaswe - tests on higher scale didn't show any problems. So it's fine from performance/scalability POV (I didn't look into the code).,,,,,,Anecdotal,comment,,,,,,,,2018-11-15,github/wojtek-t,https://github.com/kubernetes/kubernetes/pull/70144#issuecomment-439072245,repo: kubernetes/kubernetes | issue: Add mTLS encription between etcd and kube-apiserver in GCE | keyword: lesson learned
Reasons to merge this: * It only affects GCE cloud provider * It's a security defense-in-depth change Reasons not to: * We're in code freeze * This is a large amount of bash ;) I'm happy to add the requisite labels if you can get the release team to ack.,,,,,,Anecdotal,comment,,,,,,,,2018-11-26,github/lavalamp,https://github.com/kubernetes/kubernetes/pull/70144#issuecomment-441764295,repo: kubernetes/kubernetes | issue: Add mTLS encription between etcd and kube-apiserver in GCE | keyword: lesson learned
"Since we are just days before lifting code freeze, I feel its late in the release cycle to take in a feature when its not critical-urgent. Speaking with @wenjiaswe offline, we determined this will be a good candidate for 1.13.1 patch release. /cc @aleksandra-malinowska @tpepper",,,,,,Anecdotal,comment,,,,,,,,2018-11-26,github/AishSundar,https://github.com/kubernetes/kubernetes/pull/70144#issuecomment-441771736,repo: kubernetes/kubernetes | issue: Add mTLS encription between etcd and kube-apiserver in GCE | keyword: lesson learned
@gmarek @cjcullen would you please take another look and see if you could help lgtm and approve? I addressed all the comments and rebased the PR.,,,,,,Anecdotal,comment,,,,,,,,2019-01-07,github/wenjiaswe,https://github.com/kubernetes/kubernetes/pull/70144#issuecomment-452119875,repo: kubernetes/kubernetes | issue: Add mTLS encription between etcd and kube-apiserver in GCE | keyword: lesson learned
"Can folks comment on whether issue #70143 is being viewed as a feature or a bug? And then whether this PR is going to be intended to be a cherry pick candidate to prior branches? I ask because this PR is labeled feature, the issue is not labeled but has textual description as both bug and feature, and patch release managers were brought in early on cherry picks which were later closed. It feels a bit up in the air right now...",,,,,,Anecdotal,comment,,,,,,,,2019-01-09,github/tpepper,https://github.com/kubernetes/kubernetes/pull/70144#issuecomment-452822283,repo: kubernetes/kubernetes | issue: Add mTLS encription between etcd and kube-apiserver in GCE | keyword: lesson learned
@tpepper I am sorry for the confusion. This is a feature. And #70143 was labeled as feature (uncommented /kind feature) but somehow the markdown didn't show up right. I corrected that one. I am not going to cherry pick this back to previous versions. Thanks!,,,,,,Anecdotal,comment,,,,,,,,2019-01-09,github/wenjiaswe,https://github.com/kubernetes/kubernetes/pull/70144#issuecomment-452885044,repo: kubernetes/kubernetes | issue: Add mTLS encription between etcd and kube-apiserver in GCE | keyword: lesson learned
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/70144#issuecomment-454713123"" title=""Approved"">gmarek</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/70144#"" title=""Author self-approved"">wenjiaswe</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https://git.k8s.io…",,,,,,Anecdotal,comment,,,,,,,,2019-01-16,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/70144#issuecomment-454713346,repo: kubernetes/kubernetes | issue: Add mTLS encription between etcd and kube-apiserver in GCE | keyword: lesson learned
"Pass pod annotations to Allocate **What this PR does / why we need it** At the moment device plugin APIs in terms of Allocate call has no information about workload that is requesting particular HW device. Passing pod annotations to Allocate GRPC call should help with: - development and debugging: annotations can trigger some development behavior inside device plugins (e.g. additional logging, experimental features) - flexibility: some devices might have fine-tune switches (e.g. power managemen…",,,,,,Anecdotal,issue,,,,,,,,2018-03-27,github/bart0sh,https://github.com/kubernetes/kubernetes/pull/61775,repo: kubernetes/kubernetes | keyword: lesson learned | state: closed
"/lgtm from my side except for nits in comment. This alias with what we discussed during the sig meeting. Could anyone take a final pass please? Maybe we also want to check if some specified annotation keys are allowed or sth else to avoid abuse this API. > we expect to merge the n-Allocate calls to a single call (PodAdmission?), though that's not yet decided I believe it's the direction, while no need to be included in this patch.",,,,,,Anecdotal,comment,,,,,,,,2018-03-31,github/resouer,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-377717512,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
"@bart0sh and @resouer from last week's f2f meetup, I think @derekwaynecarr expressed some concern on passing totally unstructured annotation data through the API and mentioned he will publish some strawman guideline on the annotation model specific for resources, and then perhaps tie that to DP API. So I would like to wait for his inputs.",,,,,,Anecdotal,comment,,,,,,,,2018-04-03,github/jiayingz,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-378341248,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
@jiayingz Thanks! Derek's concern make sense to me. I also expressed similar thing in previous comment: > Maybe we also want to check if some specified annotation keys are allowed or sth else to avoid abuse this API. I'm happy to know any future inputs from sig-node.,,,,,,Anecdotal,comment,,,,,,,,2018-04-03,github/resouer,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-378411937,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
"There are already many ""well-known"" annotation keys defined in pkg/apis/core/annotation_key_constants.go. Most of them are not relevant to the use cases the PR is attempting to solve. So ... maybe an alternative is to add a new key to wrap things specific to device plugins, e.g. `deviceplugin.alpha.kubernetes.io/options`.",,,,,,Anecdotal,comment,,,,,,,,2018-04-04,github/tengqm,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-378466136,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
"@jiayingz I'd propose to merge this PR as is and then implement filtering out unwanted annotations. There is a very little chance that people start abusing this feature immediately after it's merged. However, having it merged would give them ability to play with it and understand better what kind of annotations would make sense to pass to the plugin. Thoughts?",,,,,,Anecdotal,comment,,,,,,,,2018-04-04,github/bart0sh,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-378552534,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
"@bart0sh I know this looks like a small change, but when it comes to API level, it is usually much harder to remove things than adding things. I think from the lessons learned from the past experiences, we are now trying to be more careful on passing annotations in K8s now. That is why I would like to get more inputs from folks like Derek who has seen such issues in the past.",,,,,,Anecdotal,comment,,,,,,,,2018-04-04,github/jiayingz,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-378687014,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
"FYI, well known annotation keys will not address the issue if the concern is about DP plugins may consume annotations which contains harmful/hack data. We'll need constraints on the annotation values, and verify that in DP, i.e.: > he will publish some strawman guideline on the annotation model specific for resources, and then perhaps tie that to DP API So I would like to know the use cases input from @derekwaynecarr.",,,,,,Anecdotal,comment,,,,,,,,2018-04-04,github/resouer,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-378752896,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
/hold We do not want to pass annotations yet via device plugin APIs since Annotations are free formed and not captured by any policies in k8s yet.,,,,,,Anecdotal,comment,,,,,,,,2018-04-04,github/vishh,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-378755515,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
"> from last week's f2f meetup, I think @derekwaynecarr expressed some concern on passing totally unstructured annotation data through the API and mentioned he will publish some strawman guideline on the annotation model specific for resources, and then perhaps tie that to DP API. So I would like to wait for his inputs. @derekwaynecar We're waiting for your input here. @vishh @resouer @jiayingz While we're waiting I'm going to update this PR with simple filtering out annotations that don't match…",,,,,,Anecdotal,comment,,,,,,,,2018-04-13,github/bart0sh,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-381097284,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
"A prefix will not help. We do not want to pass annotations since there is no way to policy gate them today. On Fri, 13 Apr 2018 at 3:42 am, Ed Bartosh <notifications@github.com> wrote: > from last week's f2f meetup, I think @derekwaynecarr > <https://github.com/derekwaynecarr> expressed some concern on passing > totally unstructured annotation data through the API and mentioned he will > publish some strawman guideline on the annotation model specific for > resources, and then perhaps tie that …",,,,,,Anecdotal,comment,,,,,,,,2018-04-14,github/vishh,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-381334639,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
> We do not want to pass annotations since there is no way to policy gate them today. That sounds hopeless. Any chance to have a policy in near future? How can we speed up this?,,,,,,Anecdotal,comment,,,,,,,,2018-04-15,github/bart0sh,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-381398896,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
I'd recommend following up with Resource APIs effort. I filed an issue that should give you some clarity on alternate possibilities - https://github.com/kubernetes/kubernetes/issues/62598#issuecomment-381425715,,,,,,Anecdotal,comment,,,,,,,,2018-04-15,github/vishh,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-381425761,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
Everytime a vendor or user chooses to use annotations for compute resources I'd recommend considering the following issues: 1. It creates an opaque API as opposed to concrete fields 2. It creates a possibility of mixing infrastructure and workload constraints. What fields are a user expected to set vs a cluster admin? How will that separation be enforced? 3. Conformance of k8s APIs - What would be the minimal feature set that can be enforced as part of k8s conformance for a given device(s) and …,,,,,,Anecdotal,comment,,,,,,,,2018-04-15,github/vishh,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-381426419,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
"Just to clarify, based on the current Resource API design draft, we have no plan to pass special information through the ComputeResource API to device plugin. We are still discussing the best API format to represent ComputeResource. To help us understand the urgency of this request, @bart0sh @kad or @RenaudWasTaken can any of you describe the most urgent use case you want to enable through this change? Maybe we can brainstorm alternative ways to enable these use cases without compromising API p…",,,,,,Anecdotal,comment,,,,,,,,2018-04-16,github/jiayingz,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-381689249,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
"@jiayingz Thank you for asking! It would be really great to brainstorm possible alternatives. The best use case I can come up with is passing required FPGA function to the FPGA device plugin as annotation. That function may or may not be already preprogrammed on the device, hence workload can either use it if it's already programmed or program it first and then use. In case the function needs to be programmed the workload needs to know which bitstream to use to program that function, so bitstre…",,,,,,Anecdotal,comment,,,,,,,,2018-04-17,github/bart0sh,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-381947148,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
"@jiayingz What I am looking for, is similar to what was done in the storage area with `mountOptions`: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#mount-options I explained use cases in description of #59109: our most burning ones are: 1. during development of plugins, we want to allow users to get experimental features of the devices (e.g. different power management/frequency options for each workload). In this scenario resource is the same from selection/scheduling point of…",,,,,,Anecdotal,comment,,,,,,,,2018-04-25,github/kad,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-384282310,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
"(1) could be dealt with as per-node / per-plugin property (2) can this be dealt with via per-plugin property too? Why would a user choose between OSS and closed source libraries? I assume by multiple devices you meant devices of different types (FPGA, NICs,...) Coz running multiple devices of the same type feels unrealistic in production environments. To clarify, I meant passing custom properties via resource classes which then get passed down to PodStatus as part of the concrete resources bein…",,,,,,Anecdotal,comment,,,,,,,,2018-04-25,github/vishh,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-384317135,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
@vishh for (1) granularity needed is Pod. per-plugin or per-node it means that every device instance on that node will be affected. It does not allow to fine-tune parameters depending on workload. It does not allow to fine-tune one of the instances of the devices on the same node (2 identical FPGA cards with running different workloads with different power management/frequency settings). For (2): choose of libraries: one example can be differences in OpenCL implementations. E.g. some open sourc…,,,,,,Anecdotal,comment,,,,,,,,2018-04-25,github/kad,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-384343020,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
"Thanks a lot for the detailed explanation, @kad and @bart0sh I think FPGA perhaps represents quite advanced use cases and it is great for us to learn these special requirements, especially at this time that we are proposing a new resource API design that hopefully can address some of these concerns. I think your described use cases are quite valid. Using pod annotation can perhaps allow some short-term experiment for such features, but it also has drawbacks such as no version control, no access…",,,,,,Anecdotal,comment,,,,,,,,2018-04-25,github/jiayingz,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-384371087,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
"> By multiple devices I meant that node can have NVidia GPU and Intel GPU at the same time. If each of those will require custom runtime, that it will be problematic to maintain this ""sandwich"" of runtimes. @kad as mentioned by @jiayingz, we don't need a custom runtime if we can simply inject a prestart hook. Docker doesn't allow that today, so we have to replace the whole runtime. Here it's different, we have way more control on the container runtime. > For use case 2), I think Nvidia also men…",,,,,,Anecdotal,comment,,,,,,,,2018-04-25,github/flx42,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-384381298,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
"@jiayingz for (2) and prestart hook: I don't have opinion at the moment. would love to see some example for Nvidia scenarios. For us it was enough to be able to return values in `ContainerAllocateResponse.Mount` based on workload parameters. Nothing fancy. For (1): we are in discussion phase, indeed, and it is good to keep in mind lessons learned from storage area, where parameters are similarly dependent on provider of the storage. Similar with devices, we shouldn't try to design or implement …",,,,,,Anecdotal,comment,,,,,,,,2018-04-25,github/kad,https://github.com/kubernetes/kubernetes/pull/61775#issuecomment-384384429,repo: kubernetes/kubernetes | issue: Pass pod annotations to Allocate | keyword: lesson learned
"Add container.lifecycle.type to api (sidecars KEP) **What type of PR is this?** /kind feature **What this PR does / why we need it**: This implements the API for the Sidecar containers KEP https://github.com/kubernetes/enhancements/issues/753 It adds a new alpha optional field `type` to `container.lifecycle`, this is a string alias which can either be ""Standard"" or ""Sidecar"". I will have follow up PRs based off of this to implement the logic in the Kubelet. **Special notes for your reviewer**: …",,,,,,Anecdotal,issue,,,,,,,,2019-07-02,github/Joseph-Irving,https://github.com/kubernetes/kubernetes/pull/79649,repo: kubernetes/kubernetes | keyword: lesson learned | state: closed
"Hi @Joseph-Irving. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once t…",,,,,,Anecdotal,comment,,,,,,,,2019-07-02,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-507625330,repo: kubernetes/kubernetes | issue: Add container.lifecycle.type to api (sidecars KEP) | keyword: lesson learned
"This PR [may require API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#what-apis-need-to-be-reviewed). If so, when the changes are ready, [complete the pre-review checklist and request an API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#mechanics). Status of requested reviews is tracked in the [API Review project](https://github.com/orgs/kubernetes/projects/13).",,,,,,Anecdotal,comment,,,,,,,,2019-07-02,github/fejta-bot,https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-507639317,repo: kubernetes/kubernetes | issue: Add container.lifecycle.type to api (sidecars KEP) | keyword: lesson learned
"thanks for the review @nilebox, I've hopefully addressed your comments. I had to regenerate the test data to get the bazel-test to work, I did this by running `UPDATE_COMPATIBILITY_FIXTURE_DATA=true make test`, I couldn't find this documented anywhere, but appears to have fixed it, I put the new test data in second commit to make it more readable. /test pull-kubernetes-integration",,,,,,Anecdotal,comment,,,,,,,,2019-07-03,github/Joseph-Irving,https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-508102910,repo: kubernetes/kubernetes | issue: Add container.lifecycle.type to api (sidecars KEP) | keyword: lesson learned
"> I had to regenerate the test data to get the bazel-test to work, I did this by running `UPDATE_COMPATIBILITY_FIXTURE_DATA=true make test`, I couldn't find this documented anywhere, but appears to have fixed it Doc in https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/api/testdata/README.md#current-version (also, `make update` will do this along with other regeneration). > I put the new test data in second commit to make it more readable. Perfect, thanks",,,,,,Anecdotal,comment,,,,,,,,2019-07-03,github/liggitt,https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-508103666,repo: kubernetes/kubernetes | issue: Add container.lifecycle.type to api (sidecars KEP) | keyword: lesson learned
/assign @thockin @smarterclayton seemed to have the most context from https://github.com/kubernetes/enhancements/pull/919#pullrequestreview-244133225,,,,,,Anecdotal,comment,,,,,,,,2019-07-08,github/liggitt,https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-509329785,repo: kubernetes/kubernetes | issue: Add container.lifecycle.type to api (sidecars KEP) | keyword: lesson learned
"For this to work for Istio's sidecar proxies, we must make sure that Sidecar proxy containers are ready before the pod's application init containers start, in order for those init containers to have network connectivity. **So the `Sidecar` containers must be running before any init container starts.** So pod startup must look like: 1. sidecar init containers start 1. sidecar init containers finish 1. sidecar containers start 1. sidecar containers become ready 1. init containers start 1. init co…",,,,,,Anecdotal,comment,,,,,,,,2019-07-08,github/rlenglet,https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-509400034,repo: kubernetes/kubernetes | issue: Add container.lifecycle.type to api (sidecars KEP) | keyword: lesson learned
"@rlenglet I don't think this is the right place to have this discussion, I'd rather keep PR comments about what's in the PR. I think the enhancements issue https://github.com/kubernetes/enhancements/issues/753 or at sig-apps meeting/mailing-list are probably better places. short answer: I don't think what you've described is required for this proposal to work with istio, I think you've described how to solve the problem of istio not working for init containers. We have discussed this, we have l…",,,,,,Anecdotal,comment,,,,,,,,2019-07-08,github/Joseph-Irving,https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-509407371,repo: kubernetes/kubernetes | issue: Add container.lifecycle.type to api (sidecars KEP) | keyword: lesson learned
"Is there a summary of the discussion that we can share? I may have been part of it, but I don't remember it all :) I just want to buy some modicum of confidence that we don't regret this design later :) On Mon, Jul 8, 2019 at 2:58 PM Joseph Irving <notifications@github.com> wrote: > @rlenglet <https://github.com/rlenglet> I don't think this is the right > place to have this discussion, I'd rather keep PR comments about what's in > the PR. I think the enhancements issue kubernetes/enhancements#7…",,,,,,Anecdotal,comment,,,,,,,,2019-07-08,github/thockin,https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-509408922,repo: kubernetes/kubernetes | issue: Add container.lifecycle.type to api (sidecars KEP) | keyword: lesson learned
"@thockin, I have tried my best to find any comments relating to init (you were indeed involved). Is it a bit tricky when split between two repos and a mixture of prs/issues etc. As well as some of this discussion taking place at various sig meetings. Maybe I should write something up and add it to the KEP? My TLDR version: - there are some use cases for having sidecars during the init phase (mainly istio) - some sidecars require starting after init containers - a proposed solution is to allow `…",,,,,,Anecdotal,comment,,,,,,,,2019-07-08,github/Joseph-Irving,https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-509414500,repo: kubernetes/kubernetes | issue: Add container.lifecycle.type to api (sidecars KEP) | keyword: lesson learned
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/79649#"" title=""Author self-approved"">Joseph-Irving</a>* To complete the [pull request process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process), please assign **smarterclayton** You can assign the PR to them by writing `/assign @smarterclayton` in a comment when ready. The full list of commands accepted by this bot can b…",,,,,,Anecdotal,comment,,,,,,,,2019-07-23,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-514161229,repo: kubernetes/kubernetes | issue: Add container.lifecycle.type to api (sidecars KEP) | keyword: lesson learned
"Hey @thockin @smarterclayton, I don't know if either of you have had the time to look at this yet, please let me know if there's anything I should be doing!",,,,,,Anecdotal,comment,,,,,,,,2019-08-15,github/Joseph-Irving,https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-521729196,repo: kubernetes/kubernetes | issue: Add container.lifecycle.type to api (sidecars KEP) | keyword: lesson learned
"@Joseph-Irving: You must be a member of the [kubernetes/milestone-maintainers](https://github.com/orgs/kubernetes/teams/milestone-maintainers/members) GitHub team to set the milestone. If you believe you should be able to issue the /milestone command, please contact your and have them propose you as an additional delegate for this responsibility. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-525627837): >/milestone v1.17 Instructions for intera…",,,,,,Anecdotal,comment,,,,,,,,2019-08-28,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-525627841,repo: kubernetes/kubernetes | issue: Add container.lifecycle.type to api (sidecars KEP) | keyword: lesson learned
"> So the Sidecar containers must be running before any init container starts. Wow, as a user with workloads that currently have both init and sidecar containers, I would definitely not expect that, mostly because that's not how it happens right now...",,,,,,Anecdotal,comment,,,,,,,,2019-10-23,github/lavalamp,https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-545530693,repo: kubernetes/kubernetes | issue: Add container.lifecycle.type to api (sidecars KEP) | keyword: lesson learned
"I ack @rlenglet concerns. I don't know if the right design would be for sidecars to start before init or for there to be distinct init sidecars. Other than mesh proxies, do we have other examples we can draw on?",,,,,,Anecdotal,comment,,,,,,,,2019-10-23,github/thockin,https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-545599161,repo: kubernetes/kubernetes | issue: Add container.lifecycle.type to api (sidecars KEP) | keyword: lesson learned
"> I ack @rlenglet concerns. I don't know if the right design would be for sidecars to start before init or for there to be distinct init sidecars. Other than mesh proxies, do we have other examples we can draw on? Hi @thockin thanks for checking out the sidecar code review. Regarding feature readiness, we (Pinterest) and Lyft has our own sidecar impl running in large scale production and we’ve summarize lessons we learned on implement this. See thread https://github.com/kubernetes/kubernetes/pu…",,,,,,Anecdotal,comment,,,,,,,,2019-10-23,github/zhan849,https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-545664050,repo: kubernetes/kubernetes | issue: Add container.lifecycle.type to api (sidecars KEP) | keyword: lesson learned
"thank you very much for the review @thockin, I think I've addressed all your comments. In regards to the actual implementation, that is currently located in this PR https://github.com/kubernetes/kubernetes/pull/80744 which builds off the two commits from this PR. I thought it would be easier to review if they were separate PRs (perhaps not). So I can merge them together into one (or just close this one?), whatever is easiest for people.",,,,,,Anecdotal,comment,,,,,,,,2019-10-25,github/Joseph-Irving,https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-546394097,repo: kubernetes/kubernetes | issue: Add container.lifecycle.type to api (sidecars KEP) | keyword: lesson learned
"@Joseph-Irving, @thockin: This is Bug Triage for release 1.18. I would like to leave a friendly reminder that code freeze is in a few days (Thursday 5 March), so please take this into account. Thanks!",,,,,,Anecdotal,comment,,,,,,,,2020-03-02,github/smourapina,https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-593238902,repo: kubernetes/kubernetes | issue: Add container.lifecycle.type to api (sidecars KEP) | keyword: lesson learned
"@Joseph-Irving: The following tests **failed**, say `/retest` to rerun all failed tests: Test name | Commit | Details | Rerun command --- | --- | --- | --- pull-kubernetes-e2e-kind-ipv6 | ccba2e3a03cdeb731d0d653948d0d125904f9319 | [link](https://prow.k8s.io/view/gcs/kubernetes-jenkins/pr-logs/pull/79649/pull-kubernetes-e2e-kind-ipv6/1235486274463207424) | `/test pull-kubernetes-e2e-kind-ipv6` pull-kubernetes-e2e-gce | ccba2e3a03cdeb731d0d653948d0d125904f9319 | [link](https://prow.k8s.io/view/gc…",,,,,,Anecdotal,comment,,,,,,,,2020-03-05,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-595154440,repo: kubernetes/kubernetes | issue: Add container.lifecycle.type to api (sidecars KEP) | keyword: lesson learned
"api review has been completed and the commits from here are contained in this PR https://github.com/kubernetes/kubernetes/pull/80744, so I will close this one so that the api and implementation and api can be merged together",,,,,,Anecdotal,comment,,,,,,,,2020-03-06,github/Joseph-Irving,https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-595759269,repo: kubernetes/kubernetes | issue: Add container.lifecycle.type to api (sidecars KEP) | keyword: lesson learned
"@Joseph-Irving: PR needs rebase. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. </details>",,,,,,Anecdotal,comment,,,,,,,,2020-03-06,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/79649#issuecomment-595759315,repo: kubernetes/kubernetes | issue: Add container.lifecycle.type to api (sidecars KEP) | keyword: lesson learned
"reform function fetchKubeConfigWithTimeout for simplicity <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pu…",,,,,,Anecdotal,issue,,,,,,,,2019-09-18,github/beautytiger,https://github.com/kubernetes/kubernetes/pull/82822,repo: kubernetes/kubernetes | keyword: lesson learned | state: closed
"Hi @beautytiger. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once the…",,,,,,Anecdotal,comment,,,,,,,,2019-09-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/82822#issuecomment-532618511,repo: kubernetes/kubernetes | issue: reform function fetchKubeConfigWithTimeout for simplicity | keyword: lesson learned
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/82822#"" title=""Author self-approved"">beautytiger</a>* To complete the [pull request process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process), please assign **neolit123** You can assign the PR to them by writing `/assign @neolit123` in a comment when ready. The full list of commands accepted by this bot can be found [her…",,,,,,Anecdotal,comment,,,,,,,,2019-09-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/82822#issuecomment-532618916,repo: kubernetes/kubernetes | issue: reform function fetchKubeConfigWithTimeout for simplicity | keyword: lesson learned
"> > Close, my bad, lesson learned. > > hi, i was on vacation. why did you close this PR? @neolit123 I found the code is good, no need to change. Sorry to bother you.",,,,,,Anecdotal,comment,,,,,,,,2019-11-18,github/beautytiger,https://github.com/kubernetes/kubernetes/pull/82822#issuecomment-555077955,repo: kubernetes/kubernetes | issue: reform function fetchKubeConfigWithTimeout for simplicity | keyword: lesson learned
CRD AllowUnconditionalUpdate modify Kubenetes built-in resource objects StatefulSet、Deployment、DaemonSet ... of fields AllowUnconditionalUpdate default is true。 ``` // AllowUnconditionalUpdate is the default update policy for daemon set objects. func (daemonSetStrategy) AllowUnconditionalUpdate() bool { return true } ``` Why CRD resource of fields AllowUnconditionalUpdate default is false? ``` func (customResourceStrategy) AllowCreateOnUpdate() bool { return false } ``` **What would you like to…,,,,,,Anecdotal,issue,,,,,,,,2019-09-27,github/ZP-AlwaysWin,https://github.com/kubernetes/kubernetes/issues/83222,repo: kubernetes/kubernetes | keyword: lesson learned | state: closed
"@ZP-AlwaysWin: The label(s) `sig/wg/component-standard` cannot be applied. These labels are supported: `api-review, community/discussion, community/maintenance, community/question, cuj/build-train-deploy, cuj/multi-user, platform/aws, platform/azure, platform/gcp, platform/minikube, platform/other` <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/83222#issuecomment-535766293): >/kind feature >/sig wg/component-standard Instructions for interacting with me using PR…",,,,,,Anecdotal,comment,,,,,,,,2019-09-27,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/83222#issuecomment-535766297,repo: kubernetes/kubernetes | issue: CRD AllowUnconditionalUpdate modify | keyword: lesson learned
"@ZP-AlwaysWin: The label(s) `sig/component-standard` cannot be applied. These labels are supported: `api-review, community/discussion, community/maintenance, community/question, cuj/build-train-deploy, cuj/multi-user, platform/aws, platform/azure, platform/gcp, platform/minikube, platform/other` <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/83222#issuecomment-535766293): >/kind feature >/sig component-standard Instructions for interacting with me using PR comme…",,,,,,Anecdotal,comment,,,,,,,,2019-09-27,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/83222#issuecomment-535766444,repo: kubernetes/kubernetes | issue: CRD AllowUnconditionalUpdate modify | keyword: lesson learned
"/assign @deads2k hi @deads2k I looked at the source code, this part of the code is written by you, can you specify this parameter when creating CRD? Best Wishes",,,,,,Anecdotal,comment,,,,,,,,2019-09-29,github/ZP-AlwaysWin,https://github.com/kubernetes/kubernetes/issues/83222#issuecomment-536275365,repo: kubernetes/kubernetes | issue: CRD AllowUnconditionalUpdate modify | keyword: lesson learned
I don't know if we want to encourage `AllowUnconditionalUpdate` in our API. In fact it was proposed that we should disallow unconditional update. See related discussion: https://github.com/kubernetes/kubernetes/issues/21330 /unassign /sig architecture,,,,,,Anecdotal,comment,,,,,,,,2019-09-30,github/roycaihw,https://github.com/kubernetes/kubernetes/issues/83222#issuecomment-536784088,repo: kubernetes/kubernetes | issue: CRD AllowUnconditionalUpdate modify | keyword: lesson learned
@roycaihw Do we really need this to be discussed in sig-arch? can you please start a thread on the mailing list and/or line up an item in the next meeting. I think this might be just handled in api-machinery if not please do as mentioned before.,,,,,,Anecdotal,comment,,,,,,,,2019-10-30,github/dims,https://github.com/kubernetes/kubernetes/issues/83222#issuecomment-547923426,repo: kubernetes/kubernetes | issue: CRD AllowUnconditionalUpdate modify | keyword: lesson learned
"Custom resources bake in many of the lessons we've learned over the last five years building APIs, and allowing unconditional update is not a best practice. It makes it too easy to overwrite data unintentionally. You can use patch updates without a resourceVersion set to overwrite fields regardless of current content, if desired. /close",,,,,,Anecdotal,comment,,,,,,,,2019-11-13,github/liggitt,https://github.com/kubernetes/kubernetes/issues/83222#issuecomment-553408478,repo: kubernetes/kubernetes | issue: CRD AllowUnconditionalUpdate modify | keyword: lesson learned
"@liggitt: Closing this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/83222#issuecomment-553408478): >Custom resources bake in many of the lessons we've learned over the last five years building APIs, and allowing unconditional update is not a best practice. It makes it too easy to overwrite data unintentionally. You can use patch updates without a resourceVersion set to overwrite fields regardless of current content, if desired. > >/close Instructions fo…",,,,,,Anecdotal,comment,,,,,,,,2019-11-13,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/83222#issuecomment-553408492,repo: kubernetes/kubernetes | issue: CRD AllowUnconditionalUpdate modify | keyword: lesson learned
"Frequent churn between EndpointSlice objects ### What happened? We are running a single control-plane k3s cluster in our lab. The cluster has about 56 high-power bare metal nodes. The cluster is used primarily by our developers and our CICD. We create kubevirt VMs for dev and testing. Recently as our cluster grew to serve a 100+ kubevirt VMs (running in pods), we noticed significant uptick in the number of ssh and nslookup failures by our CICD system when trying to communicate with the VMs. On …",,,,,,Anecdotal,issue,,,,,,,,2025-08-11,github/Arvinderpal,https://github.com/kubernetes/kubernetes/issues/133474,repo: kubernetes/kubernetes | keyword: workaround | state: open
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-08-11,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/133474#issuecomment-3176989201,repo: kubernetes/kubernetes | issue: Frequent churn between EndpointSlice objects | keyword: workaround
"1.30 is out of support upstream: https://kubernetes.io/releases/ Is this reproducible on a supported version? We may have already fixed this. > Unfortunately, this is not easy to produce, primarily because the issue seems to arise when the control-plane is under load and is having a hard time sending updates to all informers. May be possible to replicate under synthetic load with a toy cluster?",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/133474#issuecomment-3177304067,repo: kubernetes/kubernetes | issue: Frequent churn between EndpointSlice objects | keyword: workaround
"We'll look into upgrading to 1.31. I did look through issues and commit history for endpointslice controller. Didn't see anything related, but I could be wrong. https://github.com/kubernetes/endpointslice",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/Arvinderpal,https://github.com/kubernetes/kubernetes/issues/133474#issuecomment-3180245906,repo: kubernetes/kubernetes | issue: Frequent churn between EndpointSlice objects | keyword: workaround
"I think Ben's comment is probably key: Can you reproduce this: * Current version of Kubernetes? One thing to note is that 1.31 is also quite old, which means that you may want to consider upgrading to a more recent version. * If the behavior is localized to EndpointSlice controller, a smaller repro may be possible with a Kind cluster or other minimal setup with a large number of endpoints (> 100). I know that we put explicit logic in the EndpointSlice controller for some amount of hysteresis to…",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/bowei,https://github.com/kubernetes/kubernetes/issues/133474#issuecomment-3189064083,repo: kubernetes/kubernetes | issue: Frequent churn between EndpointSlice objects | keyword: workaround
HPA controller reorders the spec.metrics list <!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!--> **What happened**: I have an HPA spec as follows. ```yaml apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: argocd-server spec: minReplicas: 1 maxReplicas: 2 metrics: - resource: name: cpu targetAverageUtilization: 76 type: Resource - resource: nam…,,,,,,Anecdotal,issue,,,,,,,,2019-02-15,github/jessesuen,https://github.com/kubernetes/kubernetes/issues/74099,repo: kubernetes/kubernetes | keyword: workaround | state: open
"Issues go stale after 90d of inactivity. Mark the issue as fresh with `/remove-lifecycle stale`. Stale issues rot after an additional 30d of inactivity and eventually close. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2019-05-26,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-495964398,repo: kubernetes/kubernetes | issue: HPA controller reorders the spec.metrics list | keyword: workaround
"Stale issues rot after 30d of inactivity. Mark the issue as fresh with `/remove-lifecycle rotten`. Rotten issues close after an additional 30d of inactivity. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /lifecycle rotten",,,,,,Anecdotal,comment,,,,,,,,2019-06-25,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-505270989,repo: kubernetes/kubernetes | issue: HPA controller reorders the spec.metrics list | keyword: workaround
"Rotten issues close after 30d of inactivity. Reopen the issue with `/reopen`. Mark the issue as fresh with `/remove-lifecycle rotten`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /close",,,,,,Anecdotal,comment,,,,,,,,2019-07-25,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-514892131,repo: kubernetes/kubernetes | issue: HPA controller reorders the spec.metrics list | keyword: workaround
"@fejta-bot: Closing this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-514892131): >Rotten issues close after 30d of inactivity. >Reopen the issue with `/reopen`. >Mark the issue as fresh with `/remove-lifecycle rotten`. > >Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). >/close Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contribu…",,,,,,Anecdotal,comment,,,,,,,,2019-07-25,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-514892157,repo: kubernetes/kubernetes | issue: HPA controller reorders the spec.metrics list | keyword: workaround
"@Kassadd: You can't reopen an issue/PR unless you authored it or you are a collaborator. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-573405540): >/reopen Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes…",,,,,,Anecdotal,comment,,,,,,,,2020-01-12,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-573405546,repo: kubernetes/kubernetes | issue: HPA controller reorders the spec.metrics list | keyword: workaround
"@chancez: Reopened this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-596815309): >/reopen > >I believe this still needs to be fixed Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issu…",,,,,,Anecdotal,comment,,,,,,,,2020-03-09,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-596815324,repo: kubernetes/kubernetes | issue: HPA controller reorders the spec.metrics list | keyword: workaround
"Rotten issues close after 30d of inactivity. Reopen the issue with `/reopen`. Mark the issue as fresh with `/remove-lifecycle rotten`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /close",,,,,,Anecdotal,comment,,,,,,,,2020-04-08,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-611234564,repo: kubernetes/kubernetes | issue: HPA controller reorders the spec.metrics list | keyword: workaround
"@fejta-bot: Closing this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-611234564): >Rotten issues close after 30d of inactivity. >Reopen the issue with `/reopen`. >Mark the issue as fresh with `/remove-lifecycle rotten`. > >Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). >/close Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contribu…",,,,,,Anecdotal,comment,,,,,,,,2020-04-08,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-611234678,repo: kubernetes/kubernetes | issue: HPA controller reorders the spec.metrics list | keyword: workaround
"@chancez: Reopened this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-611772253): >/reopen Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. </…",,,,,,Anecdotal,comment,,,,,,,,2020-04-09,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-611772814,repo: kubernetes/kubernetes | issue: HPA controller reorders the spec.metrics list | keyword: workaround
"Rotten issues close after 30d of inactivity. Reopen the issue with `/reopen`. Mark the issue as fresh with `/remove-lifecycle rotten`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /close",,,,,,Anecdotal,comment,,,,,,,,2020-05-09,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-626240791,repo: kubernetes/kubernetes | issue: HPA controller reorders the spec.metrics list | keyword: workaround
"@fejta-bot: Closing this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-626240791): >Rotten issues close after 30d of inactivity. >Reopen the issue with `/reopen`. >Mark the issue as fresh with `/remove-lifecycle rotten`. > >Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). >/close Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contribu…",,,,,,Anecdotal,comment,,,,,,,,2020-05-09,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-626240815,repo: kubernetes/kubernetes | issue: HPA controller reorders the spec.metrics list | keyword: workaround
"@daurnimator: You can't reopen an issue/PR unless you authored it or you are a collaborator. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-636563468): >/reopen Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubern…",,,,,,Anecdotal,comment,,,,,,,,2020-06-01,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-636563528,repo: kubernetes/kubernetes | issue: HPA controller reorders the spec.metrics list | keyword: workaround
"Not really a solution, but I'm ignoring it for now: ``` ignoreDifferences: - group: apps kind: Deployment jsonPointers: - /spec/replicas - group: autoscaling kind: HorizontalPodAutoscaler jsonPointers: - /spec/metrics ```",,,,,,Anecdotal,comment,,,,,,,,2020-06-25,github/test-account-0,https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-649495381,repo: kubernetes/kubernetes | issue: HPA controller reorders the spec.metrics list | keyword: workaround
"I've opened a [PR](https://github.com/helm/charts/pull/22626) to a the nginx helm chart we've using a few months ago. It fixed the problem for a while but now I see the metrics are reversed again. Was something changed in recent version? We have recently upgraded our clusters from v1.14 to v1.16, could this be related?",,,,,,Anecdotal,comment,,,,,,,,2020-08-18,github/yaron-idan,https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-675663171,repo: kubernetes/kubernetes | issue: HPA controller reorders the spec.metrics list | keyword: workaround
"Also seeing this issue in Kubernetes 1.17. A metrics list of cpu then memory gets re-ordered to memory then cpu, which causes our CI tools to detect a difference in the configuration and attempt to fix it (we're using Argo CD).",,,,,,Anecdotal,comment,,,,,,,,2020-08-25,github/jcmcken,https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-680057392,repo: kubernetes/kubernetes | issue: HPA controller reorders the spec.metrics list | keyword: workaround
"@abdennour: You can't reopen an issue/PR unless you authored it or you are a collaborator. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-809373162): >/reopen Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernet…",,,,,,Anecdotal,comment,,,,,,,,2021-03-29,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-809373254,repo: kubernetes/kubernetes | issue: HPA controller reorders the spec.metrics list | keyword: workaround
"I see questions about this being resolved or closed, is it possible that this can be confirmed as open/closed or an updated related issue linked. This issue is linked from other 3rd parties who are challenged with this issue (such as ArgoCD)",,,,,,Anecdotal,comment,,,,,,,,2021-04-21,github/sellers,https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-824126695,repo: kubernetes/kubernetes | issue: HPA controller reorders the spec.metrics list | keyword: workaround
"@jessesuen: Reopened this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-849234783): >/reopen Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. …",,,,,,Anecdotal,comment,,,,,,,,2021-05-27,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/74099#issuecomment-849234856,repo: kubernetes/kubernetes | issue: HPA controller reorders the spec.metrics list | keyword: workaround
HPA stuck at maxReplicas even though metric under target ### What happened? HPA does not reduce `Deployment` replica count even though resource metric is below target. It is stuck at `maxReplicas`. ### What did you expect to happen? Deployment replica count should be reduced. ### How can we reproduce it (as minimally and precisely as possible)? We can see multiple examples in our clusters but not sure how to reproduce it exactly. Here's some relevant `kubectl` output: ```console $ kubectl top p…,,,,,,Anecdotal,issue,,,,,,,,2023-09-25,github/max-rocket-internet,https://github.com/kubernetes/kubernetes/issues/120875,repo: kubernetes/kubernetes | keyword: workaround | state: open
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2023-09-25,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-1733937296,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
So is this the old classic HPA issue of the deployment selector labels matching pods outside of that deployment? e.g. `application-one-xxxx-ar-7df9868cbf-h4db8` in the above output?,,,,,,Anecdotal,comment,,,,,,,,2023-09-26,github/max-rocket-internet,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-1735087804,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"If it is caused by `spec.selector.matchLabels` labels of `Deployment/application-one-ar` matching pods outside of that deployment, then this is quite disappointing: - It's very poor UX to see the metric presented as way under the target but still `desiredReplicas=maxReplicas`. This makes no sense to a user. - This issue seems to be [over 4 years old](https://github.com/kubernetes/kubernetes/issues/78761), constant comments in that issue also - I think it's mentioned in the docs [here](https://g…",,,,,,Anecdotal,comment,,,,,,,,2023-09-26,github/max-rocket-internet,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-1735127941,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"@max-rocket-internet I've been following #78761 since I experienced the same problem before and I agree that this should be treated as a bug. While the selector conflict may be seen as a configuration bug on the part of the operator, and easily worked around by ensuring a minimal set of unique selectors between all `Deployment`s of a given namespace, e.g. I always include `app.kubernetes.io/component` so I can differentiate between several processes from the same application, the selection beha…",,,,,,Anecdotal,comment,,,,,,,,2023-09-26,github/rochacon,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-1736021400,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
What worked for me to resolve the issue with HPA not scaling down despite the fact the CPU/Memory utilization was below target was to remove `spec.replicas` from the deployment,,,,,,Anecdotal,comment,,,,,,,,2023-09-28,github/IgalSc,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-1740021027,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"Commenting on my previous statement - the moment the deployment scales up, the hpa does not scale it down until the `spec.replicas` is removed again",,,,,,Anecdotal,comment,,,,,,,,2023-09-29,github/IgalSc,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-1741089699,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
I solved my problem like this ``` behavior: scaleDown: stabilizationWindowSeconds: 0 policies: - type: Pods value: 1 periodSeconds: 60 selectPolicy: Min,,,,,,Anecdotal,comment,,,,,,,,2023-10-03,github/wpferreira01,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-1745529858,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"> Commenting on my previous statement - the moment the deployment scales up, the hpa does not scale it down until the spec.replicas is removed again So it's not resolved for you then.",,,,,,Anecdotal,comment,,,,,,,,2023-10-04,github/max-rocket-internet,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-1746466564,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
I get the same problem with a HPA scaling based on memory ``` apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: sessions-bus-autoscaler spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: sessions-bus maxReplicas: 3 minReplicas: 1 metrics: - type: Resource resource: name: memory target: type: Utilization averageUtilization: 90 behavior: scaleDown: stabilizationWindowSeconds: 0 policies: - type: Pods value: 1 periodSeconds: 60 scaleUp: stabilizationWindowSecon…,,,,,,Anecdotal,comment,,,,,,,,2023-10-10,github/BogdanGeorge,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-1754492256,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"To maybe help the conversation, I was investigating on that matter as well for the last 4 hours. I'm using minikube and I'm fairly new to k8s. For my case I have an app that scaled on CPU utilization only. It needed 5-10 minutes to scale back to minimum replicas but eventually it did.",,,,,,Anecdotal,comment,,,,,,,,2023-12-17,github/George-Spanos,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-1859175622,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"I am also having similar problem, but i am using custom metrics from prometheus adapter. After scaling to the max pods, it is not scaling down. Even though metric is current zero below the threshold defined in hpa. And I also have checked the label issue @max-rocket-internet was talking about. I don't have any other pods with the same labels. Here is my HPA spec, ``` apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: annotations: kubectl.kubernetes.io/last-applied-configuration:…",,,,,,Anecdotal,comment,,,,,,,,2024-01-28,github/sharadregoti,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-1913480515,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
We have the same problem on K8s 1.25 and using HPA autoscaling/v2. The OP is on 1.25 also. Is anyone on 1.26+ having the issue as well? The only solution we could find was to disambiguate our `matchLabels` by including another label so there is no possible substring match on label combination. The saddest part about this workaround is that `matchLabels` are immutable. So you have to delete the Deployment and suffer the downtime until the new Deployment has rolled out. Not only that you lose all…,,,,,,Anecdotal,comment,,,,,,,,2024-04-17,github/ccmcbeck,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-2062615449,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"I moved from k8s to EKS on AWS, with 3 clusters, one of them 1.28 and two 1.29 hpa autoscaling/v2, same behaviour on all three",,,,,,Anecdotal,comment,,,,,,,,2024-04-18,github/IgalSc,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-2062919986,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"> I moved from k8s to EKS on AWS, with 3 clusters, one of them 1.28 and two 1.29 Please reach out to AWS/EKS support @IgalSc",,,,,,Anecdotal,comment,,,,,,,,2024-04-18,github/dims,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-2063603859,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"@dims sorry, why do i need to reach to AWS/EKS support? We are talking about hpa autoscaling/v2 not scaling down. That's nothing to do with AWS or EKS",,,,,,Anecdotal,comment,,,,,,,,2024-04-18,github/IgalSc,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-2064487253,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"But, to be fair, AWS contributes a lot of code to the K8s upstream https://chat.openai.com/share/52255931-cf9a-4a60-a450-730b2bb10220. We will escalate this within AWS and report back.",,,,,,Anecdotal,comment,,,,,,,,2024-04-19,github/ccmcbeck,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-2066409729,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"I don't know if this is the case with AWS support... I'm using Azure and the same thing happens. Throughout the thread we have replication in minikube, I imagine it is something with kubernetes itself",,,,,,Anecdotal,comment,,,,,,,,2024-04-19,github/gsGabriel,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-2066742741,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"> I don't know if this is the case with AWS support... I'm using Azure and the same thing happens. Throughout the thread we have replication in minikube, I imagine it is something with kubernetes itself Roger that. Maybe we can ""prod"" AWS to submit a fix. Meanwhile, I guess we have to perform a workaround similar to what I am proposing in https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-2062615449",,,,,,Anecdotal,comment,,,,,,,,2024-04-21,github/ccmcbeck,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-2068099591,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"> The saddest part about this workaround is that `matchLabels` are immutable. So you have to delete the Deployment and suffer the downtime until the new Deployment has rolled out. Not only that you lose all your Helm history so a rollback is not possible. Bleh For ZERO downtime on Production I guess you can try 1. A second deployment with a different ""instance name"" 2. Migrate to that second deployment 3. Destroy the original deployment 4. Redeploy the original with new `matchLabels` 5. Migrate…",,,,,,Anecdotal,comment,,,,,,,,2024-04-21,github/ccmcbeck,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-2068107320,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2024-09-03,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-2325453703,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"/remove-lifecycle stale This is still observed in K8s v1.29 and 1.30. There are no ambiguous label selectors in deployments affected by this, so some of the earlier comments in this thread about label matching do not apply.",,,,,,Anecdotal,comment,,,,,,,,2024-09-16,github/awsitcloudpro,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-2353193189,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"Same issue here.. The pods are below 80%, but they are not scaling down. 80% of 6GiB is approximately 4915.2MB, which should trigger scaling up. However, since the pods are below this value, the HPA (Horizontal Pod Autoscaler) should be scaling down. <img width=""1427"" alt=""image"" src=""https://github.com/user-attachments/assets/b1e95e7b-c0cd-46bb-b9e3-71900bbfa9d4"">",,,,,,Anecdotal,comment,,,,,,,,2024-10-08,github/ariretiarno,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-2399979501,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"leave a trace here... got same issue with this, in GKE 1.28 the fix is to update the deployments `.spec.selector.matchLabels` as mentioned in the below comment > If it is caused by `spec.selector.matchLabels` labels of `Deployment/application-one-ar` matching pods outside of that deployment, then this is quite disappointing: > > * It's very poor UX to see the metric presented as way under the target but still `desiredReplicas=maxReplicas`. This makes no sense to a user. > * This issue seems to …",,,,,,Anecdotal,comment,,,,,,,,2024-11-19,github/kholisrag,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-2484657748,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"Anyone found a workaround or fix on this, I upgraded my kubernetes to v1.30 and deduct load testing on the environment then the autoscaler start to acting weird and keep scaling down and up on schedule even the metrics don't seem to be over the threshold. first of all I though it is a metrics server issue but I did upgrade it and tried to do scaling policies but without luck to fix the issue. Any update please?",,,,,,Anecdotal,comment,,,,,,,,2025-01-19,github/yahiya-ayoub,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-2601022786,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"> Anyone found a workaround or fix on this, I upgraded my kubernetes to v1.30 and deduct load testing on the environment then the autoscaler start to acting weird and keep scaling down and up on schedule even the metrics don't seem to be over the threshold. first of all I though it is a metrics server issue but I did upgrade it and tried to do scaling policies but without luck to fix the issue. Any update please? I did it using Keda autoscalling instead of default HPA",,,,,,Anecdotal,comment,,,,,,,,2025-01-19,github/gsGabriel,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-2601035476,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"Wow. Keda. Gracias, @gsGabriel Here is a YouTube video from the DevOps Toolkit about Keda https://youtu.be/3lcaawKAv6s?si=qWZ2as6AixzH_6EN",,,,,,Anecdotal,comment,,,,,,,,2025-01-20,github/ccmcbeck,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-2603271653,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"Thank you, @gsGabriel and @ccmcbeck, for the quick reply. I want to hint at the root cause of my issue here for anyone who may be stuck like me. One of my team members faced a problem while implementing readiness and liveness for our pods. They mistakenly set the deployment's replica field to the maximum value of the Horizontal Pod Autoscaler (HPA) but forgot to revert it to the original value. This oversight caused unusual behavior, as the HPA kept attempting to scale down the replica count. A…",,,,,,Anecdotal,comment,,,,,,,,2025-01-21,github/yahiya-ayoub,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-2604799188,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"Add simple ResourceVersion tracking in fake.Clientset Currently, the `fake.Clientset` does not maintain any ResourceVersions on the objects that are added via it's API functions. For example: ```go client := fake.NewClientset() namespaces := clientset.CoreV1().Namespaces() // Create a namespace via the API: ns := &corev1.Namespace{ ObjectMeta: metav1.ObjectMeta{ Name: ""test-namespace"", }, } _, _ := namespaces.Create(ctx, ns, metav1.CreateOptions{}) // Retrieve the namespace by name ns, _ = name…",,,,,,Anecdotal,issue,,,,,,,,2024-10-30,github/stippi2,https://github.com/kubernetes/kubernetes/issues/133557,repo: kubernetes/kubernetes | keyword: workaround | state: open
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2025-01-28,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/133557#issuecomment-3190560769,repo: kubernetes/kubernetes | issue: Add simple ResourceVersion tracking in fake.Clientset | keyword: workaround
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle rotten` - Close this issue with `/…",,,,,,Anecdotal,comment,,,,,,,,2025-02-27,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/133557#issuecomment-3190560771,repo: kubernetes/kubernetes | issue: Add simple ResourceVersion tracking in fake.Clientset | keyword: workaround
"Hi, @stippi @stippi2 could you share your implementation of watching the PatchAction? I am having trouble implementing it for my project as `PatchActionImpl` does not have a `GetObject()` method (while CreateActionImpl and UpdateActionImpl have this method) https://github.com/kubernetes/client-go/blob/387edb880f47ba8dcb85da09aa916d0c10fee37a/testing/actions.go#L728",,,,,,Anecdotal,comment,,,,,,,,2025-03-24,github/mostafaCamel,https://github.com/kubernetes/kubernetes/issues/133557#issuecomment-3190560777,repo: kubernetes/kubernetes | issue: Add simple ResourceVersion tracking in fake.Clientset | keyword: workaround
"> Hi, [@stippi](https://github.com/stippi) [@stippi2](https://github.com/stippi2) could you share your implementation of watching the PatchAction? I am having trouble implementing it for my project as `PatchActionImpl` does not have a `GetObject()` method (while CreateActionImpl and UpdateActionImpl have this method) Yes, [this](https://github.com/stippi2/kubernetes-replicator/blob/use-fake-client/replicate/secret/secrets_test.go#L99-L168) is how I got it working. Also note that I am returning …",,,,,,Anecdotal,comment,,,,,,,,2025-03-25,github/stippi2,https://github.com/kubernetes/kubernetes/issues/133557#issuecomment-3190560779,repo: kubernetes/kubernetes | issue: Add simple ResourceVersion tracking in fake.Clientset | keyword: workaround
Thank you! EDIT: in case Stippii2's branch gets deleted: here is his permalink https://github.com/stippi2/kubernetes-replicator/blob/3950d938fe84f13486a38414e0aa83fd8172c131/replicate/secret/secrets_test.go#L99-L168 and I attached his function as a text file as his workaround is crucial [Stippi2SetupFakeClientSet.txt](https://github.com/user-attachments/files/21417445/Stippi2SetupFakeClientSet.txt),,,,,,Anecdotal,comment,,,,,,,,2025-03-25,github/mostafaCamel,https://github.com/kubernetes/kubernetes/issues/133557#issuecomment-3190560781,repo: kubernetes/kubernetes | issue: Add simple ResourceVersion tracking in fake.Clientset | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2025-06-25,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/133557#issuecomment-3190560783,repo: kubernetes/kubernetes | issue: Add simple ResourceVersion tracking in fake.Clientset | keyword: workaround
"/remove-lifecycle stale /transfer kubernetes /sig api-machinery This is the wrong place to develop client-go, it is a read-only export. We are making this clearer by closing the issue tracker and moving old issues to the repo where it is developed, there will be some other updates to make this clearer as well. see: https://github.com/kubernetes/client-go/blob/master/CONTRIBUTING.md",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/133557#issuecomment-3190560786,repo: kubernetes/kubernetes | issue: Add simple ResourceVersion tracking in fake.Clientset | keyword: workaround
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/133557#issuecomment-3190560888,repo: kubernetes/kubernetes | issue: Add simple ResourceVersion tracking in fake.Clientset | keyword: workaround
"How to reinitialize client? Hello, this is more support question that a bug (i hope so). I'm using client-go in small app that calls cluster API and checks nodes in NotReady state (like cloud-provider code does). I've stuck with a problem that after pod (guest cluster kubernetes master vm) hard killed, i have dead tcp connection in my pod. Client-go tries to reuse it for next 10-17 minutes until TCP is dropped. To workaround this problem (and not mess with deep networking stuff), i've tried to …",,,,,,Anecdotal,issue,,,,,,,,2017-12-08,github/r7vme,https://github.com/kubernetes/kubernetes/issues/133556,repo: kubernetes/kubernetes | keyword: workaround | state: open
"Issues go stale after 90d of inactivity. Mark the issue as fresh with `/remove-lifecycle stale`. Stale issues rot after an additional 30d of inactivity and eventually close. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2018-03-08,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/133556#issuecomment-3190560472,repo: kubernetes/kubernetes | issue: How to reinitialize client? | keyword: workaround
"/transfer kubernetes /sig api-machinery This is the wrong place to develop client-go, it is a read-only export. We are making this clearer by closing the issue tracker and moving old issues to the repo where it is developed, there will be some other updates to make this clearer as well. see: https://github.com/kubernetes/client-go/blob/master/CONTRIBUTING.md",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/133556#issuecomment-3190560481,repo: kubernetes/kubernetes | issue: How to reinitialize client? | keyword: workaround
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/133556#issuecomment-3190560584,repo: kubernetes/kubernetes | issue: How to reinitialize client? | keyword: workaround
"drop automaxprocs hacks now that go 1.25 handles this built in <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if th…",,,,,,Anecdotal,issue,,,,,,,,2025-08-12,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/133492,repo: kubernetes/kubernetes | keyword: workaround | state: open
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.34` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.34.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133492#issuecomment-3181199814,repo: kubernetes/kubernetes | issue: drop automaxprocs hacks now that go 1.25 handles this built in | keyword: workaround
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133492#issuecomment-3181199957,repo: kubernetes/kubernetes | issue: drop automaxprocs hacks now that go 1.25 handles this built in | keyword: workaround
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133492#"" title=""Author self-approved"">BenTheElder</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/133492#pullrequestreview-3122597345"" title=""Approved"">ylink-lfs</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https…",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133492#issuecomment-3190372194,repo: kubernetes/kubernetes | issue: drop automaxprocs hacks now that go 1.25 handles this built in | keyword: workaround
"Remove the MD5 hash function for FIPS compliance ### What would you like to be added? For now, there seems to be a hardcoded usage of MD5 in the [source code](https://github.com/kubernetes/kubernetes/blob/master/pkg/api/v1/endpoints/util.go#L157), which is not FIPS compliant, and there is no configurable way to avoid it by declaring to use other hash functions like SHA256. When using the K8S with FIPS-compliant Go, `panic: openssl: unsupported hash function: 2` error is expected. It would be gr…",,,,,,Anecdotal,issue,,,,,,,,2025-01-15,github/jazzplato,https://github.com/kubernetes/kubernetes/issues/129652,repo: kubernetes/kubernetes | keyword: workaround | state: open
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-01-15,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-2593733794,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
"/sig network If you have specific scenarios where MD5's use has caused issues like (integration failures, and compliance errors), sharing those examples would help prioritize and shape the implementation.",,,,,,Anecdotal,comment,,,,,,,,2025-01-16,github/AmarNathChary,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-2594554347,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
Can we first explain in the issue what is the proposal? There is a long discussion about FIPS complain here https://github.com/kubernetes/kubernetes/issues/129075 and it seems better to tackle all these related issues in a single theme instead of individual PRs,,,,,,Anecdotal,comment,,,,,,,,2025-01-16,github/aojea,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-2595274252,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
"I think we can change the MD5 to pretty much any other hash function. It's not security-sensitive, but ACK that FIPS just blanket disallows MD5. The only risk I see here is that a different hash MIGHT change the results order, but it's been a LONG time since I looked at that code. If someone were to send a PR and proof that it doesn't break anyone (i.e. it won't cause instant churn when the cluster is upgraded, which I am pretty sure it won't) I would be happy to consider it. Since it is not se…",,,,,,Anecdotal,comment,,,,,,,,2025-01-16,github/thockin,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-2596398148,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
"This might be the first one that errors on building, but it's not the only md5 usage in the project. https://github.com/search?q=repo%3Akubernetes%2Fkubernetes%20md5.New()&type=code https://github.com/search?q=repo%3Akubernetes%2Fkubernetes%20crypto%2Fmd5&type=code",,,,,,Anecdotal,comment,,,,,,,,2025-01-16,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-2596929617,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
"Note that there are currently at least 6 places using md5, presumably removing only one is of limited utility. It seems like we'd need to remove all of them AND ensure that no new usage is added. We have tooling to mark ""unwanted imports"".",,,,,,Anecdotal,comment,,,,,,,,2025-04-15,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-2807627435,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
"> I think we can change the MD5 to pretty much any other hash function. It's not security-sensitive, but ACK that FIPS just blanket disallows MD5. > > The only risk I see here is that a different hash MIGHT change the results order, but it's been a LONG time since I looked at that code. > > If someone were to send a PR and proof that it doesn't break anyone (i.e. it won't cause instant churn when the cluster is upgraded, which I am pretty sure it won't) I would be happy to consider it. Since it…",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/redwrasse,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-3145208989,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
"For the specific case of Endpoints - I would craft an `Endpoints` YAML which is ill-formed on input (which will trigger the `RepackSubsets` logic), create an instance of it, and record the resulting object in the API. Do this with md5 (as is) and with your replacement and compare the resulting objects.",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/thockin,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-3145345159,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
"I am new here, trying to work on this issue :) > Note that there are currently at least 6 places using md5, presumably removing only one is of limited utility. It seems like we'd need to remove all of them AND ensure that no new usage is added. > > We have tooling to mark ""unwanted imports"". @BenTheElder Here we are using standard crypto/md5 library and do not think it is possible to mark as unwanted in `unwanted-dependencies.json` as the tooling is based on the output of `go mod graph` that do…",,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/harzallah,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-3182961914,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
"> I think we can change the MD5 to pretty much any other hash function. Probably. It may be somewhat disruptive to performance and or caching depending on the algorithm and usage. md5 is cheap. > It's not security-sensitive, but ACK that FIPS just blanket disallows MD5. I think it's not that simple and ultimately it's up to the distributor to certify their usage, libraries, etc. https://crypto.stackexchange.com/a/92075 The new go FIPS crypto stuff has a ""strict"" (now called ""only"") mode that ma…",,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-3186151215,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
"> @BenTheElder Here we are using standard crypto/md5 library and do not think it is possible to mark as unwanted in unwanted-dependencies.json as the tooling is based on the output of go mod graph that do not show standard libraries. Good point. This might not be the right tool for the job. In the meantime, @harzallah, want to take a look at the endpoints issue outlined above? https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-2596398148 https://github.com/kubernetes/kubernetes…",,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-3186196627,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
"Instead of rewriting unwanted-dependencies, we can use `forbidigo` for preventing new usage: https://github.com/kubernetes/kubernetes/pull/133511 While doing that, I noticed the kubeadm usage is extremely trivial and self-contained, so I migrated it. The endpoints usage still needs working through, as do the others. /sig architecture /area code-organization",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-3186234245,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
"BTW ""good first issue"" is a label we use with a higher bar for documenting exactly how to get started etc: https://github.com/kubernetes/kubernetes/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22good%20first%20issue%22 https://www.kubernetes.dev/docs/guide/help-wanted/ https://www.kubernetes.dev/docs/guide/ I think this doesn't meet that bar, e.g.: > If someone were to send a PR and proof that it doesn't break anyone (i.e. it won't cause instant churn when the cluster is upgraded, which I am p…",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-3186244125,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
"> I'm not sure if changing the pod UID bit would be non-disruptive ...? The actual UID is only internal to the kubelet... mirror pods have their UID stomped by the API server. The kubelet records the calculated UID in an annotation, but that's fine to have change between minor versions (pod annotation change doesn't break or restart or do anything disruptive)",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/liggitt,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-3189313920,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
"> For the specific case of Endpoints - I would craft an `Endpoints` YAML which is ill-formed on input (which will trigger the `RepackSubsets` logic), create an instance of it, and record the resulting object in the API. Do this with md5 (as is) and with your replacement and compare the resulting objects. I was expecting a much narrower tweak to switch the hasher to something fast like fnv and prove performance is equivalent with a local unit test benchmark.",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/liggitt,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-3189319486,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
"I would like to avoid forcing all Endpoints objects to be refreshed in some different order, if we can. It seems unlikely to cause problems (it's not really clearly ordered anyway) but ... Too paranoid for Jordan? Maybe that's a warning sign :)",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/thockin,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-3189548518,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
"oh, I missed the hash informs the slice sorting for what gets written to the API... ick. I thought it was just used for uniquing internally.",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/liggitt,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-3189591092,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
"`staging/src/k8s.io/kubectl/pkg/util/util.go` `HashObject` doesn't appear to be used by us: https://cs.k8s.io/?q=%5C.HashObject&i=nope&literal=nope&files=&excludeFiles=&repos= ... but it is exported The others, aside from the kubelet change (discussed above, should be fine in a new minor version) and the endpoints bit, are pretty obviously fine to switch. https://github.com/kubernetes/kubernetes/pull/133511 has preventing new usage via linter, switching the others was so trivial it didn't seem …",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-3189660560,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
"> How terrible would this be on minor version upgrade? Seems like we would only shuffle them when there was an incoming update anyhow? That's what I mean, it's PROBABLY not a hug deal, I am not even 100% sure it will happen, but we should check it.",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/thockin,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-3189748482,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
"imagePullPolicy to pull latest image OR use existing image if repository unavailable ### What would you like to be added? Add another option for a container's `imagePullPolicy`, which attempts to pull the image, and falls back to using a local cached copy if the image could not be pulled. ### Why is this needed? In development, images are frequently updated, so it's essential to grab the latest version of the image to test against. In production, single points of failure should be minimized, so…",,,,,,Anecdotal,issue,,,,,,,,2022-08-12,github/Alibirb,https://github.com/kubernetes/kubernetes/issues/111822,repo: kubernetes/kubernetes | keyword: workaround | state: open
"@Alibirb: This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related …",,,,,,Anecdotal,comment,,,,,,,,2022-08-12,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-1213307168,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
"In the best practice guide of my company's platform - user can use images' latest or v1 tag(that can change) with the Always pull policy in development env. - - so every CD or restart of Pod can download the latest image. - user should use a specific tag(that cannot change) of an image with `IfNotPresent` in production env. - - specific tags should not be updated in the image repository - - specific tags can ensure that the image behavior can be predicted (Even if they are in a different node, …",,,,,,Anecdotal,comment,,,,,,,,2022-08-15,github/pacoxu,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-1214592591,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
"Hi, I'm agree with @pacoxu The recommend for developers team is using tags. Kubernetes need to be a microservice and faster as posible. If anytime that you are creating the docker image the kubernetes need to pull the docker image will take some time. I'm my opinion is better to follow tag strategy. If you want to pull always just put the image pull policy as always. But from my perspective I don't see this request as a optimal improvement.",,,,,,Anecdotal,comment,,,,,,,,2022-08-15,github/JJotah,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-1215688529,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
"I had really hoped we could avoid turning this into an argument about the validity of using mutable tags, but here goes. According to [the Kubernetes website](https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/#what-kubernetes-is-not), ""Kubernetes aims to support an extremely diverse variety of workloads"". I recognize that not everyone would have any use for this new `imagePullPolicy`, or mutable tags, but that's okay; nobody is going to force you to use it. I know the typical reco…",,,,,,Anecdotal,comment,,,,,,,,2022-08-15,github/Alibirb,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-1215918166,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
"> * user should use a specific tag(that cannot change) of an image with `IfNotPresent` in production env. > * * specific tags should not be updated in the image repository > * * specific tags can ensure that the image behavior can be predicted (Even if they are in a different node, the image should be the same for a specific tag.) > * Even some users use [alwayspullimages](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages) admission control. They pr…",,,,,,Anecdotal,comment,,,,,,,,2022-08-15,github/Alibirb,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-1215979737,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
"It makes sense in your auto-upgrade cases. However, it sounds like the image pulling sometimes failed in your env. (Is this the key reason for adding this policy?) I'd like to know why there are registry failures. - `IfAvailable` is like `BestEffortAlways` that allow image pull failure. (It seems to be a strategy for edge env whose network is not stable.)",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/pacoxu,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-1216048799,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
"Hi, I'm sorry. I understand your point but this feature works for local environments to test the image. Dev environment, staging a production if you have some network issues you can't test the service or the image outside the cluster. Anyway understanding your point is better to add new functionality into image pull policy like imagePullPolicyconnectionError true. Or something like that.",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/JJotah,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-1216348759,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
"Our registry is fairly reliable, but everything has to be really fault-tolerant, because if something stops working in production, the users need to be able to either quickly fix it by rebooting (before the unavailability causes much bigger issues), or carry out their duties without it. So having our registry be a single point of failure for everything in the cluster is a tough sell, and would become even more difficult if any subsystem ever moves one of their safety-critical components into th…",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/Alibirb,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-1217260058,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
+1. I'd be interested in taking a stab at this potentially. Should be pretty easy to implement. Just don't want to do the work if the issue isn't signed off on.,,,,,,Anecdotal,comment,,,,,,,,2022-10-20,github/droctothorpe,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-1285726574,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
+1 this would be very handy in edge cases. With unstable networks `IfNotPresent` is my only option. It's a total pain to update all deployments with new tags for every release,,,,,,Anecdotal,comment,,,,,,,,2022-12-07,github/dwitzig,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-1340665247,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
"> Hi, I'm sorry. I understand your point but this feature works for local environments to test the image. Dev environment, staging a production if you have some network issues you can't test the service or the image outside the cluster. Anyway understanding your point is better to add new functionality into image pull policy like imagePullPolicyconnectionError true. Or something like that. (Sorry for the late response; somehow I didn't notice your comment before.) Just because the registry isn'…",,,,,,Anecdotal,comment,,,,,,,,2023-01-11,github/Alibirb,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-1378840474,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2023-04-11,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-1503560169,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
"I'd be interested in implementing this myself if we can reach some sort of tentative agreement to accept the feature. I'm honestly amazed Kubernetes has gone so long without this, as it seems like the ideal behavior for any use case where operational availability is a greater concern than potentially outdated images. /remove-lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2023-04-11,github/Alibirb,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-1503829029,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
@Alibirb You could propose the feature and get some more feedback at the [sig-node meeting](https://github.com/kubernetes/community/blob/master/sig-node/README.md) before actually implementing it.,,,,,,Anecdotal,comment,,,,,,,,2023-04-18,github/gjkim42,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-1512546894,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2023-07-17,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-1637532517,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
"Definitely agree with the operational downtime concern mentioned above. My understanding is that ImagePullPolicy: Always is part of the CIS benchmark hardening (and probably the STIG). However, if the registry is down or credentials are expired, and your container crashes and restarts (not a human initiated pod rescheduling event), that container will attempt to re-pull and fail, causing a downtime. So now we are faced with the decision of following K8s hardening OR ensuring we have zero downti…",,,,,,Anecdotal,comment,,,,,,,,2023-11-07,github/clayvan,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-1799385059,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
"> Am I missing something? Is there another solution out there? Looks like someone linked to this issue in kube-image-keeper, which is a proxy to locally cache images from remote registries. It doesn't support mutable tags but it sounds like they're working on it. If they get that working, it would provide *somewhat* of a workaround for this issue in some cases, by making the local caching proxy the point of failure instead of the remote registry. Whether that's a better or a worse point of fail…",,,,,,Anecdotal,comment,,,,,,,,2023-11-07,github/Alibirb,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-1800277335,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
"Another work-around in some cases might be to create a mutating webhook that replaces mutable tags and `Always` with the latest hash for that tag at the time of pod creation and changes the policy to `IfNotPresent`. This would achieve a similar effect as `IfAvailable`, but could still fail in some circumstances, e.g. if the webhook can reach the registry but the node the pod is assigned to can't (due to network issues, credentials, rate limiting, or poor timing), and the image for that hash is …",,,,,,Anecdotal,comment,,,,,,,,2023-11-07,github/Alibirb,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-1800307112,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2024-02-05,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-1928404145,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2024-05-06,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-2096179556,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
"Mostly, I just wanted some kind of agreement that this was something that might actually get accepted in before I devoted the time to making the change. I keep meaning to attend the SIG node meeting and bring it up, but then something comes up at work at the time.",,,,,,Anecdotal,comment,,,,,,,,2024-05-16,github/Alibirb,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-2115665118,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2024-11-06,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-2460858664,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
"feat: add image parsing library to CEL <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted…",,,,,,Anecdotal,issue,,,,,,,,2025-03-15,github/vishal-chdhry,https://github.com/kubernetes/kubernetes/pull/130834,repo: kubernetes/kubernetes | keyword: workaround | state: open
Welcome @vishal-chdhry! <br><br>It looks like this is your first PR to <a href='https://github.com/kubernetes/kubernetes'>kubernetes/kubernetes</a> 🎉. Please refer to our [pull request process documentation](https://git.k8s.io/community/contributors/guide/pull-requests.md) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands docu…,,,,,,Anecdotal,comment,,,,,,,,2025-03-15,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130834#issuecomment-2726146900,repo: kubernetes/kubernetes | issue: feat: add image parsing library to CEL | keyword: workaround
"Hi @vishal-chdhry. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once t…",,,,,,Anecdotal,comment,,,,,,,,2025-03-15,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130834#issuecomment-2726146905,repo: kubernetes/kubernetes | issue: feat: add image parsing library to CEL | keyword: workaround
"Thanks for the PR. To add a new library, please also - define the cost in https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/cel/library/cost.go - add tests in - https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/cel/library/cost_test.go - https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/cel/library/library_compatibility_test.go - https://github.com/kubernetes/kubernetes/blob/master/stagin…",,,,,,Anecdotal,comment,,,,,,,,2025-03-15,github/cici37,https://github.com/kubernetes/kubernetes/pull/130834#issuecomment-2726175424,repo: kubernetes/kubernetes | issue: feat: add image parsing library to CEL | keyword: workaround
"@cici37 Thank you so much for looking into this, I added all the mentioned tests. Regarding tests in [apiextensions-apiserver/pkg/apiserver/schema/cel/celcoststability_test.go](https://github.com/kubernetes/kubernetes/blame/master/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/celcoststability_test.go) , I have added the tests, but the test [`TestCelEstimatedCostStability`](https://github.com/vishal-chdhry/kubernetes/blob/4cbfe27305ab25a0f50a52b7092255c44bfd5d8b/staging/src…",,,,,,Anecdotal,comment,,,,,,,,2025-03-16,github/vishal-chdhry,https://github.com/kubernetes/kubernetes/pull/130834#issuecomment-2727307265,repo: kubernetes/kubernetes | issue: feat: add image parsing library to CEL | keyword: workaround
@vishal-chdhry Thanks for adding all those tests! The failure in [apiextensions-apiserver/pkg/apiserver/schema/cel/celcoststability_test.go](https://github.com/kubernetes/kubernetes/blame/master/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/celcoststability_test.go) is expected due to the way we introduce the new cel library. The new library will be able to be used in the next release(1.34) due to backward compatibility concern. It would be great if you could add the simil…,,,,,,Anecdotal,comment,,,,,,,,2025-03-17,github/cici37,https://github.com/kubernetes/kubernetes/pull/130834#issuecomment-2728261928,repo: kubernetes/kubernetes | issue: feat: add image parsing library to CEL | keyword: workaround
"/hold for CI job to pass. For the vendor updates, please run ./hack/update-vendor.sh for the new added dir and dependency",,,,,,Anecdotal,comment,,,,,,,,2025-03-17,github/cici37,https://github.com/kubernetes/kubernetes/pull/130834#issuecomment-2728268972,repo: kubernetes/kubernetes | issue: feat: add image parsing library to CEL | keyword: workaround
"@cici37 I have updated the implementation. I had to remove gcr's name package because it has multiple dependencies that are marked as unwanted in `hack/unwanted-dependencies.json` I have replaced it with `github.com/distribution/reference` which is already in use in the project. I have also removed the default registry behaviour, now `image(""nginx"").registry()` returns `""""` I can update that implementation with this a normalised reference, which will add `docker.io` as default registry and `lib…",,,,,,Anecdotal,comment,,,,,,,,2025-03-17,github/vishal-chdhry,https://github.com/kubernetes/kubernetes/pull/130834#issuecomment-2730229069,repo: kubernetes/kubernetes | issue: feat: add image parsing library to CEL | keyword: workaround
"Thanks for the work! My 2 cents: The current default behavior makes sense to me(image(""nginx"").registry() returns """"). I would prefer having an option provided for normalization like what we did for semver if we plan to support normalization. :)",,,,,,Anecdotal,comment,,,,,,,,2025-03-18,github/cici37,https://github.com/kubernetes/kubernetes/pull/130834#issuecomment-2731493815,repo: kubernetes/kubernetes | issue: feat: add image parsing library to CEL | keyword: workaround
Sorry for the confusion. My bad memory on when we use stored env drives me nuts... The changes should be ready to go. Thank you for your patience. /lgtm,,,,,,Anecdotal,comment,,,,,,,,2025-03-18,github/cici37,https://github.com/kubernetes/kubernetes/pull/130834#issuecomment-2734482654,repo: kubernetes/kubernetes | issue: feat: add image parsing library to CEL | keyword: workaround
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/130834#"" title=""Author self-approved"">vishal-chdhry</a>* **Once this PR has been reviewed and has the lgtm label**, please ask for approval from [liggitt](https://github.com/liggitt). For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted by…",,,,,,Anecdotal,comment,,,,,,,,2025-03-20,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130834#issuecomment-2739137486,repo: kubernetes/kubernetes | issue: feat: add image parsing library to CEL | keyword: workaround
"@cici37 @liggitt I pushed a commit to fix a comment and import hash algorithms: [`ddff560` (#130834)](https://github.com/kubernetes/kubernetes/pull/130834/commits/ddff560e4e2ebc2c110a2e25f2b1771ce310e924) This change is not required for converting the digest to string, but the package mentions that its needed for digest verification, if we choose to verify the digest in the future",,,,,,Anecdotal,comment,,,,,,,,2025-03-20,github/vishal-chdhry,https://github.com/kubernetes/kubernetes/pull/130834#issuecomment-2739141019,repo: kubernetes/kubernetes | issue: feat: add image parsing library to CEL | keyword: workaround
"Since the Code Freeze is in a couple hours, trying to understand how urgent this changes are. Is there urgent need for this library to be used sooner? To make it clear, if it makes it into 1.33, it will have to wait for 1.34 to use anyway. If the usage is not urgent, we could also wait after code freeze. Thanks",,,,,,Anecdotal,comment,,,,,,,,2025-03-20,github/cici37,https://github.com/kubernetes/kubernetes/pull/130834#issuecomment-2741436002,repo: kubernetes/kubernetes | issue: feat: add image parsing library to CEL | keyword: workaround
"> Related #124490 I'm trying to get oriented here... I don't see image spec parsing mentioned in https://github.com/kubernetes/kubernetes/issues/124490 ... is there a specific KEP or feature this is associated with? Image spec parsing is sort of a niche thing to add a CEL library for, I wasn't sure if this had been discussed elsewhere or if I was missing context.",,,,,,Anecdotal,comment,,,,,,,,2025-03-20,github/liggitt,https://github.com/kubernetes/kubernetes/pull/130834#issuecomment-2741461881,repo: kubernetes/kubernetes | issue: feat: add image parsing library to CEL | keyword: workaround
"In general, every CEL library / function we add fragments the Kubernetes ecosystem, since manifests that use that library / function will fail on older Kubernetes versions. We've typically tried to limit use of those to things which are ~impossible or extremely difficult to do with existing basic CEL validations (e.g. Kubernetes quantity parsing) or are very broadly applicable (URL / IP / CIDR parsing and intersection computation, etc). Before we get into the mechanics of adding a library / imp…",,,,,,Anecdotal,comment,,,,,,,,2025-03-20,github/liggitt,https://github.com/kubernetes/kubernetes/pull/130834#issuecomment-2741468075,repo: kubernetes/kubernetes | issue: feat: add image parsing library to CEL | keyword: workaround
"cc @jpbetz for visibility / input on whether a CEL library is justified here, or if an openapi format string would be a lighter-weight more preferred way to verify a given field is a valid image spec",,,,,,Anecdotal,comment,,,,,,,,2025-03-20,github/liggitt,https://github.com/kubernetes/kubernetes/pull/130834#issuecomment-2741497968,repo: kubernetes/kubernetes | issue: feat: add image parsing library to CEL | keyword: workaround
"I could see the image library used in more area other than ""verify a given field is a valid image spec"". e.g. the repository/tag/identifier/digest check could possibly used in policy enforcement area. We have similar use case for validation webhook before such like checking if the image is pulling from a specific org registry(It should have a workaround support with regex). I agree that cel library has a larger surface area than things like openapi format. Maybe a clear use case analysis would …",,,,,,Anecdotal,comment,,,,,,,,2025-03-20,github/cici37,https://github.com/kubernetes/kubernetes/pull/130834#issuecomment-2741507809,repo: kubernetes/kubernetes | issue: feat: add image parsing library to CEL | keyword: workaround
"We've been cautiously adopting support for formats where a plain OpenAPI format is insufficient for major use cases. Deconstruction (breaking apart a complex format and inspecting sub parts), and comparisons are good examples of operations beyond format validation that have motivated recent additions. Some examples: IP and CIDR (API gateway use case in particular), semver and quantity (DRA use cases). Because image is an important format in Kubernetes, and because it has a complex structure whe…",,,,,,Anecdotal,comment,,,,,,,,2025-03-21,github/jpbetz,https://github.com/kubernetes/kubernetes/pull/130834#issuecomment-2742021011,repo: kubernetes/kubernetes | issue: feat: add image parsing library to CEL | keyword: workaround
"> Image spec parsing is sort of a niche thing to add a CEL library for Hi @liggitt, I agree with joe here, Since images are integral to kubernetes, having an image parsing library makes sense, Some of the common usecases for this library in VAPs are: 1. verifying that container images are from a specific registry/tag etc 2. ensuring that images contain digest For MAPs 1. Replacing registries for images 2. Adding proxy registry prefixes to images from a specific repo This library is helpful beca…",,,,,,Anecdotal,comment,,,,,,,,2025-03-21,github/vishal-chdhry,https://github.com/kubernetes/kubernetes/pull/130834#issuecomment-2743797414,repo: kubernetes/kubernetes | issue: feat: add image parsing library to CEL | keyword: workaround
"> Since the Code Freeze is in a couple hours, trying to understand how urgent this changes are. Is there urgent need for this library to be used sooner? Has the code freeze happened? We wanted to use this as a library in our CEL environment",,,,,,Anecdotal,comment,,,,,,,,2025-03-22,github/vishal-chdhry,https://github.com/kubernetes/kubernetes/pull/130834#issuecomment-2744935651,repo: kubernetes/kubernetes | issue: feat: add image parsing library to CEL | keyword: workaround
"Writable cgroup for unprivileged containers ### What happened? Except for the case when the `privileged` field is enabled in a container `securityContext`, `cgroup` fs is mounted in `/sys/fs/cgroup` as read only. While this behavior is desirable for most workloads, it simply prevents further/nested containerization (e.g. docker/podman in kubernetes) inside kubernetes pods unless insecure and/or hairy and hacky ways are applied to expose writable cgroup inside an unprivileged container. As discu…",,,,,,Anecdotal,issue,,,,,,,,2023-10-12,github/acromc,https://github.com/kubernetes/kubernetes/issues/121190,repo: kubernetes/kubernetes | keyword: workaround | state: open
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2023-10-12,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/121190#issuecomment-1760142531,repo: kubernetes/kubernetes | issue: Writable cgroup for unprivileged containers | keyword: workaround
"Another use case for writable cgroup fs mount in the container is to be able to use `/sys/fs/cgroup/memory/cgroup.event_control` interface for getting memory levels / pressure notifications, which requires that file to be writable",,,,,,Anecdotal,comment,,,,,,,,2023-10-17,github/igor-anferov,https://github.com/kubernetes/kubernetes/issues/121190#issuecomment-1766807866,repo: kubernetes/kubernetes | issue: Writable cgroup for unprivileged containers | keyword: workaround
have you tried running cri-o? we have an experimental annotation `io.kubernetes.cri-o.cgroup2-mount-hierarchy-rw` which should do this,,,,,,Anecdotal,comment,,,,,,,,2023-10-19,github/haircommander,https://github.com/kubernetes/kubernetes/issues/121190#issuecomment-1771494063,repo: kubernetes/kubernetes | issue: Writable cgroup for unprivileged containers | keyword: workaround
"@haircommander I have no control over the runtime. It could be containerd for example. I was hoping for a more generic solution, of course if it's permitted by the current CRI API, something like `mask` and `unmask` options in Podman and Docker which you can use with the `--security-opt` flag. Exposing this via the Pod spec would be ideal in order to not have this issue with every masked path by default and not just `/sys/fs/cgroup`.",,,,,,Anecdotal,comment,,,,,,,,2023-10-20,github/acromc,https://github.com/kubernetes/kubernetes/issues/121190#issuecomment-1772930850,repo: kubernetes/kubernetes | issue: Writable cgroup for unprivileged containers | keyword: workaround
"@dgl and I were chatting about this yesterday. we're thinking of starting with an annotation cri-o will interpret to start, but in the 1.30 cycle could attempt a similar thing as ProcMountType for /sys as well",,,,,,Anecdotal,comment,,,,,,,,2023-10-20,github/haircommander,https://github.com/kubernetes/kubernetes/issues/121190#issuecomment-1773050833,repo: kubernetes/kubernetes | issue: Writable cgroup for unprivileged containers | keyword: workaround
@haircommander To be quit honest the `procMount` field is not a very good example of how it should be exposed. If I am not being wrong it's either `default` or `none` string enum and you can't even tell which paths are being masked or which are read-only unless you have look into the source code and such paths could change over time. I guess having a unified option of `maskedPaths` and `unmaskedPaths` string arrays that can override the default paths is much clearer like it's done in podman and…,,,,,,Anecdotal,comment,,,,,,,,2023-10-20,github/acromc,https://github.com/kubernetes/kubernetes/issues/121190#issuecomment-1773164439,repo: kubernetes/kubernetes | issue: Writable cgroup for unprivileged containers | keyword: workaround
> have you tried running cri-o? we have an experimental annotation `io.kubernetes.cri-o.cgroup2-mount-hierarchy-rw` which should do this Do we have the similar annotation for containerd ?,,,,,,Anecdotal,comment,,,,,,,,2023-11-14,github/kghost,https://github.com/kubernetes/kubernetes/issues/121190#issuecomment-1810189090,repo: kubernetes/kubernetes | issue: Writable cgroup for unprivileged containers | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2024-02-12,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/121190#issuecomment-1938734635,repo: kubernetes/kubernetes | issue: Writable cgroup for unprivileged containers | keyword: workaround
"As a workaround, I wrote a small init program to umount cgroup ``` #include <unistd.h> #include <stdio.h> #include <sys/mount.h> int main() { if (umount(""/sys/fs/cgroup"") < 0) { perror(""Unable to umount /sys/fs/cgroup""); return -1; } const char * prog = ""/lib/systemd/systemd""; char * args[] = {""/lib/systemd/systemd"", nullptr}; if (execv(prog, args) < 0) { perror(""Unable to exec /lib/systemd/systemd""); return -1; } return 0; } ```",,,,,,Anecdotal,comment,,,,,,,,2024-06-18,github/kghost,https://github.com/kubernetes/kubernetes/issues/121190#issuecomment-2175954709,repo: kubernetes/kubernetes | issue: Writable cgroup for unprivileged containers | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2024-10-22,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/121190#issuecomment-2429060793,repo: kubernetes/kubernetes | issue: Writable cgroup for unprivileged containers | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2025-01-23,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/121190#issuecomment-2608975832,repo: kubernetes/kubernetes | issue: Writable cgroup for unprivileged containers | keyword: workaround
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle rotten` - Close this issue with `/…",,,,,,Anecdotal,comment,,,,,,,,2025-02-22,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/121190#issuecomment-2676067844,repo: kubernetes/kubernetes | issue: Writable cgroup for unprivileged containers | keyword: workaround
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Reopen this issue with `/reopen` - Mark this issue as fresh with `/remove-lifecycle ro…",,,,,,Anecdotal,comment,,,,,,,,2025-03-24,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/121190#issuecomment-2747183150,repo: kubernetes/kubernetes | issue: Writable cgroup for unprivileged containers | keyword: workaround
"@k8s-triage-robot: Closing this issue, marking it as ""Not Planned"". <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/121190#issuecomment-2747183150): >The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. > >This bot triages issues according to the following rules: >- After 90d of inactivity, `lifecycle/stale` is applied >- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is ap…",,,,,,Anecdotal,comment,,,,,,,,2025-03-24,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/121190#issuecomment-2747183348,repo: kubernetes/kubernetes | issue: Writable cgroup for unprivileged containers | keyword: workaround
cc/ @chrishenzie for vis. More usecases for the writable cgroups. Also please read: https://github.com/containerd/containerd/issues/10924,,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/dchen1107,https://github.com/kubernetes/kubernetes/issues/121190#issuecomment-3156025881,repo: kubernetes/kubernetes | issue: Writable cgroup for unprivileged containers | keyword: workaround
"Another use case: In an app I work on we need to start external commands in the container for some tasks and those external commands can use an unpredictable amount of memory. One solution would be to enable `singleProcessOOMKill` and manage cgroups within the container to limit the memory use of such external commands. (A little off topic: But that also shows that there should be a method to set `singleProcessOOMKill` (which only modifies memory.oom.group in the pod cgroup) per POD, dunno why …",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/typetetris,https://github.com/kubernetes/kubernetes/issues/121190#issuecomment-3167271661,repo: kubernetes/kubernetes | issue: Writable cgroup for unprivileged containers | keyword: workaround
"@Divya063 wooo! Thank you. I'm not familiar with KEPs, is there any way we can throw support behind that and encourage traction?",,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/jeanbza,https://github.com/kubernetes/kubernetes/issues/121190#issuecomment-3184697350,repo: kubernetes/kubernetes | issue: Writable cgroup for unprivileged containers | keyword: workaround
"@Divya063 Thank you for starting this! I'm very happy to see that this is being worked on. I had also started looking into what it would take to add support for writable cgroups and I'm glad to see that you are owning this. I'd be happy to help in any way that I can, whether it's providing feedback on the design or helping with the implementation. Please feel free to loop me in on any design reviews.",,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/chrishenzie,https://github.com/kubernetes/kubernetes/issues/121190#issuecomment-3185952607,repo: kubernetes/kubernetes | issue: Writable cgroup for unprivileged containers | keyword: workaround
"Is sharing GPU to multiple containers feasible? <!-- This form is for bug reports and feature requests ONLY! If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/). --> **Is this a BUG REPORT or FEATURE REQUEST?**: feature request /kind feature **What happened**: As far, we do not support sharing GPU to multiple containers, one GPU can on…",,,,,,Anecdotal,issue,,,,,,,,2017-09-20,github/tianshapjq,https://github.com/kubernetes/kubernetes/issues/52757,repo: kubernetes/kubernetes | keyword: workaround | state: open
"/sig node until we have a wg-resource-management label From @flx42: > By default, kernels from different processes can't run on one GPU simultaneously (concurrency but not parallelism), they are time sliced. The Pascal architecture brings instruction-level preemption instead of block-level preemption, but context switches are not free. > Also, there is no way of partitioning GPU resources (SMs, memory), or even assignin priorities when sharing a card. You also have [MPS](https://docs.nvidia.com…",,,,,,Anecdotal,comment,,,,,,,,2017-09-20,github/RenaudWasTaken,https://github.com/kubernetes/kubernetes/issues/52757#issuecomment-330766518,repo: kubernetes/kubernetes | issue: Is sharing GPU to multiple containers feasible? | keyword: workaround
"@RenaudWasTaken thanks! But another question, where to place the GPU enablement code if we separate the GPU from kubelet? Seems it's not appropriate to place the GPU code in the vendor pkg anymore, do we have to create a new repo related to kubernetes?",,,,,,Anecdotal,comment,,,,,,,,2017-09-20,github/tianshapjq,https://github.com/kubernetes/kubernetes/issues/52757#issuecomment-330773087,repo: kubernetes/kubernetes | issue: Is sharing GPU to multiple containers feasible? | keyword: workaround
@tianshapjq see the device plugin design document for 1.8 which is how we plan to support GPUs in the future: https://github.com/kubernetes/community/pull/695/,,,,,,Anecdotal,comment,,,,,,,,2017-09-20,github/RenaudWasTaken,https://github.com/kubernetes/kubernetes/issues/52757#issuecomment-330773465,repo: kubernetes/kubernetes | issue: Is sharing GPU to multiple containers feasible? | keyword: workaround
Sharing GPUs is out of scope for the foreseeable future (at-least until v1.11). Our current focus is to get gpus per container working in production.,,,,,,Anecdotal,comment,,,,,,,,2017-10-05,github/vishh,https://github.com/kubernetes/kubernetes/issues/52757#issuecomment-334533604,repo: kubernetes/kubernetes | issue: Is sharing GPU to multiple containers feasible? | keyword: workaround
@flx42 Did you mean sharing a single GPU between different containers belonging to the same pod? What isolation do your users/customers expect in such scenarios?,,,,,,Anecdotal,comment,,,,,,,,2018-02-28,github/rohitagarwal003,https://github.com/kubernetes/kubernetes/issues/52757#issuecomment-369085362,repo: kubernetes/kubernetes | issue: Is sharing GPU to multiple containers feasible? | keyword: workaround
"@flx42 yes, seems the isolation is the blocker at this moment. GPU doesn't support secure isolation in production-grade, which would cause fatal damage if we simply assign one gpu to multiple containers IMO. If any news about gpu isolation please let me know :)",,,,,,Anecdotal,comment,,,,,,,,2018-02-28,github/tianshapjq,https://github.com/kubernetes/kubernetes/issues/52757#issuecomment-369088547,repo: kubernetes/kubernetes | issue: Is sharing GPU to multiple containers feasible? | keyword: workaround
">FWIW, I'm seeing more and more users/customers asking for a way to share a single GPU across a pod. I'm one of the many users.",,,,,,Anecdotal,comment,,,,,,,,2018-02-28,github/WIZARD-CXY,https://github.com/kubernetes/kubernetes/issues/52757#issuecomment-369093866,repo: kubernetes/kubernetes | issue: Is sharing GPU to multiple containers feasible? | keyword: workaround
"@flx42 @mindprince BTW, as device plugin has been counted into extended resource, is that sharing would not be acceptable at present?",,,,,,Anecdotal,comment,,,,,,,,2018-02-28,github/tianshapjq,https://github.com/kubernetes/kubernetes/issues/52757#issuecomment-369094806,repo: kubernetes/kubernetes | issue: Is sharing GPU to multiple containers feasible? | keyword: workaround
OpenStack is now supporting **Intel GVT-g** and **nVIDIA vGPU** technology to share GPUs among guest vms. But is it possible for k8s to manage kernel modules and do the same work? Maybe it needs support from Intel and nVIDIA.,,,,,,Anecdotal,comment,,,,,,,,2018-06-12,github/cruse123,https://github.com/kubernetes/kubernetes/issues/52757#issuecomment-396540192,repo: kubernetes/kubernetes | issue: Is sharing GPU to multiple containers feasible? | keyword: workaround
"hi guys, any further updates with this user story/case? we have more than 30 services need share GPUs in dev(qa / beta) ENV 😭😭",,,,,,Anecdotal,comment,,,,,,,,2018-07-03,github/ericjee,https://github.com/kubernetes/kubernetes/issues/52757#issuecomment-402083857,repo: kubernetes/kubernetes | issue: Is sharing GPU to multiple containers feasible? | keyword: workaround
"Can you post the workloads for which you intend to share GPUs? How do you plan on handling gpu memory requirements? Is it ok to over provision GPU memory? On Tue, Jul 3, 2018, 2:50 AM ericshen <notifications@github.com> wrote: > hi guys, any further update with this user story/case? we have more than > 30 services need share GPUs in dev(qa / beta) ENV 😭😭 > > — > You are receiving this because you were assigned. > Reply to this email directly, view it on GitHub > <https://github.com/kubernetes/k…",,,,,,Anecdotal,comment,,,,,,,,2018-07-03,github/vishh,https://github.com/kubernetes/kubernetes/issues/52757#issuecomment-402182250,repo: kubernetes/kubernetes | issue: Is sharing GPU to multiple containers feasible? | keyword: workaround
@vishh thx for reply:) ``` | NVIDIA-SMI 375.39 Driver Version: 375.39 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... On | 0000:00:08.0 Off | 0 | | N/A 34C P0 30W / 250W | 13691MiB / 16276MiB | 0% Default | +---------------------…,,,,,,Anecdotal,comment,,,,,,,,2018-07-03,github/ericjee,https://github.com/kubernetes/kubernetes/issues/52757#issuecomment-402209689,repo: kubernetes/kubernetes | issue: Is sharing GPU to multiple containers feasible? | keyword: workaround
"@ericjee So your workload is tensorflow. Is it training or inference? Do you care about just memory or GPU core isolation as well? I assume you are aware that TF support for unified memory is [still experimental](https://github.com/tensorflow/tensorflow/commit/b1139814). Also, Nvidia does not support memory isolation yet in their GPU stack. Also, it is not clear yet if MPS can and should be used for sharing. When it comes to inference, TF Serving can already handle bin packing models so k8s lev…",,,,,,Anecdotal,comment,,,,,,,,2018-07-03,github/vishh,https://github.com/kubernetes/kubernetes/issues/52757#issuecomment-402278985,repo: kubernetes/kubernetes | issue: Is sharing GPU to multiple containers feasible? | keyword: workaround
"Hey guys, we have the use-case that we want to run multiple inference models (with low request rate) on a single GPU. While inferences with large request income should be assigned to dedicated GPUs. We have investigated: While it is possible to share one GPU among multiple containers (using nvidia-docker2), it is not possible to do this on K8s (using Nvidia Device Plugin, nvidia-docker2 etc. pre-delivered by https://github.com/NVIDIA/kubernetes/, deployed on GCP). K8s returns the error that `1 …",,,,,,Anecdotal,comment,,,,,,,,2018-07-05,github/samedguener,https://github.com/kubernetes/kubernetes/issues/52757#issuecomment-402636344,repo: kubernetes/kubernetes | issue: Is sharing GPU to multiple containers feasible? | keyword: workaround
"Have you looked into serving multiple models with tensorflow serving - https://stackoverflow.com/questions/45749024/how-can-i-use-tensorflow-serving-for-multiple-models On Thu, Jul 5, 2018, 1:53 AM Samed Güner <notifications@github.com> wrote: > Hey guys, > > we have the use-case that we want to run multiple inference models (with > low request rate) on a single GPU. While inferences with large request > income should be assigned to dedicated GPUs. > > We have investigated: While it is possible…",,,,,,Anecdotal,comment,,,,,,,,2018-07-05,github/vishh,https://github.com/kubernetes/kubernetes/issues/52757#issuecomment-402758019,repo: kubernetes/kubernetes | issue: Is sharing GPU to multiple containers feasible? | keyword: workaround
"Support basic arithmetic operations for environment variable value ### What would you like to be added? K8s already support to derive environment variable from existing one by [`$()`](https://kubernetes.io/docs/tasks/inject-data-application/define-interdependent-environment-variables/), currently it supports string substitution only, it's not enough if derived environment variable need some arithmetic operations. For example: ```yaml env: - name: POD_INDEX valueFrom: fieldRef: fieldPath: metada…",,,,,,Anecdotal,issue,,,,,,,,2025-02-26,github/quaff,https://github.com/kubernetes/kubernetes/issues/130435,repo: kubernetes/kubernetes | keyword: workaround | state: open
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-02-26,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/130435#issuecomment-2683870553,repo: kubernetes/kubernetes | issue: Support basic arithmetic operations for environment variable value | keyword: workaround
"> Why not do this in a shell script in your image? > > I don't think we should attempt to partially implement sh. The image is not under my control, I tried `lifecycle.postStart` it doesn't work. ```yaml lifecycle: postStart: exec: command: [""/bin/sh"", ""-c"", ""export WORKER_GROUP=$((POD_INDEX%3))""] ``` Is there any other workaround?",,,,,,Anecdotal,comment,,,,,,,,2025-02-27,github/quaff,https://github.com/kubernetes/kubernetes/issues/130435#issuecomment-2686538334,repo: kubernetes/kubernetes | issue: Support basic arithmetic operations for environment variable value | keyword: workaround
"It would also be good for resourceRef variables, for something like: ```yaml env: - name: CPU_LIMITS valueFrom: resourceFieldRef: resource: limits.cpu - name: GOMAXPROCS value: ""$(CPU_LIMITS > 1? CPU_LIMITS : 2)"" ```",,,,,,Anecdotal,comment,,,,,,,,2025-05-14,github/m-messiah,https://github.com/kubernetes/kubernetes/issues/130435#issuecomment-2879468202,repo: kubernetes/kubernetes | issue: Support basic arithmetic operations for environment variable value | keyword: workaround
"> The image is not under my control, I tried lifecycle.postStart it doesn't work. You could always extend it (FROM $image ...), or mutate the pod. I don't think we're adding computations to pod variables, but if we did it would require a KEP. > It would also be good for resourceRef variables, for something like: We've been talking to the go team about GOMAXPROCS and there is a proposal out to handle this more directly: https://github.com/golang/go/issues/73193 With in place pod resize becoming …",,,,,,Anecdotal,comment,,,,,,,,2025-05-14,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/130435#issuecomment-2880471097,repo: kubernetes/kubernetes | issue: Support basic arithmetic operations for environment variable value | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/130435#issuecomment-3179782046,repo: kubernetes/kubernetes | issue: Support basic arithmetic operations for environment variable value | keyword: workaround
"@m-messiah re: https://github.com/kubernetes/kubernetes/issues/130435#issuecomment-2879468202 > We've been talking to the go team about GOMAXPROCS and there is a proposal out to handle this more directly: https://github.com/golang/go/issues/73193 Go has a fix for this released today with go 1.25, and we're discussing how to approach this in https://github.com/kubernetes/website/issues/51594 https://go.dev/doc/go1.25#container-aware-gomaxprocs You shouldn't need to do this sort of computation fo…",,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/130435#issuecomment-3181787920,repo: kubernetes/kubernetes | issue: Support basic arithmetic operations for environment variable value | keyword: workaround
"> cc [@kubernetes/sig-apps-leads](https://github.com/orgs/kubernetes/teams/sig-apps-leads) re: feature request Actually, that part is handled by the kubelet, when injecting values into containers. None of our controllers is modifying anything under the podTemplateSpec. So moving this accordingly. /remove-sig apps /sig node cc @kubernetes/sig-node-proposals",,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/soltysh,https://github.com/kubernetes/kubernetes/issues/130435#issuecomment-3182916674,repo: kubernetes/kubernetes | issue: Support basic arithmetic operations for environment variable value | keyword: workaround
With this KEP: https://github.com/kubernetes/enhancements/issues/3721 you can have an init container that will do whatever calculation for env variables you wish to do. My personal opinion is that there are too many tiny use cases for each new operator we may come up with. And I don't think developing this language for variables substitutions is what we need to be doing. Enabling any calculation using init container sounds like a universal solution. I can be convinced otherwise if there is a us…,,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/SergeyKanzhelev,https://github.com/kubernetes/kubernetes/issues/130435#issuecomment-3184604112,repo: kubernetes/kubernetes | issue: Support basic arithmetic operations for environment variable value | keyword: workaround
"failed to delete cgroup paths ### What happened? Pods stuck in terminating state lots of log entries like this one ``` journalctl -u kubelet --since -1m -f | grep ""failed to delete cgroup paths"" Feb 28 05:57:26 worker kubelet[592400]: E0228 05:57:26.352096 592400 pod_workers.go:1300] ""Error syncing pod, skipping"" err=""failed to delete cgroup paths for [kubepods burstable podc5659845-8967-408a-aa34-1223384f0ade] : unable to destroy cgroup paths for cgroup [kubepods burstable podc5659845-8967-408…",,,,,,Anecdotal,issue,,,,,,,,2024-03-06,github/vitality411,https://github.com/kubernetes/kubernetes/issues/123766,repo: kubernetes/kubernetes | keyword: workaround | state: open
@harche I don't have the kubelet logs from that run anymore. Attached you will find a current problem. This is just the last 10 minutes of kubelet logs. syslog since last rotation 7h ago is already 1.2G. [journal.gz](https://github.com/kubernetes/kubernetes/files/14597445/journal.gz),,,,,,Anecdotal,comment,,,,,,,,2024-03-14,github/vitality411,https://github.com/kubernetes/kubernetes/issues/123766#issuecomment-1996583329,repo: kubernetes/kubernetes | issue: failed to delete cgroup paths | keyword: workaround
"I am not sure this was fixed, but as a note you're running a version of kubernetes that's out of upstream support. Would you mind upgrading to 1.27 (or better 1.28, as 1.30 is coming soon) and verify you still are hitting it there?",,,,,,Anecdotal,comment,,,,,,,,2024-03-29,github/haircommander,https://github.com/kubernetes/kubernetes/issues/123766#issuecomment-2027454041,repo: kubernetes/kubernetes | issue: failed to delete cgroup paths | keyword: workaround
"I think we'll need logs from around when the issue happened. Since you can reproduce, can you get us logs from when the issue is occurring? my guess is containerd is not cleaning up all of the processes in the cgroup, which causes runc to fail to clean up the cgroup when the pod is being deleted.",,,,,,Anecdotal,comment,,,,,,,,2024-04-10,github/haircommander,https://github.com/kubernetes/kubernetes/issues/123766#issuecomment-2048132848,repo: kubernetes/kubernetes | issue: failed to delete cgroup paths | keyword: workaround
@haircommander I can't reproduce as I don't know yet what is causing this. I have to wait until it occurs again. I attached logs from yesterday. [syslog.1.gz](https://github.com/kubernetes/kubernetes/files/15020482/syslog.1.gz) Edit: another one with some containerd cgroup errors [syslog.2.gz](https://github.com/kubernetes/kubernetes/files/15020782/syslog.2.gz),,,,,,Anecdotal,comment,,,,,,,,2024-04-18,github/vitality411,https://github.com/kubernetes/kubernetes/issues/123766#issuecomment-2063066729,repo: kubernetes/kubernetes | issue: failed to delete cgroup paths | keyword: workaround
"> I think we'll need logs from around when the issue happened. Since you can reproduce, can you get us logs from when the issue is occurring? > > my guess is containerd is not cleaning up all of the processes in the cgroup, which causes runc to fail to clean up the cgroup when the pod is being deleted. @haircommander I think your guess is correct, because I have also encountered this problem. Indeed, containerd did not clean up the cgroup, but the container was cleaned up and the process was le…",,,,,,Anecdotal,comment,,,,,,,,2024-07-25,github/fengxidi,https://github.com/kubernetes/kubernetes/issues/123766#issuecomment-2249231241,repo: kubernetes/kubernetes | issue: failed to delete cgroup paths | keyword: workaround
"I imagine this issue has passed by now (sorry for the late reply) but in case not: what state is the process? if it's in Z state (zombie, waiting to be reaped by pid 1) then that's a bug in pid 1. If it's in D state (uninterruptable sleep, maybe stuck in a kernel thread) then it's like a bug with the application or even potentially the kernel. unfortunately, once a container passes graceful termination period and is attempted to be SIGKILLed by the runtime, and the runtime fails to do so, it's …",,,,,,Anecdotal,comment,,,,,,,,2024-08-29,github/haircommander,https://github.com/kubernetes/kubernetes/issues/123766#issuecomment-2318478063,repo: kubernetes/kubernetes | issue: failed to delete cgroup paths | keyword: workaround
"I'm experiencing the exact issue on my k8s cluster, I'm able to repro it when I run our reliability tests. The issue is observed when k8s oom killer kicks in due to memory starvation, looks like kubelet service consumes close to 1TiB in memory, kubelet continues to kill all other services. This happens specifically on one particular node. ![image](https://github.com/user-attachments/assets/a2037173-a798-42d0-ab0a-e3ba6952d391) `sudo journalctl -u kubelet` on the node lists endless entries of th…",,,,,,Anecdotal,comment,,,,,,,,2024-12-05,github/aakashhemadri,https://github.com/kubernetes/kubernetes/issues/123766#issuecomment-2519728580,repo: kubernetes/kubernetes | issue: failed to delete cgroup paths | keyword: workaround
"> I'm experiencing the exact issue on my k8s cluster, I'm able to repro it when I run our reliability tests. > > The issue is observed when k8s oom killer kicks in due to memory starvation, looks like kubelet service consumes close to 1TiB in memory, kubelet continues to kill all other services. This happens specifically on one particular node. > > ![image](https://private-user-images.githubusercontent.com/24451176/392735609-a2037173-a798-42d0-ab0a-e3ba6952d391.png?jwt=eyJhbGciOiJIUzI1NiIsInR5c…",,,,,,Anecdotal,comment,,,,,,,,2025-01-11,github/jichen2002,https://github.com/kubernetes/kubernetes/issues/123766#issuecomment-2585220910,repo: kubernetes/kubernetes | issue: failed to delete cgroup paths | keyword: workaround
the same trouble ``` [root@xos-47ixerbj log]# docker ps | wc -l 129 [root@xos-47ixerbj log]# ps -aux |grep containerd-shim | wc -l 5251 ```,,,,,,Anecdotal,comment,,,,,,,,2025-02-06,github/kwenzh,https://github.com/kubernetes/kubernetes/issues/123766#issuecomment-2639136781,repo: kubernetes/kubernetes | issue: failed to delete cgroup paths | keyword: workaround
"> I met the problem too. Have you found a solution yet? @jichen2002 upgrading to k8s 1.31.4 fixed the issue for me, i looked at later changelogs from the version I was on and saw a bunch memory leak issues had been fixed. We haven't experienced this issue since.",,,,,,Anecdotal,comment,,,,,,,,2025-02-06,github/aakashhemadri,https://github.com/kubernetes/kubernetes/issues/123766#issuecomment-2639148715,repo: kubernetes/kubernetes | issue: failed to delete cgroup paths | keyword: workaround
"> > I think we'll need logs from around when the issue happened. Since you can reproduce, can you get us logs from when the issue is occurring? > > my guess is containerd is not cleaning up all of the processes in the cgroup, which causes runc to fail to clean up the cgroup when the pod is being deleted. > > [@haircommander](https://github.com/haircommander) > > I think your guess is correct, because I have also encountered this problem. Indeed, containerd did not clean up the cgroup, but the c…",,,,,,Anecdotal,comment,,,,,,,,2025-06-22,github/HirazawaUi,https://github.com/kubernetes/kubernetes/issues/123766#issuecomment-2994008599,repo: kubernetes/kubernetes | issue: failed to delete cgroup paths | keyword: workaround
> This problem may be related to this [opencontainers/runc#4481](https://github.com/opencontainers/runc/issues/4481) I've already fixed this issue in the runc repo: ref: https://github.com/opencontainers/runc/pull/4757 These two issues do have some similarities. I've assign this issue myself to continue tracking it. /assign,,,,,,Anecdotal,comment,,,,,,,,2025-06-22,github/HirazawaUi,https://github.com/kubernetes/kubernetes/issues/123766#issuecomment-2994010969,repo: kubernetes/kubernetes | issue: failed to delete cgroup paths | keyword: workaround
"> ### What happened? > Pods stuck in terminating state > > lots of log entries like this one > > ``` > journalctl -u kubelet --since -1m -f | grep ""failed to delete cgroup paths"" > Feb 28 05:57:26 worker kubelet[592400]: E0228 05:57:26.352096 592400 pod_workers.go:1300] ""Error syncing pod, skipping"" err=""failed to delete cgroup paths for [kubepods burstable podc5659845-8967-408a-aa34-1223384f0ade] : unable to destroy cgroup paths for cgroup [kubepods burstable podc5659845-8967-408a-aa34-1223384…",,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/vagabond2522,https://github.com/kubernetes/kubernetes/issues/123766#issuecomment-3181748344,repo: kubernetes/kubernetes | issue: failed to delete cgroup paths | keyword: workaround
"Allow implicit extended resource name to be used no matter explicit extendedResourceName field is set or not in device class <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request accord…",,,,,,Anecdotal,issue,,,,,,,,2025-08-03,github/yliaog,https://github.com/kubernetes/kubernetes/pull/133363,repo: kubernetes/kubernetes | keyword: workaround | state: open
@macsko both kubelet and scheuduler changes are included in this PR. also added a util func IsDRAExtendedResourceName(). PTAL,,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/yliaog,https://github.com/kubernetes/kubernetes/pull/133363#issuecomment-3155805017,repo: kubernetes/kubernetes | issue: Allow implicit extended resource name to be used no matter explicit extendedResourceName field is set or not in device class | keyword: workaround
"@yliaog: The following tests **failed**, say `/retest` to rerun all failed tests or `/retest-required` to rerun all mandatory failed tests: Test name | Commit | Details | Required | Rerun command --- | --- | --- | --- | --- pull-kubernetes-dra-integration | d961af4f33560e0c46546f2bc42e746718c6a098 | [link](https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/133363/pull-kubernetes-dra-integration/1954964821073989632) | false | `/test pull-kubernetes-dra-integration` pull-kubernetes-kind-…",,,,,,Anecdotal,comment,,,,,,,,2025-08-11,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133363#issuecomment-3176261484,repo: kubernetes/kubernetes | issue: Allow implicit extended resource name to be used no matter explicit extendedResourceName field is set or not in device class | keyword: workaround
"The test failure is due to the release, as seen from the error below. it is not related to the test, or code change. [FAILED] FATAL ERROR: get https://dl.k8s.io/release/stable-1.34.txt: 404 - 404 Not Found @SergeyKanzhelev PTAL",,,,,,Anecdotal,comment,,,,,,,,2025-08-11,github/yliaog,https://github.com/kubernetes/kubernetes/pull/133363#issuecomment-3176269049,repo: kubernetes/kubernetes | issue: Allow implicit extended resource name to be used no matter explicit extendedResourceName field is set or not in device class | keyword: workaround
"> The test failure is due to the release, as seen from the error below. it is not related to the test, or code change. [FAILED] FATAL ERROR: get https://dl.k8s.io/release/stable-1.34.txt: 404 - 404 Not Found > > @SergeyKanzhelev PTAL The upgrade/downgrade test will resolve when 1.34 is released. Right now in master it sees 1.35 and tries then to look for 1.34 to download, but it's not available yet.",,,,,,Anecdotal,comment,,,,,,,,2025-08-11,github/johnbelamaric,https://github.com/kubernetes/kubernetes/pull/133363#issuecomment-3176334698,repo: kubernetes/kubernetes | issue: Allow implicit extended resource name to be used no matter explicit extendedResourceName field is set or not in device class | keyword: workaround
"Is this critical enough for 1.34? My underdstanding was that for alpha feature with the workaround we can postpone this to 1.35, but if this will be breaking all use cases we can consider it for 1.34. Any comments on priority?",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/SergeyKanzhelev,https://github.com/kubernetes/kubernetes/pull/133363#issuecomment-3180328272,repo: kubernetes/kubernetes | issue: Allow implicit extended resource name to be used no matter explicit extendedResourceName field is set or not in device class | keyword: workaround
"@SergeyKanzhelev this bug fix is for 1.34, which introduced the feature DRA Extended Resource, but due to this bug, the implicit extened resource name does not work as expected.",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/yliaog,https://github.com/kubernetes/kubernetes/pull/133363#issuecomment-3180334491,repo: kubernetes/kubernetes | issue: Allow implicit extended resource name to be used no matter explicit extendedResourceName field is set or not in device class | keyword: workaround
"Increase maximum pods per node As discussed on the sig-node call on March 22: max-pods on kube-1.1 was 40, kube-1.2 is 110 pods per node. We have use-cases expressed by customers for increased node vertical scalability. This is (generally) for environments using fewer larger capacity nodes and perhaps running lighter-weight pods. For kube-1.3 we would like to discuss targeting a 100 node cluster running 500 pods per node. This will require coordination with @kubernetes/sig-scalability as it wou…",,,,,,Anecdotal,issue,,,,,,,,2016-03-22,github/jeremyeder,https://github.com/kubernetes/kubernetes/issues/23349,repo: kubernetes/kubernetes | keyword: workaround | state: open
"As discussed in the meeting, using a single number (max pods) can be misleading for the users, given the huge variation in machine specs, workload, and environment. If we have a node benchmark, we can let users profile their nodes and decide what is the best configuration for them. The benchmark can exist as a node e2e test, or in the contrib repository. @jeremyeder, you mentioned you've tried running more pods in a test environment. What's the machine spec and could you share the numbers?",,,,,,Anecdotal,comment,,,,,,,,2016-03-22,github/yujuhong,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-200004786,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"> How do we vet that a given node configuration is ""qualified enough"" to meet the 500 pods per node goal? That's where the benchmark can play an important role. Community can also share the results on different platform with each other using the standardized benchmark. I'd suggest we should look at: - management overhead in terms of resource usage - performance (responsiveness) in terms of latency for various operations (create/delete pod, etc). Or even detecting the container changes. So far w…",,,,,,Anecdotal,comment,,,,,,,,2016-03-22,github/yujuhong,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-200007893,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"@jeremyeder - from the ""cluster"" perspective, what really matter is the total number of pods. So if you have fewer nodes in the cluster, you can put more pods on them without affecting apiserver, etcd or controllers performance. So that doesn't seem to be a problem given that you are talking abour smaller deployments in terms of number of nodes. Also, we are planning to increase the total number of pods in 1.3. The final number is not decided, but I hope it will be 100.000 or even more (pods/cl…",,,,,,Anecdotal,comment,,,,,,,,2016-03-23,github/wojtek-t,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-200226992,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"@wojtek-t understood, thank you -- that's basically what I was wondering, if the pods-per-cluster limits would be increased during 1.3 cycle.",,,,,,Anecdotal,comment,,,,,,,,2016-03-23,github/jeremyeder,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-200288971,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"@jeremyeder Thanks for filing the issue so we can carry on the discussion. Here is a small summary what I talked in sig-node meeting as a record: - max-pods is configurable. The default value we chosen today is targeted for the users who want an out-of-box solution, and is decided under several constraints: - Docker's performance and limitation. Docker has improved a lot since 1.8 release, but still a lot of room to improve. Unfortunately today docker's management overhead highly depends on the…",,,,,,Anecdotal,comment,,,,,,,,2016-03-23,github/dchen1107,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-200484407,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"> Introduced --kube-reserve and --system-reserve (#17201) to Kubelet so that the admin to config the proportion of overall resource (cpu and memory) a node devote to daemons including Kubelet, docker along with other daemons. Today it is properly configured due to lack of benchmark here. :-) System reserve is arguably a systemd.slice provisioning constraint from our side, but +1 on reserve. Going forwards, I think think the only limits should be resource constraints. If needed, we could put pod…",,,,,,Anecdotal,comment,,,,,,,,2016-03-23,github/timothysc,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-200518510,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"Agree with all comments about the benchmark. @dchen1107 perhaps we should file a new issue to deliver that and leave this one to delivering the increase, should we be able to agree on something. This test is: 1. sleep 60 2. schedule 100 ""hello-openshift"" pods across 2 nodes https://github.com/openshift/origin/tree/master/examples/hello-openshift 3. wait til they are all running 4. sleep 60 5. schedule 100 more 6. loop up through 800 pods per node. 7. sleep 60 ![stacked_cpu](https://cloud.github…",,,,,,Anecdotal,comment,,,,,,,,2016-03-24,github/jeremyeder,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-201004660,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"@dchen1107 as far as what a ""benchmark"" may look like... Perhaps we generate a ""node scaling score"" out of factors like cpu_generation+core_count+GB_RAM+kube_version+other_factors. That score would set max-pods dynamically. This way we don't have to inject a ""test"" into the admission control pipeline or product install paths, the node process could compute the score/max-pods dynamically during it's startup phase. Thoughts ?",,,,,,Anecdotal,comment,,,,,,,,2016-03-24,github/jeremyeder,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-201005233,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"+1 for dynamic limits. Those limits should also take into account kubelet's internal design though, specifically around latency. On Thu, Mar 24, 2016 at 1:28 PM, Jeremy Eder notifications@github.com wrote: > @dchen1107 https://github.com/dchen1107 as far as what a ""benchmark"" > may look like... > > Perhaps we generate a ""node scaling score"" out of factors like > cpu_generation+core_count+GB_RAM+kube_version+other_factors. That score > would set max-pods dynamically. > > This way we don't have t…",,,,,,Anecdotal,comment,,,,,,,,2016-03-24,github/vishh,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-201012675,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"Another factor we need to consider is probing. As https://github.com/kubernetes/kubernetes/issues/16943#issuecomment-183544512 shows, agressive liveness / readiness probing can have a significant impact on performance. We may eventually need to figure out how to account probe usage to the containers being probed.",,,,,,Anecdotal,comment,,,,,,,,2016-03-24,github/timstclair,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-201021620,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"> Another factor we need to consider is probing. As #16943 (comment) shows, agressive liveness / readiness probing can have a significant impact on performance. We may eventually need to figure out how to account probe usage to the containers being probed. That's why I think benchmark with realistic/customizable workloads is valuable. Users can benchmark their cluster and adjust if they want (e.g., determine max pods allowed with 10% of dedicated resources).",,,,,,Anecdotal,comment,,,,,,,,2016-03-24,github/yujuhong,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-201030733,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"We also need to take disk resources into account going forwards, right now that's a level of overhead that we haven't really captured.",,,,,,Anecdotal,comment,,,,,,,,2016-03-24,github/timothysc,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-201037555,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"@jeremyeder I can file a separate benchmark issue. Actually I were in the middle of filing that, and saw this issue and everyone jumped into all over talking about the benchmark. But on another side, I think publishing the benchmark can serve the purpose without keeping increasing --max-pods per node. Node team signed up to: - together with other teams & community to define the performance SLOs - together with community to choose one or several representative workloads to generate benchmark for…",,,,,,Anecdotal,comment,,,,,,,,2016-03-24,github/dchen1107,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-201066147,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"I think we all agreed that developing a node benchmark should be the next step. The benchmark will allow users to test their nodes and adjust kubelet configuration (e.g., --max-pods) accordingly. They can also publish the results and share them with the community. The results can serve as a ballpark for users who just want some configuration to start with. In addition to that, having the published results will also help us discover issues on different platform. Some initial thoughts about what …",,,,,,Anecdotal,comment,,,,,,,,2016-03-24,github/yujuhong,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-201075373,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"@timothysc Could you please clarify the following comment? > Going forwards, I think think the only limits should be resource constraints. If needed, we could put pod limits on admission control. Which resource constraints? The containers'? Each pod and container requires some amount of resources from the management agents (Kubelet, cadvisor, docker) and kernel. These resources can't be attributed to the cgroups of the containers (we've been working on such things for years internally). Dependi…",,,,,,Anecdotal,comment,,,,,,,,2016-03-25,github/bgrant0607,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-201084228,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"**OBJECTIVE** Users would like to achieve much higher density numbers per machine with a large number of underutilized pods. Current `--max-pods` represents an artificial governor for machines which have ample resources available. Instead, admins would prefer to set some system reserve, as well as resource thresholds (watermarks) after which pods are not longer accepted. > Which resource constraints? The containers'? I meant available machine resources that exist for the kubelet + container sub…",,,,,,Anecdotal,comment,,,,,,,,2016-03-25,github/timothysc,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-201352876,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"@timothysc Other considerations: - Kubelet, Docker, network and storage plugins, etc. are not perfectly scalable - Increased load on these agents impacts quality of service from these components - An explicit, predictable limit (as opposed to opaquely just denying requests at some point) helps schedulers make better decisions (e.g., avoiding resource stranding) and helps users understand placement decisions I agree that accurate, simple, automatically set limits would be desirable.",,,,,,Anecdotal,comment,,,,,,,,2016-03-28,github/bgrant0607,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-202481599,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"There is also a realistic difference in time it takes to go from 99-100 running pods in my experience than it takes to go from 0-100 running pods. Right now, I have been running a loop in a three-node cluster that creates a NS with a single RC with 500 pods, and I wait for at least 200 of those pods to report back running before terminating the namespace (I am trying to debug a stuck terminating pod flake that is hard to reproduce), but it seems extremely obvious to me that we are less stable g…",,,,,,Anecdotal,comment,,,,,,,,2016-03-31,github/derekwaynecarr,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-204137689,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"> seems extremely obvious to me that we are less stable going from 0-100 pods > running on a node than we are going from 99-100. @derekwaynecarr, do you mean that batch creation of 100 pods makes the node less stable than when starting a single pod at 99 pods? We don't limit the docker qps in kubelet and creating/deleting pods are the heaviest operations for now. We discussed before v1.2 to [mitigate this issue](https://github.com/kubernetes/kubernetes/issues/21570#issuecomment-187965101), but …",,,,,,Anecdotal,comment,,,,,,,,2016-03-31,github/yujuhong,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-204165143,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"cc/ @gmarek @wojtek-t This is the issue I mentioned to @gmarek earlier. For 1.3 release, we plan to publish node level benchmark at https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-200484407 To do that, we need to define our performance SLO at node level.",,,,,,Anecdotal,comment,,,,,,,,2016-04-04,github/dchen1107,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-205428259,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
@yujuhong - yes. i think qps is an important thing to keep in mind as we change this number. i was able to discover the pod stuck in terminating problem by overwhelming the docker daemon with this scenario.,,,,,,Anecdotal,comment,,,,,,,,2016-04-04,github/derekwaynecarr,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-205447262,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"> @yujuhong Can this be closed? If not, the title definitely needs to be changed Updated the title. We never increased the maximum number of pods above 110. There is still work to do for scaling the pod capacity based on the machine size.",,,,,,Anecdotal,comment,,,,,,,,2017-09-15,github/yujuhong,https://github.com/kubernetes/kubernetes/issues/23349#issuecomment-329811637,repo: kubernetes/kubernetes | issue: Increase maximum pods per node | keyword: workaround
"StatefulSet: support resize pvc storage in K8s v1.11 /kind feature **What happened**: With k8s v1.11 the [new feature ""Resizing Persistent Volume""](https://kubernetes.io/blog/2018/07/12/resizing-persistent-volumes-using-kubernetes/) was prompted to beta. I tried to update the field ""statefulset.spec.volumeClaimTemplates.spec.resources.requests.storage"" by increasing the storage size from 3Gi to 4Gi, however received following error message. ""The StatefulSet ""es-data"" is invalid: spec: Forbidden…",,,,,,Anecdotal,issue,,,,,,,,2018-09-17,github/usherfu,https://github.com/kubernetes/kubernetes/issues/68737,repo: kubernetes/kubernetes | keyword: workaround | state: open
"@mlmhl As far as I understood, the PVC is created automatically based on volumeClaimTemplates, could you elaborate how to directly update the PVC object size?",,,,,,Anecdotal,comment,,,,,,,,2018-09-18,github/usherfu,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-422294157,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
"As resizing PVCs is supported now, it makes absolutely sense to support resizing from templates (i.e. statefulset) as well to benefit from rolling etc. (@mlmhl updating the PVC manually does not sound like a good idea, e.g. when you change the number of replicas in your statefulset, you'll end up with manually resized PVCs plus newly created ones with a different size...)",,,,,,Anecdotal,comment,,,,,,,,2018-12-12,github/dguendisch,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-446619449,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
"Due to this limitation, many database Operators for Kubernetes don't support PVC resizing. It is a critical issue because when your database becomes bigger than you expected - you have no choice and it is needed to backup DB and recreate new DB from the backup.",,,,,,Anecdotal,comment,,,,,,,,2019-01-03,github/delgod,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-451134133,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
Just as an FYI for anyone following this issue: 1. Kubernetes Enhancement Issue: https://github.com/kubernetes/enhancements/issues/661 2. Statefulset Volume Expansion KEP: https://github.com/kubernetes/enhancements/pull/660,,,,,,Anecdotal,comment,,,,,,,,2019-01-25,github/Freyert,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-457668190,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
"Is there a workaround for this? I'm stuck in between two abstraction layers right now, as * there is an operator for me that creates a DB Cluster. * this operator creates statefulsets with the volumeclaimtemplate and joins the cluster * I can't change either the PVC size nor the volumeclaimtemplate size. Basically it would be enough for me finding a way, how to throw one by one a pvc away and get a bigger pvc as the cluster will heal itself.",,,,,,Anecdotal,comment,,,,,,,,2019-01-29,github/alwinmarkcf,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-458510189,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
Ok maybe I should've searched a bit longer. For anybody else searching for a solution: https://kubernetes.io/blog/2018/07/12/resizing-persistent-volumes-using-kubernetes/ So basically it was for me: 1. changing the storageclass(es) by adding: `allowVolumeExpansion: true` 2. changing the PVC and change the requested size: `spec.resources.requests.storage: 200Gi` 3. then the status changes for the pvc (while the pv already has the new size) for waiting that the pod will be restarted 4. restart Po…,,,,,,Anecdotal,comment,,,,,,,,2019-01-29,github/alwinmarkcf,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-458515489,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
"@alwinmarkcf, I believe it is bad idea to do such workflow. If you lose PVC due to some reason - k8s will recreate it from a template with the wrong size. also during scaling up, you will receive new instance storage with the wrong size. a better way - delete statefullset, change size in statefullset, resize PVCs manually and apply statefullset again.",,,,,,Anecdotal,comment,,,,,,,,2019-02-04,github/delgod,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-460126813,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
"If I delete my Statefulset, it will delete all my Pods so, Ive a downtime of my services. If that field would be just editable, exact the failure secnario you mentioned would be possible (but in the other direction) even for storageclasses which does not allow resizing. Also I could finish my resize procedure and woul be able to autoscale. Currently I don't use autoscale and if I would loose my Volumens/Images on my storage provider I would have more serious problems. If I'd loose PVs or PVCs I…",,,,,,Anecdotal,comment,,,,,,,,2019-02-04,github/alwinmarkcf,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-460166400,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
"I got around this limitation on elasticsearch because of work I had to do to avoid EBS volumes being unable to be assigned because they are in the wrong availability zone, i.e. I had created a statefulset per AZ. If I want to change some storage characteristic, I create a new ""AZ"" using the same storage class, and then migrate all the data to pods in that new AZ, then destroy the old AZ.",,,,,,Anecdotal,comment,,,,,,,,2019-02-22,github/DaveWHarvey,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-466514840,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
"@alwinmarkcf You can delete the statefulset without the pods being deleted, by using `kubectl delete sts --cascade=false <statefulset>` - you can then apply a new statefulset and the pods will keep living. I know this might not be possible for you since you mentioned you have an operator that create the statefulsets, but for example when upgrading a helm chart with a new size for the PV Claim and it refuses to update your statefulset, this is a work around for that. Just delete the statefulset …",,,,,,Anecdotal,comment,,,,,,,,2019-03-05,github/polarn,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-469647348,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
"@polarn I didn't tried it out, but when the operator will recreate the statefulset, it should be fine right? Also I'm not sure if the operator is listening to the statefulset or if an additional restart needs to be done. When I find the time to try this out I'll document my experiences here.",,,,,,Anecdotal,comment,,,,,,,,2019-03-05,github/alwinmarkcf,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-469649258,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
"@polarn Thanks for that! The PVC still has to be updated manually though. After doing that, the underlying PV will get updated and the disk will grow. However, the client (Pod) will not see the new size; it either has to be restarted, or the fs has to be manually extended with `resize2fs /dev/<id>` from within the Pod. In case of restarting the pod, the resize operation is visible in the event log: ``` Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3s defaul…",,,,,,Anecdotal,comment,,,,,,,,2019-04-23,github/ervinb,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-485865262,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
"I have a problem with this approach (azure aks). Even after changing the stateful set disk size and applying it again, I still need to restart the pods as stated before but since the stateful set recreates the pods quickly, there's no time for the disk to be detached from the node and resized before the new pod attaches it again. This results on the pvc being stuck with `type: Resizing` (despite still working fine but with the old size instead of the new one). Looks like running `resize2fs` fro…",,,,,,Anecdotal,comment,,,,,,,,2019-05-17,github/guitmz,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-493433066,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
Here is what worked for me with Kubernetes 1.13 on GKE using their own storage class: 1. Directly changed the StatefulSet pvc's size on the pvc itself 2. Removed StatefulSet using: `kubectl delete sts --cascade=false [statefulset-name]` 3. Re-created the StatefulSet with the correct size on the claim Kubernetes automatically re-created the pods 1 by 1 since it detected that the PVC needed to be resized.,,,,,,Anecdotal,comment,,,,,,,,2019-06-04,github/JCMais,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-498470138,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
"> I have a problem with this approach (azure aks). Even after changing the stateful set disk size and applying it again, I still need to restart the pods as stated before but since the stateful set recreates the pods quickly, there's no time for the disk to be detached from the node and resized before the new pod attaches it again. This results on the pvc being stuck with `type: Resizing` (despite still working fine but with the old size instead of the new one). > > Looks like running `resize2f…",,,,,,Anecdotal,comment,,,,,,,,2019-09-06,github/infa-ddeore,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-528752771,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
Looks like someone tried to add support for this in https://github.com/kubernetes/enhancements/pull/660 but the PR has been open for ~10 months and awaiting requested changes for ~8 months. Are there any plans to merge the PR or is the workaround listed here the only solution?,,,,,,Anecdotal,comment,,,,,,,,2019-09-17,github/JamesBalazs,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-532206295,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
"Issues go stale after 90d of inactivity. Mark the issue as fresh with `/remove-lifecycle stale`. Stale issues rot after an additional 30d of inactivity and eventually close. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2019-12-16,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-566077734,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
"```yaml volumeMounts: - name: my-pv --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pv spec: accessModes: - ReadWriteMany storageClassName: my-storageclass resources: requests: storage: 1Gi ``` I think it can be avoided by defining yaml above and changing storage. ```diff @@ -39,4 +39,4 @@ spec: storageClassName: my-storageclass resources: requests: - storage: 1Gi + storage: 2Gi ``` If you have any problems please give us your feedback.",,,,,,Anecdotal,comment,,,,,,,,2020-02-22,github/8398a7,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-589934270,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
"It seems like this creates a new type of PVCs which is used as a volumeClaimTemplate to create specific PVCs per pod, i.e., it is creating a standalone volumeClaimTemplate resource by overloading PVCs rather than creating a new resource type. The specification above seems to be describing all pods in the stateful sets sharing the same volume. Referencing the PVC from the volumeClaimTemplate section of the statefulSet would seem more semantically consistent: regardless of whether the PVC is boun…",,,,,,Anecdotal,comment,,,,,,,,2020-02-22,github/DavidWHarvey,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-589964834,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
> It seems like this creates a new type of PVCs which is used as a volumeClaimTemplate to create specific PVCs per pod pvc is automatically created by the number of replicas. I didn't know this and the proposed method was not enough. Thank you for teaching me.,,,,,,Anecdotal,comment,,,,,,,,2020-02-23,github/8398a7,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-590045763,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
The solution to the primary problem of changing the statefulSet so that new PVCs use the new configuration seems to be blocked by the second order problem of expanding the old PVCs. Simply re-defining the volumeClaimTemplate so that it does not affect existing PVCs would address our production use cases. Our procedure to expand the size of all volumes would become: 1) Change the statefulSet volumeClaimTemplate to allow the size to change as well as the storage class. But this would have no effe…,,,,,,Anecdotal,comment,,,,,,,,2020-02-23,github/DavidWHarvey,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-590071558,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
"Issues go stale after 90d of inactivity. Mark the issue as fresh with `/remove-lifecycle stale`. Stale issues rot after an additional 30d of inactivity and eventually close. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2020-05-23,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-633063085,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
> Here is what worked for me with Kubernetes 1.13 on GKE using their own storage class: > Directly changed the StatefulSet pvc's size on the pvc itself Removed StatefulSet using: kubectl delete sts --cascade=false [statefulset-name] Re-created the StatefulSet with the correct size on the claim Kubernetes automatically re-created the pods 1 by 1 since it detected that the PVC needed to be resized. There is something necessarily wrong that this cannot be done programmatically by changing the size…,,,,,,Anecdotal,comment,,,,,,,,2020-07-31,github/bakayolo,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-666842533,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
"Consider allowing CEL validation of metadata.namespace field of embedded resource ### What would you like to be added? Some context for the issue below: I'm writing operator and generating CRD yaml using `controller-gen` I have a CRD which has a `runtime.RawExtension` field, its spec looks more or less like this: ```go type ObjectSpec struct { // Raw YAML representation of the kubernetes object to be created. // +kubebuilder:validation:EmbeddedResource // +kubebuilder:pruning:PreserveUnknownFie…",,,,,,Anecdotal,issue,,,,,,,,2023-12-03,github/aerfio,https://github.com/kubernetes/kubernetes/issues/122163,repo: kubernetes/kubernetes | keyword: workaround | state: open
"/triage accepted Add some background info: we currently only limit the access to apiVersion, kind, metadata.name and metadata.generateName as the current documentation states: ``` The apiVersion, kind, metadata.name and metadata.generateName are always accessible from the root of the object and from any x-kubernetes-embedded-resource annotated objects. No other metadata properties are accessible. ``` The consideration behind it is mainly that we wanna metadata to be homogeneous across types. Th…",,,,,,Anecdotal,comment,,,,,,,,2023-12-05,github/cici37,https://github.com/kubernetes/kubernetes/issues/122163#issuecomment-1841642772,repo: kubernetes/kubernetes | issue: Consider allowing CEL validation of metadata.namespace field of embedded resource | keyword: workaround
"Yes, I could just use ValidatingAdmissionPolicy, but tbh if that's the solution then what's the point of this restriction on CRD level, if I can just validate whatever I want using VAP? Preferably if possible I'd like to have the source of those validation policies near the Go structs in my codebase (it would be the case if I had an access to namespace), and not ""far"" away in e.g helm chart in VAP, which might get de-synchronised with the fields in the CRD by accident.",,,,,,Anecdotal,comment,,,,,,,,2023-12-06,github/aerfio,https://github.com/kubernetes/kubernetes/issues/122163#issuecomment-1842572533,repo: kubernetes/kubernetes | issue: Consider allowing CEL validation of metadata.namespace field of embedded resource | keyword: workaround
"Proposal: Allow CEL Validation for metadata.namespace in CRD Embedded Resources Overview: Currently, CRD embedded resources lack CEL validation for the metadata.namespace field. This hinders users seeking stable identities for embedded resources across different CR instances. Proposed Solution: Extend CEL validation to cover metadata.namespace in embedded resources. This aligns with existing practices for other metadata fields. Benefits: Stable Identities: Ensure stable identities for embedded …",,,,,,Anecdotal,comment,,,,,,,,2023-12-08,github/xWuWux,https://github.com/kubernetes/kubernetes/issues/122163#issuecomment-1847826806,repo: kubernetes/kubernetes | issue: Consider allowing CEL validation of metadata.namespace field of embedded resource | keyword: workaround
"This does feel a bit quirky, given that `x-kubernetes-embedded-resource` doesn't preclude a CRD author from placing further constraints on the same subschema. I tried to write a schema that would enforce immutability on metadata.namespace in an embedded resource and came up with: ```yaml apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: foos.example.com spec: group: example.com versions: - name: v1 served: true storage: true schema: openAPIV3Schema: type: object…",,,,,,Anecdotal,comment,,,,,,,,2024-01-23,github/benluddy,https://github.com/kubernetes/kubernetes/issues/122163#issuecomment-1906784419,repo: kubernetes/kubernetes | issue: Consider allowing CEL validation of metadata.namespace field of embedded resource | keyword: workaround
"> This seems to work for embedded resources that must be namespaced. It's awkward for a couple reasons: If I'm following, (1), (2) and (3) all result in sub-par validation compared with the root resource. I'd be very interested in seeing that improved. my notes: (1) - This seems like a bug. We wouldn't see it on the root because we have built-in immutability validation. Is this also a problem with OpenAPI value validations? (e.g. a regex rule or length rule)? (2) - The ability to specify if nam…",,,,,,Anecdotal,comment,,,,,,,,2024-01-24,github/jpbetz,https://github.com/kubernetes/kubernetes/issues/122163#issuecomment-1907223578,repo: kubernetes/kubernetes | issue: Consider allowing CEL validation of metadata.namespace field of embedded resource | keyword: workaround
"@aerfio A workaround is to declare all the CEL rules ""above"" the embedded resource: ```yaml apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: withembeddeds.stable.example.com spec: group: stable.example.com versions: - name: v1 served: true storage: true schema: openAPIV3Schema: type: object properties: spec: type: object required: - embedme x-kubernetes-validations: - rule: 'oldSelf.embedme.kind == self.embedme.kind' - rule: 'oldSelf.embedme.apiVersion == self.…",,,,,,Anecdotal,comment,,,,,,,,2024-01-24,github/jpbetz,https://github.com/kubernetes/kubernetes/issues/122163#issuecomment-1908351389,repo: kubernetes/kubernetes | issue: Consider allowing CEL validation of metadata.namespace field of embedded resource | keyword: workaround
"This issue has not been updated in over 1 year, and should be re-triaged. You can: - Confirm that this issue is still relevant with `/triage accepted` (org members only) - Close this issue with `/close` For more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/ /remove-triage accepted",,,,,,Anecdotal,comment,,,,,,,,2025-01-23,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/122163#issuecomment-2610364197,repo: kubernetes/kubernetes | issue: Consider allowing CEL validation of metadata.namespace field of embedded resource | keyword: workaround
"This is also an issue I have been facing when building validations into the CRD, being unable to validate `annotations`, `labels`, and `namespace`. The use cases we see in multiple operators: - Often annotations are used to control operand behaviour, in lieu of content in the spec. However we cant have CEL validations on annotations. - Some objects rely on a name+namespace combination, like `Routes` and `Services`. For this an other use cases we need to limit the name+namespace character limit,…",,,,,,Anecdotal,comment,,,,,,,,2025-03-19,github/giacomoch,https://github.com/kubernetes/kubernetes/issues/122163#issuecomment-2735890090,repo: kubernetes/kubernetes | issue: Consider allowing CEL validation of metadata.namespace field of embedded resource | keyword: workaround
"This is by design. There's a long history to this topic (#74620, #77653, #80493). This led to decision for metadata to have ""generic semantics independently from the object at hand"" which resulted in us limiting CRD metadata validation to name (and generateName), both for OpenAPI and CEL. @sttts @deads2k",,,,,,Anecdotal,comment,,,,,,,,2025-03-19,github/jpbetz,https://github.com/kubernetes/kubernetes/issues/122163#issuecomment-2736883865,repo: kubernetes/kubernetes | issue: Consider allowing CEL validation of metadata.namespace field of embedded resource | keyword: workaround
"> This is by design. There's a long history to this topic ([#74620](https://github.com/kubernetes/kubernetes/issues/74620), [#77653](https://github.com/kubernetes/kubernetes/pull/77653), [#80493](https://github.com/kubernetes/kubernetes/issues/80493)). This led to decision for metadata to have ""generic semantics independently from the object at hand"" which resulted in us limiting CRD metadata validation to name (and generateName), both for OpenAPI and CEL. > > [@sttts](https://github.com/sttts)…",,,,,,Anecdotal,comment,,,,,,,,2025-05-18,github/nunnatsa,https://github.com/kubernetes/kubernetes/issues/122163#issuecomment-2888932231,repo: kubernetes/kubernetes | issue: Consider allowing CEL validation of metadata.namespace field of embedded resource | keyword: workaround
"This is tricky. I can see the argument for being able to use namespace as an input for validation. > But namespace is a special case. It's like a part of the name. What I want to prevent is CRD authors declaring validation rules about what namespaces a custom resource can be created in. This should be done using ValidatingAdmissionPolicy. But once I provide namespace as and input to CRD validation rules, it becomes possible to do what I'm trying to prevent. I think it's best to ask that users t…",,,,,,Anecdotal,comment,,,,,,,,2025-05-19,github/jpbetz,https://github.com/kubernetes/kubernetes/issues/122163#issuecomment-2892161104,repo: kubernetes/kubernetes | issue: Consider allowing CEL validation of metadata.namespace field of embedded resource | keyword: workaround
hi all. not sure what is the conclusion in this thread but trying to define a validation rule that inspects `metadata.namespace` in an embedded resource fails. example validation rule: ```yaml spec: properties: myembedded: type: object x-kubernetes-embedded-resource: true x-kubernetes-preserve-unknown-fields: true x-kubernetes-validations: - message: namespace of myembedded is immutable rule: self.metadata.__namespace__ == oldSelf.metadata.__namespace__ ``` trying to apply this CRD gives the fo…,,,,,,Anecdotal,comment,,,,,,,,2025-08-10,github/tareksha,https://github.com/kubernetes/kubernetes/issues/122163#issuecomment-3172636042,repo: kubernetes/kubernetes | issue: Consider allowing CEL validation of metadata.namespace field of embedded resource | keyword: workaround
"There are two things at play here: - There is a general restriction on CRDs for metadata that ""only restrictions on metadata.name and metadata.generateName are allowed"". This rule pre-dates CEL and is a system wide constraint. - Only explicitly defined fields may be accessed via CEL. That is, no field (namespace or otherwise) can be accessed by CEL in a `x-kubernetes-embedded-resource` using `x-kubernetes-preserve-unknown-fields` unless the field is explicitly declared in the schema. cc @lalitc…",,,,,,Anecdotal,comment,,,,,,,,2025-08-11,github/jpbetz,https://github.com/kubernetes/kubernetes/issues/122163#issuecomment-3175311092,repo: kubernetes/kubernetes | issue: Consider allowing CEL validation of metadata.namespace field of embedded resource | keyword: workaround
"Allow scaling up to meet PDB constraints **What would you like to be added**: Currently if you have a PDB spec like `minAvailable: 1`, and a HPA defining minReplicas=1 and maxReplicas=N, you may end up in a scenario where disruptions get ""stuck"" if the HPA has scaled to 1. At any point an increase in load could cause the HPA to scale up to 2+, allowing the PDB to be satisfied, which leads to a weird scenario where a disruption can only occur if there is high load. Ideally, the PDB would be able…",,,,,,Anecdotal,issue,,,,,,,,2020-07-27,github/howardjohn,https://github.com/kubernetes/kubernetes/issues/93476,repo: kubernetes/kubernetes | keyword: workaround | state: open
It seems also that the Deployment.strategy.maxSurge will not impact this either. This seems like it should have an impact - if I allow a surge of `5` pods why not scale up then evict the draining node?,,,,,,Anecdotal,comment,,,,,,,,2020-09-22,github/howardjohn,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-696759135,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"If you are deploying a single replica of any particular pod, you're saying ""It's okay if this pod is offline for some short time"" as an involuntary disruption (eg, node failure) would result in this potentially happening. So, you have two choices: Acknowledge it's okay to have 0 pods deployed for a short amount of time by removing the PDB, or adjust your replica count to ensure that you're always at N+1, and set PDBs appropriately.",,,,,,Anecdotal,comment,,,,,,,,2020-10-07,github/michaelgugino,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-704984991,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"> Acknowledge it's okay to have 0 pods deployed for a short amount of time by removing the PDB Wouldn't removing the PDB mean we can have *voluntary* downtime, whereas with the PDB we will only have involuntary?",,,,,,Anecdotal,comment,,,,,,,,2020-10-07,github/howardjohn,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-705008995,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"> Wouldn't removing the PDB mean we can have _voluntary_ downtime, whereas with the PDB we will only have involuntary? This is true. But from my POV, by only having one replica, you're implicitly stating that losing that application for some length of time is a non-critical event for your cluster. Set an alert if the replica is down for a protracted period of time. One approach to minimizing downtime for voluntary disruptions is to set an appropriately timed gracePeriod. The pod will be marked …",,,,,,Anecdotal,comment,,,,,,,,2020-10-07,github/michaelgugino,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-705016498,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"> Ideally, the PDB would be able to trigger the HPA to scale up to meet constraints. For example, if I have 1 replica and a disruption is triggered, scale up a new replica first, then terminate the old one. Given an HPA is in place, I have already clearly specified I am OK with my pod being scaled up, so this shouldn't have negative impact on stateful workloads. this. I don't want to have to run two copies of everything just so KureD can do its thing that is mighty expensive. I understand that …",,,,,,Anecdotal,comment,,,,,,,,2020-10-26,github/Crayeth,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-716658663,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"Issues go stale after 90d of inactivity. Mark the issue as fresh with `/remove-lifecycle stale`. Stale issues rot after an additional 30d of inactivity and eventually close. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2021-01-24,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-766378536,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"> > Ideally, the PDB would be able to trigger the HPA to scale up to meet constraints. For example, if I have 1 replica and a disruption is triggered, scale up a new replica first, then terminate the old one. Given an HPA is in place, I have already clearly specified I am OK with my pod being scaled up, so this shouldn't have negative impact on stateful workloads. > > this. I don't want to have to run two copies of everything just so KureD can do its thing that is mighty expensive. I understand…",,,,,,Anecdotal,comment,,,,,,,,2021-04-09,github/michaelgugino,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-816735448,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"@howardjohn if you're still facing this issues, I did have to think about this a bit. A potential solution is to use an extra metric. ```If multiple metrics are specified in a HorizontalPodAutoscaler, this calculation is done for each metric, and then the largest of the desired replica counts is chosen. If any of these metrics cannot be converted into a desired replica count (e.g. due to an error fetching the metrics from the metrics APIs) and a scale down is suggested by the metrics which can …",,,,,,Anecdotal,comment,,,,,,,,2021-06-10,github/simonwgill,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-858516858,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"@simonwgill unfortunately that would not help us, as we are shipping a software to many users, not deploying it ourselves, so we cannot control things like this. In general cluster scaling operations tend to be fairly hands-off and automatic for some users as well",,,,,,Anecdotal,comment,,,,,,,,2021-06-10,github/howardjohn,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-858722429,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs. This bot triages issues and PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue or PR as fresh with `/remove-lifecycle stale` - Mark this issue or PR…",,,,,,Anecdotal,comment,,,,,,,,2021-09-08,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-915384997,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"I also encounters this kind of issue but without the use of HPA. I have some pods with replicaCount=1 because I am OK to have a downtime in case of **temporary and unplanned failures** but I want to avoid downtime during a planned node pool upgrade. If I do a node upgrade, I expect k8s starts a new pod to be able to drain the old node and satisfying the PDB. Basically, I have to manually do a 'kubectl rollout restart deploy' to drain a node and allows the PDB to be satisfied.",,,,,,Anecdotal,comment,,,,,,,,2021-10-01,github/olivierboudet,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-932063548,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages issues and PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue or PR as fresh with `/remove-lifecycle rotten` - Close this is…",,,,,,Anecdotal,comment,,,,,,,,2021-10-31,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-955671813,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs. This bot triages issues and PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue or PR as fresh with `/remove-lifecycle stale` - Mark this issue or PR…",,,,,,Anecdotal,comment,,,,,,,,2022-01-30,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-1025167225,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages issues and PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue or PR as fresh with `/remove-lifecycle rotten` - Close this is…",,,,,,Anecdotal,comment,,,,,,,,2022-03-01,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-1055571747,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages issues and PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Reopen this issue or PR with `/reopen` - Mark this issue or PR as fresh with `…",,,,,,Anecdotal,comment,,,,,,,,2022-03-31,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-1084767746,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"@k8s-triage-robot: Closing this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-1084767746): >The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. > >This bot triages issues and PRs according to the following rules: >- After 90d of inactivity, `lifecycle/stale` is applied >- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied >- After 30d of …",,,,,,Anecdotal,comment,,,,,,,,2022-03-31,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-1084768164,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"@Tristan971: You can't reopen an issue/PR unless you authored it or you are a collaborator. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-1179629719): >/reopen Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubern…",,,,,,Anecdotal,comment,,,,,,,,2022-07-10,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-1179629753,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"can we please reopen this. it's a very valid use case and seems like a bug in k8s I should be able to set that I want 1 pod most time yet bring up the replacement pod in case of `kubectl drain` instead of getting stuck in a loop with ``` error when evicting pods/... (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. ``` I can't think of a workaround, sadly, to achieve this rather simple mode (I would have expected indeed that either maxsurge or hpa or pdb w…",,,,,,Anecdotal,comment,,,,,,,,2022-09-26,github/ldemailly,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-1258433810,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"I have a healthcheck on a service for a pod in a deployment of `replicas=1`. Unplanned outages I want the healthcheck to alert, as it currently does. Planned node pool upgrades I'd like the deployment equivalent to Rolling Upgrade's `maxSurge=1`, to satisfy disruption budget `minAvailable=1`, and the healthcheck not to alert. I also would prefer that node upgrades not fail if due to violating PDBs given the above criteria. I can tolerate short term absence of the pod, but running more than 1 po…",,,,,,Anecdotal,comment,,,,,,,,2022-10-11,github/t-l-k,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-1275260772,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"@howardjohn: Reopened this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-1284399792): >/reopen Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository…",,,,,,Anecdotal,comment,,,,,,,,2022-10-19,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-1284399876,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"@howardjohn: This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions relat…",,,,,,Anecdotal,comment,,,,,,,,2022-10-19,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-1284399974,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Reopen this issue with `/reopen` - Mark this issue as fresh with `/remove-lifecycle ro…",,,,,,Anecdotal,comment,,,,,,,,2022-11-18,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-1320426649,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"@k8s-triage-robot: Closing this issue, marking it as ""Not Planned"". <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-1320426649): >The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. > >This bot triages issues according to the following rules: >- After 90d of inactivity, `lifecycle/stale` is applied >- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is app…",,,,,,Anecdotal,comment,,,,,,,,2022-11-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-1320426862,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"kubelet counts active page cache against memory.available (maybe it shouldn't?) <!-- Thanks for filing an issue! Before hitting the button, please answer these questions.--> **Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): No **What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply there.): active_file…",,,,,,Anecdotal,issue,,,,,,,,2017-03-31,github/vdavidoff,https://github.com/kubernetes/kubernetes/issues/43916,repo: kubernetes/kubernetes | keyword: workaround | state: open
"I'm trying to better understand how the kernel deals with the page cache in terms of active and inactive pages, and I may have just discovered that it actually does not reclaim active page cache (for example, if you echo 3 to drop_caches), or at least doesn't know what to do with it if there is no swap available (and as recommended by the Kubernetes documentation, my nodes have swap disabled). So maybe I'm just totally wrong here, and I need to better understand specifically how the system cons…",,,,,,Anecdotal,comment,,,,,,,,2017-03-31,github/vdavidoff,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-290780228,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"At this point in my research I'm wondering if drop_caches releases active page cache because it actually first moves pages to the inactive_list, then evicts from the inactive_list. And if something like that is happening, then maybe it's not possible to determine what from the active_list cold be dropped without iterating over it, which is not something cadvisor or kubelet would do. I guess I was hoping there'd be some stats exposed somewhere that could be used as a heuristic to determine with …",,,,,,Anecdotal,comment,,,,,,,,2017-04-01,github/vdavidoff,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-290884455,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"@vdavidoff the memory management convoluted for sure. To start, I would agree that subtracting the active pages from available isn't a great heuristic. It is very pessimistic about reclaimable memory and how much of the active list could be reclaimed without pushing the system into a thrashing state. Doing a `sync;echo 1 > drop_caches` will free as much pagecache as possible. Keep in mind that this doesn't drop dirty or locked pages from the cache, hence the `sync` before the drop to maximize t…",,,,,,Anecdotal,comment,,,,,,,,2017-04-03,github/sjenning,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-291211208,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"And to answer your question about how exactly `drop_caches` works, basically like this ``` for each superblock for each cached inode invalidate all page mappings to the inode and release the pages ``` It actually works backward from the filesystem to find the pages that can be freed. It doesn't consider the LRU i.e. active/invactive pages lists",,,,,,Anecdotal,comment,,,,,,,,2017-04-03,github/sjenning,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-291217095,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"Issues go stale after 90d of inactivity. Mark the issue as fresh with `/remove-lifecycle stale`. Stale issues rot after an additional 30d of inactivity and eventually close. Prevent issues from auto-closing with an `/lifecycle frozen` comment. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or `@fejta`. /lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2017-12-22,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-353679835,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"Stale issues rot after 30d of inactivity. Mark the issue as fresh with `/remove-lifecycle rotten`. Rotten issues close after an additional 30d of inactivity. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or `@fejta`. /lifecycle rotten /remove-lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2018-01-21,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-359287718,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"/remove-lifecycle rotten it seems like we're affected by this problem as well. with tightly packed containers, long running jobs involving heavy disk I/O sporadically fail. take this example: ``` apiVersion: v1 kind: PersistentVolumeClaim metadata: name: democlaim spec: accessModes: - ReadWriteOnce storageClassName: ssd resources: requests: storage: 1.2Ti --- apiVersion: batch/v1 kind: Job metadata: name: demo spec: template: spec: containers: - name: demo image: ubuntu command: [""bash"", ""-c"", …",,,,,,Anecdotal,comment,,,,,,,,2018-02-16,github/berlincount,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-366285034,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"We don't currently have a reliable reproducer for this, but we often hit this when restoring large PostgreSQL backups with `pg_basebackup`. A particularly horrible but effective hack to help the backup restore process complete is to exec into the pod and `sync; echo 1 > drop_caches` repeatedly as suggested above (it also helps to sigstop/cont the backup process while flushing the cache). Is there a good way to fix this without a change to the kernel's implementation of cgroups, though? Should t…",,,,,,Anecdotal,comment,,,,,,,,2018-02-16,github/mgomezch,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-366289865,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"This test load of mine seems to be killed reliably: ``` [..] Unpacking 1TiB_of_zeroes tar: 1TiB_of_zeroes: Wrote only 2560 of 10240 bytes $ kubectl --namespace tarsplosion get events LAST SEEN FIRST SEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE 12m 12m 1 demo-xmnr6.1514cb1745ab1ac3 Pod Normal SandboxChanged kubelet, gke-tier1-central-pool-3-3f529658-8wwx Pod sandbox changed, it will be killed and re-created. 11m 11m 1 demo-xmnr6.1514cb1e485790c7 Pod spec.containers{demo} Normal Kill…",,,,,,Anecdotal,comment,,,,,,,,2018-02-20,github/berlincount,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-366928644,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"I ran into the same issue today. On node with 32GB memory, 16+GB is cached. When the memory used + cache exceeded 29GB (~90% of 32GB), the kubelet tried to evict all the pods which shouldn't have happened since the node still had close to 50% of memory available, although in cache. Is there a fix to this issue?",,,,,,Anecdotal,comment,,,,,,,,2018-03-01,github/devopsprosiva,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-369773525,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"As part of other investigations we've been recommended to use https://github.com/Feh/nocache to wrap the corresponding calls, which helped a fairly big amount :)",,,,,,Anecdotal,comment,,,,,,,,2018-03-16,github/berlincount,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-373740496,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
Also having this problem. We had 53GB of available memory and 0.5GB free. 52.5GB is in buff/cache and it starts trying to kill pods due to SystemOOM.,,,,,,Anecdotal,comment,,,,,,,,2018-03-21,github/treacher,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-374812832,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
This is not expected behavior. The OS caching the memory has been around for a long time. Any app looking at memory usage should consider the cached memory. Using `nocache` is not an ideal solution either. Is there anyway we can bump up the severity/need on this issue? We're planning to go into production soon but can't without this issue getting fixed,,,,,,Anecdotal,comment,,,,,,,,2018-03-21,github/devopsprosiva,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-375092842,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"I've been digging in here trying to figure out why the workload @berlincount provided was OOMing. Given the simplicity, it just seemed odd. Using the exact Job spec provided by Andreas, I spun up a k8s cluster, ran the job, and watched the kernel stats for the pod. The kernel was performing sanely for the most part. When it started to come under memory pressure, it started evicting from the page cache. Eventually, the page cache values approached zero. The following is the output of the cgroup'…",,,,,,Anecdotal,comment,,,,,,,,2018-03-30,github/thefirstofthe300,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-377591694,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"Interesting...the nodes I executed the above tests on were GKE COS nodes ``` $ cat /etc/os-release BUILD_ID=10323.12.0 NAME=""Container-Optimized OS"" KERNEL_COMMIT_ID=2d7de0bde20ae17f934c2a2e44cb24b6a1471dec GOOGLE_CRASH_ID=Lakitu VERSION_ID=65 BUG_REPORT_URL=https://crbug.com/new PRETTY_NAME=""Container-Optimized OS from Google"" VERSION=65 GOOGLE_METRICS_PRODUCT_ID=26 HOME_URL=""https://cloud.google.com/compute/docs/containers/vm-image/"" ID=cos ``` ``` $ uname -a Linux gke-yolo-default-pool-a42e4…",,,,,,Anecdotal,comment,,,,,,,,2018-03-30,github/thefirstofthe300,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-377621764,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"I'd like to share some observations, though I can't say I have a good solution to offer yet, other than to set a memory limit equal to the memory request for any pod that makes use of the file cache. Perhaps it's just a matter of documenting the consequences of not having a limit set. Or perhaps an explicit declaration of cache reservation should exist in the podspec, in lieu of assuming ""inactive -> not important to reserve"". Another possibility I've not explored is cgroup soft limits, and/or …",,,,,,Anecdotal,comment,,,,,,,,2018-05-30,github/bitglue,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-393228487,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"Issues go stale after 90d of inactivity. Mark the issue as fresh with `/remove-lifecycle stale`. Stale issues rot after an additional 30d of inactivity and eventually close. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2018-09-11,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-420340484,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"I wanted to add my observations with respect to containers and caching and confirm they are correct and what is intended. I'm running Cassandra in a docker `18.03.1-ce` container, 8GB Xmx, so around 10GB JVM RSS usage. I'm running on a 20GB VM. If I set the kubernetes limit for the pod to 12GB, the system wide (file) cached usage never gets above 2GB. However, if I set the kubernetes limit to 20GB, the cached usage can get into the 10GB range. This is with RHEL 7.5 and kernel 3.10. So it appear…",,,,,,Anecdotal,comment,,,,,,,,2018-10-17,github/rdzimmer-zz,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-430759322,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"Kubernetes resource limits are implemented with cgroups. It is intended for cgroups to limit all RAM, including cache. If you want to use available RAM as cache, you must set a bigger limit, or no limit. Ostensibly, if Cassandra could run fine on 10 GB but it could be faster with up to 100 GB if it happened to be available, then you could set the RAM request to 10 GB and the limit to 100 GB. The problem, and the topic of this bug, is if you then run a process which uses a lot of cache, kubernet…",,,,,,Anecdotal,comment,,,,,,,,2018-10-18,github/bitglue,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-430841267,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"Issues go stale after 90d of inactivity. Mark the issue as fresh with `/remove-lifecycle stale`. Stale issues rot after an additional 30d of inactivity and eventually close. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2019-01-16,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-454620907,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"I have found MongoDB workloads seem to also consistently recreate this problem. I am having to give pods +50% memory (around 50% of the dataset size) to prevent them from getting evicted even when the node still has 40% of its system memory in a reclaimable state. As such I'm migrating MongoDB out of Kubernetes. Edit: As an interim solution, wouldn't it be possible to make whether kubelet considers this cache toward available configurable? Edit 2: Systemd handles this by providing both a LimitR…",,,,,,Anecdotal,comment,,,,,,,,2019-02-18,github/kinghrothgar,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-464794275,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"@kinghrothgar [`LimitRSS` doesn't do anything on Linux](https://www.freedesktop.org/software/systemd/man/systemd.exec.html#Process%20Properties). I suggest making your pod's memory request and limit equal. This will prevent the pod from being evicted. Yes, this means if there's ""extra"" memory available on the node, MongoDB can't use it. But arguably that's a _good_ thing, because if more pods are later scheduled on the node this could reduce the cache available to MongoDB to the point that appl…",,,,,,Anecdotal,comment,,,,,,,,2019-02-21,github/bitglue,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-466041186,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
hi @bitglue did not understand your last comment.. Are you saying that setting requests and limits to the same will allow the Linux kernel to limit my cache usage ? or I should allocate enough memory for my pod to account for my max cache usage?,,,,,,Anecdotal,comment,,,,,,,,2019-03-27,github/gopalvibm,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-477157590,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"Assuming a process has data on disk that is too large to fit entirely in cache, there is no useful metric such as ""max cache usage"". Once a disk block is used, it goes in the inactive cache. Once that block is used a second time, it goes in the active cache. It will stay in the cache until something needs to allocate some RAM but there's none left. So ""max cache usage"" is ""all physical RAM"", even if only a tiny fraction of that used RAM is contributing usefully to IO performance. Using a page o…",,,,,,Anecdotal,comment,,,,,,,,2019-03-27,github/bitglue,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-477214239,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"hi @bitglue even if we make an informed guess, what if we exceed that limit. If pod request/limit are set, then the pod will get evicted. So, either you are acted upon by the OOM killer or you undergo a pod eviction - the good thing is that we are penalizing the one that is using the cache rather than anyone else.",,,,,,Anecdotal,comment,,,,,,,,2019-03-27,github/gopalvibm,https://github.com/kubernetes/kubernetes/issues/43916#issuecomment-477244182,repo: kubernetes/kubernetes | issue: kubelet counts active page cache against memory.available (maybe it shouldn't?) | keyword: workaround
"[WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of …",,,,,,Anecdotal,issue,,,,,,,,2025-01-20,github/esotsal,https://github.com/kubernetes/kubernetes/pull/129719,repo: kubernetes/kubernetes | keyword: workaround | state: open
"@Chunxia202410, @hshiina , @ffromani , @AnishShah , @tallclair , @SergeyKanzhelev , @vinaykul , moved old PR here due to company transfer, now i can continue working on this. Will prioritize updating the commit with the recent changes done in InPlacePodVerticalScaling and the proposals shared by Chunxia202410.",,,,,,Anecdotal,comment,,,,,,,,2025-01-20,github/esotsal,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2602918808,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"<a href=""https://easycla.lfx.linuxfoundation.org/#/?version=2""><img src=""https://s3.amazonaws.com/cla-project-logo-prod/cla-signed.svg?v=2"" alt=""CLA Signed"" align=""left"" height=""28"" width=""328"" ></a><br/><br />The committers listed above are authorized under a signed CLA.<ul><li>:white_check_mark: login: Chunxia202410 / name: Chunxia Guo (3916b6c676e3907401611b3e4f55a8c98779b91e)</li><li>:white_check_mark: login: esotsal / name: Sotiris Salloumis (704e14bd5529071aa5a496d5e8d4da06517c61e5, eb3bf…",,,,,,Anecdotal,comment,,,,,,,,2025-02-05,github/linux-foundation-easycla[bot],https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2637447459,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"Thanks @Chunxia202410 for the contributions, merged your two PRs to check tests . I have some questions about the second PR regarding strategy , thought best to discuss here, will ask later this week, need to do some more tests. Thanks for the API PR seems is one of the options discussed. Will try to update tests tomorrow or by end of this week, to make sure test is covered and passed before the Beta in v1.33 to have a point of reference. We will need to raise the final solution or solutions in…",,,,,,Anecdotal,comment,,,,,,,,2025-02-05,github/esotsal,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2637463639,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"> Thanks @Chunxia202410 for the contributions, merged your two PRs to check tests . I have some questions about the second PR regarding strategy , thought best to discuss here, will ask later this week, need to do some more tests. @esotsal , Thank you for merging my PRs. If you have any questions, please feel free to ask me. In addition, I am also trying to do the e2e test. If there is any progress later, I will share with you. As for PR#3 (CPU strategy), the new preferAlignByUncoreCache functi…",,,,,,Anecdotal,comment,,,,,,,,2025-02-06,github/Chunxia202410,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2638939807,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"@esotsal , Hi, When I do the e2e test using patchString like `{""spec"":{""containers"":[{""name"":""c1"", ""env"":[{""name"":""mustKeepCPUs"",""value"": ""10""}], ""resources"":{""requests"":{""cpu"":""100m""},""limits"":{""cpu"":""200m""}}}]}}` It can work after I modified the code in function dropNonResizeUpdates() in file /pkg/registry/core/pod/strategy.go. `pod.Spec.Containers[idx].Env = ctr.Env //Add code for Env resize ` please note.",,,,,,Anecdotal,comment,,,,,,,,2025-02-08,github/Chunxia202410,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2644928463,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"@Chunxia202410 as discussed via slack, reverted your commits since resulted to failures in tests. Let keep your contributions on a separate PR and work in parallel, will give /ok-to-test for your PR if you push it against kubernetes/kubernetes repo in case is needed. Please feel free to copy paste code from this PR if you need so in your PR.",,,,,,Anecdotal,comment,,,,,,,,2025-02-16,github/esotsal,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2661538780,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
/test pull-kubernetes-e2e-inplace-pod-resize-containerd-main-v2 /test pull-kubernetes-node-kubelet-serial-podresize /test pull-kubernetes-node-kubelet-serial-crio-cgroupv2 /test pull-kubernetes-e2e-gce-cos-alpha-features /test pull-kubernetes-node-kubelet-serial-containerd /test pull-kubernetes-node-kubelet-serial-containerd-alpha-features /test pull-kubernetes-node-kubelet-serial-containerd-sidecar-containers /test pull-kubernetes-e2e-ec2 /test pull-kubernetes-e2e-gce-canary /test pull-kuberne…,,,,,,Anecdotal,comment,,,,,,,,2025-02-19,github/esotsal,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2669894441,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"> @Chunxia202410 as discussed via slack, reverted your commits since resulted to failures in tests. Let keep your contributions on a separate PR and work in parallel, will give /ok-to-test for your PR if you push it against kubernetes/kubernetes repo in case is needed. Please feel free to copy paste code from this PR if you need so in your PR. Update, @Chunxia202410 , seems failures are not from your commit but from changes in the framework :-( which i have missed, working on it to fix those, p…",,,,,,Anecdotal,comment,,,,,,,,2025-02-20,github/esotsal,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2671859252,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"> Update, @Chunxia202410 , seems failures are not from your commit but from changes in the framework :-( which i have missed, working on it to fix those, please gather all your commits in one PR and we will try to merge in this PR to continue the review in one PR. Apologies for the inconvenience, i have missed the refactoring trying to resolve it. No problem at all. I have resubmitted a PR(https://github.com/esotsal/kubernetes/pull/9) to your branch and merged all changes from my side into one …",,,,,,Anecdotal,comment,,,,,,,,2025-02-21,github/Chunxia202410,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2673862859,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"@Chunxia202410 [failing check in pull-kubernetes-node-kubelet-serial-containerd-alpha-features](https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/129719/pull-kubernetes-node-kubelet-serial-containerd-alpha-features/1897676535129706496) is not related with this commit, i have opened https://github.com/kubernetes/kubernetes/issues/130630 for this. We need to wait until Tims commits have been merged and this test bug has been fixed to continue with this PR.",,,,,,Anecdotal,comment,,,,,,,,2025-03-07,github/esotsal,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2705893543,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
@esotsal: The specified target(s) for `/test` were not found. The following commands are available to trigger required jobs: ``` /test pull-cos-containerd-e2e-ubuntu-gce ``` ``` /test pull-kubernetes-cmd ``` ``` /test pull-kubernetes-cmd-canary ``` ``` /test pull-kubernetes-cmd-go-canary ``` ``` /test pull-kubernetes-conformance-kind-ga-only-parallel ``` ``` /test pull-kubernetes-coverage-unit ``` ``` /test pull-kubernetes-dependencies ``` ``` /test pull-kubernetes-dependencies-go-canary ``` ``…,,,,,,Anecdotal,comment,,,,,,,,2025-03-11,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2714758456,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
/test pull-kubernetes-node-kubelet-podresize /test pull-kubernetes-node-kubelet-serial-podresources /test pull-kubernetes-e2e-inplace-pod-resize-containerd-main-v2 /test pull-kubernetes-node-kubelet-serial-podresize /test pull-kubernetes-node-kubelet-serial-crio-cgroupv2 /test pull-kubernetes-e2e-gce-cos-alpha-features /test pull-kubernetes-node-kubelet-serial-containerd /test pull-kubernetes-node-kubelet-serial-containerd-alpha-features /test pull-kubernetes-node-kubelet-serial-containerd-side…,,,,,,Anecdotal,comment,,,,,,,,2025-03-11,github/esotsal,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2715233190,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"As per sig-node meeting today, thanks for a good discussion. With regard problem to have the ability to maintain CPUs during scaling - Try to find solution making local decision, but not ok to completely delegate to NRI, even lower layer. Reasoning this will create another layer of complexity to the system. - Try to redesign the cpu accumulator with this new requirement in mind. - Check https://github.com/kubernetes/enhancements/pull/4541 for inspiration - Try to find a solution with checkpoint…",,,,,,Anecdotal,comment,,,,,,,,2025-03-11,github/esotsal,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2715258098,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"how much of this work is intrinsically tied to VPA vs how much can be extracted in a separated PR (even KEP?) for example 1. changing the cpumanager logic to extend allocation taking into account existing cpuset as (pseudo?) affinity hint 2. changing the checkpoint layout to take into account base set + extension set(s) and compute ""effective cpu set"" 3. redesigning cpu accumulator in general",,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/ffromani,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2717219156,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"> how much of this work is intrinsically tied to VPA vs how much can be extracted in a separated PR (even KEP?) for example > > 1. changing the cpumanager logic to extend allocation taking into account existing cpuset as (pseudo?) affinity hint > > 2. changing the checkpoint layout to take into account base set + extension set(s) and compute ""effective cpu set"" > > 3. redesigning cpu accumulator in general That is a great question, thanks for asking. Short answer is that it depends. I believe i…",,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/esotsal,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2717378870,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"> I will revert the commit later today to [8700407](https://github.com/kubernetes/kubernetes/pull/129719/commits/8700407022d88dfdf62291425757d66e8939f3e5), based on suggestions received yesterday at sig node meeting to abandon the API approach , continue working to find a local only solution without the introduction of a LIFO ordered cpuset and topology universal without modifications on topology in this PR @dchen1107 @Chunxia202410 @mrunalp @ffromani @kad @swatisehgal please let me know that i…",,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/esotsal,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2717418336,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"> > how much of this work is intrinsically tied to VPA vs how much can be extracted in a separated PR (even KEP?) for example > > ``` > > 1. changing the cpumanager logic to extend allocation taking into account existing cpuset as (pseudo?) affinity hint > > > > 2. changing the checkpoint layout to take into account base set + extension set(s) and compute ""effective cpu set"" > > > > 3. redesigning cpu accumulator in general > > ``` > > That is a great question, thanks for asking. Short answer i…",,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/ffromani,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2717472756,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
/test pull-kubernetes-node-kubelet-podresize /test pull-kubernetes-e2e-inplace-pod-resize-containerd-main-v2 /test pull-kubernetes-node-kubelet-serial-podresize,,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/esotsal,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2717924393,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"> > I will revert the commit later today to [8700407](https://github.com/kubernetes/kubernetes/pull/129719/commits/8700407022d88dfdf62291425757d66e8939f3e5), based on suggestions received yesterday at sig node meeting to abandon the API approach , continue working to find a local only solution without the introduction of a LIFO ordered cpuset and topology universal without modifications on topology in this PR > > @dchen1107 @Chunxia202410 @mrunalp @ffromani @kad @swatisehgal please let me know …",,,,,,Anecdotal,comment,,,,,,,,2025-03-13,github/Chunxia202410,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2720369293,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"> I would like to clarify the motivation behind the modified CPU management proposal again so that everyone can better understand it. > > This proposal aims to discuss how to add new CPUs when scaling up a Pod while keeping the existing CPUs unchanged. > > If we do not consider the existing CPUs, the combination of the original CPUs and the additional CPUs may not be what the customer wants. For example, if a Pod's existing CPUs are on one NUMA node, even if there are enough resources available…",,,,,,Anecdotal,comment,,,,,,,,2025-03-13,github/ffromani,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2720470774,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"@ffromani, Thank you for your patient explanation, but I do not understand the following points, could you please explain more? > the main problem is fitting the requirement in the current architecture. NUMA alignment is done at topology manager level using the topology hint mechanism. The cpumanager however got some of these responsabilities to implement the options like `distribute-cpus-across-numa`. > > So we have already a (growing, IMO) conflict in responsabilities. > > Then there's the pr…",,,,,,Anecdotal,comment,,,,,,,,2025-03-13,github/Chunxia202410,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2721007186,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"> > the main problem is fitting the requirement in the current architecture. NUMA alignment is done at topology manager level using the topology hint mechanism. The cpumanager however got some of these responsabilities to implement the options like `distribute-cpus-across-numa`. > > So we have already a (growing, IMO) conflict in responsabilities. > > Then there's the problem of hardware modelling, a very longstanding very big problem we know since years but we are struggling to address. The ku…",,,,,,Anecdotal,comment,,,,,,,,2025-03-13,github/ffromani,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2721135969,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"Thank you for your further explanation. I think I have a general understanding of your intentions. Based on your explanation, here are some of my understandings: Point 1. Currently, kubelet, especially the CPU manager, is facing some problems. If new features are added, a more cautious design may be needed to avoid introducing more problems. Point 2. Regarding my design, I want the newly added CPUs to follow certain rules with the existing CPUs. However, in fact, this rule may not be that impor…",,,,,,Anecdotal,comment,,,,,,,,2025-03-14,github/Chunxia202410,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2723461459,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"> I will try to find a new solution to allocate additional CPUs but not change existing core logic. [Updated] Very interesting discussion, thanks for engaging, i hope more participants from this weeks sig-node meeting to share their thoughts on this subject. My two cents on this statement, I think https://github.com/kubernetes/kubernetes/pull/129719/commits/ff33df4ea970fc0203b7fe2d27863609f8c0df08 already achieves this, without modifying topology manager/logic, maintaining CPUs [during scale up…",,,,,,Anecdotal,comment,,,,,,,,2025-03-14,github/esotsal,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2723794742,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"Yes, I agree with the stepwise approach. And thank you @ffromani and @esotsal for suggestions and feedback. From the feedback, it seems that my proposal poses too much of a challenge to the existing CPU manager and may pose some risks to the stability of the code. I agree with you that it seems that not considering the existing CPUs is the simplest and does not require more changing the existing CPU manager logic. > > I will try to find a new solution to allocate additional CPUs but not change …",,,,,,Anecdotal,comment,,,,,,,,2025-03-14,github/Chunxia202410,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2724154749,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
Schedule a CronJob on a final day of the month / revival ### What would you like to be added? Hi all Let me renew the request for Schedule a CronJob on a final day of the month like it has been made a couple of years ago here: https://github.com/kubernetes/kubernetes/issues/121088 We know that robofig/cron project is abandoned but there is a PR ready to apply this feature here: https://github.com/robfig/cron/pull/325 Could you kindly consider it? thanks in advance Luca ### Why is this needed? N…,,,,,,Anecdotal,issue,,,,,,,,2025-03-19,github/skiat,https://github.com/kubernetes/kubernetes/issues/130923,repo: kubernetes/kubernetes | keyword: workaround | state: closed
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-03-19,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/130923#issuecomment-2736295944,repo: kubernetes/kubernetes | issue: Schedule a CronJob on a final day of the month / revival | keyword: workaround
"@skiat: The label(s) `sig/scheduler` cannot be applied, because the repository doesn't have them. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/130923#issuecomment-2739320397): >/sig architecture >/sig scheduler Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs…",,,,,,Anecdotal,comment,,,,,,,,2025-03-20,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/130923#issuecomment-2739320466,repo: kubernetes/kubernetes | issue: Schedule a CronJob on a final day of the month / revival | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2025-06-18,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/130923#issuecomment-2985562806,repo: kubernetes/kubernetes | issue: Schedule a CronJob on a final day of the month / revival | keyword: workaround
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle rotten` - Close this issue with `/…",,,,,,Anecdotal,comment,,,,,,,,2025-07-18,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/130923#issuecomment-3090724180,repo: kubernetes/kubernetes | issue: Schedule a CronJob on a final day of the month / revival | keyword: workaround
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Reopen this issue with `/reopen` - Mark this issue as fresh with `/remove-lifecycle ro…",,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/130923#issuecomment-3194671815,repo: kubernetes/kubernetes | issue: Schedule a CronJob on a final day of the month / revival | keyword: workaround
"@k8s-triage-robot: Closing this issue, marking it as ""Not Planned"". <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/130923#issuecomment-3194671815): >The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. > >This bot triages issues according to the following rules: >- After 90d of inactivity, `lifecycle/stale` is applied >- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is ap…",,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/130923#issuecomment-3194671852,repo: kubernetes/kubernetes | issue: Schedule a CronJob on a final day of the month / revival | keyword: workaround
"Flag to Disable the 6-minute Force Detach Window ### What happened? Certain online upgrade workflows might need to restart kubernetes as a part of their upgrade procedure. This can lead to the following sequence: * As a part of draining a kubernetes node, a pod might get deleted. * The unmount of a pod could take more than 6 minutes. * If the node is deemed unhealthy when the 6 minute timer fires for the pod being deleted (for example, because kubelet is still restarting), then the underlying P…",,,,,,Anecdotal,issue,,,,,,,,2023-08-31,github/rohitssingh,https://github.com/kubernetes/kubernetes/issues/120328,repo: kubernetes/kubernetes | keyword: workaround | state: closed
"Hi @rohitssingh Thank you for the detailed issue report. The issue you've described with zombie iSCSI LUNs during Kubernetes upgrades sounds concerning, especially with the data corruption implications for cluster-management applications. Firstly, I noticed there's a version skew between your client (1.23) and server (1.27) that exceeds the supported minor version skew of +/-1. While this may not be directly related to the issue you're facing, it's generally a good idea to align the versions fo…",,,,,,Anecdotal,comment,,,,,,,,2023-09-01,github/Devesh-N,https://github.com/kubernetes/kubernetes/issues/120328#issuecomment-1702371954,repo: kubernetes/kubernetes | issue: Flag to Disable the 6-minute Force Detach Window | keyword: workaround
"Thanks for the detailed feedback @Devesh-N. I'm in the process of uploading an associated PR; it'll be coming out shortly. > We would like to reproduce this issue in a controlled environment to understand it better. Would you be willing to share additional logs that could be useful? I think we can reproduce this by stopping kubelet on a node, killing a pod with a PV on that node, and then also bringing down the CSI node driver pod. We would like everything to just wait for kubelet & the CSI nod…",,,,,,Anecdotal,comment,,,,,,,,2023-09-01,github/rohitssingh,https://github.com/kubernetes/kubernetes/issues/120328#issuecomment-1702394693,repo: kubernetes/kubernetes | issue: Flag to Disable the 6-minute Force Detach Window | keyword: workaround
"Hi @rohitssingh , Thank you for your prompt response, and I'm glad to hear that you're already working on a PR for this issue. Your steps for reproducing the problem by stopping kubelet and killing a pod with a PV on the affected node provide a clear path for investigating this further. We'll use this method to reproduce the issue on our end as well. Regarding the 6-Minute Timer: The 6-minute timer was designed to handle certain edge cases, but as you've pointed out, it can cause more problems …",,,,,,Anecdotal,comment,,,,,,,,2023-09-01,github/Devesh-N,https://github.com/kubernetes/kubernetes/issues/120328#issuecomment-1702397343,repo: kubernetes/kubernetes | issue: Flag to Disable the 6-minute Force Detach Window | keyword: workaround
"Thank you for posting issue report. ``` If the node is deemed unhealthy when the 6 minute timer fires for the pod being deleted (for example, because kubelet is still restarting), then the underlying PV will be force detached (via a ControllerUnpublishVolume RPC) ``` To be exact, PV force detach occurs only when `out-of-service` taint is added to the unhealthy node. So, if you don't wish PV force-detach, you can avoid it by not adding the taint. PV force-detach doesn't occur automatically. Than…",,,,,,Anecdotal,comment,,,,,,,,2024-02-02,github/YuikoTakada,https://github.com/kubernetes/kubernetes/issues/120328#issuecomment-1923641244,repo: kubernetes/kubernetes | issue: Flag to Disable the 6-minute Force Detach Window | keyword: workaround
"@YuikoTakada There are two reasons for force detach, one is due to the `out-of-service` taint, and the other one is due to the 6 minute timeout. See this PR that added reason for the force detach: https://github.com/kubernetes/kubernetes/pull/119185/files",,,,,,Anecdotal,comment,,,,,,,,2024-02-02,github/xing-yang,https://github.com/kubernetes/kubernetes/issues/120328#issuecomment-1924058521,repo: kubernetes/kubernetes | issue: Flag to Disable the 6-minute Force Detach Window | keyword: workaround
"@rohitssingh I am wondering if you ever actually hit the consequence described in the issue > If pods end up migrating to a node with such a Zombie LUN present, those pods might end up attaching themselves to the Zombie LUN leading to data corruption. We think we see it but we don't entirely understand how it's possible. (Obviously the forced storage detach isn't very safe just trying to explain the phenomenon) I can certainly produce the forced detach in https://github.com/kubernetes/kubernete…",,,,,,Anecdotal,comment,,,,,,,,2025-08-11,github/akalenyu,https://github.com/kubernetes/kubernetes/issues/120328#issuecomment-3176338231,repo: kubernetes/kubernetes | issue: Flag to Disable the 6-minute Force Detach Window | keyword: workaround
"> [@rohitssingh](https://github.com/rohitssingh) I am wondering if you ever actually hit the consequence described in the issue Yes, we did encounter this issue in our environments. > > > If pods end up migrating to a node with such a Zombie LUN present, those pods might end up attaching themselves to the Zombie LUN leading to data corruption. > > We think we see it but we don't entirely understand how it's possible. (Obviously the forced storage detach isn't very safe just trying to explain th…",,,,,,Anecdotal,comment,,,,,,,,2025-08-11,github/rohitssingh,https://github.com/kubernetes/kubernetes/issues/120328#issuecomment-3176897340,repo: kubernetes/kubernetes | issue: Flag to Disable the 6-minute Force Detach Window | keyword: workaround
"Thanks! just to make sure, the original LUN from the pod is the one getting corrupted? So the data of the original application? Or in some weird way the other ""new"" LUNs get corrupted?",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/akalenyu,https://github.com/kubernetes/kubernetes/issues/120328#issuecomment-3189563082,repo: kubernetes/kubernetes | issue: Flag to Disable the 6-minute Force Detach Window | keyword: workaround
"> Thanks! just to make sure, the original LUN from the pod is the one getting corrupted? So the data of the original application? > > Or in some weird way the other ""new"" LUNs get corrupted? There are two cases to consider: 1) In the situation where the storage system can reuse the same LUN number for two different volumes, the main worry is that the new LUN gets confused with the old LUN, and the new LUN gets corrupted because it's mapped to a LUN number that the node things belongs to the old…",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/bswartz,https://github.com/kubernetes/kubernetes/issues/120328#issuecomment-3192237394,repo: kubernetes/kubernetes | issue: Flag to Disable the 6-minute Force Detach Window | keyword: workaround
"ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty When applying a deploy with some env vars kubernetes accepts the deployment but when doing a var change gives an error with an invalid var value(which is not empty) Removing some random vars can make it work or not but if you remove the deploy and apply it again makes that the deployment is deployed again successfully **Similar issues:** https://github.com/kubernete…",,,,,,Anecdotal,issue,,,,,,,,2017-06-02,github/ferrandinand,https://github.com/kubernetes/kubernetes/issues/46861,repo: kubernetes/kubernetes | keyword: workaround | state: closed
@ferrandinand There are no sig labels on this issue. Please [add a sig label](https://github.com/kubernetes/test-infra/blob/master/commands.md) by:<br>(1) mentioning a sig: `@kubernetes/sig-<team-name>-misc`<br>(2) specifying the label manually: `/sig <label>`<br><br>_Note: method (1) will trigger a notification to the team. You can find the team list [here](https://github.com/kubernetes/community/blob/master/sig-list.md)._,,,,,,Anecdotal,comment,,,,,,,,2017-06-02,github/k8s-github-robot,https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-305808291,"repo: kubernetes/kubernetes | issue: ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty | keyword: workaround"
"Hi @suyogbarve already updated but still getting random failures. ``` kubectl version Client Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.2"", GitCommit:""477efc3cbe6a7effca06bd1452fa356e2201e1ee"", GitTreeState:""clean"", BuildDate:""2017-04-19T22:51:55Z"", GoVersion:""go1.8.1"", Compiler:""gc"", Platform:""darwin/amd64""} Server Version: version.Info{Major:""1"", Minor:""6"", GitVersion:""v1.6.4"", GitCommit:""d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae"", GitTreeState:""clean"", BuildDate:""2017-05-30T…",,,,,,Anecdotal,comment,,,,,,,,2017-06-05,github/ferrandinand,https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-306200910,"repo: kubernetes/kubernetes | issue: ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty | keyword: workaround"
@ferrandinand I just changed following and it works fine - name: KEEN_PROJECT_ID value: '3232333433243432131',,,,,,Anecdotal,comment,,,,,,,,2017-06-05,github/suyogbarve,https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-306208133,"repo: kubernetes/kubernetes | issue: ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty | keyword: workaround"
"@mengqiy: you can't close an issue unless you authored it or you are assigned to it. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-308279309): >/assign >/close Instructions for interacting with me using PR comments are available [here](https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](ht…",,,,,,Anecdotal,comment,,,,,,,,2017-06-13,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-308279312,"repo: kubernetes/kubernetes | issue: ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty | keyword: workaround"
"This issue is also cause by changing the secret type. For example, if you have an env var in the deployment as a key/value, then you change the same env var but try to read from a kubernetes secret.",,,,,,Anecdotal,comment,,,,,,,,2017-07-19,github/gabrie30,https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-316467837,"repo: kubernetes/kubernetes | issue: ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty | keyword: workaround"
"I'm having this issue on 1.14. I can't `apply` my deployment, changing env from a key-value pair to key-secret pair. Is there no other way to do this? I'd really hope to avoid a downtime on this!",,,,,,Anecdotal,comment,,,,,,,,2019-07-15,github/arcana261,https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-511345636,"repo: kubernetes/kubernetes | issue: ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty | keyword: workaround"
> Deleting the deployment and running kubectl apply on your deployment.yaml resolves the issue. Doesn't work for me. I have the cli on 1.12.7 and cluster 1.12,,,,,,Anecdotal,comment,,,,,,,,2019-08-15,github/oceaneLonneux,https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-521600320,"repo: kubernetes/kubernetes | issue: ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty | keyword: workaround"
"Just to mention my specific error: ``` Error: Deployment.apps ""my-test-service"" is invalid: spec.template.spec.containers[0].env[15].valueFrom.configMapKeyRef.name: Invalid value: """": a DNS-1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*') ```",,,,,,Anecdotal,comment,,,,,,,,2019-08-27,github/thunder-spb,https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-525339969,"repo: kubernetes/kubernetes | issue: ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty | keyword: workaround"
"> This issue is also cause by changing the secret type This error message can also be caused by accidentally specifying both `valueFrom` (from the secret) and `value` (from a literal value) as keys. Once `value` is removed, the error goes away and `kubectl apply` works correctly.",,,,,,Anecdotal,comment,,,,,,,,2019-09-05,github/adamlamar,https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-528180473,"repo: kubernetes/kubernetes | issue: ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty | keyword: workaround"
"I'm getting this error currently because my deployment YAML contains the `value` key (from a literal value), and I'm trying to overwrite it so that it's set from a secret. Perhaps the way to go here is to show a more informative error message, but I would say it makes more sense to overwrite it directly in most cases.",,,,,,Anecdotal,comment,,,,,,,,2019-09-12,github/dLobatog,https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-530901993,"repo: kubernetes/kubernetes | issue: ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty | keyword: workaround"
"Here's a way I handle this without creating an outage on a deployment: 1. Pause rollouts: ``` kubectl rollout pause <resource> ``` 2. Edit your config -- remove all env vars on the ""offending"" container (e.g. set `env: []` on the container's configuration) -- you do this either via `kubectl edit`, or if you're using templates/configs and applying those configs -- then edit the file followed by `kubectl apply -f <file>`. 3. Now revert back to your configuration that has the `env` vars set that y…",,,,,,Anecdotal,comment,,,,,,,,2020-03-26,github/bsquizz,https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-604543606,"repo: kubernetes/kubernetes | issue: ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty | keyword: workaround"
"Has there been an issue opened for this problem? The one explained after this specific issue has been closed: > This issue is also cause by changing the secret type. For example, if you have an env var in the deployment as a key/value, then you change the same env var but try to read from a kubernetes secret. Because there have been two workarounds given in the following comments but none of them seems to be the right way this should be working.",,,,,,Anecdotal,comment,,,,,,,,2020-04-22,github/Thematrixme,https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-617838488,"repo: kubernetes/kubernetes | issue: ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty | keyword: workaround"
"@Thematrixme I think the root cause of this is that resources were initially created with `kubectl create` and are later trying to be updated with `kubectl apply`. This is also mentioned in some of the related issues linked in this issue's description. I'm not able to reproduce the issue when following these steps: * Write this data to `file.yaml`: ``` apiVersion: v1 kind: List items: - apiVersion: v1 metadata: name: my-secret data: mySecretKey: ""Ym9ndXMK"" kind: Secret - apiVersion: extensions/…",,,,,,Anecdotal,comment,,,,,,,,2020-04-27,github/bsquizz,https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-620106262,"repo: kubernetes/kubernetes | issue: ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty | keyword: workaround"
"I had the same problem. Reason: In my old deployment, I defined some Environment vars literally. But in the new deployment, I was defining them in the secret. Solution: I edited current deployment using `kubectl edit <DEPLOYMENT>` and removed the literally defined vars. Then ran `kubectl apply -f <NEW DEPLOYMENT YML FILE>` Have fun.",,,,,,Anecdotal,comment,,,,,,,,2020-06-12,github/MortezaHosseini,https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-643101281,"repo: kubernetes/kubernetes | issue: ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty | keyword: workaround"
"> For me the reason was the same env var declared twice Same for me. My current deploy had the evar declared via auto-injected GitLab CI/CD variable, where as my new deploy had the evar defined in `deployment.yml`. Undeploying the current app was required to clear the evars, before the new deploy would work.",,,,,,Anecdotal,comment,,,,,,,,2020-07-10,github/amarvin,https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-656855316,"repo: kubernetes/kubernetes | issue: ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty | keyword: workaround"
"For me, removing the dash (-) before the ""image"" word solved the problem... Yay! I think it was because I created with kubectl create and tried to update with kubectl apply, as mentioned above. So, I guess those two have a little different data structure...",,,,,,Anecdotal,comment,,,,,,,,2020-10-05,github/immortalize,https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-703858205,"repo: kubernetes/kubernetes | issue: ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty | keyword: workaround"
Helm version 3.3.4 doesn't handle the `valueRef` -> `value` change with the `helm upgrade`. I needed to manually modify the Deployment (remove the `valueRef`) and then rerun the `helm upgrade` with the `value`.,,,,,,Anecdotal,comment,,,,,,,,2020-10-14,github/zoltan-fedor,https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-708471059,"repo: kubernetes/kubernetes | issue: ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty | keyword: workaround"
"Scheduler is not balancing properly the pods across the nodes in big clusters (>200 nodes) in quick massive scale ups ### What happened? In some scenarios, the k8s scheduler is not balancing the pods properly across the cluster. In _big_ clusters (> 200 nodes), doing a _quick_ massive scale up of pods that have the same resource requests (and no constraints or affinity), we have detected that the scheduling spread is ~20 pods from most utilized to least utilized nodes. This scale up scenario ha…",,,,,,Anecdotal,issue,,,,,,,,2025-03-10,github/albertogd,https://github.com/kubernetes/kubernetes/issues/130692,repo: kubernetes/kubernetes | keyword: workaround | state: closed
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/130692#issuecomment-2711244421,repo: kubernetes/kubernetes | issue: Scheduler is not balancing properly the pods across the nodes in big clusters (>200 nodes) in quick massive scale ups | keyword: workaround
"The issue is from the scheduler's optimization to reduce the number of nodes evaluated in each scheduling cycle. While this optimization improves performance, it can lead to uneven pod distribution in large clusters during rapid scaling events.",,,,,,Anecdotal,comment,,,,,,,,2025-03-11,github/thisisharrsh,https://github.com/kubernetes/kubernetes/issues/130692#issuecomment-2712970139,repo: kubernetes/kubernetes | issue: Scheduler is not balancing properly the pods across the nodes in big clusters (>200 nodes) in quick massive scale ups | keyword: workaround
"The round-robin approach doesn't guarantee that all nodes are evaluated equally over time, especially when thousands of pods are scheduled in quick succession.",,,,,,Anecdotal,comment,,,,,,,,2025-03-11,github/thisisharrsh,https://github.com/kubernetes/kubernetes/issues/130692#issuecomment-2712972058,repo: kubernetes/kubernetes | issue: Scheduler is not balancing properly the pods across the nodes in big clusters (>200 nodes) in quick massive scale ups | keyword: workaround
"As a workaround, we can try to modify the `percentageOfNodesToScore` parameter to increase the number of nodes evaluated in each scheduling cycle.",,,,,,Anecdotal,comment,,,,,,,,2025-03-11,github/thisisharrsh,https://github.com/kubernetes/kubernetes/issues/130692#issuecomment-2713010090,repo: kubernetes/kubernetes | issue: Scheduler is not balancing properly the pods across the nodes in big clusters (>200 nodes) in quick massive scale ups | keyword: workaround
This can be achieved by setting the `percentageOfNodesToScore` parameter in the scheduler configuration to a higher value,,,,,,Anecdotal,comment,,,,,,,,2025-03-11,github/thisisharrsh,https://github.com/kubernetes/kubernetes/issues/130692#issuecomment-2713014837,repo: kubernetes/kubernetes | issue: Scheduler is not balancing properly the pods across the nodes in big clusters (>200 nodes) in quick massive scale ups | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2025-06-10,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/130692#issuecomment-2958060601,repo: kubernetes/kubernetes | issue: Scheduler is not balancing properly the pods across the nodes in big clusters (>200 nodes) in quick massive scale ups | keyword: workaround
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle rotten` - Close this issue with `/…",,,,,,Anecdotal,comment,,,,,,,,2025-07-10,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/130692#issuecomment-3056369167,repo: kubernetes/kubernetes | issue: Scheduler is not balancing properly the pods across the nodes in big clusters (>200 nodes) in quick massive scale ups | keyword: workaround
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Reopen this issue with `/reopen` - Mark this issue as fresh with `/remove-lifecycle ro…",,,,,,Anecdotal,comment,,,,,,,,2025-08-09,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/130692#issuecomment-3170547221,repo: kubernetes/kubernetes | issue: Scheduler is not balancing properly the pods across the nodes in big clusters (>200 nodes) in quick massive scale ups | keyword: workaround
"@k8s-triage-robot: Closing this issue, marking it as ""Not Planned"". <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/130692#issuecomment-3170547221): >The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. > >This bot triages issues according to the following rules: >- After 90d of inactivity, `lifecycle/stale` is applied >- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is ap…",,,,,,Anecdotal,comment,,,,,,,,2025-08-09,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/130692#issuecomment-3170547261,repo: kubernetes/kubernetes | issue: Scheduler is not balancing properly the pods across the nodes in big clusters (>200 nodes) in quick massive scale ups | keyword: workaround
"kubectl cp fails on large files <!-- This form is for bug reports and feature requests ONLY! If you're looking for help check [Stack Overflow](https://stackoverflow.com/questions/tagged/kubernetes) and the [troubleshooting guide](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/). --> **Is this a BUG REPORT or FEATURE REQUEST?**: /kind bug **What happened**: Copying either a large file, or a large directory from a container via `kubectl cp` results in the error of `err…",,,,,,Anecdotal,issue,,,,,,,,2018-02-21,github/tnine,https://github.com/kubernetes/kubernetes/issues/60140,repo: kubernetes/kubernetes | keyword: workaround | state: closed
@tnine this is most likely due to a network setup in your specific environment; like a load balancer timeout. How long is the download taking before it times out? Is it always the same amount of time which would indicate a timeout?,,,,,,Anecdotal,comment,,,,,,,,2018-02-26,github/jhorwit2,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-368373406,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"I had the same issue when running kubectl from my local machine and attempting to move a large file (~700mb). I only overcame this issue by running the copy from another machine within the same network as the kubernetes cluster (an adjacent VM). The failures that I saw typically took place within 5-20 seconds of starting the transfer (and indeed, only transferred a tiny fraction of the file in question). This was over an OpenVPN connection to the cluster.",,,,,,Anecdotal,comment,,,,,,,,2018-04-04,github/ben-harack,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-378775357,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"Same issue here. `error: unexpected EOF` 5-20 seconds of copying. Only about 50-80MB copied. Tried from over VPN and not. Our instance is in AWS behind an ELB. Deployed with KOPS. Kubernetes 1.9.7. ``` Client Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.1"", GitCommit:""d4ab47518836c750f9949b9e0d387f20fb92260b"", GitTreeState:""clean"", BuildDate:""2018-04-13T22:27:55Z"", GoVersion:""go1.9.5"", Compiler:""gc"", Platform:""darwin/amd64""} Server Version: version.Info{Major:""1"", Minor:""9"", G…",,,,,,Anecdotal,comment,,,,,,,,2018-05-14,github/integrii,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-388885966,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"I found the issue here in our environment. We had blocked ICMP packets in the SG attached to the ENI of our API ELB (CLB these days). This meant that requests to fragment large packets were not getting back to the ELB. Because of that, the packet was never re-sent by the ELB, which meant it was lost. This breaks the TCP session and the connection resets. In short, make sure that ICMP is allowed between your Load Balancer and your hosts, and your MTU settings are correctly calibrated.",,,,,,Anecdotal,comment,,,,,,,,2018-06-01,github/integrii,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-393751563,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"Issues go stale after 90d of inactivity. Mark the issue as fresh with `/remove-lifecycle stale`. Stale issues rot after an additional 30d of inactivity and eventually close. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2018-08-30,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-417189032,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"Stale issues rot after 30d of inactivity. Mark the issue as fresh with `/remove-lifecycle rotten`. Rotten issues close after an additional 30d of inactivity. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /lifecycle rotten",,,,,,Anecdotal,comment,,,,,,,,2018-09-29,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-425616781,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"Same here ``` $ kubectl version Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.0"", GitCommit:""0ed33881dc4355495f623c6f22e7dd0b7632b7c0"", GitTreeState:""clean"", BuildDate:""2018-09-28T15:20:58Z"", GoVersion:""go1.11"", Compiler:""gc"", Platform:""darwin/amd64""} Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.3"", GitCommit:""2bba0127d85d5a46ab4b778548be28623b32d0b0"", GitTreeState:""clean"", BuildDate:""2018-05-21T09:05:37Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Pla…",,,,,,Anecdotal,comment,,,,,,,,2018-10-16,github/koooge,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-430110392,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"I have the same issue, files ~400-900mb ```$ kubectl version Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.2"", GitCommit:""17c77c7898218073f14c8d573582e8d2313dc740"", GitTreeState:""clean"", BuildDate:""2018-10-24T06:54:59Z"", GoVersion:""go1.10.4"", Compiler:""gc"", Platform:""linux/amd64""} Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.3"", GitCommit:""2bba0127d85d5a46ab4b778548be28623b32d0b0"", GitTreeState:""clean"", BuildDate:""2018-05-21T09:05:37Z"", GoVersion…",,,,,,Anecdotal,comment,,,,,,,,2018-11-08,github/sta-szek,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-436974412,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"If anyone is in AWS, check this out: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/network_mtu.html#path_mtu_discovery Need to ensure you have inbound ICMP on your loadbalancers to your instances. This meant that requests to fragment large packets were not getting back to the ELB. Because of that, the packet was never re-sent by the ELB, which meant it was lost. This breaks the TCP session and the connection resets.",,,,,,Anecdotal,comment,,,,,,,,2019-03-18,github/jrowinski88,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-474125674,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"What do you mean by "" inbound ICMP on your loadbalancers to your instances"" ? Do you mean allow ICMP on the instance SG from the LBs? Or allow ICMP on the LBs SG from the instances? To test, I enabled all ICMP on the nodes SG, and all ICMP on the Kubernetes API load balancer - still getting the same 'unexpected eof' error.",,,,,,Anecdotal,comment,,,,,,,,2019-05-16,github/richstokes,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-493130984,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"Any update on this one? Facing the same issue when copying large files > 600MB. Basically the following errors: `unexpected EOF` -> copy and exec outputs `Error proxying data from client to backend: unexpected EOF` -> API-server `error forwarding port <port> to pod <pod_id>, uid : EOF:` -> Kubelet This is happening from within the cluster as well. ``` Server Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.8"", GitCommit:""0c6d31a99f81476dfc9871ba3cf3f597bec29b58"", GitTreeState:""cle…",,,,,,Anecdotal,comment,,,,,,,,2019-07-17,github/zanetworker,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-512351815,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"Issues go stale after 90d of inactivity. Mark the issue as fresh with `/remove-lifecycle stale`. Stale issues rot after an additional 30d of inactivity and eventually close. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2019-10-15,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-542311654,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
same here. Trying to download Prometheus snapshots ~= 3.8G files in the pod: ``` -rw-r--r-- 1 1000 2000 4.2G Oct 17 19:09 prom-backup.tar.gz -rw-r--r-- 1 1000 2000 3.8G Oct 17 21:14 prom-snapshot.tar.gz ``` Download via `kubectl cp`: ``` $ kubectl cp default/prometheus-0:/prometheus/prom-backup.tar.gz prom-backup.tar.gz Defaulting container name to prometheus. tar: removing leading '/' from member names error: unexpected EOF ``` same for the `prom-snapshot.tar.gz` file. I am using: ``` $ kubect…,,,,,,Anecdotal,comment,,,,,,,,2019-10-17,github/fabianbaier,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-543372026,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"Stale issues rot after 30d of inactivity. Mark the issue as fresh with `/remove-lifecycle rotten`. Rotten issues close after an additional 30d of inactivity. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /lifecycle rotten",,,,,,Anecdotal,comment,,,,,,,,2019-11-16,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-554676749,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"Rotten issues close after 30d of inactivity. Reopen the issue with `/reopen`. Mark the issue as fresh with `/remove-lifecycle rotten`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /close",,,,,,Anecdotal,comment,,,,,,,,2019-12-16,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-566271688,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"@fejta-bot: Closing this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-566271688): >Rotten issues close after 30d of inactivity. >Reopen the issue with `/reopen`. >Mark the issue as fresh with `/remove-lifecycle rotten`. > >Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). >/close Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contribu…",,,,,,Anecdotal,comment,,,,,,,,2019-12-16,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-566271766,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"same error here ``` Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.2"", GitCommit:""66049e3b21efe110454d67df4fa62b08ea79a19b"", GitTreeState:""clean"", BuildDate:""2019-05-16T16:23:09Z"", GoVersion:""go1.12.5"", Compiler:""gc"", Platform:""linux/amd64""} Server Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.7"", GitCommit:""6f482974b76db3f1e0f5d24605a9d1d38fad9a2b"", GitTreeState:""clean"", BuildDate:""2019-03-25T02:41:57Z"", GoVersion:""go1.10.8"", Compiler:""gc"", Platform:""lin…",,,,,,Anecdotal,comment,,,,,,,,2020-01-06,github/Eslamanwar,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-571175032,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"@Eslamanwar: You can't reopen an issue/PR unless you authored it or you are a collaborator. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-571175032): >/reopen >same error here >``` >Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.2"", GitCommit:""66049e3b21efe110454d67df4fa62b08ea79a19b"", GitTreeState:""clean"", BuildDate:""2019-05-16T16:23:09Z"", GoVersion:""go1.12.5"", Compiler:""gc"", Platform:""linux/amd64""} >Server Version: ve…",,,,,,Anecdotal,comment,,,,,,,,2020-01-06,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-571175092,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"Same error here copying a db backup. Obviously we can work around this, but this seems like a pretty obvious use case for kubectl cp",,,,,,Anecdotal,comment,,,,,,,,2020-01-17,github/tcolgate,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-575563531,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"Does anyone have a solution for this problem. Facing the same issue. kubectl version Client Version: version.Info{Major:""1"", Minor:""11+"", GitVersion:""v1.11.0+d4cacc0"", GitCommit:""d4cacc0"", GitTreeState:""clean"", BuildDate:""2018-10-10T16:38:01Z"", GoVersion:""go1.10.3"", Compiler:""gc"", Platform:""windows/amd64""} Server Version: version.Info{Major:""1"", Minor:""11+"", GitVersion:""v1.11.0+d4cacc0"", GitCommit:""d4cacc0"", GitTreeState:""clean"", BuildDate:""2020-02-03T19:39:10Z"", GoVersion:""go1.10.8"", Compiler:…",,,,,,Anecdotal,comment,,,,,,,,2020-03-31,github/kishorejm,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-606720995,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"Hi, found nice solution called ""devspace"" (https://github.com/devspace-cloud/devspace) using this tool you can run for example : devspace sync --pod=your pod name --container-path=path --download-only for me it worked great !",,,,,,Anecdotal,comment,,,,,,,,2020-06-03,github/roeera,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-638154160,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
I ended up splitting the file into 500mb chunks and copied over each chunk 1 at a time and it worked fine. `split ./largeFile.bin -b 500m part.` copy all of the files `kubectl cp <pod>:<path_to_part.aa> part.aa` Then reassemble with cat `cat part* > largeFile.bin` I suggest you use a checksum to validate the files integrity once you are done,,,,,,Anecdotal,comment,,,,,,,,2020-08-12,github/MicahRam,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-673060019,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"/reopen Nothing above says the problem with `kubectl cp` has been solved. I am experiencing it. ``` $ kubectl version Client Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.0"", GitCommit:""9e991415386e4cf155a24b1da15becaa390438d8"", GitTreeState:""clean"", BuildDate:""2020-03-25T14:58:59Z"", GoVersion:""go1.13.8"", Compiler:""gc"", Platform:""linux/amd64""} Server Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.8"", GitCommit:""9f2892aab98fe339f3bd70e3c470144299398ace"", GitTreeS…",,,,,,Anecdotal,comment,,,,,,,,2020-09-03,github/MikeSpreitzer,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-686731927,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
PLEG is not healthy **Is this a BUG REPORT or FEATURE REQUEST?**: /kind bug **What happened**: I had a node missbehaving. journal of the kubelet showed this: ``` Mar 13 16:02:43 vk-prod4-node-v18-w3fz kubelet[1450]: I0313 16:02:43.934473 1450 kubelet.go:1778] skipping pod synchronization - [PLEG is not healthy: pleg was last seen active 3m5.30015447s ago; threshold is 3m0s] Mar 13 16:02:48 vk-prod4-node-v18-w3fz kubelet[1450]: I0313 16:02:48.934773 1450 kubelet.go:1778] skipping pod synchroniza…,,,,,,Anecdotal,issue,,,,,,,,2018-03-13,github/albertvaka,https://github.com/kubernetes/kubernetes/issues/61117,repo: kubernetes/kubernetes | keyword: workaround | state: closed
also just had this wonderful experience (with calico 2.6) on one of my nodes in Azure @albertvaka probably abandoned it in favour of https://github.com/kubernetes/kubernetes/issues/45419,,,,,,Anecdotal,comment,,,,,,,,2018-05-08,github/oivindoh,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-387370685,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
"@albertvaka out of memory on a node can cause this, and when it happens the node can't update its own node status to say so. You might find the hyperkube process has leaked to use multiple GB of RAM. This might not be your problem but one idea to check.",,,,,,Anecdotal,comment,,,,,,,,2018-07-12,github/whereisaaron,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-404533096,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
"I am getting same error message on centos 7 node, its has lot of free memory. > Reason:KubeletNotReady Message:PLEG is not healthy: pleg was last seen active 3m2.007756043s ago; threshold is 3m0s}",,,,,,Anecdotal,comment,,,,,,,,2018-11-30,github/sfgroups-k8s,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-443063899,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
"I have the same issue on K8S v1.10.11 with health CPU, memory and disk. v1.10.11 <none> CentOS Linux 7 (Core) 3.10.0-862.14.4.el7.x86_64 docker://1.13.1 Docker works normally by running ""docker ps or info"". By this way, the issue is fixed temporarily. After restarting docker deamon, there was no container running, meanwhile, kubelet logs kept printing: kubelet[788387]: W0412 00:46:38.054829 788387 pod_container_deletor.go:77] Container ""621a89ecc8d299773098d740cf9057602df1f67aba6ba85b7cae88701a…",,,,,,Anecdotal,comment,,,,,,,,2019-04-12,github/javafoot,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-482483573,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
Thanks for this - I lost two nodes over the weekend - flapping. I'll try this next time it happens. I've also brought this up with the folks at OpenEBS just to notify them. They never heard of it. (not an OpenEBS issue but I know some guys over there who are much smarter than me).,,,,,,Anecdotal,comment,,,,,,,,2019-04-15,github/gridworkz,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-483418545,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
"It seems to happen to me when I have too many application instances running and too few nodes. It doesn't matter what the size of the nodes are. I have a simple 3 node test cluster going. I create one project/namespace and run one instance of Odoo - all good. I add a few more instances of Odoo and after a week or so I'm plagued with pleg. My nodes are beefy too. This has happened on Upcloud, Hetzner and Digital Ocean. On Tue, Jun 18, 2019 at 8:08 AM Mohammed S Fadin <notifications@github.com> w…",,,,,,Anecdotal,comment,,,,,,,,2019-06-18,github/gridworkz,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-503161977,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
Is this issue resolved? I am getting PLEG issues in my cluster and observed this open issue. Is there any workaround for this?,,,,,,Anecdotal,comment,,,,,,,,2020-06-26,github/cshivashankar,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-650139472,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
"I also got this issue. My cause is memory leak. I don't know how, but collectd got memory leak and eat my memory resources until 20GB and leave no resource for my kubernetes deployment. After check, i got **Nodes not ready**.",,,,,,Anecdotal,comment,,,,,,,,2020-08-19,github/billysutomo,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-676273398,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
"I see this issue on 1.17.6, with lots of free RAM on the host ``` free -g total used free shared buff/cache available Mem: 15 3 3 0 8 11 Swap: 0 0 0 ```",,,,,,Anecdotal,comment,,,,,,,,2020-08-26,github/TheKangaroo,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-680980018,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
i got same issue on centos 7.6 with k8s 1.13.12 and docker 18.06.2-ce. it cause node reday/notready intermittently. Reason:KubeletNotReady Message:PLEG is not healthy: pleg was last seen active 3m2.007756043s ago; threshold is 3m0s},,,,,,Anecdotal,comment,,,,,,,,2020-10-23,github/wwyhy,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-714870284,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
"i am also facing the same issue on latest kubelet version which is 1.20.5 and still getting this error, there is no issue related to memory because i have not use any application till yet.",,,,,,Anecdotal,comment,,,,,,,,2021-04-12,github/gautamsoni17990,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-817935946,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
"btw: Where are you hosting your servers? On Mon, Apr 12, 2021 at 12:08 PM gautamsoni17990 ***@***.***> wrote: > i am also facing the same issue on latest kubelet version which is 1.20.5 > and still getting this error, there is no issue related to memory because i > have not use any application till yet. > > — > You are receiving this because you commented. > Reply to this email directly, view it on GitHub > <https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-817935946>, > or uns…",,,,,,Anecdotal,comment,,,,,,,,2021-04-12,github/gridworkz,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-817986321,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
"Can this be opened or is there a new issue for that? The problem can actually be found on several systems like aws, rancher etc https://github.com/rancher/rancher/issues/31793 EDIT: https://github.com/kubernetes/kubernetes/issues/101056",,,,,,Anecdotal,comment,,,,,,,,2021-04-27,github/rdxmb,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-827735211,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
"""PLEG is not healthy: pleg was last seen active 17m16.107709513s ago; threshold is 3m0s"" I am still facing this issue. This is fluctuating. Because of this some pods get stuck. I am also using Cluster Autoscaler, which starts adding node, once pods are not scheduled because of this node in error. Any help or clue? --",,,,,,Anecdotal,comment,,,,,,,,2021-04-30,github/nswarnkar,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-830218394,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
"Hi: I generally find this issue arises from network / service mesh inadequacy. The network is usually the last place people look - we just assume it's super fast. I've tried a myriad of configurations across many providers, local clusters etc., and this issue is common across all setups. The only common denominator in all these setups is the underlying network. Now, when I set up a K8 provider service like DO or Google Cloud I never run into this issue (or it's extremely rare). They obviously h…",,,,,,Anecdotal,comment,,,,,,,,2021-04-30,github/gridworkz,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-830231248,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
"I reproduced the environment. After certain number of pods, pod starts getting stuck. I got this detail when i used kubectl describe pod command: Failed to create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod ""vpnwebgate-ec4eb44654f75ea9-7c67995bbf-pnb5c"": operation timeout: context deadline exceeded In this state, Pod stucks during initialization and could not pull the docker images to make container.",,,,,,Anecdotal,comment,,,,,,,,2021-05-03,github/nswarnkar,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-831446612,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
"I am also clear, if I delete an active pod while other pod is still ( created last in order), stuck, stuck pod gets activated and comes into ""Running"" state. You may right in saying that it could be underlying network issue. How could I solve it?",,,,,,Anecdotal,comment,,,,,,,,2021-05-03,github/nswarnkar,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-831456046,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
"There is a bug in Linux kernel re: Kubernetes. - this might help: https://serverfault.com/questions/976233/context-deadline-exceeded-preventing-pods-from-being-created-in-aks On Mon, May 3, 2021 at 2:44 PM Neeraj Swarnkar ***@***.***> wrote: > I am also clear, if I delete an active pod while other pod is still ( > created last in order), stuck, stuck pod > gets activated and comes into ""Running"" state. > > You may right in saying that it could be underlying network issue. How > could I solve it…",,,,,,Anecdotal,comment,,,,,,,,2021-05-03,github/gridworkz,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-831459892,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
"Thanks @gridworkz , I saw the mentioned link. In my case, CPU is just 13% and RAM usage is 66%. Node is hosting more than 50 pods. So I am not sure how the above mentioned fix works in this case. This fix mentioned is to patch systemd to make garbage collection every hour. https://github.com/Azure/AKS/issues/750 Could you enlighten me how this fix would be applicable in my case? Thanks in advance.",,,,,,Anecdotal,comment,,,,,,,,2021-05-03,github/nswarnkar,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-831468947,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
"Ah, OK. I wasn't sure about your setup. Are you using a hosting provider i.e. with Ubuntu nodes? (or are you using a K8 service from a provider like Digital Ocean). Basically, are you self-built or using a provider K8 service? D. On Mon, May 3, 2021 at 3:06 PM Neeraj Swarnkar ***@***.***> wrote: > Thanks @gridworkz <https://github.com/gridworkz> , I saw the mentioned > link. In my case, CPU is just 13% and RAM usage is 66%. Node is hosting > more than 50 pods. So I am not sure how the above men…",,,,,,Anecdotal,comment,,,,,,,,2021-05-03,github/gridworkz,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-831556756,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
"Question: Are you using persistent storage (say at GKE / DO?). In any case, this might be a disk issue. You may consider increasing your disk pool size (or reduce the number of pods scheduled on the node). I know max throughput on SSD is around 240Mb/s and you need 500Gb min to achieve that. Your disks might be saturated causing Docker to timeout. We like to add Prometheus node exporters to all our nodes to use Grafana etc. You should see your disks going sort of wacky. Dave On Mon, May 3, 2021…",,,,,,Anecdotal,comment,,,,,,,,2021-05-03,github/gridworkz,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-831565245,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
I am using AWS. ASG is used for automatic scaling. Cluster autoscaler used ASG to scale up or scale down. The pods which we create have init container. Init container uses shared volume to generate configuration to be used by main container.,,,,,,Anecdotal,comment,,,,,,,,2021-05-04,github/nswarnkar,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-831666026,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
"High kubepods cgroup cpu.weight/shares starves kernel threads on many core systems ### What happened? Hi team, originally this issue manifested as poor performance being observed in Redpanda but for the ease of reproducing I will only talk about [fio](https://github.com/axboe/fio) below. Running fio on large servers ""inside"" of kubernetes results in less IOPS (2x in the example below) than running it ""outside"" of kubernetes natively on the host. This is caused by the cgroup settings as created …",,,,,,Anecdotal,issue,,,,,,,,2024-06-10,github/StephanDollberg,https://github.com/kubernetes/kubernetes/issues/125409,repo: kubernetes/kubernetes | keyword: workaround | state: closed
"@AnishShah: The label(s) `triage/accept` cannot be applied, because the repository doesn't have them. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/125409#issuecomment-2233793395): >sig-node triage meeting: > >@harche will look into it. > >/triage accept >/priority important-longterm Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions …",,,,,,Anecdotal,comment,,,,,,,,2024-07-17,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/125409#issuecomment-2233793578,repo: kubernetes/kubernetes | issue: High kubepods cgroup cpu.weight/shares starves kernel threads on many core systems | keyword: workaround
"@StephanDollberg, thanks for filing this issue. It seems you have already [reached out to the kernel community](https://lore.kernel.org/linux-fsdevel/CAM9ScsHJ1zQ4j+0J+jQ1fUyRvxTMCF9OKC9kcvD5uyQZKxN1Pg@mail.gmail.com/T/#u). If performance degradation is observed while running a task within a cgroup (which, in the case of Kubernetes, would typically be kubepods.slice), it needs to be addressed at the kernel level. Unfortunately, since containers are central to running workloads on Kubernetes hos…",,,,,,Anecdotal,comment,,,,,,,,2024-07-23,github/harche,https://github.com/kubernetes/kubernetes/issues/125409#issuecomment-2245920935,repo: kubernetes/kubernetes | issue: High kubepods cgroup cpu.weight/shares starves kernel threads on many core systems | keyword: workaround
"@harche: Closing this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/125409#issuecomment-2245920935): >@StephanDollberg, thanks for filing this issue. It seems you have already [reached out to the kernel community](https://lore.kernel.org/linux-fsdevel/CAM9ScsHJ1zQ4j+0J+jQ1fUyRvxTMCF9OKC9kcvD5uyQZKxN1Pg@mail.gmail.com/T/#u). > >If performance degradation is observed while running a task within a cgroup (which, in the case of Kubernetes, would typically be…",,,,,,Anecdotal,comment,,,,,,,,2024-07-23,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/125409#issuecomment-2245921049,repo: kubernetes/kubernetes | issue: High kubepods cgroup cpu.weight/shares starves kernel threads on many core systems | keyword: workaround
"> If performance degradation is observed while running a task within a cgroup (which, in the case of Kubernetes, would typically be kubepods.slice), it needs to be addressed at the kernel level. Unfortunately, since containers are central to running workloads on Kubernetes hosts, there is no way for Kubernetes to run a process (container) outside of a cgroup. The suggestion was not to run outside of a cgroup. As described in the ticket the degradation happens because Kubernetes sets the cpu wei…",,,,,,Anecdotal,comment,,,,,,,,2024-07-24,github/StephanDollberg,https://github.com/kubernetes/kubernetes/issues/125409#issuecomment-2247211401,repo: kubernetes/kubernetes | issue: High kubepods cgroup cpu.weight/shares starves kernel threads on many core systems | keyword: workaround
"This is being discussed in this issue - https://github.com/kubernetes/kubernetes/issues/72881 specifically this comments highlights how setting higher kubepods slice cpu.weight (or cpu.share for v1) and not updating system slice hurts - https://github.com/kubernetes/kubernetes/issues/72881#issuecomment-821224980 This was recently brought up in a sig-node meeting, and it was decided to isolate compressible resources like cpu and set the appropriate values on system slice. Work in progress PR - h…",,,,,,Anecdotal,comment,,,,,,,,2024-07-24,github/harche,https://github.com/kubernetes/kubernetes/issues/125409#issuecomment-2247973001,repo: kubernetes/kubernetes | issue: High kubepods cgroup cpu.weight/shares starves kernel threads on many core systems | keyword: workaround
"Thanks yeah that looks related. I will keep watching those. One thing to note here, I don't actually think adjusting the `/system.slice` would help in this case as all the kernel threads don't run in the `/system.slice` but rather as children of the `/` slice.",,,,,,Anecdotal,comment,,,,,,,,2024-07-24,github/StephanDollberg,https://github.com/kubernetes/kubernetes/issues/125409#issuecomment-2248059436,repo: kubernetes/kubernetes | issue: High kubepods cgroup cpu.weight/shares starves kernel threads on many core systems | keyword: workaround
```bash [root@ip-10-0-46-60 cgroup]# pwd /sys/fs/cgroup [root@ip-10-0-46-60 cgroup]# cat system.slice/cpu.weight 100 [root@ip-10-0-46-60 cgroup]# cat kubepods.slice/cpu.weight 137 [root@ip-10-0-46-60 cgroup]# cat cpu.weight cat: cpu.weight: No such file or directory [root@ip-10-0-46-60 cgroup]# ``` How would you set the cpu.weight for the children of `/`?,,,,,,Anecdotal,comment,,,,,,,,2024-07-24,github/harche,https://github.com/kubernetes/kubernetes/issues/125409#issuecomment-2248070341,repo: kubernetes/kubernetes | issue: High kubepods cgroup cpu.weight/shares starves kernel threads on many core systems | keyword: workaround
"You can't I think, they default to 100/1024 from my (limited) understanding. This is really part of the problem as that would for example be an easy workaround that we could apply.",,,,,,Anecdotal,comment,,,,,,,,2024-07-24,github/StephanDollberg,https://github.com/kubernetes/kubernetes/issues/125409#issuecomment-2248946986,repo: kubernetes/kubernetes | issue: High kubepods cgroup cpu.weight/shares starves kernel threads on many core systems | keyword: workaround
"volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs <!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks! If the matter is security related, please disclose it privately via https://kubernetes.io/security/ --> #### What happened: * PVC was created, & pod consuming this PVC. PVC is using the WaitForFirstConsumer policy. * controller with S…",,,,,,Anecdotal,issue,,,,,,,,2021-03-23,github/nmiculinic,https://github.com/kubernetes/kubernetes/issues/100485,repo: kubernetes/kubernetes | keyword: workaround | state: closed
/assign @verult @jsafrane to help look into this. It looks like we only remove the selected-node annotation on provisioning failure if the [error returned is resource exhausted](https://github.com/kubernetes-csi/external-provisioner/blob/1b9d55dd8c7d8c076abeea1039d261723e2d6efa/pkg/controller/controller.go#L1717). Do you think it makes sense to return rescheduling for any other final error?,,,,,,Anecdotal,comment,,,,,,,,2021-05-27,github/msau42,https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-849816408,repo: kubernetes/kubernetes | issue: volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs | keyword: workaround
"> Do you think it makes sense to return rescheduling for any other final error? It makes sense removing this annotation if node in question does not exist anymore. If the underlying PV can be attached to some other node, like many cloud PVs can, then it should be allowed to do so.",,,,,,Anecdotal,comment,,,,,,,,2021-05-27,github/nmiculinic,https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-849849876,repo: kubernetes/kubernetes | issue: volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs. This bot triages issues and PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue or PR as fresh with `/remove-lifecycle stale` - Mark this issue or PR…",,,,,,Anecdotal,comment,,,,,,,,2021-08-25,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-905803731,repo: kubernetes/kubernetes | issue: volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs | keyword: workaround
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages issues and PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue or PR as fresh with `/remove-lifecycle rotten` - Close this is…",,,,,,Anecdotal,comment,,,,,,,,2021-09-24,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-926877780,repo: kubernetes/kubernetes | issue: volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs | keyword: workaround
"We also facing this in other, similar situations: - AWS Spot instances are removed - Unexpected node terminations Removing the annotation manually solves the corrupt state and the pods + pvc get placed (eks 20, csi-driver v1.2.1)",,,,,,Anecdotal,comment,,,,,,,,2021-09-27,github/schuerle,https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-927554695,repo: kubernetes/kubernetes | issue: volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs | keyword: workaround
"Sorry, I somehow missed this issue. > It looks like we only remove the selected-node annotation on provisioning failure if the error returned is resource exhausted. Do you think it makes sense to return rescheduling for any other final error? I don't want a PVC + Pod to hop around Nodes on every little hiccup in the CSI driver. I think the provisioner should have at least configurable amount of retries (on cmdline). Then it could work for most CSI drivers. For CSI drivers with the distributed p…",,,,,,Anecdotal,comment,,,,,,,,2021-10-13,github/jsafrane,https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-942324652,repo: kubernetes/kubernetes | issue: volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs | keyword: workaround
"> Therefore it would need to be all other external-provisioners that would need to watch other CSINodes and clear the annotation from PVS that are scheduled to wrong nodes. I don't like it at all. @pohly, do you have any idea how to fix it? There is https://github.com/kubernetes-csi/external-provisioner/issues/544 for tracking that problem. The idea there is to have one central external-(de)provisioner which knows about nodes where the CSI driver should be running and clears the annotation for …",,,,,,Anecdotal,comment,,,,,,,,2021-10-13,github/pohly,https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-942377869,repo: kubernetes/kubernetes | issue: volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs. This bot triages issues and PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue or PR as fresh with `/remove-lifecycle stale` - Mark this issue or PR…",,,,,,Anecdotal,comment,,,,,,,,2022-01-11,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-1010032090,repo: kubernetes/kubernetes | issue: volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs | keyword: workaround
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages issues and PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue or PR as fresh with `/remove-lifecycle rotten` - Close this is…",,,,,,Anecdotal,comment,,,,,,,,2022-02-10,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-1035013282,repo: kubernetes/kubernetes | issue: volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs | keyword: workaround
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages issues and PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Reopen this issue or PR with `/reopen` - Mark this issue or PR as fresh with `…",,,,,,Anecdotal,comment,,,,,,,,2022-03-12,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-1065903109,repo: kubernetes/kubernetes | issue: volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs | keyword: workaround
"@k8s-triage-robot: Closing this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-1065903109): >The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. > >This bot triages issues and PRs according to the following rules: >- After 90d of inactivity, `lifecycle/stale` is applied >- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied >- After 30d of…",,,,,,Anecdotal,comment,,,,,,,,2022-03-12,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-1065903162,repo: kubernetes/kubernetes | issue: volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs | keyword: workaround
"@andyzhangx: Reopened this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-1217357288): >/reopen Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repositor…",,,,,,Anecdotal,comment,,,,,,,,2022-08-17,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-1217357428,repo: kubernetes/kubernetes | issue: volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs | keyword: workaround
"I believe I had this issue: the PVC with annotation for dead node. My workaround: ```bash #!/bin/bash declare -A nodes while read node; do nodes[""${node#node/}""]=exists done < <(kubectl get nodes -o name) kubectl get pvc -A -o json | jq '.items[].metadata | [.namespace, .name, .annotations[""volume.kubernetes.io/selected-node""]] | @tsv' -r | while read namespace name node; do test -n ""$node"" || continue if ! [[ ${nodes[$node]-} == ""exists"" ]]; then kubectl annotate -n ""${namespace}"" ""pvc/${name}…",,,,,,Anecdotal,comment,,,,,,,,2022-10-08,github/dex4er,https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-1272411687,repo: kubernetes/kubernetes | issue: volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs | keyword: workaround
"Thanks for sharing @dex4er. We just got this issue randomly on 1.21 (it's the first time it happens to us and we've been scaling down instances at night for more than a year, in multiple environments. Surprising ! ) What makes the issue sporadic ?",,,,,,Anecdotal,comment,,,,,,,,2022-12-09,github/org-ci-cd,https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-1344353121,repo: kubernetes/kubernetes | issue: volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs | keyword: workaround
"Discussed today. There may be some complications with handling final vs non-final state. It may be possible to split up a fix and leave handling non-final state, and distributed mode separately, and focus on fixing the scenario of central provisioning with autoscalers. /help",,,,,,Anecdotal,comment,,,,,,,,2023-01-11,github/msau42,https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-1379363455,repo: kubernetes/kubernetes | issue: volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs | keyword: workaround
Facing similar issue. Autoscaler deleted the node and the PVC had the `volume.kubernetes.io/selected-node` annotations. Then the EBS CSI driver kept on complaining that node didn't exist.,,,,,,Anecdotal,comment,,,,,,,,2023-02-21,github/sabinayakc,https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-1439083472,repo: kubernetes/kubernetes | issue: volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs | keyword: workaround
We are using Karpenter and have also seen this issue. The workaround of removing the `volume.kubernetes.io/selected-node` annotation works but this is still causing disruption.,,,,,,Anecdotal,comment,,,,,,,,2023-04-04,github/DanniRiggin,https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-1496204251,repo: kubernetes/kubernetes | issue: volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs | keyword: workaround
This issue is identical to https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner/issues/121 by my understanding. A fix has been submitted and the next step if cherry-pick the fix.,,,,,,Anecdotal,comment,,,,,,,,2023-04-04,github/sunnylovestiramisu,https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-1496257145,repo: kubernetes/kubernetes | issue: volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs | keyword: workaround
The release has been published in external-provisioner: https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.3.1 https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.2.2 https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.4.1 /close,,,,,,Anecdotal,comment,,,,,,,,2023-04-05,github/sunnylovestiramisu,https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-1497878875,repo: kubernetes/kubernetes | issue: volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs | keyword: workaround
@sunnylovestiramisu: Closing this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-1497878875): >The release has been published in external-provisioner: > >https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.3.1 >https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.2.2 >https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.4.1 > >/close Instructions for interacting with me using PR co…,,,,,,Anecdotal,comment,,,,,,,,2023-04-05,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/100485#issuecomment-1497879040,repo: kubernetes/kubernetes | issue: volume.kubernetes.io/selected-node never cleared for non-existent nodes on PVC without PVs | keyword: workaround
"Kubelet accepting pod, setting OutOfCpu on scheduled pods ### What happened? I'm using Knative to run services that scale up and down with load. Knative creates Deployments, which create ReplicaSets, and then calls the Kubernetes API to scale up and scale down those Deployments based on metrics. We've been able to identify pods being scheduled onto nodes that do not have enough resources when the pod is accepted. This has been reported and supposedly fixed several times in 1.22.9 and 1.23.6, ho…",,,,,,Anecdotal,issue,,,,,,,,2023-01-25,github/mbrancato,https://github.com/kubernetes/kubernetes/issues/115325,repo: kubernetes/kubernetes | keyword: workaround | state: closed
"Still seeing this under heavy load on 1.23.14. They all seem to happen together - in the screenshot below, you can see all the `OutOfcpu` state pod happened at two times. They never started. <img width=""624"" alt=""image"" src=""https://user-images.githubusercontent.com/8493275/225180606-731026b3-4a2e-47ec-a9bd-b234d50936fd.png"">",,,,,,Anecdotal,comment,,,,,,,,2023-03-15,github/mbrancato,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1469128444,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"Yes, still seeing this issue in v1.23.10. What's the best way to recover from this state and ensure the pod is scheduled?",,,,,,Anecdotal,comment,,,,,,,,2023-04-08,github/ramgdev,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1500773513,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"Seeing OutOfCpu on GKE Autopilot 1.24.10-gke.2300 using https://github.com/actions/actions-runner-controller The cluster isn't busy. The existing nodes might be low on resources since AutoPilot conserves them. Run one github actions job. It schedules a pod, gets the ""OutOfCpu"" error, the actions job fails, and there are no retries. Should the scheduler avoid placing a pod on a node that doesn't have much CPU or memory available? Rather than scheduling it, getting ""OutOfCpu"", and then the CI tas…",,,,,,Anecdotal,comment,,,,,,,,2023-04-22,github/sdarwin,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1518464908,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"/sig node cc @ruiwen-zhao > Should the scheduler avoid placing a pod on a node that doesn't have much CPU or memory available? It does. The issues you refer to in older patches of 1.22 were additional circumstances that were fixed, to the best of my knowledge. However, it is possible that kubelet didn't have a chance to report the creation of `kube-proxy`, which is a static pod in GKE. This is still true in 1.27. In general, Pods are ephemeral and can fail for multiple reasons. You shouldn't us…",,,,,,Anecdotal,comment,,,,,,,,2023-04-24,github/alculquicondor,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1520617879,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"> However, it is possible that kubelet didn't have a chance to report the creation of kube-proxy, which is a static pod in GKE. This is still true in 1.27. @mbrancato - can you try searching the kubelet log and see if there's anything like ` Failed creating a mirror pod` [1]? If kubelet fails to report static pods to api-server, then it's possible that the scheduler over-schedules pods onto a node. [1] https://github.com/kubernetes/kubernetes/blob/c0804231b97e418429f5f353c94e2c59db0d4c7a/pkg/ku…",,,,,,Anecdotal,comment,,,,,,,,2023-04-24,github/ruiwen-zhao,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1520665747,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"@ruiwen-zhao I currently have pods in the ""OutOfcpu"" state, but I do not see that anywhere in the logs for the past few days. Current cluster version is 1.24.10",,,,,,Anecdotal,comment,,,,,,,,2023-04-24,github/mbrancato,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1520714233,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"update: As far as I understand at the moment, actions-runner-controller is purposely trying to place a pod onto the same node as another pod, so they can share a directory, or a similar idea. With that in mind, it would not be surprising to see ""OutOfcpu"" (or ""OutofMemory"") if the node happens to be busy. That means it's not a kubernetes bug, but if anything, a design decision of that other app. Thanks.",,,,,,Anecdotal,comment,,,,,,,,2023-04-24,github/sdarwin,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1520744179,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"> update: As far as I understand at the moment, actions-runner-controller is purposely trying to place a pod onto the same node as another pod, so they can share a directory, or a similar idea. wait... what? yeah, a pod-placing controller that is ignoring available resources is definitely likely to hit this, and it's an expected outcome (a controller told the node to run more things than it has capacity for) I'm more interested in the issue reported by @mbrancato using pods scheduled by the kub…",,,,,,Anecdotal,comment,,,,,,,,2023-04-24,github/liggitt,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1520748322,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"> wait... what? LOL. exactly. I had commented in this thread because it looked like the same issue about ""OutOfCpu"". but it's not the same thing.",,,,,,Anecdotal,comment,,,,,,,,2023-04-24,github/sdarwin,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1520755028,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"I'm seeing this on deployments that sometimes are rapidly scaled, and should trigger a node pool scale up. They are, however, just regular Deployments from the standard controller. The Knative autoscaler is simply adjusting the replicas value on the Deployment.",,,,,,Anecdotal,comment,,,,,,,,2023-04-24,github/mbrancato,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1520809207,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"Kubernetes is a distributed system, and as such, it might run into these kind of conditions sometimes. If you are using Deployment, eventually there will be pods running once the steady state is reached. If that is the case, I wouldn't consider what you observe much of a problem. One thing we could consider is reducing the pod GC threshold so that these failed pods are removed faster. But... this could break a lot of third-party operators that rely on Failed pods to remain in the cluster :(",,,,,,Anecdotal,comment,,,,,,,,2023-04-24,github/alculquicondor,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1520819095,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"Hi @ruiwen-zhao, I'm facing the same issue on GKE with OutOfpods/OutOfcpu. Below is the kubelet error with failing to create the mirror pod. ``` May 02 17:53:32 gke-*** kubelet[2254]: E0502 17:53:32.099174 2254 kubelet.go:1693] ""Failed creating a mirror pod for"" err=""pods \""kube-proxy-gke-***\"" is forbidden: a mirror pod may not reference secrets"" pod=""kube-system/kube-proxy-gke-***"" # no secrets mention in the manifest grep secrets /etc/kubernetes/manifests/kube-proxy.manifest ``` I don't see …",,,,,,Anecdotal,comment,,,,,,,,2023-05-02,github/dmichel1,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1531931241,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"@dmichel1 do you have mutating admission webhooks installed that are intercepting pods? if so, I suspect one might be trying to inject secret references into pods as they are created",,,,,,Anecdotal,comment,,,,,,,,2023-05-02,github/liggitt,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1531937157,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"@liggitt 💡 - I do, its injecting `imagePullSecrets`. Let me ignore these pods from the admission webhook. ``` pod.Spec.ImagePullSecrets = append(pod.Spec.ImagePullSecrets, corev1.LocalObjectReference{ Name: cfg.SecretName, }) ``` --- update 1: no longer seeing any errors in the kubelet and I can see the shadow pods. will report back if I see any outofcpu/pod errors again. and big thanks, this was impacting me for a while.",,,,,,Anecdotal,comment,,,,,,,,2023-05-02,github/dmichel1,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1532012996,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"@alculquicondor I think closing this an ignoring it leaves things in a poor quality state. In 1.22.8 the code was updated to specifically prevent `OutOfcpu` state so that: ``` The Kubelet now waits to report the phase of a pod as terminal in the API until all running containers are guaranteed to have stopped and no new containers can be started ``` but that obviously that isn't solving the issue here. @liggitt I have a mutating webhooks, however, there are no secrets injected. I also do not see…",,,,,,Anecdotal,comment,,,,,,,,2023-05-03,github/mbrancato,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1532870103,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"@mbrancato there are multiple possible causes for this symptom, some of which are resolvable and some are not Examples of external things that interfere with the scheduler understanding the resources a kubelet has available: * webhooks interfering with kubelet reporting static pods * force deletion of pods from the API * alternate schedulers assigning pods to the same node as kube-scheduler * direct creation of pods assigned to specific node names using `spec.nodeName` There's also a long-stand…",,,,,,Anecdotal,comment,,,,,,,,2023-05-03,github/liggitt,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1532942811,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"> It does look like it happens (at least most recently) after a node pool triggered scale up: I suspect that the new node is reporting ready and the scheduler is scheduling new pods onto it before all of the static pods have corresponding mirror pods created in the API. The kubelet is correct to reject those pods with OutOfCpu, since it doesn't actually have resources to run those",,,,,,Anecdotal,comment,,,,,,,,2023-05-03,github/liggitt,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1532945449,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"if that is indeed the cause, we can narrow this issue to track the race between a new node being marked ready and completing reporting of static pods, rather than the OutOfCPU symptom",,,,,,Anecdotal,comment,,,,,,,,2023-05-03,github/liggitt,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1532956455,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"/triage needs-information if this is a static pods issue, there likely a dup somewhere tracking it. @mbrancato, can you confirm?",,,,,,Anecdotal,comment,,,,,,,,2023-05-10,github/SergeyKanzhelev,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1542577568,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"@SergeyKanzhelev if by static pod you mean using the `Pod` resource type directly, then no. These are `Deployments` where the `.spec.replicas` value is being modified. The `Deployments` are created by https://github.com/knative/serving which also modifies the `.spec.replicas` value.",,,,,,Anecdotal,comment,,,,,,,,2023-05-10,github/mbrancato,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1542609728,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"> @SergeyKanzhelev if by static pod you mean using the `Pod` resource type directly, then no. These are `Deployments` where the `.spec.replicas` value is being modified. > > The `Deployments` are created by https://github.com/knative/serving which also modifies the `.spec.replicas` value. So this is neither of issues listed https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1532942811? Can you please provide step by step repro to understand this new failure scenario?",,,,,,Anecdotal,comment,,,,,,,,2023-05-17,github/SergeyKanzhelev,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1551819901,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"It could still be the same problem. The ""static pod issue"" is not about the user's Pod, it's about the static kube-proxy pod, provided by GKE. Here it's important to know what are the requests you Pod has, compared to the allocatable of the Node.",,,,,,Anecdotal,comment,,,,,,,,2023-05-17,github/alculquicondor,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1551824887,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
it seems we are facing the same issue on GKE 1.27. GKE 1.27 has a confirmed bug (https://github.com/kubernetes/kubernetes/issues/118472). Might this be related?,,,,,,Anecdotal,comment,,,,,,,,2023-06-12,github/philipp-hb,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1587209258,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"> GKE 1.27 has a confirmed bug (#118472). Might this be related? The #118472 bug (regression) was introduced in 1.27 (https://github.com/kubernetes/kubernetes/pull/115331), so not related in a direct way at least.",,,,,,Anecdotal,comment,,,,,,,,2023-06-12,github/mimowo,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1587222267,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"Was any resolution/mitigation ever found for this issue? Since upgrading our cluster to 1.27.3 we've been seeing this issue intermittently during load tests, where both the nodes and the deployment (using HPA) are autoscaling in order to meet increased levels of traffic. In some cases this results in hundreds of `OutOfcpu` pods. In our case our pods were requesting 500 mCPU each, on nodes with 4 CPU each. We were not close to our maximum nodes per our autoscaler (scaled from 3 to 6 nodes in the…",,,,,,Anecdotal,comment,,,,,,,,2023-12-05,github/devinmgrant,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-1840900054,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2024-04-06,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/115325#issuecomment-2041107408,"repo: kubernetes/kubernetes | issue: Kubelet accepting pod, setting OutOfCpu on scheduled pods | keyword: workaround"
"topologySpreadConstraint spanning multiple namespaces ### What would you like to be added? I desperately need a way to spread similar prods from different namespaces evenly across clusters nodes. Right now official docummentation for topologySpreadConstraints explicitly states this: > Only the Pods holding the same namespace as the incoming Pod can be matching candidates. I also taken into consideration cluster-wide topologySpreadConstraints, but they seems not covering my case as well, as pods…",,,,,,Anecdotal,issue,,,,,,,,2023-06-20,github/dene14,https://github.com/kubernetes/kubernetes/issues/118749,repo: kubernetes/kubernetes | keyword: workaround | state: closed
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2023-06-20,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/118749#issuecomment-1598051575,repo: kubernetes/kubernetes | issue: topologySpreadConstraint spanning multiple namespaces | keyword: workaround
"Kubernetes has an extension point that may help: you can [write your own scheduler](https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/), and you can either start from scratch or use the [extension points](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/#extension-points) that the existing scheduler provides. If you want this change to happen in-tree @dene14, you should add some context about why you recommend that approach. To read mor…",,,,,,Anecdotal,comment,,,,,,,,2023-06-20,github/sftim,https://github.com/kubernetes/kubernetes/issues/118749#issuecomment-1599154858,repo: kubernetes/kubernetes | issue: topologySpreadConstraint spanning multiple namespaces | keyword: workaround
"@sftim I've describe the use case that I'm having, I don't quite understand what details are missing, could you please elaborate? I've explored custom scheduler and scheduler extension plugins approach, it sounds like a huge overkill for what I'm trying to solve. topologySpreadConstraint has all the functionality needed, except it's namespaced.",,,,,,Anecdotal,comment,,,,,,,,2023-06-21,github/dene14,https://github.com/kubernetes/kubernetes/issues/118749#issuecomment-1600592291,repo: kubernetes/kubernetes | issue: topologySpreadConstraint spanning multiple namespaces | keyword: workaround
"I think the key here is how can we define `similar pods` ? I don't think we can remove the ns constrain directly, but I think you can bring this to sig/scheduling slack channel to raise a discussion.",,,,,,Anecdotal,comment,,,,,,,,2023-07-05,github/AxeZhan,https://github.com/kubernetes/kubernetes/issues/118749#issuecomment-1620951090,repo: kubernetes/kubernetes | issue: topologySpreadConstraint spanning multiple namespaces | keyword: workaround
"> @sftim I've describe the use case that I'm having, I don't quite understand what details are missing, could you please elaborate? You can solve this by writing your own code; if just one person has this request, then we can keep Kubernetes simple by allowing that one person to solve it themselves (outside of Kubernetes). Why is this improvement important enough that the Kubernetes project should write that code and make it part of Kubernetes' core? I know that _you_ are desperate to have this…",,,,,,Anecdotal,comment,,,,,,,,2023-07-05,github/sftim,https://github.com/kubernetes/kubernetes/issues/118749#issuecomment-1621257935,repo: kubernetes/kubernetes | issue: topologySpreadConstraint spanning multiple namespaces | keyword: workaround
"Hi, I usually refrain from posting a me-too, but the last comment is asking for examples of other interested parties. For our use case, we are looking to balance instances of our application across AWS AZs. We run each instance of the app in its own namespace. We want to balance the usage of a few AZs primarily to avoid resource exhaustion in that AZ. We are looking at other areas where this could be addressed, for example within karpenter. That said, we would use this feature if available.",,,,,,Anecdotal,comment,,,,,,,,2023-08-07,github/dougbyrne,https://github.com/kubernetes/kubernetes/issues/118749#issuecomment-1667966464,repo: kubernetes/kubernetes | issue: topologySpreadConstraint spanning multiple namespaces | keyword: workaround
"Hi, Same ask and our use case is similar to @dougbyrne . Same application is deployed in multiple namespaces and we want to distribute PODs of high CPU consuming app to be distributed across available cluster nodes.",,,,,,Anecdotal,comment,,,,,,,,2023-08-28,github/hiteshcj,https://github.com/kubernetes/kubernetes/issues/118749#issuecomment-1696572220,repo: kubernetes/kubernetes | issue: topologySpreadConstraint spanning multiple namespaces | keyword: workaround
"Hi, I'll also add my 2ct here, we also have a couple of use cases for this: - same as @hiteshcj, we deploy the same application to multiple namespaces, and would like to distribute pods of the same ""component"" even across namespaces - in some cases, we want to do the opposite and _co-locate_ the same components across namespaces (e.g. because we know they use the same image that is expensive to pull, and pulling that image only on a few nodes as opposed to many nodes saves resources) - ideally,…",,,,,,Anecdotal,comment,,,,,,,,2023-08-29,github/lauraseidler,https://github.com/kubernetes/kubernetes/issues/118749#issuecomment-1697342404,repo: kubernetes/kubernetes | issue: topologySpreadConstraint spanning multiple namespaces | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2024-01-27,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/118749#issuecomment-1913013832,repo: kubernetes/kubernetes | issue: topologySpreadConstraint spanning multiple namespaces | keyword: workaround
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle rotten` - Close this issue with `/…",,,,,,Anecdotal,comment,,,,,,,,2024-02-26,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/118749#issuecomment-1963414164,repo: kubernetes/kubernetes | issue: topologySpreadConstraint spanning multiple namespaces | keyword: workaround
"If folks want this, the best way to track that is to add a :+1: reaction to the issue description. I see there are already two upvotes to build on the original request.",,,,,,Anecdotal,comment,,,,,,,,2024-02-26,github/sftim,https://github.com/kubernetes/kubernetes/issues/118749#issuecomment-1963669337,repo: kubernetes/kubernetes | issue: topologySpreadConstraint spanning multiple namespaces | keyword: workaround
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Reopen this issue with `/reopen` - Mark this issue as fresh with `/remove-lifecycle ro…",,,,,,Anecdotal,comment,,,,,,,,2024-03-27,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/118749#issuecomment-2022408783,repo: kubernetes/kubernetes | issue: topologySpreadConstraint spanning multiple namespaces | keyword: workaround
"@k8s-triage-robot: Closing this issue, marking it as ""Not Planned"". <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/118749#issuecomment-2022408783): >The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. > >This bot triages issues according to the following rules: >- After 90d of inactivity, `lifecycle/stale` is applied >- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is ap…",,,,,,Anecdotal,comment,,,,,,,,2024-03-27,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/118749#issuecomment-2022409084,repo: kubernetes/kubernetes | issue: topologySpreadConstraint spanning multiple namespaces | keyword: workaround
The issue was closed due to inactivity. Is this something community is still interested in? There's over 20 thumps up. Would it make sense to reopen this?,,,,,,Anecdotal,comment,,,,,,,,2025-07-30,github/ingvagabund,https://github.com/kubernetes/kubernetes/issues/118749#issuecomment-3136532689,repo: kubernetes/kubernetes | issue: topologySpreadConstraint spanning multiple namespaces | keyword: workaround
"DRA: Implementation of Consumable Capacity (KEP-5075) <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a r…",,,,,,Anecdotal,issue,,,,,,,,2025-06-25,github/sunya-ch,https://github.com/kubernetes/kubernetes/pull/132522,repo: kubernetes/kubernetes | keyword: workaround | state: closed
Welcome @sunya-ch! <br><br>It looks like this is your first PR to <a href='https://github.com/kubernetes/kubernetes'>kubernetes/kubernetes</a> 🎉. Please refer to our [pull request process documentation](https://www.kubernetes.dev/docs/guide/pull-requests/) to help your PR have a smooth ride to approval. <br><br>You will be prompted by a bot to use commands during the review process. Do not be afraid to follow the prompts! It is okay to experiment. [Here is the bot commands documentation](https:…,,,,,,Anecdotal,comment,,,,,,,,2025-06-25,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/132522#issuecomment-3003060645,repo: kubernetes/kubernetes | issue: DRA: Implementation of Consumable Capacity (KEP-5075) | keyword: workaround
"Hi @sunya-ch. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once the pa…",,,,,,Anecdotal,comment,,,,,,,,2025-06-25,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/132522#issuecomment-3003060742,repo: kubernetes/kubernetes | issue: DRA: Implementation of Consumable Capacity (KEP-5075) | keyword: workaround
"This PR [may require API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#what-apis-need-to-be-reviewed). If so, when the changes are ready, [complete the pre-review checklist and request an API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#mechanics). Status of requested reviews is tracked in the [API Review project](https://github.com/orgs/kubernetes/projects/169).",,,,,,Anecdotal,comment,,,,,,,,2025-06-25,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/132522#issuecomment-3003312650,repo: kubernetes/kubernetes | issue: DRA: Implementation of Consumable Capacity (KEP-5075) | keyword: workaround
"> Handling it in the allocator means computing the state for each allocator, but I think it keeps the code a bit simpler and contained. > > @pohly Do you have any preference here? We have to keep it in the allocator because the same code is also used by the autoscaler. If we move more logic up the stack, then that implies making the autoscaler more complex. The ""autoscaler_contract"" changes highlight that - ideally, that directory should not get modified.",,,,,,Anecdotal,comment,,,,,,,,2025-07-06,github/pohly,https://github.com/kubernetes/kubernetes/pull/132522#issuecomment-3041166591,repo: kubernetes/kubernetes | issue: DRA: Implementation of Consumable Capacity (KEP-5075) | keyword: workaround
"> We have to keep it in the allocator because the same code is also used by the autoscaler. If we move more logic up the stack, then that implies making the autoscaler more complex. The ""autoscaler_contract"" changes highlight that - ideally, that directory should not get modified. Ack. So sounds like we want to find a way to avoid the changes in the `ResourceClaimTracker` interface. We already do a pass through the ResourceSlices to identify the allocated devices in https://github.com/kubernete…",,,,,,Anecdotal,comment,,,,,,,,2025-07-06,github/mortent,https://github.com/kubernetes/kubernetes/pull/132522#issuecomment-3042204408,repo: kubernetes/kubernetes | issue: DRA: Implementation of Consumable Capacity (KEP-5075) | keyword: workaround
"> > We have to keep it in the allocator because the same code is also used by the autoscaler. If we move more logic up the stack, then that implies making the autoscaler more complex. The ""autoscaler_contract"" changes highlight that - ideally, that directory should not get modified. > > Ack. So sounds like we want to find a way to avoid the changes in the `ResourceClaimTracker` interface. We already do a pass through the ResourceSlices to identify the allocated devices in https://github.com/kub…",,,,,,Anecdotal,comment,,,,,,,,2025-07-07,github/sunya-ch,https://github.com/kubernetes/kubernetes/pull/132522#issuecomment-3043334775,repo: kubernetes/kubernetes | issue: DRA: Implementation of Consumable Capacity (KEP-5075) | keyword: workaround
"> The result of consumed capacity is not in the resourceslice but in the allocated resource claims which is not passed to the allocator. Do you think we should also run through all the resource claims in the allocator during allocation? I thought we did, but it's actually handled here: https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/framework/plugins/dynamicresources/allocateddevices.go",,,,,,Anecdotal,comment,,,,,,,,2025-07-07,github/pohly,https://github.com/kubernetes/kubernetes/pull/132522#issuecomment-3043631294,repo: kubernetes/kubernetes | issue: DRA: Implementation of Consumable Capacity (KEP-5075) | keyword: workaround
"@lmktfy @LionelJouin @pohly @mortent Thank you so much for your comments and suggestions. To reduce the number of comments, I would like to summarize my [change](https://github.com/kubernetes/kubernetes/compare/249d6036defdd39cd65566e5c2693f27e0d62fb7..a24ce5b59c7933e083ef543ecce3b55836dd9aa4) according to some of your suggestions. @lmktfy's suggestion - Get rid of present tense logging, `Constraint does not apply to request` is removed. `Device has no enough capacity` -> `Device capacity not e…",,,,,,Anecdotal,comment,,,,,,,,2025-07-07,github/sunya-ch,https://github.com/kubernetes/kubernetes/pull/132522#issuecomment-3044902217,repo: kubernetes/kubernetes | issue: DRA: Implementation of Consumable Capacity (KEP-5075) | keyword: workaround
"> @mortent The result of consumed capacity is not in the resourceslice but in the allocated resource claims which is not passed to the allocator. Do you think we should also run through all the resource claims in the allocator during allocation? Ah, I didn't remember that we don't actually look at the ResourceClaims in the logic for Partitionable Devices. I think having access to the existing allocated ResourceClaims in the allocator might be useful for future changes. And the changes to the DR…",,,,,,Anecdotal,comment,,,,,,,,2025-07-07,github/mortent,https://github.com/kubernetes/kubernetes/pull/132522#issuecomment-3045736280,repo: kubernetes/kubernetes | issue: DRA: Implementation of Consumable Capacity (KEP-5075) | keyword: workaround
This [change](https://github.com/kubernetes/kubernetes/compare/2324b45ec7777a876310d5bef63e0f6c2ffc7b8e..170bd3102b30d7cac75b4c91cc0e8521ac86226a) includes: - add fake markup // ^^^ with summary of all discussions as suggested https://github.com/kubernetes/kubernetes/pull/132522#discussion_r2189074846 and https://github.com/kubernetes/kubernetes/pull/132522#discussion_r2185638722 - replace description of AllowMultipleAllocations https://github.com/kubernetes/kubernetes/pull/132522#discussion_r2…,,,,,,Anecdotal,comment,,,,,,,,2025-07-08,github/sunya-ch,https://github.com/kubernetes/kubernetes/pull/132522#issuecomment-3047376608,repo: kubernetes/kubernetes | issue: DRA: Implementation of Consumable Capacity (KEP-5075) | keyword: workaround
"This [change](https://github.com/kubernetes/kubernetes/compare/170bd3102b30d7cac75b4c91cc0e8521ac86226a..9c7e9060382e624d6364b2062cd75cb9356bac3a) includes the update for strategy to keep fields if in-use in the old resource (ResourceSlice, ResourceClaim, ResourceClaimTemplate) and also fix the incorrect validation on DeviceCapacity (should not check sharingPolicy if feature is disabled). This should address - https://github.com/kubernetes/kubernetes/pull/132522#discussion_r2190533857 - https:/…",,,,,,Anecdotal,comment,,,,,,,,2025-07-08,github/sunya-ch,https://github.com/kubernetes/kubernetes/pull/132522#issuecomment-3049551823,repo: kubernetes/kubernetes | issue: DRA: Implementation of Consumable Capacity (KEP-5075) | keyword: workaround
@mortent Thank you. This [changes](https://github.com/kubernetes/kubernetes/pull/132522/commits/378d77f930ae2d5f9f9df504d116d827ef5f3964) should address your following recent comments. - https://github.com/kubernetes/kubernetes/pull/132522#discussion_r2198949177 - https://github.com/kubernetes/kubernetes/pull/132522#discussion_r2198950363 - https://github.com/kubernetes/kubernetes/pull/132522#discussion_r2198959080 - https://github.com/kubernetes/kubernetes/pull/132522#discussion_r2198994863 - …,,,,,,Anecdotal,comment,,,,,,,,2025-07-11,github/sunya-ch,https://github.com/kubernetes/kubernetes/pull/132522#issuecomment-3060090996,repo: kubernetes/kubernetes | issue: DRA: Implementation of Consumable Capacity (KEP-5075) | keyword: workaround
@macsko Thank you so much for your suggestions. I have applied your suggestions in this temporary commit https://github.com/kubernetes/kubernetes/pull/132522/commits/51f6603e2a60894f060ab9cd5f69ed6a54a6ac10. I will rebase and squeeze this temporary commit later.,,,,,,Anecdotal,comment,,,,,,,,2025-07-15,github/sunya-ch,https://github.com/kubernetes/kubernetes/pull/132522#issuecomment-3072140223,repo: kubernetes/kubernetes | issue: DRA: Implementation of Consumable Capacity (KEP-5075) | keyword: workaround
"Hello! Appreciate all of your efforts with this PR! A gentle reminder that the code freeze has started [02:00 UTC Friday 25th July 2025](https://everytimezone.com/s/a2c01c54). Please make sure any PRs have both lgtm and approved labels ASAP, and file an [Exception](https://github.com/kubernetes/sig-release/blob/master/releases/EXCEPTIONS.md) if you haven't done it yet. Thanks!",,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/tico88612,https://github.com/kubernetes/kubernetes/pull/132522#issuecomment-3100685881,repo: kubernetes/kubernetes | issue: DRA: Implementation of Consumable Capacity (KEP-5075) | keyword: workaround
"Approving for `pkg/scheduler/framework/autoscaler_contract/OWNERS`. The CA/scheduler contract change LGTM apart from the fact that `AllocatedState` is imported from an external module (because changes to the struct can break the contract without flagging it during review). I'm okay with merging this as-is for alpha given the time constraints, and the fact that the feature won't work with Cluster Autoscaler until beta anyway. But IMO we should definitely revisit this for beta. /approve",,,,,,Anecdotal,comment,,,,,,,,2025-07-23,github/towca,https://github.com/kubernetes/kubernetes/pull/132522#issuecomment-3109537412,repo: kubernetes/kubernetes | issue: DRA: Implementation of Consumable Capacity (KEP-5075) | keyword: workaround
"@liggitt @pohly I have updated the API according to what we discuss yesterday. ### Before - If allowMultipleAllocations set, all capacity is non-consumable (act as attribute) - Total capacity requests can overcommitted (sum(capacity.requests) > capacity value) if it is non-consumable capacity - If sharingPolicy set, capacity is consumable based on defined policy. - If no device resource request, default value consumed. ### After - If allowMultipleAllocations set, all capacity is consumable and …",,,,,,Anecdotal,comment,,,,,,,,2025-07-24,github/sunya-ch,https://github.com/kubernetes/kubernetes/pull/132522#issuecomment-3112678283,repo: kubernetes/kubernetes | issue: DRA: Implementation of Consumable Capacity (KEP-5075) | keyword: workaround
"Three specific things to pin down for API review: - [x] Clarify docs and ensure the implementation for `AllocationMode: All` behaves as discussed in https://github.com/kubernetes/kubernetes/pull/132522#discussion_r2229312943 - [x] Report `consumedCapacity` for all device capacities for multi-allocatable device, not just ones with `requestPolicy` - https://github.com/kubernetes/kubernetes/pull/132522#discussion_r2229381142 - [x] Drop `zeroConsumption` from this first iteration - https://github.c…",,,,,,Anecdotal,comment,,,,,,,,2025-07-24,github/liggitt,https://github.com/kubernetes/kubernetes/pull/132522#issuecomment-3114724673,repo: kubernetes/kubernetes | issue: DRA: Implementation of Consumable Capacity (KEP-5075) | keyword: workaround
> Please extend > > https://github.com/kubernetes/kubernetes/blob/b93bcbfba1327b18efc1689a667700dd41af2bac/staging/src/k8s.io/dynamic-resource-allocation/resourceslice/resourceslicecontroller.go#L271-L288 > > > to also detect that DRAConsumableCapacity was disabled. Thank you and I updated. Could you please confirm this patch https://github.com/kubernetes/kubernetes/compare/6a4dac245fa2aa56b7c1d480c71b3c5589643318..46dcc7a26754ac58ba4f519ca2df98a48d5f7ec5?,,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/sunya-ch,https://github.com/kubernetes/kubernetes/pull/132522#issuecomment-3126192205,repo: kubernetes/kubernetes | issue: DRA: Implementation of Consumable Capacity (KEP-5075) | keyword: workaround
"@liggitt @johnbelamaric Thank you. I have updated the API, validation and scheduler implementation as follows. > Three specific things to pin down for API review: > > * [ ] Clarify docs and ensure the implementation for `AllocationMode: All` behaves as discussed in [DRA: Implementation of Consumable Capacity (KEP-5075) #132522 (comment)](https://github.com/kubernetes/kubernetes/pull/132522#discussion_r2229312943) I updated the API doc as you suggested. To ensure the behavior that has been discu…",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/sunya-ch,https://github.com/kubernetes/kubernetes/pull/132522#issuecomment-3126447220,repo: kubernetes/kubernetes | issue: DRA: Implementation of Consumable Capacity (KEP-5075) | keyword: workaround
The failing test in the alpha/beta job also fails elsewhere. I filed: https://github.com/kubernetes/kubernetes/issues/133250 /skip,,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/pohly,https://github.com/kubernetes/kubernetes/pull/132522#issuecomment-3127001607,repo: kubernetes/kubernetes | issue: DRA: Implementation of Consumable Capacity (KEP-5075) | keyword: workaround
"kubelet and scheduler for extended resource backed by DRA <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is…",,,,,,Anecdotal,issue,,,,,,,,2025-03-07,github/yliaog,https://github.com/kubernetes/kubernetes/pull/130653,repo: kubernetes/kubernetes | keyword: workaround | state: closed
"This PR [may require API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#what-apis-need-to-be-reviewed). If so, when the changes are ready, [complete the pre-review checklist and request an API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#mechanics). Status of requested reviews is tracked in the [API Review project](https://github.com/orgs/kubernetes/projects/169).",,,,,,Anecdotal,comment,,,,,,,,2025-05-14,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130653#issuecomment-2878373041,repo: kubernetes/kubernetes | issue: kubelet and scheduler for extended resource backed by DRA | keyword: workaround
/assign @pohly @johnbelamaric @klueska the implementation for https://github.com/kubernetes/enhancements/pull/5136 is ready for review,,,,,,Anecdotal,comment,,,,,,,,2025-05-14,github/yliaog,https://github.com/kubernetes/kubernetes/pull/130653#issuecomment-2878490461,repo: kubernetes/kubernetes | issue: kubelet and scheduler for extended resource backed by DRA | keyword: workaround
What will be the `k8s.io/kubelet/pkg/apis/podresources/v1.ListPodResourcesResponse.container.GetDevices()[0].GetResourceName()` with these changes? Right now it returns the extended resourcename used in podspec like `nvidia.com/gou`,,,,,,Anecdotal,comment,,,,,,,,2025-06-09,github/guptaNswati,https://github.com/kubernetes/kubernetes/pull/130653#issuecomment-2957277223,repo: kubernetes/kubernetes | issue: kubelet and scheduler for extended resource backed by DRA | keyword: workaround
@yliaog: there are quite a few test failures that point to genuine problems with the code. Can you start looking into those?,,,,,,Anecdotal,comment,,,,,,,,2025-07-10,github/pohly,https://github.com/kubernetes/kubernetes/pull/130653#issuecomment-3058036359,repo: kubernetes/kubernetes | issue: kubelet and scheduler for extended resource backed by DRA | keyword: workaround
@yliaog: you keep pushing changes that don't even pass simple testing in the CI. Do you need help with setting up an environment to check your changes locally?,,,,,,Anecdotal,comment,,,,,,,,2025-07-14,github/pohly,https://github.com/kubernetes/kubernetes/pull/130653#issuecomment-3070986318,repo: kubernetes/kubernetes | issue: kubelet and scheduler for extended resource backed by DRA | keyword: workaround
"i pushed the commit from the reviewer suggest, that caused the CI run failures. I have local test env for e2e and integration, i would check the tests locally before manually pushing the commits.",,,,,,Anecdotal,comment,,,,,,,,2025-07-14,github/yliaog,https://github.com/kubernetes/kubernetes/pull/130653#issuecomment-3071018079,repo: kubernetes/kubernetes | issue: kubelet and scheduler for extended resource backed by DRA | keyword: workaround
"> i pushed the commit from the reviewer suggest, that caused the CI run failures. Yes, that's a pitfall. Putting the PR back into draft mode avoids it, or manually applying the changes locally.",,,,,,Anecdotal,comment,,,,,,,,2025-07-15,github/pohly,https://github.com/kubernetes/kubernetes/pull/130653#issuecomment-3072061048,repo: kubernetes/kubernetes | issue: kubelet and scheduler for extended resource backed by DRA | keyword: workaround
"@johnbelamaric @pohly @mrunalp @liggitt @macsko thanks all for the feedback, addressed all coments, also reorganized the PR into 4 commits: api, codegen, impl, and test. PTAL.",,,,,,Anecdotal,comment,,,,,,,,2025-07-17,github/yliaog,https://github.com/kubernetes/kubernetes/pull/130653#issuecomment-3084870673,repo: kubernetes/kubernetes | issue: kubelet and scheduler for extended resource backed by DRA | keyword: workaround
"API bits lgtm, tag me once the implementation has review/approval and tests are green and I can tag for the API packages",,,,,,Anecdotal,comment,,,,,,,,2025-07-18,github/liggitt,https://github.com/kubernetes/kubernetes/pull/130653#issuecomment-3090024984,repo: kubernetes/kubernetes | issue: kubelet and scheduler for extended resource backed by DRA | keyword: workaround
"@pohly the single test failure is about windows (--- FAIL: TestPlugin/timeout (0.12s) Error: Error ""cannot allocate all claims"" does not contain ""timed out trying to allocate devices"" ), is it something we need to be concerned about? @liggitt is it a good idea to file a separate PR for apis only, i.e., take api types, codegen and some validation code from this PR?",,,,,,Anecdotal,comment,,,,,,,,2025-07-19,github/yliaog,https://github.com/kubernetes/kubernetes/pull/130653#issuecomment-3091449040,repo: kubernetes/kubernetes | issue: kubelet and scheduler for extended resource backed by DRA | keyword: workaround
"> the single test failure is about windows (--- FAIL: TestPlugin/timeout (0.12s) Error: Error ""cannot allocate all claims"" does not contain ""timed out trying to allocate devices"" ), is it something we need to be concerned about? It's unrelated to this PR, please ignore. Windows tests of kubelet features are problematic. /skip",,,,,,Anecdotal,comment,,,,,,,,2025-07-19,github/pohly,https://github.com/kubernetes/kubernetes/pull/130653#issuecomment-3092236326,repo: kubernetes/kubernetes | issue: kubelet and scheduler for extended resource backed by DRA | keyword: workaround
"> is it a good idea to file a separate PR for apis only, i.e., take api types, codegen and some validation code from this PR? It's good to structure those changes so that they are separate commits (like you have it), but we cannot merge the API separately because it would be non-functional unless the rest of the changes also get merged.",,,,,,Anecdotal,comment,,,,,,,,2025-07-19,github/pohly,https://github.com/kubernetes/kubernetes/pull/130653#issuecomment-3092236945,repo: kubernetes/kubernetes | issue: kubelet and scheduler for extended resource backed by DRA | keyword: workaround
"> It's good to structure those changes so that they are separate commits (like you have it) Well, almost :grin: Your ""implementation"" commit contains parts of the API implementation (relevant for an API reviewer) and the feature implementation (relevant for DRA reviewers). I would keep those separate, but you can also leave it as it is. Also, the commit message just says ""fix"".",,,,,,Anecdotal,comment,,,,,,,,2025-07-19,github/pohly,https://github.com/kubernetes/kubernetes/pull/130653#issuecomment-3092308234,repo: kubernetes/kubernetes | issue: kubelet and scheduler for extended resource backed by DRA | keyword: workaround
"@pohly I've pulled your commit https://github.com/yliaog/kubernetes/pull/1 into this PR, with some fixes to make e2e and unittests pass. PTAL.",,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/yliaog,https://github.com/kubernetes/kubernetes/pull/130653#issuecomment-3104494541,repo: kubernetes/kubernetes | issue: kubelet and scheduler for extended resource backed by DRA | keyword: workaround
"Support scheduling hint in Pod.Spec to have sticky scheduling ### What would you like to be added? Provide a mechanism to hint the scheduler to schedule a pod, which is to be created, to a node where the pod's owner controller believe it's optimal to schedule that specific pod to that node. ### Why is this needed? Some batch workloads have checkpoints saved in local storage, although they can restore checkpoints from previous node's storage via network, the network overhead is non-trivial. This…",,,,,,Anecdotal,issue,,,,,,,,2025-07-24,github/linxiulei,https://github.com/kubernetes/kubernetes/issues/133181,repo: kubernetes/kubernetes | keyword: workaround | state: closed
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-07-24,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/133181#issuecomment-3114189935,repo: kubernetes/kubernetes | issue: Support scheduling hint in Pod.Spec to have sticky scheduling | keyword: workaround
"If the hint is about a future Pod, this change will need a KEP (if it's about a hint about where an existing Pod might schedule, we already support that - for example with preferred-during-scheduling hints) You can use a Mutating admission policy to turn an annotation value into a suggested node for scheduling; or, if you cannot, watch this space.",,,,,,Anecdotal,comment,,,,,,,,2025-07-24,github/lmktfy,https://github.com/kubernetes/kubernetes/issues/133181#issuecomment-3115327798,repo: kubernetes/kubernetes | issue: Support scheduling hint in Pod.Spec to have sticky scheduling | keyword: workaround
Looks like `pod.spec.affinity.nodeAffinity.preferredDuringSchedulingIgnoredDuringExecution` is exactly what I need. Thanks @lmktfy !,,,,,,Anecdotal,comment,,,,,,,,2025-07-25,github/linxiulei,https://github.com/kubernetes/kubernetes/issues/133181#issuecomment-3115408372,repo: kubernetes/kubernetes | issue: Support scheduling hint in Pod.Spec to have sticky scheduling | keyword: workaround
Kubernetes controller manager may fail to manage deployment rollout with minReadySeconds and pod restarts ### What happened? ``` kubectl rollout restart deployment alpine ; kubectl rollout status deployment alpine ``` may exceed the deployment rollout progress deadlines if the deployment sets `minReadySeconds` and a new pod restarts during the rollout before the minimum ready seconds. ### What did you expect to happen? Deployment rollout is successful. ### How can we reproduce it (as minimally …,,,,,,Anecdotal,issue,,,,,,,,2022-02-21,github/rtheis,https://github.com/kubernetes/kubernetes/issues/108266,repo: kubernetes/kubernetes | keyword: workaround | state: closed
It appears that deleting the Pending pod gets the rollout to continue - maybe anything that the right controller sees as a change would do the trick.,,,,,,Anecdotal,comment,,,,,,,,2022-03-26,github/jmcmeek,https://github.com/kubernetes/kubernetes/issues/108266#issuecomment-1079692589,repo: kubernetes/kubernetes | issue: Kubernetes controller manager may fail to manage deployment rollout with minReadySeconds and pod restarts | keyword: workaround
Looking at: https://github.com/kubernetes/kubernetes/blob/542a979c036e7cab16f12c324aa93b0149a81c1c/pkg/controller/replicaset/replica_set.go#L466-L471 I wonder if that needs to check if the pod is not yet available - [IsPodAvailable](https://github.com/kubernetes/kubernetes/blob/ea0764452222146c47ec826977f49d7001b0ea8c/staging/src/k8s.io/kubectl/pkg/util/podutils/podutils.go#L32) takes into account the ready condition and minReadySeconds - and add that as an OR condition for requeuing the reques…,,,,,,Anecdotal,comment,,,,,,,,2022-03-26,github/jmcmeek,https://github.com/kubernetes/kubernetes/issues/108266#issuecomment-1079757323,repo: kubernetes/kubernetes | issue: Kubernetes controller manager may fail to manage deployment rollout with minReadySeconds and pod restarts | keyword: workaround
"That tweak didn't do anything obvious. I think I need to understand the deployment / rollout code. It seem odd to see: ``` $ kubectl get deploy,rs;kubectl get pod -o wide NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/alpine 2/2 2 1 3h50m NAME DESIRED CURRENT READY AGE replicaset.apps/alpine-55f4d79f77 2 2 1 71m replicaset.apps/alpine-846b9df9cb 1 1 1 104m replicaset.apps/alpine-946db6d6f 0 0 0 110m replicaset.apps/alpine-c466659dd 0 0 0 3h49m replicaset.apps/alpine-f8b84b8d7 0 0 0 3h50m N…",,,,,,Anecdotal,comment,,,,,,,,2022-03-27,github/jmcmeek,https://github.com/kubernetes/kubernetes/issues/108266#issuecomment-1080015529,repo: kubernetes/kubernetes | issue: Kubernetes controller manager may fail to manage deployment rollout with minReadySeconds and pod restarts | keyword: workaround
"Testing with a StatefulSet works as expected. So the problem seems isolated to Deployments. ``` kubectl apply -f - <<EOF kind: StatefulSet apiVersion: apps/v1 metadata: labels: app: alpine name: alpine spec: minReadySeconds: 30 replicas: 3 revisionHistoryLimit: 2 selector: matchLabels: app: alpine serviceName: ""alpine"" template: metadata: labels: app: alpine spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: I…",,,,,,Anecdotal,comment,,,,,,,,2022-04-06,github/rtheis,https://github.com/kubernetes/kubernetes/issues/108266#issuecomment-1090599488,repo: kubernetes/kubernetes | issue: Kubernetes controller manager may fail to manage deployment rollout with minReadySeconds and pod restarts | keyword: workaround
Changing `maxSurge: 3` to `maxSurge: 1` still hits the problem as long as `podAntiAffinity` yields a pending pod during the rollout.,,,,,,Anecdotal,comment,,,,,,,,2022-04-25,github/rtheis,https://github.com/kubernetes/kubernetes/issues/108266#issuecomment-1109031335,repo: kubernetes/kubernetes | issue: Kubernetes controller manager may fail to manage deployment rollout with minReadySeconds and pod restarts | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs. This bot triages issues and PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue or PR as fresh with `/remove-lifecycle stale` - Mark this issue or PR…",,,,,,Anecdotal,comment,,,,,,,,2022-07-27,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/108266#issuecomment-1197307185,repo: kubernetes/kubernetes | issue: Kubernetes controller manager may fail to manage deployment rollout with minReadySeconds and pod restarts | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs. This bot triages issues and PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue or PR as fresh with `/remove-lifecycle stale` - Mark this issue or PR…",,,,,,Anecdotal,comment,,,,,,,,2022-10-25,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/108266#issuecomment-1291118987,repo: kubernetes/kubernetes | issue: Kubernetes controller manager may fail to manage deployment rollout with minReadySeconds and pod restarts | keyword: workaround
"### Replica sets As mentioned, the most problematic parts of the issue mainly apply to replica sets. The timeline of a problematic pod (there needs to be other unready pods for this to occurr) ``` 0s pod NotReady -> Ready (check for available scheduled in 30s) 15s pod Ready -> NotReady 15.5s pod NotReady -> Ready (wishing for a check for available in 30s, but only previous check in 15s is kept and no new check is scheduled) 30s replica set checks the replica for availability, which it is not (s…",,,,,,Anecdotal,comment,,,,,,,,2022-11-03,github/atiratree,https://github.com/kubernetes/kubernetes/issues/108266#issuecomment-1302639795,repo: kubernetes/kubernetes | issue: Kubernetes controller manager may fail to manage deployment rollout with minReadySeconds and pod restarts | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs. This bot triages issues and PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue or PR as fresh with `/remove-lifecycle stale` - Mark this issue or PR…",,,,,,Anecdotal,comment,,,,,,,,2023-02-01,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/108266#issuecomment-1412759432,repo: kubernetes/kubernetes | issue: Kubernetes controller manager may fail to manage deployment rollout with minReadySeconds and pod restarts | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2023-05-02,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/108266#issuecomment-1532260639,repo: kubernetes/kubernetes | issue: Kubernetes controller manager may fail to manage deployment rollout with minReadySeconds and pod restarts | keyword: workaround
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2024-01-19,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/108266#issuecomment-1900474482,repo: kubernetes/kubernetes | issue: Kubernetes controller manager may fail to manage deployment rollout with minReadySeconds and pod restarts | keyword: workaround
"@rtheis: Closing this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/108266#issuecomment-1900478084): >/close Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository. </d…",,,,,,Anecdotal,comment,,,,,,,,2024-01-19,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/108266#issuecomment-1900478236,repo: kubernetes/kubernetes | issue: Kubernetes controller manager may fail to manage deployment rollout with minReadySeconds and pod restarts | keyword: workaround
"@atiratree: Reopened this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/108266#issuecomment-1960500473): >not seeing a resolution >/reopen Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=…",,,,,,Anecdotal,comment,,,,,,,,2024-02-22,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/108266#issuecomment-1960500540,repo: kubernetes/kubernetes | issue: Kubernetes controller manager may fail to manage deployment rollout with minReadySeconds and pod restarts | keyword: workaround
"This issue has not been updated in over 1 year, and should be re-triaged. You can: - Confirm that this issue is still relevant with `/triage accepted` (org members only) - Close this issue with `/close` For more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/ /remove-triage accepted",,,,,,Anecdotal,comment,,,,,,,,2025-03-13,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/108266#issuecomment-2722798654,repo: kubernetes/kubernetes | issue: Kubernetes controller manager may fail to manage deployment rollout with minReadySeconds and pod restarts | keyword: workaround
"nftables kube-proxy TODO #### For 1.30: - [x] decide on required kernel/`nft` versions (@danwinship, #124152) - discussion in the first half of #122296, plus some comments in this issue, and then #122743 - [ ] add some useful metrics (@aojea / @npinaeva ?) - we don't know exactly what these will be: [discussion in the KEP](https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/3866-nftables-proxy/README.md#monitoring-requirements) - [x] clean up existing metrics stuff a bit at …",,,,,,Anecdotal,issue,,,,,,,,2024-01-03,github/danwinship,https://github.com/kubernetes/kubernetes/issues/122572,repo: kubernetes/kubernetes | keyword: workaround | state: closed
"I can easily switch kernels in my env (and ""nft"" for that matter), so I can check that. But what to check, and how precise? Is it enough to verify basic traffic, or is a full K8s e2e required? Even if I can switch kernels, download and building them is a tedious task (especially if there are gcc version problems), so the fewer the better. #### Update K8s e2e with: ``` FOCUS='\[sig-network\].*ervice' SKIP='Disruptive|Serial|ESIPP|DNS|GCE|finalizer' ``` runs fine on `linux-6.6.9` (latest at the t…",,,,,,Anecdotal,comment,,,,,,,,2024-01-04,github/uablrek,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1876311340,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"> actually, we've pretty much decided on kernel 5.6, though we will probably implement this via an nft --check-based check rather than via a kernel version number check. @danwinship Any idea on what to check with ""nft --check""? And, is it OK to discuss in this PR? I can run on linux-5.6.10 without problems, but on linux-5.6 and linux-5.6.4 I get errors: ``` E0104 13:16:35.176593 405 proxier.go:1561] ""nftables sync failed"" err=< /dev/stdin:50:1-116: Error: Could not process rule: Invalid argumen…",,,,,,Anecdotal,comment,,,,,,,,2024-01-04,github/uablrek,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1877082016,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"> I can run on linux-5.6.10 without problems, but on linux-5.6 and linux-5.6.4 I get errors: @uablrek For v1.29.0, the minimal kernel version is linux-5.6.9 according to previous investigation: https://github.com/kubernetes/kubernetes/pull/122296#issuecomment-1864347600 > Yes, I managed to run it with 5.6 but only after 5.6.9. It's because nftables CLI ""Set NFT_SET_CONCAT flag for sets with concatenated ranges"" since [0.9.5](https://www.netfilter.org/projects/nftables/files/changes-nftables-0.9…",,,,,,Anecdotal,comment,,,,,,,,2024-01-04,github/tnqn,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1877130093,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"> And, is it OK to discuss in this PR? I assume this PR is going to get a lot of discussion about a lot of things, and that's fine, but if you're planning to grab one of the items anyway, maybe file a separate issue for it (or an initial/placeholder PR) and move discussion there? > Is it enough to verify basic traffic, or is a full K8s e2e required? We need to verify that every kind of rule the nftables Proxier can generate works correctly. So somewhere in between; full e2e would do the trick, …",,,,,,Anecdotal,comment,,,,,,,,2024-01-04,github/danwinship,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1877189103,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"> After #122296, the minimal version may be relaxed a lot. I guess it's [4.1](https://wiki.nftables.org/wiki-nftables/index.php/List_of_updates_since_Linux_kernel_3.13#4.1) but haven't tested it on that version yet. Currently we use `th` expressions, which were added in 5.3, though there's a workaround for that if needed. Other than that though, there are also bugfixes to worry about; the list of updates in the wiki includes really major ones, but I worry that the further back we go, the more m…",,,,,,Anecdotal,comment,,,,,,,,2024-01-04,github/danwinship,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1877196450,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"> Any idea on what to check with ""nft --check""? So, per [the wiki](https://wiki.nftables.org/wiki-nftables/index.php/List_of_updates_since_Linux_kernel_3.13), if we want to require 5.4, I guess you could try a rule with ""`meta time`""... though it seems a little weird to have the test be based on something we aren't actually using, so maybe `th` would be better?",,,,,,Anecdotal,comment,,,,,,,,2024-01-04,github/danwinship,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1877201969,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"Also, I haven't investigated the `nft` version situation at all, and there may be constraints there that make us need a newer `nft` than ""whatever version added support for the oldest kernel feature we're using"". Especially, I'm not sure exactly when `--json` support was added, and we definitely need that.",,,,,,Anecdotal,comment,,,,,,,,2024-01-04,github/danwinship,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1877212874,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"> Currently we use `th` expressions, which were added in 5.3 It seems `th` expressions is only an alias and the change is needed in CLI only: [proto: add pseudo th protocol to match d/sport in generic way](https://git.netfilter.org/nftables/commit/?h=v0.9.2&id=a43a696443a150f448d182fe708a43d7279de715). See `As ""th"" is an alias for the raw expression, no dependency is generated`. And kernel [5.3](https://kernelnewbies.org/Linux_5.3) doesn't mention related change in kernel at all. I successfully…",,,,,,Anecdotal,comment,,,,,,,,2024-01-04,github/tnqn,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1877463641,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"> See `As ""th"" is an alias for the raw expression, no dependency is generated`. ""dependency"" here is nftables-maintainer-speak. It's warning you that `th` doesn't imply `{tcp,udp,sctp}` so if you're careless with your rules, a `th` expression might end up matching bits inside an ICMP packet or something. (I got the ""5.3"" requirement from [here](https://wiki.nftables.org/wiki-nftables/index.php/Matching_packet_headers#Matching_UDP.2FTCP_headers_in_the_same_rule) but I agree, I can't see any comm…",,,,,,Anecdotal,comment,,,,,,,,2024-01-04,github/danwinship,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1877614647,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
I have tested some versions of `nft` ([archives](https://netfilter.org/projects/nftables/files/)): ``` nftables v0.9.2 (Scram) 2019-08-19 nftables v1.0.0 (Fearless Fosdick #2) 2021-08-19 ``` Both works with K8s e2e with FOCUS/SKIP as in https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1876311340. Can we settle for nft $\ge$ v0.9.2? #### Update There is an aspect of dynamic lib's that I haven't covered. For instance I have used `libnftnl` 1.2.6 ([archives](https://netfilter.or…,,,,,,Anecdotal,comment,,,,,,,,2024-01-05,github/uablrek,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1878157602,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"> Can we settle for nft ≥ v0.9.2? Yes. I think we'll officially declare that we support kernel 5.4+ and nftables command-line 0.9.2+, and we can use a `th` expression to check that. (Which means really we'll support kernel 5.3, but we won't advertise it that way.) FTR (and maybe worth documenting in a comment wherever we put the check): - If we decide we need concatenations-with-ranges support again later, that would imply 5.6/0.9.4. - If we want to use `typeof` in sets/maps, that would require…",,,,,,Anecdotal,comment,,,,,,,,2024-01-05,github/danwinship,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1878809431,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
For perf job do we need something similar to [ci-kubernetes-e2e-gce-scale-performance](https://github.com/kubernetes/test-infra/blob/dd8dfec9b2dd99999a1187fe861c90a2694556d3/config/jobs/kubernetes/sig-scalability/sig-scalability-release-blocking-jobs.yaml#L62C9-L62C48) configured for NFTables with lower cadence and nodes?,,,,,,Anecdotal,comment,,,,,,,,2024-01-09,github/aroradaman,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1883561467,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"I'm not sure exactly what we want for perf... @aojea may have thoughts on that, and sig-scalability may have more information on what sort of big tests we're _able_ to run. (I assume we can't spin up a 2000 node cluster just because we're bored...) Maybe open an issue to start the discussion on that? (Antonio also said he was going to start thinking about metrics that the perf job could be measuring.)",,,,,,Anecdotal,comment,,,,,,,,2024-01-09,github/danwinship,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1883625682,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"> * change `--nodeport-addresses` behavior to default to ""primary node IP(s) only"" rather than ""all node IPs"". Do we need to implement it **only** in nftables mode or in all modes(both in iptables and ipvs)? And by the way, I think we should create new issue to track every task. This issue is not a good place to discuss details.",,,,,,Anecdotal,comment,,,,,,,,2024-01-10,github/nayihz,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1884246087,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"Will try to tackle `ensure unit test parity with iptables.` and `change --nodeport-addresses behavior` (@nayihz if you wanted to work on the latter, lmk and I will pick something else)",,,,,,Anecdotal,comment,,,,,,,,2024-01-10,github/npinaeva,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1884403689,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"> (@nayihz if you wanted to work on the latter, lmk and I will pick something else) Yes, I want to work on it after we confirm some details.",,,,,,Anecdotal,comment,,,,,,,,2024-01-10,github/nayihz,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1884422631,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"picking up `reject connections on invalid ports of service IPs`. Instead of directly linking the services chain to service port chain (using verdict maps), can we create a new chain(service-ports) which links services chain to service-ports chain if daddr matches any of {cluster|loadbalancer|external}IP. We can then add a rule to match on service port verdict map in this new chain and simply reject if nothing matches. ```go // create set of IPs (cluster + loadbalancer + external) add set ip kub…",,,,,,Anecdotal,comment,,,,,,,,2024-01-10,github/aroradaman,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1884474235,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"> Do we need to implement it **only** in nftables mode or in all modes(both in iptables and ipvs)? We're only changing behavior of nftables mode. All of the todo items here are for nftables mode only, except for the ""Additional metrics for iptables mode"" section. > And by the way, I think we should create new issue to track every task. This issue is not a good place to discuss details. Yes, people should comment here if they want to claim an item, but it's best to split out discussion. You can …",,,,,,Anecdotal,comment,,,,,,,,2024-01-10,github/danwinship,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1885039699,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"I will check [making it possible to run multiple instances of kube-proxy](https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/3866-nftables-proxy/README.md#multiple-instances-of-kube-proxy). Not the most urgent, but fun :smile: It can be tested as a PoC with Multus, independent of [K8s multi-network](https://github.com/kubernetes/enhancements/pull/3700), but will be really useful with it. Load balancing is not yet considered in `K8s multi-network`, but I think is must at som…",,,,,,Anecdotal,comment,,,,,,,,2024-01-10,github/uablrek,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1885137117,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"### An update on `nft` versions On KinD nft v0.9.8 segv's. On a KinD node: ``` apt update apt install -y nftables nft list ruleset Segmentation fault (core dumped) ``` Stack trace: ``` Core was generated by `nft list ruleset'. Program terminated with signal SIGSEGV, Segmentation fault. #0 0x0000000000000000 in ?? () (gdb) bt #0 0x0000000000000000 in ?? () #1 0x00007f3691345743 in set_make_key (attr=attr@entry=0x558676fe2f1c) at netlink.c:767 #2 0x00007f3691347bba in netlink_delinearize_set ( ct…",,,,,,Anecdotal,comment,,,,,,,,2024-01-12,github/uablrek,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1889780672,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
`registry.k8s.io/kube-proxy:v1.29.0` was built with `v1.0.6 (Lester Gooch #5)` I exec into kube-proxy pod to run nft in a kind cluster 😅,,,,,,Anecdotal,comment,,,,,,,,2024-01-12,github/aroradaman,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1889790041,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"@danwinship Is there a reason why we have a check on daddr, port and proto for individual service mark-for-maq jump? Packets will only be jumped to this chain when they match on daddr, port and proto. ``` chain service-JKB34KST-default/test/tcp/http { ip daddr 10.96.83.93 tcp dport 80 ip saddr != 10.244.0.0/16 jump mark-for-masquerade numgen random ... } # maybe this can be simplified to chain service-JKB34KST-default/test/tcp/http { ip saddr != 10.244.0.0/16 jump mark-for-masquerade numgen ran…",,,,,,Anecdotal,comment,,,,,,,,2024-05-07,github/aroradaman,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-2098796857,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
> maybe reuse the knftables.Transaction.operation arrays somehow? How about a **Reset** method on transactions? We can call **tx.Reset** in sync loop.,,,,,,Anecdotal,comment,,,,,,,,2024-05-22,github/aroradaman,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-2124755371,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"This issue is labeled with `priority/important-soon` but has not been updated in over 90 days, and should be re-triaged. Important-soon issues must be staffed and worked on either currently, or very soon, ideally in time for the next release. You can: - Confirm that this issue is still relevant with `/triage accepted` (org members only) - Deprioritize it with `/priority important-longterm` or `/priority backlog` - Close this issue with `/close` For more details on the triage process, see https:…",,,,,,Anecdotal,comment,,,,,,,,2024-10-07,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-2396865174,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
I don't see `nftables` mentioned as a `ProxyMode` option in [the documentation](https://kubernetes.io/docs/reference/config-api/kube-proxy-config.v1alpha1/#kubeproxy-config-k8s-io-v1alpha1-ProxyMode). ref.: https://kubernetes.slack.com/archives/C09QYUH5W/p1733790390408679,,,,,,Anecdotal,comment,,,,,,,,2024-12-10,github/performantdata,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-2532199162,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"fix data race in OIDC integration tests by serializing test server starts /kind flake fixes https://github.com/kubernetes/kubernetes/issues/133082 With the change: ``` ➜ KUBE_RACE=""-race"" SHOW=--short=false make test-integration WHAT=./test/integration/apiserver/oidc KUBE_TEST_ARGS=""-v -tags e2e"" +++ [0720 19:48:24] Set GOMAXPROCS automatically to 8 +++ [0720 19:48:24] Checking etcd is on PATH /home/<>/go/src/k8s.io/kubernetes/third_party/etcd/etcd +++ [0720 19:48:24] Starting etcd instance etc…",,,,,,Anecdotal,issue,,,,,,,,2025-07-20,github/aramase,https://github.com/kubernetes/kubernetes/pull/133086,repo: kubernetes/kubernetes | keyword: workaround | state: closed
Unit test failure unrelated. ``` {Failed panic: test timed out after 3m0s running tests: TestConsistencyCheckerDigestMatches (10s) goroutine 130961 [running]: testing.(*M).startAlarm.func1() /usr/local/go/src/testing/testing.go:2484 +0x605 created by time.goFunc /usr/local/go/src/time/sleep.go:215 +0x45 ``` /test pull-kubernetes-unit,,,,,,Anecdotal,comment,,,,,,,,2025-07-20,github/aramase,https://github.com/kubernetes/kubernetes/pull/133086#issuecomment-3094765480,repo: kubernetes/kubernetes | issue: fix data race in OIDC integration tests by serializing test server starts | keyword: workaround
Please open a separate issue for fixing `kubeapiserverapptesting.StartTestServer` directly (i.e. the admission logic) as it should tolerate parallel starts. This is an okay workaround to deflake CI. /lgtm /approve,,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/enj,https://github.com/kubernetes/kubernetes/pull/133086#issuecomment-3097083848,repo: kubernetes/kubernetes | issue: fix data race in OIDC integration tests by serializing test server starts | keyword: workaround
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/133086#"" title=""Author self-approved"">aramase</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/133086#issuecomment-3097083848"" title=""Approved"">enj</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https://git.k8s.io/c…",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/133086#issuecomment-3097084617,repo: kubernetes/kubernetes | issue: fix data race in OIDC integration tests by serializing test server starts | keyword: workaround
feat(kubelet): migrate kuberuntime to contextual logging #### What type of PR is this? /kind feature #### What this PR does / why we need it: This PR migrates the `pkg/kubelet/kuberuntime` package to use contextual logging as part of the ongoing effort to improve structured logging in Kubernetes. The main changes include: - Replace all `klog` calls with contextual logging using `logger` from `klog.FromContext(ctx)` - Add context parameters to functions that need to pass context for logging - Up…,,,,,,Anecdotal,issue,,,,,,,,2025-06-20,github/soma00333,https://github.com/kubernetes/kubernetes/pull/132427,repo: kubernetes/kubernetes | keyword: workaround | state: closed
"Hi @soma00333. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once the p…",,,,,,Anecdotal,comment,,,,,,,,2025-06-20,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/132427#issuecomment-2991246653,repo: kubernetes/kubernetes | issue: feat(kubelet): migrate kuberuntime to contextual logging | keyword: workaround
"/lgtm @soma00333 As your PR is quite big, I'd suggest to ask approvers on Slack or even on SIG-Node meeting to look at it. Otherwise it's going to be conflicting with other changes and you'll end up rebasing it again and again. /assign @mrunalp @SergeyKanzhelev @dchen1107 for approval",,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/bart0sh,https://github.com/kubernetes/kubernetes/pull/132427#issuecomment-3018773268,repo: kubernetes/kubernetes | issue: feat(kubelet): migrate kuberuntime to contextual logging | keyword: workaround
"Remove deprecated LegacySidecarContainers feature gate <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a …",,,,,,Anecdotal,issue,,,,,,,,2025-04-25,github/gjkim42,https://github.com/kubernetes/kubernetes/pull/131463,repo: kubernetes/kubernetes | keyword: workaround | state: closed
"Skipping CI for Draft Pull Request. If you want CI signal for your change, please convert it to an actual PR. You can still manually trigger a test run with `/test all`",,,,,,Anecdotal,comment,,,,,,,,2025-04-25,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131463#issuecomment-2829534674,repo: kubernetes/kubernetes | issue: Remove deprecated LegacySidecarContainers feature gate | keyword: workaround
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-04-25,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131463#issuecomment-2829534872,repo: kubernetes/kubernetes | issue: Remove deprecated LegacySidecarContainers feature gate | keyword: workaround
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/131463#"" title=""Author self-approved"">gjkim42</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/131463#discussion_r2211150482"" title=""Approved"">SergeyKanzhelev</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https://g…",,,,,,Anecdotal,comment,,,,,,,,2025-07-16,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131463#issuecomment-3079679105,repo: kubernetes/kubernetes | issue: Remove deprecated LegacySidecarContainers feature gate | keyword: workaround
"etcd and kube-apiserver does not start after incorrect machine shutdown I was told that this is correct issue tracker for my problem. Previously I've post this issue [here ](https://github.com/kubernetes/kubeadm/issues/2041) ## Environment ``` mcajkovs@ubuntu:~$ cat /etc/os-release NAME=""Ubuntu"" VERSION=""18.04.3 LTS (Bionic Beaver)"" ID=ubuntu ID_LIKE=debian PRETTY_NAME=""Ubuntu 18.04.3 LTS"" VERSION_ID=""18.04"" HOME_URL=""https://www.ubuntu.com/"" SUPPORT_URL=""https://help.ubuntu.com/"" BUG_REPORT_UR…",,,,,,Anecdotal,issue,,,,,,,,2020-02-26,github/mcajkovs,https://github.com/kubernetes/kubernetes/issues/88574,repo: kubernetes/kubernetes | keyword: workaround | state: closed
"> > > @mcajkovs: There are no sig labels on this issue. Please add an appropriate label by using one of the following commands: > > * `/sig <group-name>` > > * `/wg <group-name>` > > * `/committee <group-name>` > > > Please see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available. sorry but how? I have no gear icon on right side next to labels",,,,,,Anecdotal,comment,,,,,,,,2020-02-26,github/mcajkovs,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-591411383,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
> > > What is the content of the etcd container logs? > > /cc @jpbetz Is this what you want? ``` mcajkovs@ubuntu:~$ docker ps -a | grep etcd | awk '{print $NF}' | while read i; do docker logs -t $i; done 2020-02-26T13:48:24.613993533Z [WARNING] Deprecated '--logger=capnslog' flag is set; use '--logger=zap' flag instead 2020-02-26T13:48:24.614022261Z 2020-02-26 13:48:24.613797 I | etcdmain: etcd Version: 3.4.3 2020-02-26T13:48:24.614026299Z 2020-02-26 13:48:24.613832 I | etcdmain: Git SHA: 3cf2f…,,,,,,Anecdotal,comment,,,,,,,,2020-02-26,github/mcajkovs,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-591441875,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
> > > is the `/var/lib/etcd/member` directory present after you have rebooted? > is there a side-process that cleans it up? Yes it is presented. No there is no side-process job that cleans it up. Should I clean it now? ``` mcajkovs@ubuntu:~$ sudo tree /var/lib/etcd/member /var/lib/etcd/member ├── snap │ ├── 0000000000000005-0000000000068fdb.snap │ ├── 0000000000000005-000000000006b6ec.snap │ ├── 0000000000000005-000000000006ddfd.snap │ ├── 0000000000000005-000000000007050e.snap │ ├── 0000000000…,,,,,,Anecdotal,comment,,,,,,,,2020-02-26,github/mcajkovs,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-591466352,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
"> Should I clean it now? no. does this cluster have more etcd instances / control-plane Nodes? if yes, are you seeing the same issue on those?",,,,,,Anecdotal,comment,,,,,,,,2020-02-26,github/neolit123,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-591468787,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
"> > > > Should I clean it now? > > no. > > does this cluster have more etcd instances / control-plane Nodes? if yes, are you seeing the same issue on those? no, it is a single node cluster",,,,,,Anecdotal,comment,,,,,,,,2020-02-26,github/mcajkovs,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-591469859,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
"From etcd log: ``` 2020-02-26T13:48:24.702625099Z 2020-02-26 13:48:24.702261 C | etcdserver: recovering backend from snapshot error: failed to find database snapshot file (snap: snapshot file doesn't exist) ``` It seems the panic is by design if the recovery fails (from server.go): ``` if be, err = recoverSnapshotBackend(cfg, be, *snapshot); err != nil { cfg.Logger.Panic(""failed to recover v3 backend from snapshot"", zap.Error(err)) ```",,,,,,Anecdotal,comment,,,,,,,,2020-02-26,github/tedyu,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-591539823,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
I've tried to rename` /var/lib/etcd/member` to `/var/lib/etcd/member.bak` and then isssue `sudo systemctl restart kubelet` but after those steps I get only one service running in cluster ``` mcajkovs@ubuntu:~$ kubectl get all -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 61s ``` If I understand correct the issue is due to broken etcd database. Are there any best practices for periodically backing up etcd database during c…,,,,,,Anecdotal,comment,,,,,,,,2020-02-27,github/mcajkovs,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-591836322,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
> I've tried to rename /var/lib/etcd/member to /var/lib/etcd/member.bak and then isssue sudo systemctl restart kubelet but after those steps I get only one service running in cluster the folder contains the data of your existing cluster. deleting it would mean data loss. > If I understand correct the issue is due to broken etcd database. this seems more like a bug in the etcd server where it cannot restore it's previous state. > Are there any best practices for periodically backing up etcd data…,,,,,,Anecdotal,comment,,,,,,,,2020-02-27,github/neolit123,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-591931659,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
> Are there any best practices for periodically backing up etcd database during cluster operation? if you have used `kubeadm upgrade apply` on that Node you should have an etcd backup under `/etc/kubernetes/`.,,,,,,Anecdotal,comment,,,,,,,,2020-02-27,github/neolit123,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-591932194,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
"@fedebongio: GitHub didn't allow me to assign the following users: jingyih. Note that only [kubernetes members](https://github.com/orgs/kubernetes/people), repo collaborators and people who have commented on this issue/PR can be assigned. Additionally, issues/PRs can only have 10 assignees at the same time. For more information please see [the contributor guide](https://git.k8s.io/community/contributors/guide/#issue-assignment-in-github) <details> In response to [this](https://github.com/kubern…",,,,,,Anecdotal,comment,,,,,,,,2020-02-27,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-592181599,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
"> Or is this type of issue normal when cluster is incorrectly shut down? No this is not normal. etcd is designed to be recoverable from unexpected shut down. While I am trying to reproduce this issue, on your side could you add ""--logger=zap"" to etcd starting command (in etcd.yaml). It will print more info such as exactly which snapshot file it fails to find.",,,,,,Anecdotal,comment,,,,,,,,2020-02-28,github/jingyih,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-592611174,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
"> > > > Or is this type of issue normal when cluster is incorrectly shut down? > > No this is not normal. etcd is designed to be recoverable from unexpected shut down. > > While I am trying to reproduce this issue, on your side could you add ""--logger=zap"" to etcd starting command (in etcd.yaml). It will print more info such as exactly which snapshot file it fails to find. ``` mcajkovs@ubuntu:~$ docker ps -a | grep etcd | awk '{print $NF}' | while read i; do docker logs -t $i; done 2020-03-09T2…",,,,,,Anecdotal,comment,,,,,,,,2020-03-09,github/mcajkovs,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-596832735,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
"@mcajkovs, The latest etcd log you provided suggests that the etcd server was started successfully. The following log entry suggests that there was no snapshot file missing. Is there anything changed since the last panic? ``` 2020-03-09T23:46:27.122089841Z {""level"":""info"",""ts"":""2020-03-09T23:46:27.120Z"",""caller"":""etcdserver/server.go:461"",""msg"":""recovered v3 backend from snapshot"",""backend-size-bytes"":1724416,""backend-size"":""1.7 MB"",""backend-size-in-use-bytes"":790528,""backend-size-in-use"":""790 …",,,,,,Anecdotal,comment,,,,,,,,2020-03-10,github/jingyih,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-597108463,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
"> > > @mcajkovs, The latest etcd log you provided suggests that the etcd server was started successfully. The following log entry suggests that there was no snapshot file missing. Is there anything changed since the last panic? > > ``` > 2020-03-09T23:46:27.122089841Z {""level"":""info"",""ts"":""2020-03-09T23:46:27.120Z"",""caller"":""etcdserver/server.go:461"",""msg"":""recovered v3 backend from snapshot"",""backend-size-bytes"":1724416,""backend-size"":""1.7 MB"",""backend-size-in-use-bytes"":790528,""backend-size-i…",,,,,,Anecdotal,comment,,,,,,,,2020-03-10,github/mcajkovs,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-597129784,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
"Got it. The relevant log entries are: ``` 2020-03-10T14:50:16.844626183Z {""level"":""warn"",""ts"":""2020-03-10T14:50:16.844Z"",""caller"":""snap/db.go:92"",""msg"":""failed to find [SNAPSHOT-INDEX].snap.db"",""snapshot-index"":470047,""snapshot-file-path"":""/var/lib/etcd/member/snap/0000000000072c1f.snap.db"",""error"":""snap: snapshot file doesn't exist""} 2020-03-10T14:50:16.844728992Z {""level"":""panic"",""ts"":""2020-03-10T14:50:16.844Z"",""caller"":""etcdserver/server.go:454"",""msg"":""failed to recover v3 backend from snaps…",,,,,,Anecdotal,comment,,,,,,,,2020-03-10,github/jingyih,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-597193025,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
"@mcajkovs how reproducible is this issue on your VM. Let's say we create a fresh etcd (fresh k8s cluster), and then the VM is shutdown unexpectedly (such as due to power cut). Does it always comes to this state where the restarting etcd panics?",,,,,,Anecdotal,comment,,,,,,,,2020-03-10,github/jingyih,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-597212049,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
@mcajkovs Did you remove any files in the snap directory? Or could be old snapshots which's been already discarded in the local node (but old incoming snapshot handler should not even reach this code path...),,,,,,Anecdotal,comment,,,,,,,,2020-03-10,github/gyuho,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-597323362,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
"We encountered the same issue with etcd-3.4.3 after a power cut. The three nodes cluster had two of them failed to recover, with the following message: ``` aller"":""etcdserver/backend.go:79"",""msg"":""opened backend db"",""path"":""/var/lib/etcd/member/snap/db"",""took"":""3.557338ms""} aller"":""etcdserver/server.go:443"",""msg"":""recovered v2 store from snapshot"",""snapshot-index"":3700038,""snapshot-size"":""11 kB""} aller"":""snap/db.go:92"",""msg"":""failed to find [SNAPSHOT-INDEX].snap.db"",""snapshot-index"":3700038,""sn…",,,,,,Anecdotal,comment,,,,,,,,2020-06-04,github/kidlj,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-638747804,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
"Since one of the nodes can recover, we add `--force-new-cluster` to it to make a new cluster using its old snaps, and added the other two nodes back. I'm wondering why it can recover with the same(?) snap db but the other two can't? The recovery process seems to be looking for a`snap.db` file which doesn't exist in any node. ``` func (s *Snapshotter) dbFilePath(id uint64) string { return filepath.Join(s.dir, fmt.Sprintf(""%016x.snap.db"", id)) } ```",,,,,,Anecdotal,comment,,,,,,,,2020-06-04,github/kidlj,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-638760468,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
"I am having the same issue with single node ETCD: ``` 2020-07-29 10:38:33.880447 I | etcdmain: etcd Version: 3.3.10 2020-07-29 10:38:33.880566 I | etcdmain: Git SHA: 27fc7e2 2020-07-29 10:38:33.880576 I | etcdmain: Go Version: go1.10.4 2020-07-29 10:38:33.880586 I | etcdmain: Go OS/Arch: linux/amd64 2020-07-29 10:38:33.880596 I | etcdmain: setting maximum number of CPUs to 8, total number of available CPUs is 8 2020-07-29 10:38:33.880674 N | etcdmain: the server is already initialized as member…",,,,,,Anecdotal,comment,,,,,,,,2020-07-29,github/dmitrievav,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-665591948,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
"Same here after power loss, one of my 3 nodes cluster cannot start with the same panic. I still have the pod/data in case you want to extract some info.",,,,,,Anecdotal,comment,,,,,,,,2020-08-22,github/n0rad,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-678695769,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
"Same problem here with single node cluster with `etcd:3.3.10, k8s:v1.15.2`. After switch the `etcd 3.4.3`, the similar log appears: ```json {""level"":""warn"",""ts"":""2020-09-25T06:40:27.621Z"",""caller"":""snap/db.go:92"",""msg"":""failed to find [SNAPSHOT-INDEX].snap.db"",""snapshot-index"":55435591,""snapshot-file-path"":""/var/lib/etcd/member/snap/00000000034de147.snap.db"",""error"":""snap: snapshot file doesn't exist""} ```",,,,,,Anecdotal,comment,,,,,,,,2020-09-25,github/llhuii,https://github.com/kubernetes/kubernetes/issues/88574#issuecomment-698757886,repo: kubernetes/kubernetes | issue: etcd and kube-apiserver does not start after incorrect machine shutdown | keyword: workaround
"configMap and secrets volumeMount are always mounted readOnly in 1.9.6 **Is this a BUG REPORT or FEATURE REQUEST?**: /kind bug **What happened**: After upgrading from 1.9.4 to 1.9.6 configMap and secrets volumes are always mounted ReadOnly even when the deployment specs don't set the option and ""kubectl describe pod"" show the mount ad rw Deployment specs and kubectl describe show RW: ``` $ kubectl get deployment -n infra-services ldaps-proxy -o yaml | egrep -A 6 ""volumeMounts:"" volumeMounts: - …",,,,,,Anecdotal,issue,,,,,,,,2018-04-04,github/primeroz,https://github.com/kubernetes/kubernetes/issues/62099,repo: kubernetes/kubernetes | keyword: workaround | state: closed
This was intentional. More details https://github.com/kubernetes/kubernetes/pull/60258/files . If you really want `configMap` or `secrets` to be `rw` then you can disable the feature gate - `ReadOnlyAPIDataVolumes`.,,,,,,Anecdotal,comment,,,,,,,,2018-04-04,github/gnufied,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-378770047,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
"making atomic writer volumes (secret, configmap, downwardAPI, projected) readonly was part of the fix for https://github.com/kubernetes/kubernetes/issues/60814 > Fix impact: > Secret, configMap, downwardAPI and projected volumes will be mounted as read-only volumes. Applications that attempt to write to these volumes will receive read-only filesystem errors. Previously, applications were allowed to make changes to these volumes, but those changes were reverted at an arbitrary interval by the sy…",,,,,,Anecdotal,comment,,,,,,,,2018-04-05,github/liggitt,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-378809922,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
My approach is to copy the config map to `emptyDir` volume if I need rw capabilities https://github.com/kubernetes/charts/pull/3591/commits/e4b7d0b8740cd81e10a9ead3d89ee50328ca596a,,,,,,Anecdotal,comment,,,,,,,,2018-04-16,github/komljen,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-381530656,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
I don't agree with this. I should have the option to control readOnly flag. In fact when I describe the pod the confmap and secrets do show rw.,,,,,,Anecdotal,comment,,,,,,,,2018-04-25,github/mingfang,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-384179991,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
"Hello - I totally agree with config maps being readonly. But it was extremely surprising for this to happen within a minor version upgrade (1.9.x release). Currently we use GKE with automatic upgrades since these sort of minor version upgrades seemed safe, but this time the upgrade broke an old version of the grafana helm chart we were using - and indeed going from rw to ro is a breaking change IMO. So just wondering is there a semantic versioning policy for Kubernetes to prevent breaking chang…",,,,,,Anecdotal,comment,,,,,,,,2018-05-18,github/anuraaga,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-390212514,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
"@anuraaga It does (https://github.com/thtanaka/kubernetes/blob/master/docs/design/versioning.md), but unfortunately the policy was ignored in this case.",,,,,,Anecdotal,comment,,,,,,,,2018-05-30,github/jsravn,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-393172350,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
"> But it was extremely surprising for this to happen within a minor version upgrade (1.9.x release). Currently we use GKE with automatic upgrades since these sort of minor version upgrades seemed safe, but this time the upgrade broke an old version of the grafana helm chart we were using - and indeed going from rw to ro is a breaking change IMO. As noted in https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-378809922, the breaking change was required to close a significant secur…",,,,,,Anecdotal,comment,,,,,,,,2018-05-30,github/liggitt,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-393175515,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
"@liggitt you should be able to control the readOnly option and accept the security vulnerability. Default should be set to true if this is preferred, but you should be able to take the risk with this breaking change.",,,,,,Anecdotal,comment,,,,,,,,2018-06-01,github/ricklagerweij,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-393873490,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
"Thanks for the info, I can see the security implications here. But considering the fact that it is completely breaking, I still wonder whether a minor version update is appropriate for it, even when there is a fallback. It might be but needs to be coordinated with the major cloud providers, I follow GKE release notes continuously but there was no mention that an auto-upgrade would introduce breaking changes. This doesn't seem natural to me, but perhaps GKE's auto-upgrade process can be blamed.",,,,,,Anecdotal,comment,,,,,,,,2018-06-01,github/anuraaga,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-393878637,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
"> Default should be set to true if this is preferred, but you should be able to take the risk with this breaking change. Setting the feature gate `ReadOnlyAPIDataVolumes=false` allows just that as noted in the release notes - https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.9.md#changelog-since-v193",,,,,,Anecdotal,comment,,,,,,,,2018-06-01,github/liggitt,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-393878668,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
"@anuraaga is being nice about it but I agree with him completely, and probably more strongly. Breaking your contract with users is about the worse thing an open source project can do. Breaking change in a patch version is never the right choice, except in very extreme situations. This should have been opt-in not opt-out. Release notes are not good enough either when the versioning docs imply we can do auto updates to patch versions safely, like GKE does.",,,,,,Anecdotal,comment,,,,,,,,2018-06-01,github/jsravn,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-393904127,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
"> Breaking change in a patch version is never the right choice, except in very extreme situations. I agree with you. Preventing data loss and closing severe security vulnerabilities are among the few reasons such a break would even be considered, and both of those factors were part of this decision.",,,,,,Anecdotal,comment,,,,,,,,2018-06-02,github/liggitt,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-394058442,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
"I just wonder whether the proper communication was taken with the cloud vendors. It's a hard decision to make a breaking change with a patch release but at least can be better if users are made aware, at least in e.g., the GKE release notes. If there's a process for such a notification via the cloud vendors, I think it will allow people to worry less when using auto-upgrade. Or is the ball in their court for establishing such a process?",,,,,,Anecdotal,comment,,,,,,,,2018-06-02,github/anuraaga,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-394059204,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
"I'm dismayed that this option was broken *while still silently allowing* `readOnly: false`. If this feature is now unsupported, the API machinery should now refuse a pod spec when a secret or configmap has `readOnly: false` and it won't be enforced.",,,,,,Anecdotal,comment,,,,,,,,2018-06-03,github/atombender,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-394135186,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
"> do we need to do anything by the time we ship 1.11? No, rejecting previously persisted API objects as invalid would cause far more compatibility problems. It would prevent all updates to those objects, including those required to delete them (removing finalizers, etc). Additionally, serialization of that particular field does not currently distinguish between ""unset"" and ""explicitly set to false"" (JSON omits the readOnly field in both cases, protobuf persists readOnly=0 in both cases). Changi…",,,,,,Anecdotal,comment,,,,,,,,2018-06-03,github/liggitt,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-394160840,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
"> Fix impact: Secret, configMap, downwardAPI and projected volumes will be mounted as read-only volumes. Applications that attempt to write to these volumes will receive read-only filesystem errors. Previously, applications were allowed to make changes to these volumes, but those changes were reverted at an arbitrary interval by the system. Applications should be re-configured to write derived files to another location. Wonder what happens if the app can not `be re-configured to write derived f…",,,,,,Anecdotal,comment,,,,,,,,2018-06-06,github/igoratencompass,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-395029966,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
"For those encountering this, I would recommend using an init container to copy over the contents of the secret / config map directory into a shared empty dir: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/",,,,,,Anecdotal,comment,,,,,,,,2018-08-27,github/weisjohn,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-416393651,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
"@liggitt Hello, I still see the `ReadOnlyAPIDataVolumes` feature gate in the release 1.11 it should not be removed? and just to clarify this readonly will be a default? cant we set that to false? thanks",,,,,,Anecdotal,comment,,,,,,,,2018-08-28,github/cpanato,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-416574306,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
"> I still see the ReadOnlyAPIDataVolumes feature gate in the release 1.11 it should not be removed? > > and just to clarify this readonly will be a default? cant we set that to false? that is still present in 1.11 only to satisfy the deprecation period for kubelet CLI flags (6 months or 1 release, whichever is longer). it will be removed in 1.12",,,,,,Anecdotal,comment,,,,,,,,2018-08-28,github/liggitt,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-416584906,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
"> @liggitt now 1.15 support config configmap as `rw`? no, configmap/secret/downwardAPI/projected volumes are read only",,,,,,Anecdotal,comment,,,,,,,,2019-09-03,github/liggitt,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-527286207,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
"Should throw an error in the pod description logs if readOnly:false isn't possible and doesn't actually set the `readOnly` flag. The deployment YAML description is the same as it is on 1.8 as well -- if I specify readOnly:false, the mounts won't say readOnly:false in 1.8. I assumed when we upgraded our cluster and the mounts didn't say readOnly:false, the behavior was the same, only to be met that our service won't work because of this. It's honestly little ridiculous that the behavior is incon…",,,,,,Anecdotal,comment,,,,,,,,2019-11-06,github/ischang,https://github.com/kubernetes/kubernetes/issues/62099#issuecomment-550525107,repo: kubernetes/kubernetes | issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6 | keyword: workaround
"scheduler memory increases after uninstalled pods. ### What happened? We try to install apllication with using helm in our environment, and we found that the scheduler's memory usage after uninstalled pods is higher than pods are deployed. As the heap alloc bytes collect by prometheus shown , after uninstall pods, scheduler merory is higher than before. <img width=""1258"" height=""415"" alt=""Image"" src=""https://github.com/user-attachments/assets/19695634-7243-469e-8ad8-6920d126c946"" /> Also we ana…",,,,,,Anecdotal,issue,,,,,,,,2025-08-03,github/gdzzdesti,https://github.com/kubernetes/kubernetes/issues/133365,repo: kubernetes/kubernetes | keyword: gotcha | state: open
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-08-03,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/133365#issuecomment-3148289801,repo: kubernetes/kubernetes | issue: scheduler  memory increases after uninstalled pods. | keyword: gotcha
> May you provide the raw pprof.gz files? sure. [heap.out_after_install_with_1.33.gz](https://github.com/user-attachments/files/21592129/heap.out_after_install_with_1.33.gz) [heap.out_after_uninstall_with_1.33.gz](https://github.com/user-attachments/files/21592160/heap.out_after_uninstall_with_1.33.gz),,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/gdzzdesti,https://github.com/kubernetes/kubernetes/issues/133365#issuecomment-3153758880,repo: kubernetes/kubernetes | issue: scheduler  memory increases after uninstalled pods. | keyword: gotcha
Will the memory usage continue to increase if you keep repeating the steps to reproduce? can you show your kube-scheduler `kubernetes_build_info` metric? aslo show your kube-scheduer config.,,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/olderTaoist,https://github.com/kubernetes/kubernetes/issues/133365#issuecomment-3154899811,repo: kubernetes/kubernetes | issue: scheduler  memory increases after uninstalled pods. | keyword: gotcha
"> Will the memory usage continue to increase if you keep repeating the steps to reproduce? can you show your kube-scheduler `kubernetes_build_info` metric? aslo show your kube-scheduer config. scheduler memory will not continue to increase, but will decrease if we deploy pods again, and increase when uninstall all pods. <img width=""1284"" height=""428"" alt=""Image"" src=""https://github.com/user-attachments/assets/440980a6-35b2-4597-a097-a5b014edbfc0"" /> **kubernetes_build_info：** {__name__=""kuberne…",,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/gdzzdesti,https://github.com/kubernetes/kubernetes/issues/133365#issuecomment-3155216937,repo: kubernetes/kubernetes | issue: scheduler  memory increases after uninstalled pods. | keyword: gotcha
"Seems the profile you provided is 27.36M vs 30.98M, which is not so obvious as you show in screenshot. before: https://flamegraph.com/share/a981bfc9-726c-11f0-9ac4-4e554e3cebd6 after: https://flamegraph.com/share/b17ece26-726c-11f0-bb22-8ac84e7fe810 diff: https://flamegraph.com/share/73d85ce5-726c-11f0-bb22-8ac84e7fe810/73e5c532-726c-11f0-bb22-8ac84e7fe810",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/haosdent,https://github.com/kubernetes/kubernetes/issues/133365#issuecomment-3157209220,repo: kubernetes/kubernetes | issue: scheduler  memory increases after uninstalled pods. | keyword: gotcha
"Hi @gdzzdesti, Interesting find with the pprof analysis showing the memory increase tied to pod watching. From the flamegraphs @haosdent shared, the difference seems more subtle than your initial prometheus metrics suggested (27.36M vs 30.98M), but the pattern is definitely worth investigating. A few thoughts on this: 1. The scheduler maintains watch connections and internal state for all pods - after uninstalling, some cached data structures might not be getting properly cleaned up 2. With 100…",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/AadiDev005,https://github.com/kubernetes/kubernetes/issues/133365#issuecomment-3157597088,repo: kubernetes/kubernetes | issue: scheduler  memory increases after uninstalled pods. | keyword: gotcha
"I discovered now that memory leak is potentially happening in the Kubernetes framework, in particular pods removal code path. It was caused by improper slice element removal in the [removeFromSlice()](https://github.com/kubernetes/kubernetes/blob/13ced7b7ddc2e4b86bcb098e7876030ae2eb8802/pkg/scheduler/framework/types.go#L363) When pods were removed from NodeInfo slices (Pods, PodsWithAffinity, PodsWithRequiredAntiAffinity), the function was: 1. Moving the last element to the deleted position `s[…",,,,,,Anecdotal,comment,,,,,,,,2025-08-07,github/ravisastryk,https://github.com/kubernetes/kubernetes/issues/133365#issuecomment-3164829384,repo: kubernetes/kubernetes | issue: scheduler  memory increases after uninstalled pods. | keyword: gotcha
"[@ravisastryk](https://github.com/ravisastryk)! This makes total sense now. I was thinking it had to be something with the watch cache not cleaning up properly it's that removeFromSlice() function. The fact that it's not clearing the reference after moving elements is such a classic Go gotcha. This explains exactly what we're seeing - memory stays high after pod deletion because those PodInfo objects are still referenced even though they're ""removed"" from the slice. With 100 replicas and anti-a…",,,,,,Anecdotal,comment,,,,,,,,2025-08-07,github/AadiDev005,https://github.com/kubernetes/kubernetes/issues/133365#issuecomment-3164944949,repo: kubernetes/kubernetes | issue: scheduler  memory increases after uninstalled pods. | keyword: gotcha
"The results from analyzing the pprof file same with pprof that a memory leak caused by [SchedulerQueueingHint issue](https://github.com/kubernetes/kubernetes/issues/120622) . Could @sanposhiho provide some troubleshooting tips? Alternatively, run the following tests: Scenario 1: Enabling and disabling SchedulerQueueingHint Scenario 2: Testing older versions for reproduce problem",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/olderTaoist,https://github.com/kubernetes/kubernetes/issues/133365#issuecomment-3166460162,repo: kubernetes/kubernetes | issue: scheduler  memory increases after uninstalled pods. | keyword: gotcha
"Thank you @AadiDev005 and @olderTaoist. yes, I have seen similar Go-related gotchas in production especially when it comes to dealing with slices and I can see how this fix helps mitigate the memory leak. My plan is to add unit tests, specifically targeting the memory leak so that I can demonstrate how the fix improves things, and also include some benchmark results after fixing tests. I would appreciate validation from @olderTaoist and the original reporter in a production setting. That said, …",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/ravisastryk,https://github.com/kubernetes/kubernetes/issues/133365#issuecomment-3168877216,repo: kubernetes/kubernetes | issue: scheduler  memory increases after uninstalled pods. | keyword: gotcha
"> I discovered now that memory leak is potentially happening in the Kubernetes framework, in particular pods removal code path. > > It was caused by improper slice element removal in the [removeFromSlice()](https://github.com/kubernetes/kubernetes/blob/13ced7b7ddc2e4b86bcb098e7876030ae2eb8802/pkg/scheduler/framework/types.go#L363) When pods were removed from NodeInfo slices (Pods, PodsWithAffinity, PodsWithRequiredAntiAffinity), the function was: > > 1. Moving the last element to the deleted po…",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/gdzzdesti,https://github.com/kubernetes/kubernetes/issues/133365#issuecomment-3188608201,repo: kubernetes/kubernetes | issue: scheduler  memory increases after uninstalled pods. | keyword: gotcha
"correct the usage of ephemeral storage volumes in the eviction message <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especial…",,,,,,Anecdotal,issue,,,,,,,,2025-02-26,github/Chaunceyctx,https://github.com/kubernetes/kubernetes/pull/130446,repo: kubernetes/kubernetes | keyword: gotcha | state: open
"Adding the ""do-not-merge/release-note-label-needed"" label because no release-note block was detected, please follow our [release note process](https://git.k8s.io/community/contributors/guide/release-notes.md) to remove it. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https:…",,,,,,Anecdotal,comment,,,,,,,,2025-02-26,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130446#issuecomment-2685085412,repo: kubernetes/kubernetes | issue: correct the usage of ephemeral storage volumes in the eviction message | keyword: gotcha
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-02-26,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130446#issuecomment-2685086501,repo: kubernetes/kubernetes | issue: correct the usage of ephemeral storage volumes in the eviction message | keyword: gotcha
"Hi @Chaunceyctx. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should [join the org](https://git.k8s.io/community/community-membership.md#member) to skip this step. Once the…",,,,,,Anecdotal,comment,,,,,,,,2025-02-26,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130446#issuecomment-2685086621,repo: kubernetes/kubernetes | issue: correct the usage of ephemeral storage volumes in the eviction message | keyword: gotcha
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/130446#"" title=""Author self-approved"">Chaunceyctx</a>* **Once this PR has been reviewed and has the lgtm label**, please assign [sergeykanzhelev](https://github.com/sergeykanzhelev) for approval. For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands…",,,,,,Anecdotal,comment,,,,,,,,2025-02-26,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130446#issuecomment-2685088150,repo: kubernetes/kubernetes | issue: correct the usage of ephemeral storage volumes in the eviction message | keyword: gotcha
"I think you need to add an e2e test for it. We already have some e2e tests validating eviction events, and I think you can add this case there. ref: https://github.com/kubernetes/kubernetes/blob/88d2355c412f7fa5b220ff1cf63a97e81d227da3/test/e2e_node/eviction_test.go#L820",,,,,,Anecdotal,comment,,,,,,,,2025-02-28,github/HirazawaUi,https://github.com/kubernetes/kubernetes/pull/130446#issuecomment-2690807799,repo: kubernetes/kubernetes | issue: correct the usage of ephemeral storage volumes in the eviction message | keyword: gotcha
"> I think you need to add an e2e test for it. We already have some e2e tests validating eviction events, and I think you can add this case there. > > ref: > > https://github.com/kubernetes/kubernetes/blob/88d2355c412f7fa5b220ff1cf63a97e81d227da3/test/e2e_node/eviction_test.go#L820 yep, I will complete this code :)",,,,,,Anecdotal,comment,,,,,,,,2025-03-01,github/Chaunceyctx,https://github.com/kubernetes/kubernetes/pull/130446#issuecomment-2691870620,repo: kubernetes/kubernetes | issue: correct the usage of ephemeral storage volumes in the eviction message | keyword: gotcha
"The Kubernetes project currently lacks enough contributors to adequately respond to all PRs. This bot triages PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the PR is closed You can: - Mark this PR as fresh with `/remove-lifecycle stale` - Close this PR with `/close` - Offer to help out with [I…",,,,,,Anecdotal,comment,,,,,,,,2025-07-08,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130446#issuecomment-3047681287,repo: kubernetes/kubernetes | issue: correct the usage of ephemeral storage volumes in the eviction message | keyword: gotcha
"PR needs rebase. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository. </details>",,,,,,Anecdotal,comment,,,,,,,,2025-07-26,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130446#issuecomment-3121805667,repo: kubernetes/kubernetes | issue: correct the usage of ephemeral storage volumes in the eviction message | keyword: gotcha
"Facilitate ConfigMap rollouts / management To do a rolling update of a ConfigMap, the user needs to create a new ConfigMap, update a Deployment to refer to it, and delete the old ConfigMap once no pods are using it. This is similar to the orchestration Deployment does for ReplicaSets. One solution could be to add a ConfigMap template to Deployment and do the management there. Another could be to support garbage collection of unused ConfigMaps, which is the hard part. That would be useful for Se…",,,,,,Anecdotal,issue,,,,,,,,2016-03-02,github/bgrant0607,https://github.com/kubernetes/kubernetes/issues/22368,repo: kubernetes/kubernetes | keyword: gotcha | state: open
"This is one approach. I still want to write a demo, using the live-update feature of configmap volumes to do rollouts without restarts. It's a little scarier, but I do think it's useful. On Mar 2, 2016 9:26 AM, ""Brian Grant"" notifications@github.com wrote: > To do a rolling update of a ConfigMap, the user needs to create a new > ConfigMap, update a Deployment to refer to it, and delete the old ConfigMap > once no pods are using it. This is similar to the orchestration Deployment > does for Repl…",,,,,,Anecdotal,comment,,,,,,,,2016-03-23,github/thockin,https://github.com/kubernetes/kubernetes/issues/22368#issuecomment-200470010,repo: kubernetes/kubernetes | issue: Facilitate ConfigMap rollouts / management | keyword: gotcha
Yeah I think it should be trivial to set a parent for a config map so it automatically gets cleaned up. (Why not just add a configmap template section to deployment anyway? Seems like a super common thing people will want to do.),,,,,,Anecdotal,comment,,,,,,,,2016-03-25,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/22368#issuecomment-201365088,repo: kubernetes/kubernetes | issue: Facilitate ConfigMap rollouts / management | keyword: gotcha
"@lavalamp, I guess you mean we can set replicas sets as the parent of a config map, and delete the config map when all the replica sets are deleted?",,,,,,Anecdotal,comment,,,,,,,,2016-03-25,github/caesarxuchao,https://github.com/kubernetes/kubernetes/issues/22368#issuecomment-201420796,repo: kubernetes/kubernetes | issue: Facilitate ConfigMap rollouts / management | keyword: gotcha
"> Recent discussion: > https://groups.google.com/forum/#!topic/google-containers/-em3So0KBnA Thinking out loud: In OpenShift we have the concept of triggers. For example when an image tag is referenced by a DeploymentConfig and there is a new image for that tag, we detect it via a controller loop and update the DeploymentConfig by resolving the tag to the full spec of the image (thus triggering a new deployment since it's a template change). Could we possibly do something similar here? A contro…",,,,,,Anecdotal,comment,,,,,,,,2016-03-30,github/0xmichalis,https://github.com/kubernetes/kubernetes/issues/22368#issuecomment-203330071,repo: kubernetes/kubernetes | issue: Facilitate ConfigMap rollouts / management | keyword: gotcha
"Fundamentally, there need to be multiple ConfigMap objects if we're going to have some pods referring to the new one and others referring to the old one(s), just as with ReplicaSets.",,,,,,Anecdotal,comment,,,,,,,,2016-03-30,github/bgrant0607,https://github.com/kubernetes/kubernetes/issues/22368#issuecomment-203454883,repo: kubernetes/kubernetes | issue: Facilitate ConfigMap rollouts / management | keyword: gotcha
"On Wed, Mar 30, 2016 at 01:56:24AM -0700, Michail Kargakis wrote: > > Recent discussion: > > https://groups.google.com/forum/#!topic/google-containers/-em3So0KBnA > > Thinking out loud: In OpenShift we have the concept of triggers. For example when an image tag is referenced by a DeploymentConfig and there is a new image for that tag, we detect it via a controller loop and update the DeploymentConfig by resolving the tag to the full spec of the image (thus triggering a new deployment since it's…",,,,,,Anecdotal,comment,,,,,,,,2016-03-30,github/rata,https://github.com/kubernetes/kubernetes/issues/22368#issuecomment-203458756,repo: kubernetes/kubernetes | issue: Facilitate ConfigMap rollouts / management | keyword: gotcha
"What work is needed, if we agree deployment is the best path ? On Tue, Apr 5, 2016 at 5:10 PM, rata notifications@github.com wrote: > @bgrant0607 https://github.com/bgrant0607 ping? > > — > You are receiving this because you were mentioned. > Reply to this email directly or view it on GitHub > https://github.com/kubernetes/kubernetes/issues/22368#issuecomment-206042613",,,,,,Anecdotal,comment,,,,,,,,2016-04-06,github/thockin,https://github.com/kubernetes/kubernetes/issues/22368#issuecomment-206113385,repo: kubernetes/kubernetes | issue: Facilitate ConfigMap rollouts / management | keyword: gotcha
"@rata Sorry, I get zillions of notifications every day. Are you volunteering to help with the implementation? @thockin We need to ensure that the parent/owner on ConfigMap is set to the referencing ReplicaSet(s) when we implement cascading deletion / GC.",,,,,,Anecdotal,comment,,,,,,,,2016-04-06,github/bgrant0607,https://github.com/kubernetes/kubernetes/issues/22368#issuecomment-206115312,repo: kubernetes/kubernetes | issue: Facilitate ConfigMap rollouts / management | keyword: gotcha
"@bgrant0607 no problem! I can help, yes. Not sure I can get time from my work and I'm quite busy with university, but I'd love to help and probably can find some time. I've never really dealt with kube code (I did a very simple patch only), but I'd love to do it :) Also, I guess that a ConfigMap can have several owners, right? I think right now it can be used in several RSs and that should be taken into account when doing the cascade deletion/GC (although maybe is something obvious). Any pointe…",,,,,,Anecdotal,comment,,,,,,,,2016-04-06,github/rata,https://github.com/kubernetes/kubernetes/issues/22368#issuecomment-206398159,repo: kubernetes/kubernetes | issue: Facilitate ConfigMap rollouts / management | keyword: gotcha
"@rata > But I'm not sure if the configmap should be updated, as you propose, or if it should be a different one (for kube internals, at least). If we manage kube internals with deployments, we have to find the right thing to do for both user-consumed configs and internals. > Also, I guess that a ConfigMap can have several owners, right? @bgrant0607 I also have the same Q here -- I think we will need to reference-count configmaps / secrets since they can be referred to from pods owned by multipl…",,,,,,Anecdotal,comment,,,,,,,,2016-04-06,github/pmorie,https://github.com/kubernetes/kubernetes/issues/22368#issuecomment-206454523,repo: kubernetes/kubernetes | issue: Facilitate ConfigMap rollouts / management | keyword: gotcha
"@rata, I'm working on cascading deletion and am putting together a PR that adds the necessary API, including the ""OwnerReferences"". I'll cc you there.",,,,,,Anecdotal,comment,,,,,,,,2016-04-06,github/caesarxuchao,https://github.com/kubernetes/kubernetes/issues/22368#issuecomment-206464250,repo: kubernetes/kubernetes | issue: Facilitate ConfigMap rollouts / management | keyword: gotcha
"On Wed, Apr 06, 2016 at 09:37:25AM -0700, Paul Morie wrote: > @rata > > > But I'm not sure if the configmap should be updated, as you propose, or if it should be a different one (for kube internals, at least). > > If we manage kube internals with deployments, we have to find the right thing to do for both user-consumed configs and internals. Sure, but I guess the same should probably work for boths, right? I imagine for example the ""internals"" configMaps to use a name like <some-name>-v<kube-ve…",,,,,,Anecdotal,comment,,,,,,,,2016-04-06,github/rata,https://github.com/kubernetes/kubernetes/issues/22368#issuecomment-206468545,repo: kubernetes/kubernetes | issue: Facilitate ConfigMap rollouts / management | keyword: gotcha
@caesarxuchao @rata We'll likely need a custom mechanism in Deployment to ensure that a referenced ConfigMap is owned by the generated ReplicaSets that reference it.,,,,,,Anecdotal,comment,,,,,,,,2016-04-10,github/bgrant0607,https://github.com/kubernetes/kubernetes/issues/22368#issuecomment-208081910,repo: kubernetes/kubernetes | issue: Facilitate ConfigMap rollouts / management | keyword: gotcha
"@bgrant0607: not sure why you clarification now. But, just in case, the cascading deletion PR @caesarxuchao created a few days ago is this one: https://github.com/kubernetes/kubernetes/pull/23928 (you are on Cc: there)",,,,,,Anecdotal,comment,,,,,,,,2016-04-11,github/rata,https://github.com/kubernetes/kubernetes/issues/22368#issuecomment-208098653,repo: kubernetes/kubernetes | issue: Facilitate ConfigMap rollouts / management | keyword: gotcha
"@bgrant0607: Cascade deletion should work for both, right? If a new secret is created (instead of updated), then the old will be deleted when is not used. Or am I missing something? In any case, if @IanLewis is working on this, or just to get another opinion, is nice to have :)",,,,,,Anecdotal,comment,,,,,,,,2016-04-13,github/rata,https://github.com/kubernetes/kubernetes/issues/22368#issuecomment-209565217,repo: kubernetes/kubernetes | issue: Facilitate ConfigMap rollouts / management | keyword: gotcha
"For people who don't want progressive rollout of config changes, we need a notification mechanism for configmap volume updates since our atomic-update dance hides doesn't look like file mutations to the application. We have previously talked about: - a sentinel file/pipe/socket that could be monitored (e.g., file change detection via inotify) - HUP, which the user would need to ensure is correctly propagated from pid 1 to their app - expose configmap resourceVersion (or data hash?) via either c…",,,,,,Anecdotal,comment,,,,,,,,2016-04-28,github/bgrant0607,https://github.com/kubernetes/kubernetes/issues/22368#issuecomment-215589969,repo: kubernetes/kubernetes | issue: Facilitate ConfigMap rollouts / management | keyword: gotcha
"The atomic update is inotify-able, you just have to watch the `..data` symlink. I do think that some way to signal would still be useful.",,,,,,Anecdotal,comment,,,,,,,,2016-04-29,github/thockin,https://github.com/kubernetes/kubernetes/issues/22368#issuecomment-215607318,repo: kubernetes/kubernetes | issue: Facilitate ConfigMap rollouts / management | keyword: gotcha
"Storage upgrade mechanism Before we remove an API object version, we need to be certain that all stored objects have been upgraded to a version that will be readable in the future. The old plan for that was to run the cluster/upgrade-storage-objects.sh script after each upgrade. Unfortunately, there are a number of problems with this: * The script is old and incomplete, as api authors haven't been consistently adding their API objects to it (no doubt due to a lack of documentation in the right …",,,,,,Anecdotal,issue,,,,,,,,2017-09-08,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/52185,repo: kubernetes/kubernetes | keyword: gotcha | state: open
"Thanks, Daniel. Good summary. To echo: > it is not safe to remove API object versions, so we are instating a moratorium on API object version removal Emphasis on *removal*. It sucks to carry old APIs forward, but this is a liability I think we can't ignore. We've gotten lucky this far. @kubernetes/sig-network-api-reviews @cmluciano @dcbw @danwinship @kubernetes/api-approvers @kubernetes/api-reviewers",,,,,,Anecdotal,comment,,,,,,,,2017-09-08,github/thockin,https://github.com/kubernetes/kubernetes/issues/52185#issuecomment-328173356,repo: kubernetes/kubernetes | issue: Storage upgrade mechanism | keyword: gotcha
"For context the openshift approach is we force a migration before every upgrade starts, and the migration has to complete. Migration is a kubectl cmd that reads all objects and writes a no-op change (which forces storage turn over). We use this for protobuf migration, field defaulting, new versions, storage versions conversion, self link fixup, etc. It's basically a production version of upgrade-storage-objects.sh - depending on how complicated we want to get, i'd say it's the minimum viable pa…",,,,,,Anecdotal,comment,,,,,,,,2017-09-08,github/smarterclayton,https://github.com/kubernetes/kubernetes/issues/52185#issuecomment-328174525,repo: kubernetes/kubernetes | issue: Storage upgrade mechanism | keyword: gotcha
"Downstream, it is a bunch of utility code for a migration framework (we have other migrators for things like RBAC, alpha annotations to beta fields, image references) and then a set of simple commands. I don't know that I think of this as `kubectl` as much as `kubeadm` or a separate command for admins (in openshift, `oc adm` is all admin focused commands and `oc` is all end user focused, but we don't quite have the equiv in kube)",,,,,,Anecdotal,comment,,,,,,,,2017-09-08,github/smarterclayton,https://github.com/kubernetes/kubernetes/issues/52185#issuecomment-328193212,repo: kubernetes/kubernetes | issue: Storage upgrade mechanism | keyword: gotcha
"@smarterclayton Gotcha. I will take a look at it but that still doesn't sound like an ideal way to solve the problem. Like, it's good if you're going to run one or two of these things with super-well-trained humans doing the upgrade, but Kubernetes isn't installed like that :)",,,,,,Anecdotal,comment,,,,,,,,2017-09-08,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/52185#issuecomment-328205938,repo: kubernetes/kubernetes | issue: Storage upgrade mechanism | keyword: gotcha
"It's just part of general cluster upgrade operations. Someone has to roll the masters. Someone has to lay down new config. I agree there can be magic involved. But magic may have higher cost in some scenarios. On Sep 8, 2017, at 4:28 PM, Daniel Smith <notifications@github.com> wrote: @smarterclayton <https://github.com/smarterclayton> Gotcha. I will take a look at it but that still doesn't sound like an ideal way to solve the problem. Like, it's good if you're going to run one or two of these t…",,,,,,Anecdotal,comment,,,,,,,,2017-09-09,github/smarterclayton,https://github.com/kubernetes/kubernetes/issues/52185#issuecomment-328245266,repo: kubernetes/kubernetes | issue: Storage upgrade mechanism | keyword: gotcha
"I have worked closely with OpenShift's installer/upgrade team on their use of the `oc adm migrate storage` command. I will say running it automatically pre and post upgrade is the trivial part. The command is surprisingly good at finding random edge cases. It could be turned into a controller of sort, though I am not sure what state the controller would try to drive the system from/to.",,,,,,Anecdotal,comment,,,,,,,,2017-09-13,github/enj,https://github.com/kubernetes/kubernetes/issues/52185#issuecomment-329307129,repo: kubernetes/kubernetes | issue: Storage upgrade mechanism | keyword: gotcha
"Per conversation on sig-api-machinery, there was general consensus that it makes more sense to have this as a controllers responsibility with some set of strategies.",,,,,,Anecdotal,comment,,,,,,,,2017-09-14,github/timothysc,https://github.com/kubernetes/kubernetes/issues/52185#issuecomment-329497151,repo: kubernetes/kubernetes | issue: Storage upgrade mechanism | keyword: gotcha
"We need to figure out downgrades, also, which are similar. Right now, a downgrade will strand storage of any new APIs that are created.",,,,,,Anecdotal,comment,,,,,,,,2017-09-14,github/bgrant0607,https://github.com/kubernetes/kubernetes/issues/52185#issuecomment-329560892,repo: kubernetes/kubernetes | issue: Storage upgrade mechanism | keyword: gotcha
"> We need to figure out downgrades, also, which are similar. Right now, a downgrade will strand storage of any new APIs that are created. Brand new resources will get orphaned on downgrade, but should be inert. New versions of existing resources shouldn't start persisting with the new version until n+1 versions from when they are released (or rolling HA API upgrades won't work). If we only support single version downgrade, there shouldn't be any issues.",,,,,,Anecdotal,comment,,,,,,,,2017-09-14,github/liggitt,https://github.com/kubernetes/kubernetes/issues/52185#issuecomment-329562786,repo: kubernetes/kubernetes | issue: Storage upgrade mechanism | keyword: gotcha
"Issues go stale after 90d of inactivity. Mark the issue as fresh with `/remove-lifecycle stale`. Stale issues rot after an additional 30d of inactivity and eventually close. Prevent issues from auto-closing with an `/lifecycle frozen` comment. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or `@fejta`. /lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2018-01-06,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/52185#issuecomment-355716313,repo: kubernetes/kubernetes | issue: Storage upgrade mechanism | keyword: gotcha
"I haven't seen a better suggestion than a no-op read/write cycle via the API on the affected resources. Enforcing that is done seems like the responsibility of the deployment mechanism, so it can be staged after all apiservers in a HA deployment are upgraded.",,,,,,Anecdotal,comment,,,,,,,,2018-01-17,github/liggitt,https://github.com/kubernetes/kubernetes/issues/52185#issuecomment-358152122,repo: kubernetes/kubernetes | issue: Storage upgrade mechanism | keyword: gotcha
"In the meantime, you can start disabling *serving* the deprecated object by default. The types still remain, so they can be read from storage, and we still have the in tree debt, but it pushes people to use the new types as new clusters are deployed.",,,,,,Anecdotal,comment,,,,,,,,2018-01-17,github/liggitt,https://github.com/kubernetes/kubernetes/issues/52185#issuecomment-358154463,repo: kubernetes/kubernetes | issue: Storage upgrade mechanism | keyword: gotcha
"Speaking of HA, today each apiserver has its own independent default storage versions, which are imposed immediately upon apiserver upgrade. That doesn't seem desirable.",,,,,,Anecdotal,comment,,,,,,,,2018-01-22,github/bgrant0607,https://github.com/kubernetes/kubernetes/issues/52185#issuecomment-359530800,repo: kubernetes/kubernetes | issue: Storage upgrade mechanism | keyword: gotcha
"@lavalamp @caesarxuchao If this issue has been triaged, please comment `/remove-triage unresolved`. If you aren't able to handle this issue, consider unassigning yourself and/or adding the `help-wanted` label. 🤖 I am a bot run by vllry. 👩‍🔬",,,,,,Anecdotal,comment,,,,,,,,2019-05-06,github/athenabot,https://github.com/kubernetes/kubernetes/issues/52185#issuecomment-489714953,repo: kubernetes/kubernetes | issue: Storage upgrade mechanism | keyword: gotcha
"This is marked as `priority/important-soon` but given the last updates, that wouldn't seem to be the case. Can someone catch me up on what the status is here? Is this simply stale since we don't have anyone to move it forward?",,,,,,Anecdotal,comment,,,,,,,,2024-03-22,github/shaneutt,https://github.com/kubernetes/kubernetes/issues/52185#issuecomment-2014642702,repo: kubernetes/kubernetes | issue: Storage upgrade mechanism | keyword: gotcha
/lifecycle frozen The foundational pieces are being worked on here: https://github.com/kubernetes/enhancements/issues/2339 https://github.com/kubernetes/enhancements/issues/4192,,,,,,Anecdotal,comment,,,,,,,,2024-04-05,github/enj,https://github.com/kubernetes/kubernetes/issues/52185#issuecomment-2039379717,repo: kubernetes/kubernetes | issue: Storage upgrade mechanism | keyword: gotcha
The [#sig-api-machinery-storageversion-dev](https://kubernetes.slack.com/archives/C06S7LHB06B) slack channel is available as well.,,,,,,Anecdotal,comment,,,,,,,,2024-04-05,github/enj,https://github.com/kubernetes/kubernetes/issues/52185#issuecomment-2039385988,repo: kubernetes/kubernetes | issue: Storage upgrade mechanism | keyword: gotcha
"Validating Admission Policy: Uniform workload type policy enforcement This is a feature suggestion for ValidatingAdmissionPolicy (VAP). It may depend on other enhancements to workload types before it becomes tractable. Writing a VAP that verifies all PodSpecs is difficult/impossible. PodSpecs are embedded in multiple built-in types (Deployments, Jobs...) as well as CRDs. Even writing a VAP to validate the containers in Pod requires writing validation code in triplicate to handle `containers`, `…",,,,,,Anecdotal,issue,,,,,,,,2025-03-04,github/jpbetz,https://github.com/kubernetes/kubernetes/issues/130565,repo: kubernetes/kubernetes | keyword: gotcha | state: open
"We need to decide whether the kube API is an opaque document store, or a type system. I think it started as a document store. The ""registry"" pattern describes each resource almost completely independently of any other resource. But then we define types in Go and reuse them all over. Things like defaulting are defined on types, not on resources. This has made things like referential integrity hard, which is something we continuously struggle with as our apis get more and more modular. I personal…",,,,,,Anecdotal,comment,,,,,,,,2025-03-05,github/thockin,https://github.com/kubernetes/kubernetes/issues/130565#issuecomment-2701415243,repo: kubernetes/kubernetes | issue: Validating Admission Policy: Uniform workload type policy enforcement | keyword: gotcha
"Duplicated with https://github.com/kubernetes/kubernetes/issues/129939 Notes: it is not a blocker now. People currently using expressions like `object.kind == Pod ? object.spec : object.kind == PodTemplate ? object.template.spec : ['Deployment','ReplicaSet','DaemonSet','StatefulSet','Job', 'ReplicationController'].exists(kind, object.kind == kind) ? object.spec.template.spec : object.kind != 'CronJob' ? object.spec.jobTemplate.spec.template.spec. : nil` to address such use case.",,,,,,Anecdotal,comment,,,,,,,,2025-04-07,github/cici37,https://github.com/kubernetes/kubernetes/issues/130565#issuecomment-2783922186,repo: kubernetes/kubernetes | issue: Validating Admission Policy: Uniform workload type policy enforcement | keyword: gotcha
"RV vs object-wide logical clock (followup from SIG API Machinery meeting of 2022-09-21) (note, I'm keeping this text up to date so you don't have to read all the discussion below if you don't want to) We are trying to unblock a class of caching behaviors, example PR: https://github.com/kubernetes/kubernetes/pull/112202 The problem is that today there is no way for a client to know which of two object states is ""more recent"". This makes it very hard to e.g. put the result of a write into a cache…",,,,,,Anecdotal,issue,,,,,,,,2022-09-22,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/112684,repo: kubernetes/kubernetes | keyword: gotcha | state: open
"> We are trying to unblock a class of caching behaviors, example PR: https://github.com/kubernetes/kubernetes/pull/112202 Did we decide this was the right thing to do? The alternative pattern that I've seen us adopt (I believe) is not to write to the cache, and simply to operate on changes as the cache observes them, waiting for changes to come in over the watch even if we made those changes ourselves.",,,,,,Anecdotal,comment,,,,,,,,2022-09-22,github/justinsb,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1255466275,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"Also: We should check with all the aggregated apiserver implementations, as I think they are free to use strings if it makes them happy. (Disclaimer: I'm working on an aggregated apiserver implementation in kpt/porch, and we're not doing resource-version right today, though I think/hope 63 bit ints should be enough for us)",,,,,,Anecdotal,comment,,,,,,,,2022-09-22,github/justinsb,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1255474940,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"> The alternative pattern that I've seen us adopt (I believe) is not to write to the cache, and simply to operate on changes as the cache observes them, waiting for changes to come in over the watch even if we made those changes ourselves. You need one of these changes to make that safe, since you're not guaranteed to see every watch event.",,,,,,Anecdotal,comment,,,,,,,,2022-09-22,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1255480035,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"If we implement option 2 today, I feel like in a couple of years we could drop the ""RV must be an int64"" requirement (I have no evidence for this belief but parsing RV seems like uncommon behavior to me). However, it is unclear to me how deletes would work with option 2 (since resetting the object version would be confusing to clients).",,,,,,Anecdotal,comment,,,,,,,,2022-09-22,github/enj,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1255481567,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"> how deletes would work with option 2 (since resetting the object version would be confusing to clients) Clients already have to watch for UID changes. Yes, it would reset to zero.",,,,,,Anecdotal,comment,,,,,,,,2022-09-22,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1255484872,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"> Also: We should check with all the aggregated apiserver implementations, as I think they are free to use strings if it makes them happy. I would not say they are free to do that. I would say it's inadvisable, and clients that parse are rare enough that they might not discover the problem for a long time. > (Disclaimer: I'm working on an aggregated apiserver implementation in kpt/porch, and we're not doing resource-version right today, though I think/hope 63 bit ints should be enough for us) 🙉",,,,,,Anecdotal,comment,,,,,,,,2022-09-22,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1255493759,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"@lavalamp are we more focused on solving the **intra**object ordering problem (for two snapshots in time of an object, which of $a_0$ and $a$ occurred first) or the **inter**object ordering problem (for two snapshots in time of two objects [of the same resource], which of $a$ and $b$ occurred first)? Your post seems to mention both, unless I'm reading it wrong. Option 1 invites users to write logic that attempts to handle **inter**object ordering, but I fear that is (almost) always going to be …",,,,,,Anecdotal,comment,,,,,,,,2022-09-22,github/stevekuznetsov,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1255506677,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"Both, but not everyone has the interobject (""cross-object"" in the OP) ordering problem. > Option 1 invites users to write logic that attempts to handle interobject ordering, but I fear that is (almost) always going to be based on incorrect assumptions - e.g. ""this ServiceAccount token was written after the ServiceAccount was updated, so the token contents must be up-to-date"" Hopefully people will realize that just because SOMETHING wrote the object, does not mean the controller of interest has …",,,,,,Anecdotal,comment,,,,,,,,2022-09-22,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1255517746,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"> You need one of these changes to make that safe, since you're not guaranteed to see every watch event. I think the assumption in all of our controllers is that a watch sees _a_ state of all changed objects eventually, and in practice within a few seconds as long as the watch channel isn't overwhelmed. I think a controller that performed the fast-forward optimization would actually behave _worse_ if we don't assume that, as the world would be even less self-consistent. But anyway, I think I ag…",,,,,,Anecdotal,comment,,,,,,,,2022-09-22,github/justinsb,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1255520072,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
I feel like option 1 is reasonable. I think offering a guarantee serves the community better at this point than holding on to option to change the behavior.,,,,,,Anecdotal,comment,,,,,,,,2022-09-22,github/jpbetz,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1255521141,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"@justinsb the problem is with controllers that list all objects of some type and then do something with that. If one object has gone forward in time, some other number of objects have effectively gone back in time in comparison. Like, imagine computing the free space on a Node by adding up all the pods on it; if one of your pods is from time T+3 and you haven't yet observed a pod deletion from T+2, you will add up the free space wrong and possibly evict a pod you needn't. It is really hard to d…",,,,,,Anecdotal,comment,,,,,,,,2022-09-22,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1255525855,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"@stevekuznetsov It does (ish), see this bullet point: > More difficult to solve the cross-object ordering problem (**you have to wait for object X to hit at least objectVersion Y, instead of waiting to see any object with RV > Z**)",,,,,,Anecdotal,comment,,,,,,,,2022-09-22,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1255537283,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"@lavalamp I'm sorry, I think I'm missing something here - if you have an independent logical clock per object, what mechanism is there to sync the timelines in order to allow causality comparisons across them?",,,,,,Anecdotal,comment,,,,,,,,2022-09-22,github/stevekuznetsov,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1255539106,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"Today, if I have two `Secret`s I have no mechanism to tell which of them is ""newer"" than the other. The proposal in Option 2 is: > Alternatively, we could add a field to metadata, .objectVersion; this field would be a 63 bit int, a per-object logical clock. The server would enforce that it functions like the current .generation field, but for changes in the whole object, not just spec. So, now I have two `Secret`s and I can tell *for each of them* how many times its' spec and status have been c…",,,,,,Anecdotal,comment,,,,,,,,2022-09-22,github/stevekuznetsov,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1255542903,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
You cannot in the abstract. I'm only seeking to solve this problem for people running a watch & a cache. (In fact it's almost a pro for option 2!),,,,,,Anecdotal,comment,,,,,,,,2022-09-22,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1255543914,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"Gotcha. In my understanding at least, since you could not use the new field from Option 2 to take a list of `Secret`s and order them by recency, the option solves only the **intra**object problem, which is why my original question was asking which of the two problems was more pressing. The linked issue also only attempts to answer the **intra**object problem.",,,,,,Anecdotal,comment,,,,,,,,2022-09-22,github/stevekuznetsov,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1255545804,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"> The linked issue also only attempts to answer the intraobject problem. Right, and this is a defect that needs to be fixed before we could merge it, see why in the example I gave in [this comment](https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1255525855).",,,,,,Anecdotal,comment,,,,,,,,2022-09-22,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1255548408,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"I see. To my eyes, Option 2 gives a cache author a simple way to provide either behavior (consistent or inconsistent collections), generalizes the `generation` concept clearly, and does not invite users to make broader, more potentially foot-gun-ey assumptions like **inter**object recency comparison outside of the tight constraints of LIST+WATCH. Tighter control over the API surface might be a valuable property as well for SIG APIMachinery.",,,,,,Anecdotal,comment,,,,,,,,2022-09-22,github/stevekuznetsov,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1255558654,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"Just FYI, implementing option 2 might be easier than you think. Etcd's KV has a concept of ""version"", which is exactly what you want to have IIUC - https://github.com/etcd-io/etcd/blob/main/api/mvccpb/kv.proto#L22",,,,,,Anecdotal,comment,,,,,,,,2022-09-23,github/mm4tt,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1256147218,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"Option 2 seems like it would be easy to implement and leaves us the flexibility to change RV in the future (with appropriate depreciation). Expanding object meta is a pretty fundamental API change, and I believe both options represent an expansion. Option 2 is limited and explicit. Option 1 requires no effort outside of documentation, but ties our ability to change things in the future. Is there an option 3? Could we define resource version as an unsized unsigned integer?",,,,,,Anecdotal,comment,,,,,,,,2022-09-26,github/enj,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1258688886,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"> Is there an option 3? Could we define resource version as an unsized unsigned integer? Unfortunately this would probably also break most or all clients parsing it, should we actually use more than 63 bits.",,,,,,Anecdotal,comment,,,,,,,,2022-09-26,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1258692009,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"> Unfortunately this would probably also break most or all clients parsing it, should we actually use more than 63 bits. Ideally, if we wanted to ""reclaim resourceVersion"" we'd probably need to be smarter, as Daniel notes. Provide a way for clients to get pre-broken as a feature gate, or per client, or opt-in. All of those are expensive... and perhaps better addressed after we provide people a correct, relatively unobjectionable near term fix for intraobject. You could argue that option 2 is th…",,,,,,Anecdotal,comment,,,,,,,,2022-09-26,github/smarterclayton,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1258730686,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"> in the future a safely opaque RV can be mixed with the cluster ID of the etcd state, allowing client operations (watch, list, precondition edit) to be invalidated during a restore Today you can accomplish this by incrementing the etcd global index prior to reconnecting the cluster. But yes, that could be cleaner. (shall I put you down as voting for option 2 then?)",,,,,,Anecdotal,comment,,,,,,,,2022-09-26,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1258735249,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"> > Is there an option 3? Could we define resource version as an unsized unsigned integer? > > Unfortunately this would probably also break most or all clients parsing it, should we actually use more than 63 bits. In an abstract sense I was thinking: 1. Expand the schema of resource version (i.e. it can the current format plus some new format) 1. Document the full schema 1. Expand our libraries to support the new schema 1. Continue using the existing schema for N releases (minimum for N is prob…",,,,,,Anecdotal,comment,,,,,,,,2022-09-26,github/enj,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1258736113,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
The problem with a discontinuous switch over is that you don't really have a good way of knowing if clients have been updated or not. I prefer a 2nd field for any sort of change (RV can get set to eg the large number mod 2^63).,,,,,,Anecdotal,comment,,,,,,,,2022-09-26,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1258740581,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"> Today you can accomplish this by incrementing the etcd global index prior to reconnecting the cluster Right, if you know the increment of your lost control plane, which in a few cases has safety implications (if you lose two, you might not actually know how far ahead they were). I would say I prefer option 2 because it fixes a concrete problem (intraobject) cleanly, is tractable for almost every storage system, and having solved it gives us more flexibility to tackle resourceVersion for cases…",,,,,,Anecdotal,comment,,,,,,,,2022-09-26,github/smarterclayton,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1258757964,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"Feature request: A way to signal pods This has come up a bunch of times in conversations. The idea is that some external orchestration is being performed and the user needs to signal (SIGHUP usually, but arbitrary) pods. Sometimes this is related to ConfigMap (#22368) and sometimes not. This also can be used to ""bounce"" pods. We can't currently signal across containers in a pod, so any sort of sidecar is out for now. We can do it by `docker kill`, but something like `kubectl signal` is clumsy -…",,,,,,Anecdotal,issue,,,,,,,,2016-04-29,github/thockin,https://github.com/kubernetes/kubernetes/issues/24957,repo: kubernetes/kubernetes | keyword: gotcha | state: open
"We can just use config map as the kube-event-bus. It's what I was going to do for petset. I need to notify petset peers about changes in cluster membership. The idea is people will write ""on-change"" hook handlers, the petset will create a config map and write to it anytime it creates a new pet. The kubelet runs a handler exec/http-probe style, on every change to the config map. I'd rather have a hook than an explicity signal because I don't need a proxy to listen for the signal and reload the p…",,,,,,Anecdotal,comment,,,,,,,,2016-04-29,github/bprashanth,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-215887148,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"I opened this as a separate PR because I have been asked several times for literal signals. Asking people to do something instead of signals, especially something kube-centric is a non-starter for people who have apps that need signals. We could ask them to bundle sidecars as bridges, but because we can not signal across containers, they have to bundle into their own containers. That is gross to max. On Fri, Apr 29, 2016 at 2:32 PM, Prashanth B notifications@github.com wrote: > We can just use …",,,,,,Anecdotal,comment,,,,,,,,2016-04-29,github/thockin,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-215897186,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"You won't need a sidecar, you'll need a shell script that sends `pkill -HUP nginx`, right? the kubelet will exec-probe that script when the config map changes. This will require modifcation of pod spec but is more flexible imo.",,,,,,Anecdotal,comment,,,,,,,,2016-04-29,github/bprashanth,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-215897771,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"You're missing the point. There are other things people want to do that involve sending a signal and ""create a config map, modify your container to include bash and pkill, modify your pod to mount the config map, write a script that runs your process and then watches the configmap and signals your process, and then run that instead of your real app"" is a crappy solution. It's a kludgey workaround for lack of a real feature. On Fri, Apr 29, 2016 at 3:27 PM, Prashanth B notifications@github.com w…",,,,,,Anecdotal,comment,,,,,,,,2016-04-30,github/thockin,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-215940195,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"> write a script that runs your process and then watches the configmap and signals > your process, and then run that instead of your real app"" That's not part of what i want. I explicitly _don't_ want a long running process aware of the config map. > create a config map, modify your container to > include bash and pkill, modify your pod to mount the config map, I think this is trivial. Not bash, sh or ash. Or http. The same things you need for a probe, or a post start hook, or pre stop hook. co…",,,,,,Anecdotal,comment,,,,,,,,2016-04-30,github/bprashanth,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-215940524,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
And I'm trying not to end up with 2 overlapping concepts when you can achieve one with the other. Somehow I dont think people will be against a probe like thing instead of a straight signal.,,,,,,Anecdotal,comment,,,,,,,,2016-04-30,github/bprashanth,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-215940594,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"It feels like a hack to me. I'd be fine with notifiers, and with notifiers attached to configmap changes, and with notifiers that included ""send a signal"". None of that obviates the utility of ""i want to signal my pods"" On Fri, Apr 29, 2016 at 10:55 PM, Prashanth B notifications@github.com wrote: > And I'm trying not to end up with 2 overlapping concepts when you can > achieve one with the other. Somehow I dont think people will be against a > probe like thing instead of a straight signal. > > …",,,,,,Anecdotal,comment,,,,,,,,2016-04-30,github/thockin,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-215940810,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"Hmm, my feature request also doesn't require a config map, it's the common case. Maybe a notifier of type=probe,exec,signal?",,,,,,Anecdotal,comment,,,,,,,,2016-04-30,github/bprashanth,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-215941619,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"Regarding signals and restarts, is kill the right signal, or do users want to restart the pod itself? I fee like the granularity on signals is processes (of which container is a relatively good proxy), but the granularity on restart / exec actions is containers or pods.",,,,,,Anecdotal,comment,,,,,,,,2016-04-30,github/smarterclayton,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-215979078,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"I could see an argument for ""restart"" as a verb on pods (albeit with he same issues around imperatives), distinct from signal, though I assumed signalling a pod meant to signal all containers or a self-nominated signal receiver container. On Apr 30, 2016 10:03 AM, ""Clayton Coleman"" notifications@github.com wrote: > Regarding signals and restarts, is kill the right signal, or do users > want to restart the pod itself? I fee like the granularity on signals > is processes (of which container is a …",,,,,,Anecdotal,comment,,,,,,,,2016-04-30,github/thockin,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-215979439,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"Is ""restart"" a synthetic signal then? If we add exec it's a little wierd because we would potentially have to secure exec differently than signal (since it could be running with higher priviliges) - do pods authors define signals the container supports (which is more like invokable hooks), or are they arbitrary (can I find crazy posix signal X that causes the machine to crash)?",,,,,,Anecdotal,comment,,,,,,,,2016-04-30,github/smarterclayton,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-215982668,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"I'm still kind of stuck at the point where we're special casing a specific form of linux IPC. I really want a cluster notification system, the last hop of which is some implementation of a node local ipc, i.e, say i have a pod with: ``` notification: exec: /on-change.sh stdin: """" ``` and I write ""foo"" to stdin, the kubelet will deliver that at least once, to the one container with the hook. I then observe generation number and know that my container has received `echo foo | on-change.sh`. Now s…",,,,,,Anecdotal,comment,,,,,,,,2016-04-30,github/bprashanth,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-215987996,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"I kind of agree signal is a subset of ""pod notification"". The security aspects of exec make it something I'd prefer to require the pod author to define, in which case it's a named hook / notification. On Sat, Apr 30, 2016 at 3:11 PM, Prashanth B notifications@github.com wrote: > I'm still kind of stuck at the point where we're special casing a specific > form of linux IPC. I really want a cluster notification system, the last > hop of which is some implementation of a node local ipc, i.e, say i…",,,,,,Anecdotal,comment,,,,,,,,2016-04-30,github/smarterclayton,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-215988189,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"I would like to have a concrete way to force and guarantee a restart of a pod from the outside without hacking. One option that is not signals is a monotonic value on the pod that a user can update (essentially, generation). That's more ""whole pod"" signaling. On Sat, Apr 30, 2016 at 3:15 PM, Clayton Coleman ccoleman@redhat.com wrote: > I kind of agree signal is a subset of ""pod notification"". The security > aspects of exec make it something I'd prefer to require the pod author to > define, in w…",,,,,,Anecdotal,comment,,,,,,,,2016-05-10,github/smarterclayton,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-218218890,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"> it should be optional and probably support rollout restart, If it's too dangerous to restart containers _in_ a pod at the same time, put them in different pods/services? Everything else in the system will treat the pod as a unit, so there's probably no avoiding this. Or are you asking for a dependency chain of restarts on containers in a pod?",,,,,,Anecdotal,comment,,,,,,,,2016-05-10,github/bprashanth,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-218233028,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"Classic cases: - service registers itself on start with a system, that systems failed - server has caches that need to be flushed because of a failure elsewhere - server is flaking in a way that liveness probe doesn't/can't catch - debugging a pod on a particular node - database that needs a restart after a schema change In all of these cases the invariant enforced by pod deletion is excessive.",,,,,,Anecdotal,comment,,,,,,,,2016-05-10,github/smarterclayton,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-218233826,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"@bprashanth: for example we have replication controller which has N of replicas, if we are going to restart all of them at the same time (e.g. we rollout new config change) we will affect customer traffic.",,,,,,Anecdotal,comment,,,,,,,,2016-05-10,github/eghobo,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-218236876,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"More use cases: some software can be told to increase logging or debugging verbosity with SIGUSR1 and SIGUSR2, to rotate logs, open diagnostic ports, etc.",,,,,,Anecdotal,comment,,,,,,,,2016-05-12,github/therc,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-218786916,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"I also view this as notification. I would prefer to expose an intentional API instead of something in terms of POSIX signals. I would take two actions to start with: 1. `bump` - signal the container somehow that a reload should happen of any config 2. `restart` - what it says on the tin The problems with an intentional API are: 1. The ways of carrying out intent are not going to be universal across platforms; we need to define them for each platform we support; we can say that on Linux, `bump` …",,,,,,Anecdotal,comment,,,,,,,,2016-05-24,github/pmorie,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-221173305,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"Also, for the record, seems plausible that we would constrain the list of signals you could send in an API using `PodSecurityPolicy`.",,,,,,Anecdotal,comment,,,,,,,,2016-05-24,github/pmorie,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-221173462,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"I have no strong preference about ""intentional API vs POSIX"" problem. The only proposition I'm opposed is using any king of exec or scripts as a workaround. Docker has `docker kill --signal`, rkt has the issue where the idea of supporting custom signals is considered coreos/rkt#1496. I'd prefer to go with these features. @pmorie sorry for my ignorance, but what exact non-Linux platforms we have to care about in k8s? I had no luck with googling/greping that information.",,,,,,Anecdotal,comment,,,,,,,,2016-06-03,github/nhlfr,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-223504290,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
There is ongoing work to bring Kubernetes to Windows. We need to be aware of how we would address differences in those mappings.,,,,,,Anecdotal,comment,,,,,,,,2016-06-03,github/smarterclayton,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-223571207,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"For the record we should be able to exec scripts from sidecars for notification delivery with shared pidnamespaces, which is coming in docker 1.12 apparently (https://github.com/kubernetes/kubernetes/issues/1615)",,,,,,Anecdotal,comment,,,,,,,,2016-06-03,github/bprashanth,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-223632387,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"@smarterclayton OK, so in that case intentional API sounds better. So, for now we're going to implement the `bump` and `restart` actions as 1. API actions for pod 2. Actions which may be triggered by event bus ? Also, as far as I understand, this event bus is something which we have to implement? Sorry if I'm missing some already existing feature. If I understand correctly the API part, then I'm eager to implement this.",,,,,,Anecdotal,comment,,,,,,,,2016-06-03,github/nhlfr,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-223632944,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"@nhlfr For a change like this the best way to start is a proposal document that soecifies the yse-cases and proposes the API changes. Implementation concerns come later, but to answer your question, we already have a watch emchnaism in the API that kubelet uses to watch for changes in pods, and can be used with any API type.",,,,,,Anecdotal,comment,,,,,,,,2016-06-03,github/pmorie,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-223676957,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"Sorry, I walked away from this for a while. I _dislike_ an opinionated ""bump"" operation because that isn't what I asked for. I have been asked repeatedly for ""I want to send a signal"". Yes, I know that signals are a POSIXism and Windows support is approximately inevitable. That doesn't mean that I am going to change my app from expecting SIGUSR1 to now expect SIGHUP, even if SIGHUP is more obviously correct. We don't get to choose that. I would be OK to cage this in terms of a notifier API, whe…",,,,,,Anecdotal,comment,,,,,,,,2016-06-09,github/thockin,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-224801160,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"I think both @bprashanth and I were suggesting pod or container defined named hooks at various points that were then treated as one or more ""signals"", and that's not terribly different from what's being proposed. If a pod author wanted to create: ``` notification: - name: bump signal: SIGUSR1 ``` that's both intentional and extensible, as you note. I don't think there's an ""event bus"" per se, but we do need to talk about delivery guarantees. As a client author I should not have to write a retry…",,,,,,Anecdotal,comment,,,,,,,,2016-06-09,github/smarterclayton,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-224890183,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"I agree with this last message in its entirety. Sorry for being obtuse before. This would satisfy what I understand as requirements. On Thu, Jun 9, 2016 at 6:14 AM, Clayton Coleman notifications@github.com wrote: > I think both @bprashanth https://github.com/bprashanth and I were > suggesting pod or container defined named hooks at various points that were > then treated as one or more ""signals"", and that's not terribly different > from what's being proposed. If a pod author wanted to create: >…",,,,,,Anecdotal,comment,,,,,,,,2016-06-09,github/thockin,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-224959659,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"Validation Tightening is not Generally Possible I have made this comment on a few PRs so maybe it's time to document and/or build a mechanism. You cannot generally make validation tighter, because any existing objects that violate the new rule will not be able to be updated, and in many cases, deleted (because deletions involving finalizers require a sequence of updates). This is because, to validate updates, we validate BOTH the transition, AND the final state of the object. It's this last par…",,,,,,Anecdotal,issue,,,,,,,,2018-06-06,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/64841,repo: kubernetes/kubernetes | keyword: gotcha | state: open
xref #59984 #60635 #57615 #52936 #59593 #60934 #61623 #61984 #62987 #63426 #65470 for wanting to tighten validation wanting to loosen validation: * https://github.com/kubernetes/kubernetes/pull/71384#discussion_r243438895 * https://github.com/kubernetes/kubernetes/pull/69415#issuecomment-452397121 * https://github.com/kubernetes/enhancements/pull/660#discussion_r248502696,,,,,,Anecdotal,comment,,,,,,,,2018-06-06,github/liggitt,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-395141013,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
a methodical way to do ratcheting validation would be ideal: * apply tightened validation on create * apply tightened validation on update where the old object passed this validation (don't let a valid object become invalid) * skip tightened validation on update where the old object fails this validation,,,,,,Anecdotal,comment,,,,,,,,2018-06-06,github/liggitt,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-395141509,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
"Yes, that's the mechanism I had in mind when I wrote the two bullet points in the original post. (also thanks for the extensive list of cross references)",,,,,,Anecdotal,comment,,,,,,,,2018-06-06,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-395214033,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
"Sorry for not noticing this earlier. Tightening validation can break clients. I don't see much of a way around that. Special-casing updates protects the apiserver but not clients and users. It seems like the only recourse would be to downgrade to the prior release until the clients can be fixed. And downgrade is known to not work in the general case. Maybe we need an advisory mode that also applies to create, so that users can debug problems before they irreversibly hose their clusters?",,,,,,Anecdotal,comment,,,,,,,,2018-07-17,github/bgrant0607,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-405431399,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
"https://github.com/kubernetes/community/blob/master/contributors/devel/api_changes.md#backward-compatibility-gotchas Changing any validation rules always has the potential of breaking some client, since it changes the assumptions about part of the API, similar to adding new enum values. Validation rules on spec fields can neither be relaxed nor strengthened. Strengthening cannot be permitted because any requests that previously worked must continue to work. Weakening validation has the potentia…",,,,,,Anecdotal,comment,,,,,,,,2018-07-17,github/bgrant0607,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-405431983,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
"The list of cross references was useful. We should document how to handle common cases, such as duplicate list entries. It looks like maybe a few of them wouldn't have worked, but with asynchronous failures rather than synchronous ones. However, it's possible for cases we didn't intend to be supported, such as multiple instances of the same environment variable, to be working for someone.",,,,,,Anecdotal,comment,,,,,,,,2018-07-17,github/bgrant0607,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-405433891,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
Other thoughts: We also need to demand validation tests that fail for every field that doesn't accept all values of the base type. This doesn't handle cases where contributors introduce new validation checks outside the ratcheting mechanism. What are we trying to defend against? The cluster admin will want to know if any of potential problems. We need to introduce exported variables that surface affected resource types.,,,,,,Anecdotal,comment,,,,,,,,2018-07-17,github/bgrant0607,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-405436348,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
> Tightening validation can break clients. I don't see much of a way around that. Special-casing updates protects the apiserver but not clients and users. What are we protecting clients and users from? My intent with these was to only address validation gaps that were allowing in fatally flawed objects. > It seems like the only recourse would be to downgrade to the prior release until the clients can be fixed. Is it a great loss to block a client that is submitting deployments/pods that can not…,,,,,,Anecdotal,comment,,,,,,,,2018-07-17,github/liggitt,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-405436597,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
"@liggitt I agree with respect to requests that were fatally flawed. However, we haven't always been correct when trying to identify such cases in the past.",,,,,,Anecdotal,comment,,,,,,,,2018-07-17,github/bgrant0607,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-405437865,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
"Example: https://github.com/kubernetes/kubernetes/blob/c4f3102b1ff8f971612abb0d8986ae63aaac3e47/pkg/volume/empty_dir/empty_dir_linux.go#L59 It looks like unknown emptyDir medium values succeed: ``` if buf.Type == linuxTmpfsMagic { return v1.StorageMediumMemory, !notMnt, nil } else if int64(buf.Type) == linuxHugetlbfsMagic { return v1.StorageMediumHugePages, !notMnt, nil } return v1.StorageMediumDefault, !notMnt, nil ```",,,,,,Anecdotal,comment,,,,,,,,2018-07-17,github/bgrant0607,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-405438484,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
"> It looks like unknown emptyDir medium values succeed If that's the case, I agree it shouldn't be tightened. I was going off of https://github.com/kubernetes/kubernetes/pull/52936#issuecomment-341754780",,,,,,Anecdotal,comment,,,,,,,,2018-07-17,github/liggitt,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-405438917,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
"Sorry, that's not the right check. So, maybe not: https://github.com/kubernetes/kubernetes/blob/c4f3102b1ff8f971612abb0d8986ae63aaac3e47/pkg/volume/empty_dir/empty_dir.go#L217 But, anyway, my point is sometimes these can unintentionally succeed",,,,,,Anecdotal,comment,,,,,,,,2018-07-17,github/bgrant0607,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-405438932,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
"Yeah, point taken. I put a hold on the PR until we can think through whether this approach is a net positive. If we end up moving forward with it, we'll need to be really clear about the types of scenarios where it is an appropriate tool to use.",,,,,,Anecdotal,comment,,,,,,,,2018-07-17,github/liggitt,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-405440098,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
@liggitt One approach could be to write tests that demonstrate asynchronous failure and commit them before enabling new validation. I also think we need to start requiring tests for issues such as duplicate entries in associative lists ASAP.,,,,,,Anecdotal,comment,,,,,,,,2018-07-17,github/bgrant0607,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-405441160,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
"Along those lines, I actually don't think the integer memory requirement should be included. I can successfully start a pod with fractional memory requirements, which just get [rounded when necessary](https://github.com/kubernetes/kubernetes/blob/0e097af8d8208a02dc477a874e8dac39835c1983/pkg/api/resource/helpers.go#L113).",,,,,,Anecdotal,comment,,,,,,,,2018-07-17,github/tallclair,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-405441200,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
"Would it be possible to implement a tighter validation as a validating webhook, that the cluster admin can decide to enable/disable? There could even be a way (annotation?) to ignore specific objects known to be broken.",,,,,,Anecdotal,comment,,,,,,,,2018-07-17,github/apelisse,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-405442253,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
> One approach could be to write tests that demonstrate asynchronous failure and commit them before enabling new validation. Good idea. > I also think we need to start requiring tests for issues such as duplicate entries in associative lists ASAP. Yes. Validation in general tends to get little attention in new API PRs.,,,,,,Anecdotal,comment,,,,,,,,2018-07-17,github/liggitt,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-405442346,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
"Thinking about this more, my original comment & title calling this impossible seem to have aged well, haha. I think @liggitt's ratcheting approach is close to fixing this, but it ratchets too quickly, breaking unfixed clients. For it to be safe, I think it needs to additionally: * Continue to accept new objects that the old validator accepts but the new one does not. Each time it falls back like this, the time is recorded. * Once an old validator has not been fallen back upon for T time period,…",,,,,,Anecdotal,comment,,,,,,,,2018-08-14,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-413035093,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
"> I think @liggitt's ratcheting approach is close to fixing this, but it ratchets too quickly, breaking unfixed clients. For it to be safe, I think it needs to additionally: > > * Continue to accept new objects that the old validator accepts but the new one does not. Each time it falls back like this, the time is recorded. > * Once an old validator has not been fallen back upon for T time period, it is then safe to turn the ratchet--drop it from the list of acceptable validators for that cluste…",,,,,,Anecdotal,comment,,,,,,,,2018-08-15,github/liggitt,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-413218452,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
> We would need to make this data visible to admins I do not think it is reasonable to expect admins to understand how to deal with this stuff.,,,,,,Anecdotal,comment,,,,,,,,2018-08-15,github/enj,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-413223338,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
> Two different clusters at the same version with default config could validate creation of a new object differently. I find that very difficult to reason about. We could serve a status page / resource which states validations have ratcheted and/or how close they are to ratcheting. Error messages can direct users to that page. I don't find this confusing. The status page can state a clear english description for what each ratchet changes. Perhaps the resource could even let you un-ratchet in an…,,,,,,Anecdotal,comment,,,,,,,,2018-08-15,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-413263824,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
> > We would need to make this data visible to admins > I do not think it is reasonable to expect admins to understand how to deal with this stuff. We currently make admins deal with a lot of things that IMO they shouldn't have to. Sometime soon I want to have a philosophy discussion on the SIG call about how we should treat admins. In this case there must be a way to unbreak a deployment instantly. The status page should be the sort of thing that you don't have to even be aware of until a warn…,,,,,,Anecdotal,comment,,,,,,,,2018-08-15,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-413265059,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
"Issues go stale after 90d of inactivity. Mark the issue as fresh with `/remove-lifecycle stale`. Stale issues rot after an additional 30d of inactivity and eventually close. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2018-11-13,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/64841#issuecomment-438351212,repo: kubernetes/kubernetes | issue: Validation Tightening is not Generally Possible | keyword: gotcha
"Add the enabled field to NetworkPolicy Sometimes, I want to disable the networkpolicy temporarily. Now, I must delete it or edit it to match none. I wonder why not we add a field to control it",,,,,,Anecdotal,issue,,,,,,,,2022-09-19,github/zlcnju,https://github.com/kubernetes/kubernetes/issues/112560,repo: kubernetes/kubernetes | keyword: gotcha | state: open
"@kerthcet: The label(s) `kind/?` cannot be applied, because the repository doesn't have them. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/112560#issuecomment-1251090432): >/sig network >/kind feature ? Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](…",,,,,,Anecdotal,comment,,,,,,,,2022-09-19,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/112560#issuecomment-1251090493,repo: kubernetes/kubernetes | issue: Add the enabled field to NetworkPolicy  | keyword: gotcha
"Hi, thanks for the suggestion. This one is tricky :) What is a disabled Network Policy? The whole Network Policy? Just some rules? Also adding a `enabled` field takes some semantics problems: what if a CNI doesn't know the `enabled`, and even you setting it as `enabled: false` it keeps the NetPol enabled always. I have some concerns on this approach (I guess we discussed this on past? @astoycos @danwinship) and I'm not sure if you are talking about the whole policy, how burden is to delete/read…",,,,,,Anecdotal,comment,,,,,,,,2022-09-24,github/rikatz,https://github.com/kubernetes/kubernetes/issues/112560#issuecomment-1256989193,repo: kubernetes/kubernetes | issue: Add the enabled field to NetworkPolicy  | keyword: gotcha
"This would need to be discussed as a KEP. That said, the functionality you want sort of already exists; you should be able to set `policyTypes` to `[]` without changing anything else. Then the policy will not affect either ingress or egress, which is to say, it will not affect anything. (Of course, to re-enable it, you need to know the correct value to set `policyTypes` back to.)",,,,,,Anecdotal,comment,,,,,,,,2022-09-26,github/danwinship,https://github.com/kubernetes/kubernetes/issues/112560#issuecomment-1258109061,repo: kubernetes/kubernetes | issue: Add the enabled field to NetworkPolicy  | keyword: gotcha
"> This would need to be discussed as a KEP. That said, the functionality you want sort of already exists; you should be able to set `policyTypes` to `[]` without changing anything else. Then the policy will not affect either ingress or egress, which is to say, it will not affect anything. (Of course, to re-enable it, you need to know the correct value to set `policyTypes` back to.) PolicyTypes is a choice, but I still need to remember a little config about it. I think the addiontial field is be…",,,,,,Anecdotal,comment,,,,,,,,2022-09-27,github/zlcnju,https://github.com/kubernetes/kubernetes/issues/112560#issuecomment-1258954620,repo: kubernetes/kubernetes | issue: Add the enabled field to NetworkPolicy  | keyword: gotcha
"> Hi, thanks for the suggestion. > > This one is tricky :) What is a disabled Network Policy? The whole Network Policy? Just some rules? > > Also adding a `enabled` field takes some semantics problems: what if a CNI doesn't know the `enabled`, and even you setting it as `enabled: false` it keeps the NetPol enabled always. > > I have some concerns on this approach (I guess we discussed this on past? @astoycos @danwinship) and I'm not sure if you are talking about the whole policy, how burden is …",,,,,,Anecdotal,comment,,,,,,,,2022-09-27,github/zlcnju,https://github.com/kubernetes/kubernetes/issues/112560#issuecomment-1258959711,repo: kubernetes/kubernetes | issue: Add the enabled field to NetworkPolicy  | keyword: gotcha
"FWIW, I was also looking for a way to have the policies stored in the cluster, but with an enabled/disabled flag. Storing disabled policies (as is, without changes required in `policyTypes`) in the cluster also gives a `--dry-run` like functionality where users would just want to have knowledge of the network policies waiting to be applied and can enable those explicitly. There could be controllers that would automatically create network policies, but not apply them. Also wrt not specifying pol…",,,,,,Anecdotal,comment,,,,,,,,2022-09-27,github/chauhanshubham,https://github.com/kubernetes/kubernetes/issues/112560#issuecomment-1259018317,repo: kubernetes/kubernetes | issue: Add the enabled field to NetworkPolicy  | keyword: gotcha
"> Also wrt not specifying policyTypes, the [documentation](https://pkg.go.dev/k8s.io/kubernetes/pkg/apis/networking#NetworkPolicySpec) says otherwise. It seems to default to egress/ingress based on the rules specified. Ah... well, that only happens at creation time. So you wouldn't be able to _create_ a disabled policy this way, but after the object already exists, you could enable and disable it that way.",,,,,,Anecdotal,comment,,,,,,,,2022-10-03,github/danwinship,https://github.com/kubernetes/kubernetes/issues/112560#issuecomment-1265956968,repo: kubernetes/kubernetes | issue: Add the enabled field to NetworkPolicy  | keyword: gotcha
"I'm happy to help where I can with a KEP here, the [networkPolicyStatus KEP](https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/2943-networkpolicy-status) may be a good one to follow? Please try to attend the [network-policy-api meeting](https://github.com/kubernetes/community/tree/master/sig-network) if there are more questions!",,,,,,Anecdotal,comment,,,,,,,,2022-10-03,github/astoycos,https://github.com/kubernetes/kubernetes/issues/112560#issuecomment-1265975192,repo: kubernetes/kubernetes | issue: Add the enabled field to NetworkPolicy  | keyword: gotcha
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs. This bot triages issues and PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue or PR as fresh with `/remove-lifecycle stale` - Mark this issue or PR…",,,,,,Anecdotal,comment,,,,,,,,2023-01-01,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/112560#issuecomment-1368530078,repo: kubernetes/kubernetes | issue: Add the enabled field to NetworkPolicy  | keyword: gotcha
"/remove-lifecycle stale Adding this to the sig-network-policy API agenda, however we'll still need someone to open/own a KEP if this feature has hope of getting agreed on.",,,,,,Anecdotal,comment,,,,,,,,2023-01-17,github/astoycos,https://github.com/kubernetes/kubernetes/issues/112560#issuecomment-1385603827,repo: kubernetes/kubernetes | issue: Add the enabled field to NetworkPolicy  | keyword: gotcha
"> Sometimes, I want to disable the networkpolicy temporarily. Now, I must delete it or edit it to match none. I wonder why not we add a field to control it So when I read ""disable network policy"" -> to me it sounds like what you are trying to achieve is to have a way to override all existing network policies, you basically don't want the existing network policies to take any effect. Correct @zlcnju ? This can be achieved by creating an ANP (https://github.com/kubernetes/enhancements/tree/master…",,,,,,Anecdotal,comment,,,,,,,,2023-01-17,github/tssurya,https://github.com/kubernetes/kubernetes/issues/112560#issuecomment-1385732272,repo: kubernetes/kubernetes | issue: Add the enabled field to NetworkPolicy  | keyword: gotcha
"I think that's a different use case: ""I want to bypass _someone else's_ NetworkPolicy"" as opposed to ""I want to bypass _my own_ NetworkPolicy"" which the OP was requesting. (End users can probably edit their own NPs but can't create ANPs.)",,,,,,Anecdotal,comment,,,,,,,,2023-01-17,github/danwinship,https://github.com/kubernetes/kubernetes/issues/112560#issuecomment-1385748042,repo: kubernetes/kubernetes | issue: Add the enabled field to NetworkPolicy  | keyword: gotcha
"> I think that's a different use case: ""I want to bypass _someone else's_ NetworkPolicy"" as opposed to ""I want to bypass _my own_ NetworkPolicy"" which the OP was requesting. (End users can probably edit their own NPs but can't create ANPs.) ah gotcha, got confused on the request coming from a NP/ns owner versus cluster admin.",,,,,,Anecdotal,comment,,,,,,,,2023-01-17,github/tssurya,https://github.com/kubernetes/kubernetes/issues/112560#issuecomment-1385760666,repo: kubernetes/kubernetes | issue: Add the enabled field to NetworkPolicy  | keyword: gotcha
"This issue has not been updated in over 1 year, and should be re-triaged. You can: - Confirm that this issue is still relevant with `/triage accepted` (org members only) - Close this issue with `/close` For more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/ /remove-triage accepted",,,,,,Anecdotal,comment,,,,,,,,2024-01-28,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/112560#issuecomment-1913392286,repo: kubernetes/kubernetes | issue: Add the enabled field to NetworkPolicy  | keyword: gotcha
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2024-04-27,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/112560#issuecomment-2080305833,repo: kubernetes/kubernetes | issue: Add the enabled field to NetworkPolicy  | keyword: gotcha
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2024-08-01,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/112560#issuecomment-2261686990,repo: kubernetes/kubernetes | issue: Add the enabled field to NetworkPolicy  | keyword: gotcha
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle rotten` - Close this issue with `/…",,,,,,Anecdotal,comment,,,,,,,,2024-08-31,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/112560#issuecomment-2322625404,repo: kubernetes/kubernetes | issue: Add the enabled field to NetworkPolicy  | keyword: gotcha
"Make indices of IndexedJob pods configurable ### What would you like to be added? As a user I would like to be able to configure with which index the pods of a IndexedJob are getting created. By introducing a new field to the job spec API (possible name: `indices` -> TBD) we can influence with which index the controller would create pods. This field then has a mutually exclusive relation to the `completions` field. Ideally, we could also have a new status field called `failedIndices` which cont…",,,,,,Anecdotal,issue,,,,,,,,2022-03-29,github/isibeni,https://github.com/kubernetes/kubernetes/issues/109131,repo: kubernetes/kubernetes | keyword: gotcha | state: open
"@isibeni: This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related …",,,,,,Anecdotal,comment,,,,,,,,2022-03-29,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1082424716,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
"@isibeni: The label(s) `wg/batch` cannot be applied, because the repository doesn't have them. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1082427030): >/wg batch Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/…",,,,,,Anecdotal,comment,,,,,,,,2022-03-29,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1082427040,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
"cc @soltysh The field probably needs to be a string and it should have some limit for the number of characters. As for the status, it might be harder to limit. In Indexed Jobs we limit `parallelism` to indirectly limit the number of characters in `completedIndexes`. We need to make a similar analysis to guarantee that the field doesn't grow indefinitely. https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/job-v1/#lifecycle IMO, The feature would be a nice addition to Indexed …",,,,,,Anecdotal,comment,,,,,,,,2022-03-30,github/alculquicondor,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1083140133,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
">The field probably needs to be a string and it should have some limit for the number of characters. Agree. I would propose that it should have the same format as the `status.completedIndexes` field. e.g. `1,3-5,7` for 1, 3, 4, 5 and 7. If we then would also introduce a `status.failedIndexes` field a operator would then be able to just copy the string and insert it into the indexes field to retrigger a new job",,,,,,Anecdotal,comment,,,,,,,,2022-03-30,github/isibeni,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1083580894,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
>In Indexed Jobs we limit parallelism to indirectly limit the number of characters in completedIndexes. We need to make a similar analysis to guarantee that the field doesn't grow indefinitely. @alculquicondor Could you elaborate a bit what the problem with that is? Performance and memory consumption problems for scenarios where we can't simplify the string (e.g. all pods with even indexes are completing first)?,,,,,,Anecdotal,comment,,,,,,,,2022-03-30,github/isibeni,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1083597036,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
"> You have identified the correct challenging scenario. And with introducing the new field the challenging scenario would be when every other index fails directly (e.g. `1,3,5,7,9,11,13,15`). We then would have the same value in `completedIndexes` and `failedIndexes`. Another one would be when every other index fails and every fourth index succeeds. `completedIndexes` could then contain the following value `1-3,5-7,9-11,13-15` while `failedIndexes` contains `1,3,5,7,9,11,13,15`. Based on that w…",,,,,,Anecdotal,comment,,,,,,,,2022-04-04,github/isibeni,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1087170391,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
"If an index fails, but succeeds with a follow up pod, we shouldn't list it in `failedIndexes`. For `completedIndexes`, `.spec.parallelism` limits how many non-continuous indexes can complete, because we create the indexes in order. Can the same rationale be applied for `failedIndexes`?",,,,,,Anecdotal,comment,,,,,,,,2022-04-04,github/alculquicondor,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1087597352,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
sorry I just figured out that I had a small misconception about the behavior of backofflimit on indexed jobs. I thought the backofflimit is applied for every index so that e.g. with a backofflimit of 2 each failing index will be restarted at least twice and if they reached the limit only those indexes will be considered as failed while the other indexes are allowed to run till completion. Still the overall job will be considered as failed. If indexed jobs would behave like that the field `faile…,,,,,,Anecdotal,comment,,,,,,,,2022-04-05,github/isibeni,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1088328034,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
"The backofflimit is for the entire job. I think the feature request still makes sense. The job overall is marked as Failed, but some indexes might have completed.",,,,,,Anecdotal,comment,,,,,,,,2022-04-05,github/alculquicondor,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1088745276,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
"> Maybe we can consider introducing a behaviour like a just described. That would be a separate feature request. A we would need a new field, as we cannot change the behavior of existing fields, for backwards compatibility.",,,,,,Anecdotal,comment,,,,,,,,2022-04-05,github/alculquicondor,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1088747364,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
"> That would be a separate feature request. A we would need a new field, as we cannot change the behavior of existing fields, for backwards compatibility. Agree on that. Will open another issue these days > I think the feature request still makes sense. The job overall is marked as Failed, but some indexes might have completed. Yeah you are right. But then I would propose that we don't need the status field `failedIndexes` for now. This only becomes helpful with the other feature I just proposed",,,,,,Anecdotal,comment,,,,,,,,2022-04-08,github/isibeni,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1093098735,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
"This was brought to my attention during last wg-batch call. A few ideas worth considering. 1. Do we actually need net-new field to express failed indices? Given current completions 100, for example, and a list of `completedIndexes` being 1-40,60-100 do we actually need that new filed if we should be able to calculate that using the existing fields? 2. I'd be curious to hear more about the use-cases for this particular request. @alculquicondor provided one during the call, but I'm not sure that'…",,,,,,Anecdotal,comment,,,,,,,,2022-04-20,github/soltysh,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1103837685,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
I created https://github.com/kubernetes/kubernetes/issues/109712 to track the idea of a job execution mode where every index is allowed to execute.,,,,,,Anecdotal,comment,,,,,,,,2022-04-28,github/ahg-g,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1112559925,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
"> Do we actually need net-new field to express failed indices? Given current completions 100, for example, and a list of completedIndexes being 1-40,60-100 do we actually need that new filed if we should be able to calculate that using the existing fields? Not necessarily. As of now it would be just for convenience. For some it might not be so straightforward to implement the calculation. If we would provide it all clients/users don't need to reimplement it on their own. But for this feature re…",,,,,,Anecdotal,comment,,,,,,,,2022-05-31,github/isibeni,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1142443186,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs. This bot triages issues and PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue or PR as fresh with `/remove-lifecycle stale` - Mark this issue or PR…",,,,,,Anecdotal,comment,,,,,,,,2022-08-29,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1230705814,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues and PRs. This bot triages issues and PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue or PR as fresh with `/remove-lifecycle stale` - Mark this issue or PR…",,,,,,Anecdotal,comment,,,,,,,,2022-11-27,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1328325146,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages issues and PRs according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue or PR as fresh with `/remove-lifecycle rotten` - Close this is…",,,,,,Anecdotal,comment,,,,,,,,2022-12-27,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1366162056,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2023-03-27,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1485883509,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
"@alculquicondor what specifically is blocking? Why couldn't it be the case that you can configure the indices of the job, but not allow every index to execute?",,,,,,Anecdotal,comment,,,,,,,,2023-06-14,github/vsoch,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1591851201,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
"In a way, the features are independent. But doing #109712 first makes this feature more useful. So we started with #109712 The idea is that #109712 will include a field `failedIndexes: ""1-5,7""`. After you adjust some parameters that could have cause the failures, you can simply copy-paste ""1-5,7"" into a new Job that has a field `.spec.indexes: ""1-5,7""`",,,,,,Anecdotal,comment,,,,,,,,2023-06-14,github/alculquicondor,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1591916626,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
"Gotcha - that does make sense. I'm very interested in this feature, and if they can be separate (and eventually integrated in the way you described) would it be OK if I started on it? I haven't contributed to Kubernetes proper and would like to try that out (so I'm more empowered to in the future). But if you have someone in mind and/or absolutely want to wait, I can respect that too! I was able to get a prototype for bursting with the Flux Operator, albeit I used a hack to get around this part…",,,,,,Anecdotal,comment,,,,,,,,2023-06-14,github/vsoch,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1592048060,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
"I don't think there were any takers previously. Note for a feature like this, you need to follow the KEP process: https://github.com/kubernetes/enhancements/tree/master/keps/sig-architecture/0000-kep-process Ideally, before starting, it's helpful if you introduce yourself to SIG Apps (https://github.com/kubernetes/community/tree/master/sig-apps#meetings) announcing your intent and high level ideas, so that you clear possible questions and roadblocks in the future.",,,,,,Anecdotal,comment,,,,,,,,2023-06-15,github/alculquicondor,https://github.com/kubernetes/kubernetes/issues/109131#issuecomment-1592928585,repo: kubernetes/kubernetes | issue: Make indices of IndexedJob pods configurable | keyword: gotcha
"kubectl should warn user if they attempt to authenticate to the cluster using an expired certificate <!-- Thanks for filing an issue! Before hitting the button, please answer these questions.--> **Is this a request for help?** (If yes, you should use our troubleshooting guide and community support channels, see http://kubernetes.io/docs/troubleshooting/.): **What keywords did you search in Kubernetes issues before filing this one?** (If you have found any duplicates, you should instead reply th…",,,,,,Anecdotal,issue,,,,,,,,2017-01-11,github/chrissnell,https://github.com/kubernetes/kubernetes/issues/39767,repo: kubernetes/kubernetes | keyword: gotcha | state: open
"@smarterclayton @thockin I have been wondering about this issue that was brought up. I am not sure about asking kubectl to do additional checks on certs, however, the messaging may be a bit better if we include error code and message that are part of the status struct: ```""error: You must be logged in to the server (%d %s: %s)"",``` What do you think? If so I would like to make the change.",,,,,,Anecdotal,comment,,,,,,,,2017-02-22,github/alejandroEsc,https://github.com/kubernetes/kubernetes/issues/39767#issuecomment-281727427,repo: kubernetes/kubernetes | issue: kubectl should warn user if they attempt to authenticate to the cluster using an expired certificate | keyword: gotcha
The certificate expiry check is such a little amount of code and would save users so much headache and time. Why not let them know?,,,,,,Anecdotal,comment,,,,,,,,2017-02-22,github/chrissnell,https://github.com/kubernetes/kubernetes/issues/39767#issuecomment-281729823,repo: kubernetes/kubernetes | issue: kubectl should warn user if they attempt to authenticate to the cluster using an expired certificate | keyword: gotcha
"@chrissnell not debating that fact, if anything i think it would be a nice to have. However, i suspect, and i could be wrong, that this is one case where less information we give people about what is wrong with their auth the more secure we are? I could be wrong and willing to make the change if that is the case.",,,,,,Anecdotal,comment,,,,,,,,2017-02-22,github/alejandroEsc,https://github.com/kubernetes/kubernetes/issues/39767#issuecomment-281730684,repo: kubernetes/kubernetes | issue: kubectl should warn user if they attempt to authenticate to the cluster using an expired certificate | keyword: gotcha
Agreed @liggitt . This would be a kubectl feature. Almost everyone that uses cert-based auth will run into this issue eventually.,,,,,,Anecdotal,comment,,,,,,,,2017-02-22,github/chrissnell,https://github.com/kubernetes/kubernetes/issues/39767#issuecomment-281737071,repo: kubernetes/kubernetes | issue: kubectl should warn user if they attempt to authenticate to the cluster using an expired certificate | keyword: gotcha
"@chrissnell @liggitt gotcha, is then an approval to move towards this direction? What additional checks should we propose?",,,,,,Anecdotal,comment,,,,,,,,2017-02-22,github/alejandroEsc,https://github.com/kubernetes/kubernetes/issues/39767#issuecomment-281748012,repo: kubernetes/kubernetes | issue: kubectl should warn user if they attempt to authenticate to the cluster using an expired certificate | keyword: gotcha
@chrissnell There are no sig labels on this issue. Please [add a sig label](https://github.com/kubernetes/test-infra/blob/master/commands.md) by:<br>(1) mentioning a sig: `@kubernetes/sig-<team-name>-misc`<br>(2) specifying the label manually: `/sig <label>`<br><br>_Note: method (1) will trigger a notification to the team. You can find the team list [here](https://github.com/kubernetes/community/blob/master/sig-list.md)._,,,,,,Anecdotal,comment,,,,,,,,2017-05-31,github/k8s-github-robot,https://github.com/kubernetes/kubernetes/issues/39767#issuecomment-305341092,repo: kubernetes/kubernetes | issue: kubectl should warn user if they attempt to authenticate to the cluster using an expired certificate | keyword: gotcha
@alejandroEsc sounds good to me. I think this could be combined with #29819 to do some basic cert validity checks for the `kubectl` user. One design question I have is: should `kubectl` exit with a certain exit code when the cert fails these checks?,,,,,,Anecdotal,comment,,,,,,,,2017-06-21,github/chrissnell,https://github.com/kubernetes/kubernetes/issues/39767#issuecomment-310216327,repo: kubernetes/kubernetes | issue: kubectl should warn user if they attempt to authenticate to the cluster using an expired certificate | keyword: gotcha
"We do not have custom exit codes today, and if we do add those we would do it holistically. On Wed, Jun 21, 2017 at 5:55 PM, Chris Snell <notifications@github.com> wrote: > @alejandroEsc <https://github.com/alejandroesc> sounds good to me. I > think this could be combined with #29819 > <https://github.com/kubernetes/kubernetes/issues/29819> to do some basic > cert validity checks for the kubectl user. > > One design question I have is: should kubectl exit with a certain exit > code when the cer…",,,,,,Anecdotal,comment,,,,,,,,2017-06-22,github/smarterclayton,https://github.com/kubernetes/kubernetes/issues/39767#issuecomment-310248467,repo: kubernetes/kubernetes | issue: kubectl should warn user if they attempt to authenticate to the cluster using an expired certificate | keyword: gotcha
"It feels like overkill to handle this on the server-side but if that's where everyone wants it, it doesn't matter to me. Maybe server-side would also help those using alternate API clients? I do urge the team to reconsider the wording of the ""You must be logged into the server"" message. This message is totally unhelpful to someone dealing with an auth issue.",,,,,,Anecdotal,comment,,,,,,,,2017-07-21,github/chrissnell,https://github.com/kubernetes/kubernetes/issues/39767#issuecomment-316882628,repo: kubernetes/kubernetes | issue: kubectl should warn user if they attempt to authenticate to the cluster using an expired certificate | keyword: gotcha
"Issues go stale after 90d of inactivity. Mark the issue as fresh with `/remove-lifecycle stale`. Stale issues rot after an additional 30d of inactivity and eventually close. Prevent issues from auto-closing with an `/lifecycle frozen` comment. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or `@fejta`. /lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2018-01-01,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/39767#issuecomment-354642846,repo: kubernetes/kubernetes | issue: kubectl should warn user if they attempt to authenticate to the cluster using an expired certificate | keyword: gotcha
This needs consensus on where the code should live and exactly what it should do (IIRC that is what kept #87796 from progressing).,,,,,,Anecdotal,comment,,,,,,,,2021-06-14,github/enj,https://github.com/kubernetes/kubernetes/issues/39767#issuecomment-860823793,repo: kubernetes/kubernetes | issue: kubectl should warn user if they attempt to authenticate to the cluster using an expired certificate | keyword: gotcha
"Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too **What would you like to be added**: I would like to start a conversation to understand if we may improve `GetContainerOOMScoreAdjust` to: 1. Calculate the `oom_score_adj` in a CPU-agnosticy way 2. Take in account Pod Priority too **Why is this needed**: Currently, the `kubelet` sets a `oom_score_adj` value for each container based on the Pod QoS: - `Guaranteed`: `-998` - `BestEffort`: `1000` - `Burstable`: …",,,,,,Anecdotal,issue,,,,,,,,2019-06-10,github/pracucci,https://github.com/kubernetes/kubernetes/issues/78848,repo: kubernetes/kubernetes | keyword: gotcha | state: closed
"Thanks for your idea @pracucci ! To start discussion, could you share a little more about the advantages of setting `oom_score_adj` in a CPU agnostic way? Is the main benefit that you feel it makes more sense conceptually (i.e. CPU shouldn't impact a memory only concept) or are there additional benefits? The Pod Priority point is interesting! To be sure I'm understanding, is the problem that just because a pod is high priority, doesn't mean it is necessarily in the `Guaranteed` Pod QoS? So the …",,,,,,Anecdotal,comment,,,,,,,,2019-06-11,github/mattjmcnaughton,https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-500854965,"repo: kubernetes/kubernetes | issue: Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too | keyword: gotcha"
"Thanks @mattjmcnaughton for your reply. > To start discussion, could you share a little more about the advantages of setting oom_score_adj in a CPU agnostic way? Is the main benefit that you feel it makes more sense conceptually (i.e. CPU shouldn't impact a memory only concept) or are there additional benefits? We have several pods running critical applications for which we need low latency. On such pods we do **not** set the CPU limit in order to avoid CPU throttling (I'm aware some people dis…",,,,,,Anecdotal,comment,,,,,,,,2019-06-11,github/pracucci,https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-500862870,"repo: kubernetes/kubernetes | issue: Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too | keyword: gotcha"
Gotcha gotcha. This request makes a lot of sense to me. Do you have a high level proposal for how you'd like to see `oom_score_adj` calculated? @kubernetes/sig-node-feature-requests I'm wondering whether a KEP is appropriate/necessary in this case?,,,,,,Anecdotal,comment,,,,,,,,2019-06-12,github/mattjmcnaughton,https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-501286972,"repo: kubernetes/kubernetes | issue: Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too | keyword: gotcha"
"Issues go stale after 90d of inactivity. Mark the issue as fresh with `/remove-lifecycle stale`. Stale issues rot after an additional 30d of inactivity and eventually close. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2019-09-10,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-529958020,"repo: kubernetes/kubernetes | issue: Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too | keyword: gotcha"
"Stale issues rot after 30d of inactivity. Mark the issue as fresh with `/remove-lifecycle rotten`. Rotten issues close after an additional 30d of inactivity. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /lifecycle rotten",,,,,,Anecdotal,comment,,,,,,,,2019-10-10,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-540631196,"repo: kubernetes/kubernetes | issue: Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too | keyword: gotcha"
"Rotten issues close after 30d of inactivity. Reopen the issue with `/reopen`. Mark the issue as fresh with `/remove-lifecycle rotten`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /close",,,,,,Anecdotal,comment,,,,,,,,2019-11-09,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-552111851,"repo: kubernetes/kubernetes | issue: Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too | keyword: gotcha"
"@fejta-bot: Closing this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-552111851): >Rotten issues close after 30d of inactivity. >Reopen the issue with `/reopen`. >Mark the issue as fresh with `/remove-lifecycle rotten`. > >Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). >/close Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contribu…",,,,,,Anecdotal,comment,,,,,,,,2019-11-09,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-552111865,"repo: kubernetes/kubernetes | issue: Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too | keyword: gotcha"
"/reopen In recent experiment when putting nodes under memory stress(by applying some burstable qos stress pods consume a lot of resource), we notice that stress pods can actually have lower `oom_score` than pods that have higher priority class. This is expected based on the kubelet implementation but might not be intuitive. Why expected? Based on the public documentation, the `oom_score_adj` will be set based on the pod qos class. The only exception is `system-node-critical` priorityClass. Why …",,,,,,Anecdotal,comment,,,,,,,,2023-12-14,github/h9jiang,https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-1856914165,"repo: kubernetes/kubernetes | issue: Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too | keyword: gotcha"
"@h9jiang: You can't reopen an issue/PR unless you authored it or you are a collaborator. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-1856914165): >/reopen > >In recent experiment when putting nodes under memory stress(by applying some burstable qos stress pods consume a lot of resource), we notice that stress pods can actually have lower `oom_score` than pods that have higher priority class. > > >This is expected based on the kubelet implem…",,,,,,Anecdotal,comment,,,,,,,,2023-12-14,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-1856914340,"repo: kubernetes/kubernetes | issue: Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too | keyword: gotcha"
/reopen I think this is still an feature worth to have some discussion about it. Re-opening as there is some additional requests for it.,,,,,,Anecdotal,comment,,,,,,,,2023-12-18,github/bobbypage,https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-1861460592,"repo: kubernetes/kubernetes | issue: Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too | keyword: gotcha"
"@bobbypage: Reopened this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-1861460592): >/reopen > >I think this is still an feature worth to have some discussion about it. Re-opening as there is some additional requests for it. Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please fi…",,,,,,Anecdotal,comment,,,,,,,,2023-12-18,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-1861460844,"repo: kubernetes/kubernetes | issue: Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too | keyword: gotcha"
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Reopen this issue with `/reopen` - Mark this issue as fresh with `/remove-lifecycle ro…",,,,,,Anecdotal,comment,,,,,,,,2024-01-20,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-1902079234,"repo: kubernetes/kubernetes | issue: Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too | keyword: gotcha"
"@k8s-triage-robot: Closing this issue, marking it as ""Not Planned"". <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-1902079234): >The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. > >This bot triages issues according to the following rules: >- After 90d of inactivity, `lifecycle/stale` is applied >- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is app…",,,,,,Anecdotal,comment,,,,,,,,2024-01-20,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-1902079248,"repo: kubernetes/kubernetes | issue: Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too | keyword: gotcha"
"@yujuhong: Reopened this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-1981506207): >/reopen >/remove-lifecycle rotten Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=P…",,,,,,Anecdotal,comment,,,,,,,,2024-03-06,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-1981506320,"repo: kubernetes/kubernetes | issue: Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too | keyword: gotcha"
"/triage accepted /priority backlog according to the above comments, there's interest in the community about discussing this issue",,,,,,Anecdotal,comment,,,,,,,,2024-03-12,github/ffromani,https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-1992170228,"repo: kubernetes/kubernetes | issue: Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too | keyword: gotcha"
"This issue has not been updated in over 1 year, and should be re-triaged. You can: - Confirm that this issue is still relevant with `/triage accepted` (org members only) - Close this issue with `/close` For more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/ /remove-triage accepted",,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-2718685402,"repo: kubernetes/kubernetes | issue: Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too | keyword: gotcha"
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2025-06-10,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-2960170829,"repo: kubernetes/kubernetes | issue: Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too | keyword: gotcha"
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle rotten` - Close this issue with `/…",,,,,,Anecdotal,comment,,,,,,,,2025-07-10,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-3058553770,"repo: kubernetes/kubernetes | issue: Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too | keyword: gotcha"
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. This bot triages issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Reopen this issue with `/reopen` - Mark this issue as fresh with `/remove-lifecycle ro…",,,,,,Anecdotal,comment,,,,,,,,2025-08-09,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-3172056415,"repo: kubernetes/kubernetes | issue: Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too | keyword: gotcha"
"@k8s-triage-robot: Closing this issue, marking it as ""Not Planned"". <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-3172056415): >The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs. > >This bot triages issues according to the following rules: >- After 90d of inactivity, `lifecycle/stale` is applied >- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is app…",,,,,,Anecdotal,comment,,,,,,,,2025-08-09,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/78848#issuecomment-3172056454,"repo: kubernetes/kubernetes | issue: Calculate oom_score_adj in a CPU-agnostic way, taking in consideration Pod Priority too | keyword: gotcha"
Enhance help text for the 'top' command #### What type of PR is this? /kind cleanup #### What this PR does / why we need it: The `kubectl top` command's help text was factually correct but lacked the necessary context for users to use it effectively. #### Which issue(s) this PR is related to: https://github.com/kubernetes/kubectl/issues/830 #### Does this PR introduce a user-facing change? ```release-note NONE ``` @serathius,,,,,,Anecdotal,issue,,,,,,,,2025-06-27,github/janetkuo,https://github.com/kubernetes/kubernetes/pull/132567,repo: kubernetes/kubernetes | keyword: gotcha | state: closed
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-06-27,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/132567#issuecomment-3010755555,repo: kubernetes/kubernetes | issue: Enhance help text for the 'top' command | keyword: gotcha
"Overall lgtm, couple of nits as since posting the issue we had made multiple improvements to metrics-server documentation.",,,,,,Anecdotal,comment,,,,,,,,2025-06-27,github/serathius,https://github.com/kubernetes/kubernetes/pull/132567#issuecomment-3012082514,repo: kubernetes/kubernetes | issue: Enhance help text for the 'top' command | keyword: gotcha
We occasionally get issues about the misunderstanding of `top` command. This PR will have a positive impact by elaborating the behavior. Thank you /approve /hold in case you want more review from instrumentation,,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/ardaguclu,https://github.com/kubernetes/kubernetes/pull/132567#issuecomment-3018085138,repo: kubernetes/kubernetes | issue: Enhance help text for the 'top' command | keyword: gotcha
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/132567#issuecomment-3018085138"" title=""Approved"">ardaguclu</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/132567#"" title=""Author self-approved"">janetkuo</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https://git.k…",,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/132567#issuecomment-3018085682,repo: kubernetes/kubernetes | issue: Enhance help text for the 'top' command | keyword: gotcha
"Allow specifying a directory for image credential providers json configuration @micahhausler and i were chatting with some folks about this use case where there may be multiple credential providers, each with possibly their own configuration json files. So i wanted to check if it is as easy as it looks... Some background: - Various AMI building tools typically inject a json file with a cloud provider in mind and setup the `image-credential-provider-config` and the binary https://github.com/sear…",,,,,,Anecdotal,issue,,,,,,,,2025-05-07,github/dims,https://github.com/kubernetes/kubernetes/pull/131658,repo: kubernetes/kubernetes | keyword: gotcha | state: closed
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-05-07,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131658#issuecomment-2860051223,repo: kubernetes/kubernetes | issue: Allow specifying a directory for image credential providers json configuration | keyword: gotcha
"@dims: The label(s) `kind/important-soon` cannot be applied, because the repository doesn't have them. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/131658#issuecomment-2877858969): >/kind important-soon Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](htt…",,,,,,Anecdotal,comment,,,,,,,,2025-05-13,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131658#issuecomment-2877859178,repo: kubernetes/kubernetes | issue: Allow specifying a directory for image credential providers json configuration | keyword: gotcha
"> Shouldn't we just make a new flag that makes it clear that the input is a directory? @enj if you all think it's necessary, i am happy to",,,,,,Anecdotal,comment,,,,,,,,2025-05-14,github/dims,https://github.com/kubernetes/kubernetes/pull/131658#issuecomment-2878499893,repo: kubernetes/kubernetes | issue: Allow specifying a directory for image credential providers json configuration | keyword: gotcha
> Shouldn't we just make a new flag This _would_ align with `--config` and `--config-dir`. It does seem useful for a base/default config to be provided with `--image-credential-provider-config` and a (possibly-empty) dropin dir provided with `--image-credential-provider-config-dir`.,,,,,,Anecdotal,comment,,,,,,,,2025-05-14,github/cartermckinnon,https://github.com/kubernetes/kubernetes/pull/131658#issuecomment-2878505142,repo: kubernetes/kubernetes | issue: Allow specifying a directory for image credential providers json configuration | keyword: gotcha
"> +1 to Mo's comment on having this as a separate flag to make it clear the input is directory. Expanding to allow the existing flag be a dir seems fine to me (like `--filename` in kubectl), just document the exact behavior when a dir (.json/.yaml/.yml only, ordered lexicographically). That automatically makes file-or-dir be mutually exclusive and keeps the documentation together.",,,,,,Anecdotal,comment,,,,,,,,2025-05-14,github/liggitt,https://github.com/kubernetes/kubernetes/pull/131658#issuecomment-2880426361,repo: kubernetes/kubernetes | issue: Allow specifying a directory for image credential providers json configuration | keyword: gotcha
"I think the simple merging you had before would just error in validation if the same name got defined across files, and that's probably a better behavior (indicates an unexpected / not working configuration)",,,,,,Anecdotal,comment,,,,,,,,2025-05-14,github/liggitt,https://github.com/kubernetes/kubernetes/pull/131658#issuecomment-2880877967,repo: kubernetes/kubernetes | issue: Allow specifying a directory for image credential providers json configuration | keyword: gotcha
"> I think the simple merging you had before would just error in validation if the same name got defined across files, and that's probably a better behavior (indicates an unexpected / not working configuration) will do!",,,,,,Anecdotal,comment,,,,,,,,2025-05-14,github/dims,https://github.com/kubernetes/kubernetes/pull/131658#issuecomment-2880895191,repo: kubernetes/kubernetes | issue: Allow specifying a directory for image credential providers json configuration | keyword: gotcha
"@dims: The following test **failed**, say `/retest` to rerun all failed tests or `/retest-required` to rerun all mandatory failed tests: Test name | Commit | Details | Required | Rerun command --- | --- | --- | --- | --- pull-kubernetes-unit-windows-master | be6807e6a570c643ad1e55b7b0d7a4c6345f706f | [link](https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/131658/pull-kubernetes-unit-windows-master/1928084652493377536) | false | `/test pull-kubernetes-unit-windows-master` [Full PR tes…",,,,,,Anecdotal,comment,,,,,,,,2025-05-29,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131658#issuecomment-2919510633,repo: kubernetes/kubernetes | issue: Allow specifying a directory for image credential providers json configuration | keyword: gotcha
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/131658#"" title=""Author self-approved"">dims</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/131658#issuecomment-2919541743"" title=""Approved"">enj</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/131658#issuecomment-2880843460"" title=""Approved"">liggitt</a>* The full list of commands accepted by this bot can be found [here](https://go.…",,,,,,Anecdotal,comment,,,,,,,,2025-05-29,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/131658#issuecomment-2919542670,repo: kubernetes/kubernetes | issue: Allow specifying a directory for image credential providers json configuration | keyword: gotcha
"Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue yo…",,,,,,Anecdotal,issue,,,,,,,,2025-03-14,github/hakuna-matatah,https://github.com/kubernetes/kubernetes/pull/130806,repo: kubernetes/kubernetes | keyword: gotcha | state: closed
"> @hakuna-matatah please add some context for the graphs in the PR body as well Done @dims. The graph above shows that the 5-minute P99 average of work_queue_work_duration_seconds ranges from ~1s to 8s, representing the time taken to process each item in the StatefulSet queue. Additionally, the 5-minute P99 average wait time for an item in the queue before processing is ~1s. From the pprof analysis, most cycles are spent on listing pods. Optimizing this list call will provide the following bene…",,,,,,Anecdotal,comment,,,,,,,,2025-03-14,github/hakuna-matatah,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2725213099,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
>please add some context for the graphs in the PR body as well +1 Normally I'd suggest adding a short description above or below each graph.,,,,,,Anecdotal,comment,,,,,,,,2025-03-14,github/mengqiy,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2725263343,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
@hakuna-matatah can you think of any stress/benchmark/unit tests we can add to stateful_set_test.go? that can ensure we don't regress?,,,,,,Anecdotal,comment,,,,,,,,2025-03-14,github/dims,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2725632619,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
"> @hakuna-matatah can you think of any stress/benchmark/unit tests we can add to stateful_set_test.go? that can ensure we don't regress? This is how I was thinking about it. One unit test that comes to mind is measuring the time difference between listing from the cache versus using an index and enforcing a failure if the index-based lookup isn't significantly faster in controller code. I proposed a similar test in the DaemonSet controller PR— if we reach a consensus on that approach, I can imp…",,,,,,Anecdotal,comment,,,,,,,,2025-03-17,github/hakuna-matatah,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2730373783,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
"@hakuna-matatah as soon as https://github.com/kubernetes/kubernetes/pull/130859 try to get this PR in shape so we can tag it before the freeze, hopfully :crossed_fingers:",,,,,,Anecdotal,comment,,,,,,,,2025-03-20,github/soltysh,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2741502490,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
"> @hakuna-matatah as soon as #130859 try to get this PR in shape so we can tag it before the freeze, hopfully 🤞 Yup, thank you. Waiting for it that to be merged. So I can rebase on top of it and push a change here.",,,,,,Anecdotal,comment,,,,,,,,2025-03-20,github/hakuna-matatah,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2741653374,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
"> Waiting for it that to be merged. So I can rebase on top of it and push a change here. Sorry, it was late for me already so it will have to wait for next release. Although there are still a few comments that needs addressing. /triage accepted",,,,,,Anecdotal,comment,,,,,,,,2025-03-21,github/soltysh,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2742797617,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/130806#"" title=""Author self-approved"">hakuna-matatah</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2743730383"" title=""Approved"">wojtek-t</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https://…",,,,,,Anecdotal,comment,,,,,,,,2025-03-21,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2743730891,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
"The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass. This bot retests PRs for certain kubernetes repos according to the following rules: - The PR does have any `do-not-merge/*` labels - The PR does not have the `needs-ok-to-test` label - The PR is mergeable (does not have a `needs-rebase` label) - The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels) - The PR is failing tests required for merge You can: - Review the [full test history]…",,,,,,Anecdotal,comment,,,,,,,,2025-03-21,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2744183205,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
"The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass. This bot retests PRs for certain kubernetes repos according to the following rules: - The PR does have any `do-not-merge/*` labels - The PR does not have the `needs-ok-to-test` label - The PR is mergeable (does not have a `needs-rebase` label) - The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels) - The PR is failing tests required for merge You can: - Review the [full test history]…",,,,,,Anecdotal,comment,,,,,,,,2025-03-21,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2744475735,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
"The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass. This bot retests PRs for certain kubernetes repos according to the following rules: - The PR does have any `do-not-merge/*` labels - The PR does not have the `needs-ok-to-test` label - The PR is mergeable (does not have a `needs-rebase` label) - The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels) - The PR is failing tests required for merge You can: - Review the [full test history]…",,,,,,Anecdotal,comment,,,,,,,,2025-03-22,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2744703501,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
"The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass. This bot retests PRs for certain kubernetes repos according to the following rules: - The PR does have any `do-not-merge/*` labels - The PR does not have the `needs-ok-to-test` label - The PR is mergeable (does not have a `needs-rebase` label) - The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels) - The PR is failing tests required for merge You can: - Review the [full test history]…",,,,,,Anecdotal,comment,,,,,,,,2025-03-22,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2744903825,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
"The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass. This bot retests PRs for certain kubernetes repos according to the following rules: - The PR does have any `do-not-merge/*` labels - The PR does not have the `needs-ok-to-test` label - The PR is mergeable (does not have a `needs-rebase` label) - The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels) - The PR is failing tests required for merge You can: - Review the [full test history]…",,,,,,Anecdotal,comment,,,,,,,,2025-03-22,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2744994459,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
"The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass. This bot retests PRs for certain kubernetes repos according to the following rules: - The PR does have any `do-not-merge/*` labels - The PR does not have the `needs-ok-to-test` label - The PR is mergeable (does not have a `needs-rebase` label) - The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels) - The PR is failing tests required for merge You can: - Review the [full test history]…",,,,,,Anecdotal,comment,,,,,,,,2025-03-22,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2745117592,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
"The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass. This bot retests PRs for certain kubernetes repos according to the following rules: - The PR does have any `do-not-merge/*` labels - The PR does not have the `needs-ok-to-test` label - The PR is mergeable (does not have a `needs-rebase` label) - The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels) - The PR is failing tests required for merge You can: - Review the [full test history]…",,,,,,Anecdotal,comment,,,,,,,,2025-03-22,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2745199574,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
"The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass. This bot retests PRs for certain kubernetes repos according to the following rules: - The PR does have any `do-not-merge/*` labels - The PR does not have the `needs-ok-to-test` label - The PR is mergeable (does not have a `needs-rebase` label) - The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels) - The PR is failing tests required for merge You can: - Review the [full test history]…",,,,,,Anecdotal,comment,,,,,,,,2025-03-22,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2745261983,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
"The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass. This bot retests PRs for certain kubernetes repos according to the following rules: - The PR does have any `do-not-merge/*` labels - The PR does not have the `needs-ok-to-test` label - The PR is mergeable (does not have a `needs-rebase` label) - The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels) - The PR is failing tests required for merge You can: - Review the [full test history]…",,,,,,Anecdotal,comment,,,,,,,,2025-03-22,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2745332473,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
"The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass. This bot retests PRs for certain kubernetes repos according to the following rules: - The PR does have any `do-not-merge/*` labels - The PR does not have the `needs-ok-to-test` label - The PR is mergeable (does not have a `needs-rebase` label) - The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels) - The PR is failing tests required for merge You can: - Review the [full test history]…",,,,,,Anecdotal,comment,,,,,,,,2025-03-22,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2745405285,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
"The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass. This bot retests PRs for certain kubernetes repos according to the following rules: - The PR does have any `do-not-merge/*` labels - The PR does not have the `needs-ok-to-test` label - The PR is mergeable (does not have a `needs-rebase` label) - The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels) - The PR is failing tests required for merge You can: - Review the [full test history]…",,,,,,Anecdotal,comment,,,,,,,,2025-03-22,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2745785301,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
"The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass. This bot retests PRs for certain kubernetes repos according to the following rules: - The PR does have any `do-not-merge/*` labels - The PR does not have the `needs-ok-to-test` label - The PR is mergeable (does not have a `needs-rebase` label) - The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels) - The PR is failing tests required for merge You can: - Review the [full test history]…",,,,,,Anecdotal,comment,,,,,,,,2025-03-23,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2745923639,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
"The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass. This bot retests PRs for certain kubernetes repos according to the following rules: - The PR does have any `do-not-merge/*` labels - The PR does not have the `needs-ok-to-test` label - The PR is mergeable (does not have a `needs-rebase` label) - The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels) - The PR is failing tests required for merge You can: - Review the [full test history]…",,,,,,Anecdotal,comment,,,,,,,,2025-03-23,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2745980582,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
"The Kubernetes project has merge-blocking tests that are currently too flaky to consistently pass. This bot retests PRs for certain kubernetes repos according to the following rules: - The PR does have any `do-not-merge/*` labels - The PR does not have the `needs-ok-to-test` label - The PR is mergeable (does not have a `needs-rebase` label) - The PR is approved (has `cncf-cla: yes`, `lgtm`, `approved` labels) - The PR is failing tests required for merge You can: - Review the [full test history]…",,,,,,Anecdotal,comment,,,,,,,,2025-03-23,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/130806#issuecomment-2746029975,repo: kubernetes/kubernetes | issue: Optimize Statefulset Controller Performance: Reduce Work Duration Time & Minimize Cache Locking. | keyword: gotcha
"Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime ### What would you like to be added? (Disclaimer: It's just an idea now, will confirm the technical feasibility for this after getting some feedback on this idea) **summery**: introduce WASM runtime in kubernetes scheduler. We can allow users to write WASM plugins and load them in the scheduler much more easily without building their custom schedulers. (Probably we just need to pass a path to the wasm fil…",,,,,,Anecdotal,issue,,,,,,,,2022-10-04,github/sanposhiho,https://github.com/kubernetes/kubernetes/issues/112851,repo: kubernetes/kubernetes | keyword: gotcha | state: closed
"@sanposhiho: This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions relat…",,,,,,Anecdotal,comment,,,,,,,,2022-10-04,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-1266491139,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
"I'm an author of [go-plugin](https://github.com/knqyf263/go-plugin). Please let me know if you need any help. Also, I'm sure [wazero](https://github.com/tetratelabs/wazero) maintainers (@codefromthecrypt and @mathetake ) will help.",,,,,,Anecdotal,comment,,,,,,,,2022-10-04,github/knqyf263,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-1266568878,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
"This sounds great, and is one of the exact use cases I have expected to happen with wazero. JFYI - wazero will reach 1.0 release by the end of February 2023 https://github.com/tetratelabs/wazero/issues/506#issuecomment-1162489846 - there are some existing efforts around k8s with Wasm extension based on wazero: - https://github.com/dvob/k8s-wasm cc @dvob - https://github.com/ritazh/wasm-policies cc @ritazh @sozercan",,,,,,Anecdotal,comment,,,,,,,,2022-10-04,github/mathetake,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-1267733154,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
"I appreciate that wazero has no external dependencies, but it is still on the order of 50k LOC. Since we may run untrusted code, I would expect us to carefully audit any runtime we use. This is non-trivial, to say the least. Also, Go's lack of WASI support makes it impossible to use most of the k8s library code because tinygo cannot run it (and using an alternative compiler is not a valid approach in many envs). https://github.com/prep/wasmexec is an interesting workaround for that, but such an…",,,,,,Anecdotal,comment,,,,,,,,2022-10-05,github/enj,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-1267756458,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
"interesting feedback. ps wazero has GOOS=js (wasmexec) support but I also wouldn't recommend it, as the performance is horrendous and also the wasm code in go is borderline hackweek. Seems safe to park this until there's a go source compiler that can work with k8s library code + wasi, regardless of which compiler that is!",,,,,,Anecdotal,comment,,,,,,,,2022-10-05,github/codefromthecrypt,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-1268073075,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
"Definitely agree there is a lot of work to do to both prove the safety of a runtime as well as the (go) tooling for actually building wasm modules. That said I do not think wasm code should be considered ""untrusted"". Not to say that all wasm code should be inherently trusted, but rather that the trust should come from the admin wiring up the wasm bits to the scheduler. That would be behind both a flag to enable wasm as well as explicit wiring to enable a module (even if it's just behind root pe…",,,,,,Anecdotal,comment,,,,,,,,2022-10-05,github/cpuguy83,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-1268642808,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
"since I may have caused confusion, I'll restate a couple things about wasm (and wazero) especially as pertains to syscalls. At the moment, there are only two compilers that can produce wasm golang/go and tinygo-org/tinygo. Both of these can target the ""js"" operating system, and both when doing so will be inefficient due to that. tinygo can alternatively target wasi, without forcing data through a js model. This allows it to perform better. However, tinygo currently doesn't fully support reflect…",,,,,,Anecdotal,comment,,,,,,,,2022-10-06,github/codefromthecrypt,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-1269295202,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
here's another resource that might help with the decision outcome here. @mathetake presents how FFI works in webassembly and specifically wazero also https://speakerdeck.com/mathetake/cgo-less-foreign-function-interface-with-webassembly,,,,,,Anecdotal,comment,,,,,,,,2022-10-08,github/codefromthecrypt,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-1272193457,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
"Thank everyone for much worthwhile information. --- I checked something in https://github.com/sanposhiho/kubernetes/tree/sanposhiho/poc-wasm-scheduler-plugin. It's kinda PoC, implementing a scheduler plugin that loads the `.wasm` file, checking if it works as expected and the performance. - The huge TinyGo's limitation: as @enj said, we cannot depend on the k8s packages when creating a `.wasm` plugin. - The performance: actually, the performance of calling something through WASM runtime isn't g…",,,,,,Anecdotal,comment,,,,,,,,2022-12-23,github/sanposhiho,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-1363635976,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
"I have some feedback since pinged. Waiting for go to support WASI will likely be pausing this feature for years. I would recommend waiting only unless you actively want to defer wasm support. In other words ""let's wait for WASI"" is a keyword of putting things on ice as it is unlikely it will happen. I wouldn't make it a primary requirement to implement this with go-plugin. While the kit looks convenient, it wasn't designed for performance and also has some [gotchas](https://github.com/knqyf263/…",,,,,,Anecdotal,comment,,,,,,,,2022-12-27,github/codefromthecrypt,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-1365542126,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
"yep agreed. this work should come with the thorough ABI design work as I've done in [Proxy-Wasm](https://github.com/proxy-wasm) in Envoy community and in [http-wasm](https://http-wasm.io/) as Adrian mentioned. Then the performance stuff shouldn't be a matter as I don't expect the number of nodes of k8s cluster to exceed the number of requests we handle in Dapr. The point here to make is that regardless of the Wasm runtime you choose, we have to copy the data between ""Wasm world"" and ""Go world"" …",,,,,,Anecdotal,comment,,,,,,,,2022-12-27,github/mathetake,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-1365545720,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
"> you can run for yourself and notice it isn't required for wasm to be slow Thanks. This was actually the biggest misunderstanding for me. If we can improve performance on our side (I mean outside wazero), then there is no blocker on proceeding straight ahead; we can just pursue and pursue and see if the performance after that is acceptable or not. We cannot get started on the project in k/k from the beginning either way; we need to have another place for it to gain some experience. either of s…",,,,,,Anecdotal,comment,,,,,,,,2022-12-27,github/sanposhiho,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-1365583153,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
"@sanposhiho I'm glad you are interested in getting started and gaining experience. In dapr, this is our third iteration and we are readying to do more now. If we never had iteration one and two, we'd not be able to progress mutually between core team and wasm contributors.",,,,,,Anecdotal,comment,,,,,,,,2022-12-28,github/codefromthecrypt,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-1366358250,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
"FYI https://github.com/golang/go/issues/58141 @Pryz, @johanbrandhorst, and other folks are working on pushing the ""real"" WASI target forward as a revived proposal.",,,,,,Anecdotal,comment,,,,,,,,2023-02-15,github/mathetake,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-1431142620,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
The discussion is on-going ↓. mostly about the place where we should put the experimental implementation. https://kubernetes.slack.com/archives/C09TP78DV/p1676610114431299,,,,,,Anecdotal,comment,,,,,,,,2023-02-18,github/sanposhiho,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-1435494243,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
"The repository is created. 🎉 @kerthcet and I will take ownership, but we're, of course, happy to have other contributors as well. https://github.com/kubernetes-sigs/kube-scheduler-wasm-extension",,,,,,Anecdotal,comment,,,,,,,,2023-05-16,github/sanposhiho,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-1548997511,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
"@sanposhiho I think it has been a while since [your POC](https://github.com/sanposhiho/kubernetes/tree/sanposhiho/poc-wasm-scheduler-plugin), so a lot of things have changed, do you think it is worthwhile re-basing that against latest wazero etc so we can start the conversation from there? We now have a lot of tools available, as we get to defining an ABI and measuring it. For example, @achille-roussel and gang have [pprof support for wasm](https://blog.stealthrocket.tech/performance-in-the-spo…",,,,,,Anecdotal,comment,,,,,,,,2023-05-16,github/codefromthecrypt,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-1549697076,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
"apologies I think maybe this thread should be re-created on the [new repo](https://github.com/kubernetes-sigs/kube-scheduler-wasm-extension), my mistake as I thought I was commenting on it 😊",,,,,,Anecdotal,comment,,,,,,,,2023-05-16,github/codefromthecrypt,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-1549699259,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2024-01-20,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-1902110537,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
"@sanposhiho: Closing this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-2846970673): >The project is started as a subproject. We can close for now. > >/close Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/k…",,,,,,Anecdotal,comment,,,,,,,,2025-05-02,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-2846970828,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
[Flaking-Test] [sig-scheduling] test/integration/scheduler/serving.serving ### Which jobs are flaking? sig-release-master-blocking - integration-master ### Which tests are flaking? `k8s.io/kubernetes/test/integration/scheduler/serving.serving` `TestEndpointHandlers` [Triage](https://storage.googleapis.com/k8s-triage/index.html?text=TestEndpointHandlers&test=k8s.io%2Fkubernetes%2Ftest%2Fintegration%2Fscheduler%2Fserving.serving&xjob=e2e-kops) https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/c…,,,,,,Anecdotal,issue,,,,,,,,2025-02-10,github/Rajalakshmi-Girish,https://github.com/kubernetes/kubernetes/issues/130064,repo: kubernetes/kubernetes | keyword: gotcha | state: closed
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-02-10,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/130064#issuecomment-2647408898,repo: kubernetes/kubernetes | issue: [Flaking-Test] [sig-scheduling] test/integration/scheduler/serving.serving | keyword: gotcha
/cc @macsko @dom4ha @kerthcet @yongruilin `TestEndpointHandlers` is added at- https://github.com/kubernetes/kubernetes/pull/128818,,,,,,Anecdotal,comment,,,,,,,,2025-02-10,github/sanposhiho,https://github.com/kubernetes/kubernetes/issues/130064#issuecomment-2647434514,repo: kubernetes/kubernetes | issue: [Flaking-Test] [sig-scheduling] test/integration/scheduler/serving.serving | keyword: gotcha
"I can not personally reproduce this with `/readyz with sched-handler-sync`, but always failed by `/readyz with broken apiserver`. And the `/readyz` was introduced in https://github.com/kubernetes/kubernetes/pull/118148",,,,,,Anecdotal,comment,,,,,,,,2025-02-10,github/kerthcet,https://github.com/kubernetes/kubernetes/issues/130064#issuecomment-2647748149,repo: kubernetes/kubernetes | issue: [Flaking-Test] [sig-scheduling] test/integration/scheduler/serving.serving | keyword: gotcha
"@kerthcet Can you please confirm if this issue blocks the `1.33-alpha-2` cut that would happen on Tuesday, 18th February, UTC?",,,,,,Anecdotal,comment,,,,,,,,2025-02-13,github/Rajalakshmi-Girish,https://github.com/kubernetes/kubernetes/issues/130064#issuecomment-2655717309,repo: kubernetes/kubernetes | issue: [Flaking-Test] [sig-scheduling] test/integration/scheduler/serving.serving | keyword: gotcha
"> [@kerthcet](https://github.com/kerthcet) Can you please confirm if this issue blocks the `1.33-alpha-2` cut that would happen on Tuesday, 18th February, UTC? gotcha.",,,,,,Anecdotal,comment,,,,,,,,2025-02-13,github/kerthcet,https://github.com/kubernetes/kubernetes/issues/130064#issuecomment-2655726578,repo: kubernetes/kubernetes | issue: [Flaking-Test] [sig-scheduling] test/integration/scheduler/serving.serving | keyword: gotcha
"Hi @kerthcet, is it a blocking issue? I'm collecting all statuses because the `1.33.0-alpha-2` cut will happen on Tuesday, 18th February, UTC.",,,,,,Anecdotal,comment,,,,,,,,2025-02-14,github/tico88612,https://github.com/kubernetes/kubernetes/issues/130064#issuecomment-2659621103,repo: kubernetes/kubernetes | issue: [Flaking-Test] [sig-scheduling] test/integration/scheduler/serving.serving | keyword: gotcha
"The /readyz endpoint was introduced in v1.31, so I don't think it's a block issue. And I retried locally again, can't reproduce this. I personally would like to sit around for more time to see whether it will happen again.",,,,,,Anecdotal,comment,,,,,,,,2025-02-17,github/kerthcet,https://github.com/kubernetes/kubernetes/issues/130064#issuecomment-2662177937,repo: kubernetes/kubernetes | issue: [Flaking-Test] [sig-scheduling] test/integration/scheduler/serving.serving | keyword: gotcha
This is still flaking TestEndpointHandlers//readyz failed twice in the last 6 runs of 1.33 release blocking https://testgrid.k8s.io/sig-release-1.33-blocking#integration-1.33&width=20,,,,,,Anecdotal,comment,,,,,,,,2025-04-15,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/130064#issuecomment-2807024768,repo: kubernetes/kubernetes | issue: [Flaking-Test] [sig-scheduling] test/integration/scheduler/serving.serving | keyword: gotcha
"@BenTheElder Can you put the milestone on https://github.com/kubernetes/kubernetes/pull/131288, if you think better to fix it before the release date?",,,,,,Anecdotal,comment,,,,,,,,2025-04-19,github/sanposhiho,https://github.com/kubernetes/kubernetes/issues/130064#issuecomment-2816817558,repo: kubernetes/kubernetes | issue: [Flaking-Test] [sig-scheduling] test/integration/scheduler/serving.serving | keyword: gotcha
Promote TopologyAwareHints feature-gate to GA #### What type of PR is this? /kind feature /kind cleanup <!-- Add one of the following kinds: /kind bug /kind documentation Optionally add one or more of the following kinds if applicable: /kind api-change /kind deprecation /kind failing-test /kind flake /kind regression --> #### What this PR does / why we need it: * Promote TopologyAwareHints feature-gate to GA * Remove usage of feature-gate from kube-proxy packages. Update tests accordingly. This…,,,,,,Anecdotal,issue,,,,,,,,2025-03-12,github/gauravkghildiyal,https://github.com/kubernetes/kubernetes/pull/130742,repo: kubernetes/kubernetes | keyword: gotcha | state: closed
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130742#issuecomment-2716720573,repo: kubernetes/kubernetes | issue: Promote TopologyAwareHints feature-gate to GA | keyword: gotcha
"There are some references to the feature gate elsewhere in the kube-proxy code too. (In particular, just noticed one in `pkg/proxy/endpointslicecache.go`)",,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/danwinship,https://github.com/kubernetes/kubernetes/pull/130742#issuecomment-2718474994,repo: kubernetes/kubernetes | issue: Promote TopologyAwareHints feature-gate to GA | keyword: gotcha
"ah, since you have to rebase anyway, can you fix the last commit message; there needs to be a blank line between the ""summary"" and the ""body"" of the message, or else it treats it all as the summary: ``` danw@t14s:kubernetes (master)> git log --oneline origin/pr/130742 cc1cc769b4d (origin/pr/130742) Remove usage of TopologyAwareHints feature-gate from kube-proxy packages. TopologyAwareHints feature-gate is GA'd and enabled by default since 1.33. Since it is also locked-to-default, we can remove …",,,,,,Anecdotal,comment,,,,,,,,2025-03-14,github/danwinship,https://github.com/kubernetes/kubernetes/pull/130742#issuecomment-2724635999,repo: kubernetes/kubernetes | issue: Promote TopologyAwareHints feature-gate to GA | keyword: gotcha
"> ah, since you have to rebase anyway, can you fix the last commit message; there needs to be a blank line between the ""summary"" and the ""body"" of the message, or else it treats it all as the summary: > > ``` > danw@t14s:kubernetes (master)> git log --oneline origin/pr/130742 > cc1cc769b4d (origin/pr/130742) Remove usage of TopologyAwareHints feature-gate from kube-proxy packages. TopologyAwareHints feature-gate is GA'd and enabled by default since 1.33. Since it is also locked-to-default, we c…",,,,,,Anecdotal,comment,,,,,,,,2025-03-14,github/gauravkghildiyal,https://github.com/kubernetes/kubernetes/pull/130742#issuecomment-2725535678,repo: kubernetes/kubernetes | issue: Promote TopologyAwareHints feature-gate to GA | keyword: gotcha
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/130742#issuecomment-2723005282"" title=""Approved"">danwinship</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/130742#"" title=""Author self-approved"">gauravkghildiyal</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/130742#issuecomment-2725834571"" title=""Approved"">thockin</a>* The full list of commands accepted by this bot can be found…",,,,,,Anecdotal,comment,,,,,,,,2025-03-14,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/130742#issuecomment-2725834866,repo: kubernetes/kubernetes | issue: Promote TopologyAwareHints feature-gate to GA | keyword: gotcha
"topology manager presubmit jobs seems to be skipping all the tests The topology manager presubmit jobs seems to be skipping, not running, all the actual tests. [Reproducer PR](https://github.com/kubernetes/kubernetes/pull/125919/commits/2a76171f6dfaa2b9d18b9f20938037c149634089), on which I add a `ginkgo.Fail` in these tests and trigger them with: ``` /test pull-kubernetes-node-kubelet-serial-topology-manager /test pull-kubernetes-node-kubelet-serial-topology-manager-kubetest2 ``` yet the PR is …",,,,,,Anecdotal,issue,,,,,,,,2024-07-08,github/ffromani,https://github.com/kubernetes/kubernetes/issues/125950,repo: kubernetes/kubernetes | keyword: gotcha | state: closed
"@ffromani: The label(s) `area/testing` cannot be applied, because the repository doesn't have them. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/125950#issuecomment-2213375352): >/sig node >/area testing Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](…",,,,,,Anecdotal,comment,,,,,,,,2024-07-08,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/125950#issuecomment-2213375517,repo: kubernetes/kubernetes | issue: topology manager presubmit jobs seems to be skipping all the tests | keyword: gotcha
"I probably have fix for that in https://github.com/kubernetes/kubernetes/pull/125163 where I noticed the same for the DRA presubmit. As it affects other jobs, let's merge that fix separately: https://github.com/kubernetes/kubernetes/pull/125966/files",,,,,,Anecdotal,comment,,,,,,,,2024-07-08,github/pohly,https://github.com/kubernetes/kubernetes/issues/125950#issuecomment-2214991381,repo: kubernetes/kubernetes | issue: topology manager presubmit jobs seems to be skipping all the tests | keyword: gotcha
"@ffromani: Reopened this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/125950#issuecomment-2217399858): >/reopen > >the `-kubetest2` lane seems to be skipping, but #125966 helps a lot, we have a functional job again Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kube…",,,,,,Anecdotal,comment,,,,,,,,2024-07-09,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/125950#issuecomment-2217400004,repo: kubernetes/kubernetes | issue: topology manager presubmit jobs seems to be skipping all the tests | keyword: gotcha
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2024-10-07,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/125950#issuecomment-2396710338,repo: kubernetes/kubernetes | issue: topology manager presubmit jobs seems to be skipping all the tests | keyword: gotcha
"The Kubernetes project currently lacks enough contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle stale` - Close this issue with `/close` -…",,,,,,Anecdotal,comment,,,,,,,,2025-01-08,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/125950#issuecomment-2576875199,repo: kubernetes/kubernetes | issue: topology manager presubmit jobs seems to be skipping all the tests | keyword: gotcha
"The Kubernetes project currently lacks enough active contributors to adequately respond to all issues. This bot triages un-triaged issues according to the following rules: - After 90d of inactivity, `lifecycle/stale` is applied - After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied - After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed You can: - Mark this issue as fresh with `/remove-lifecycle rotten` - Close this issue with `/…",,,,,,Anecdotal,comment,,,,,,,,2025-02-07,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/issues/125950#issuecomment-2642110562,repo: kubernetes/kubernetes | issue: topology manager presubmit jobs seems to be skipping all the tests | keyword: gotcha
"/remove-priority important-longterm /priority important-soon I was wrong, still an issue even though ""only"" the `-kubetest2` lane is broken ``` [ReportAfterSuite] Kubernetes e2e JUnit report k8s.io/kubernetes/test/e2e/framework/test_context.go:615 [ReportAfterSuite] PASSED [0.004 seconds] ------------------------------ Ran 0 of 740 Specs in 0.012 seconds SUCCESS! -- 0 Passed | 0 Failed | 0 Pending | 740 Skipped ``` I think this is my next top priority [EDIT] see recent example (https://github.c…",,,,,,Anecdotal,comment,,,,,,,,2025-03-19,github/ffromani,https://github.com/kubernetes/kubernetes/issues/125950#issuecomment-2737531773,repo: kubernetes/kubernetes | issue: topology manager presubmit jobs seems to be skipping all the tests | keyword: gotcha
"update: the root cause seems to be ``` Will use ginkgo flags as: -timeout=24h -focus=""\[Feature:TopologyManager\]"" -skip=""\[Flaky\]|\[Slow\]|\[Serial\]"" ``` which we see in the `-kubetest2` lane (example: https://storage.googleapis.com/kubernetes-ci-logs/pr-logs/pull/128728/pull-kubernetes-node-kubelet-serial-topology-manager-kubetest2/1902411340551557120/artifacts/build-log.txt) ![Image](https://github.com/user-attachments/assets/4e53b0e1-8794-4292-a190-f6cecdc08a64) ![Image](https://github.co…",,,,,,Anecdotal,comment,,,,,,,,2025-03-20,github/ffromani,https://github.com/kubernetes/kubernetes/issues/125950#issuecomment-2740885028,repo: kubernetes/kubernetes | issue: topology manager presubmit jobs seems to be skipping all the tests | keyword: gotcha
"turns out the `kubetest2` `node` tester, which we need to use to run the `e2e_node` suite, includes the `Serial` qualifier among the default skips if nothing is given explicitely. Which was exactly the case for the affected lane. See: https://github.com/kubernetes-sigs/kubetest2/blob/master/pkg/testers/node/node.go#L92 https://github.com/kubernetes/test-infra/pull/34563 should fix this.",,,,,,Anecdotal,comment,,,,,,,,2025-03-20,github/ffromani,https://github.com/kubernetes/kubernetes/issues/125950#issuecomment-2741089786,repo: kubernetes/kubernetes | issue: topology manager presubmit jobs seems to be skipping all the tests | keyword: gotcha
"This default kubetest2 skip has bitten various people over time, including @bart0sh and on another occasion myself (when converting to --label-filter). I don't think it should be opinionated like that, it's totally non-obvious when looking at a job definition. @BenTheElder, @aojea: what do you think about removing that default? It is a breaking change, though.",,,,,,Anecdotal,comment,,,,,,,,2025-03-20,github/pohly,https://github.com/kubernetes/kubernetes/issues/125950#issuecomment-2741222756,repo: kubernetes/kubernetes | issue: topology manager presubmit jobs seems to be skipping all the tests | keyword: gotcha
"> turns out the kubetest2 node tester, which we need to use to run the e2e_node suite, includes the Serial qualifier among the default skips if nothing is given explicitely. Which was exactly the case for the affected lane Yes, because by default we run test in parallel? `[Serial]` is supposed to denote a test that cannot be run safely in parallel with other tests. These tests get run in some periodic CI, but running every test serially is too slow for presubmit. > @BenTheElder, @aojea: what do…",,,,,,Anecdotal,comment,,,,,,,,2025-03-20,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/125950#issuecomment-2741237161,repo: kubernetes/kubernetes | issue: topology manager presubmit jobs seems to be skipping all the tests | keyword: gotcha
"And I can imagine that folks who specifically set up a job with the intend to run serial, slow and disruptive tests will be left scratching their heads why those tests don't run.",,,,,,Anecdotal,comment,,,,,,,,2025-03-20,github/pohly,https://github.com/kubernetes/kubernetes/issues/125950#issuecomment-2741430992,repo: kubernetes/kubernetes | issue: topology manager presubmit jobs seems to be skipping all the tests | keyword: gotcha
"> And I can imagine that folks who specifically set up a job with the intend to run serial, slow and disruptive tests will be left scratching their heads why those tests don't run. Yeah that's fair, but I'm concerned about breaking ~everyone else, since this behavior is ... pretty old. EDIT: but again, I'd be more concerned for test/e2e/e2e.test that node_e2e (which has both fewer jobs and users, especially far fewer jobs to check)",,,,,,Anecdotal,comment,,,,,,,,2025-03-20,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/125950#issuecomment-2741453270,repo: kubernetes/kubernetes | issue: topology manager presubmit jobs seems to be skipping all the tests | keyword: gotcha
We want people to migrate to kubetest2 and --label-filter also for e2e jobs. We probably need to warn about this gotcha as part of the communication. I think I had it in my email draft.,,,,,,Anecdotal,comment,,,,,,,,2025-03-21,github/pohly,https://github.com/kubernetes/kubernetes/issues/125950#issuecomment-2742491703,repo: kubernetes/kubernetes | issue: topology manager presubmit jobs seems to be skipping all the tests | keyword: gotcha
"One option would be to disable the skip default when --label-filter is used. --label-filter support in kubetest2 is new, so we wouldn't break existing jobs.",,,,,,Anecdotal,comment,,,,,,,,2025-03-21,github/pohly,https://github.com/kubernetes/kubernetes/issues/125950#issuecomment-2742493052,repo: kubernetes/kubernetes | issue: topology manager presubmit jobs seems to be skipping all the tests | keyword: gotcha
"This was already covered by previous comments, so fully acknowledging the risk of being obvious, but still, mostly for historical record: Q: Why we use `Serial` quite a lot in the `e2e_node` suite, so this bite us? A: there's a bunch of tests in the `e2e_node` suite which are `Serial` because they need to mutate and check the precise state of the node, which is (the node) a big blob of implicit shared state. At minimum we need to precisely control how many pods are running in order to make asse…",,,,,,Anecdotal,comment,,,,,,,,2025-03-21,github/ffromani,https://github.com/kubernetes/kubernetes/issues/125950#issuecomment-2743178019,repo: kubernetes/kubernetes | issue: topology manager presubmit jobs seems to be skipping all the tests | keyword: gotcha
"> Not sure there's a better/simpler way which enables parallel testing, but has not emerged yet. You can simply label your serial test with `WithSerial` and then run the suite with `ginkgo -p`. Ginkgo v2 then automatically runs parallel tests in parallel and the serial tests sequentially: https://pkg.go.dev/k8s.io/kubernetes/test/e2e/framework#WithSerial",,,,,,Anecdotal,comment,,,,,,,,2025-03-21,github/pohly,https://github.com/kubernetes/kubernetes/issues/125950#issuecomment-2743463624,repo: kubernetes/kubernetes | issue: topology manager presubmit jobs seems to be skipping all the tests | keyword: gotcha
"Require email_verified to be used when email is set as username via CEL /kind feature /kind api-change /assign liggitt aramase /triage accepted /milestone v1.30 /sig auth Fixes #123675 ```release-note When configuring a JWT authenticator: If username.expression uses 'claims.email', then 'claims.email_verified' must be used in username.expression or extra[*].valueExpression or claimValidationRules[*].expression. An example claim validation rule expression that matches the validation automaticall…",,,,,,Anecdotal,issue,,,,,,,,2024-03-05,github/enj,https://github.com/kubernetes/kubernetes/pull/123737,repo: kubernetes/kubernetes | keyword: gotcha | state: closed
"This PR [may require API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#what-apis-need-to-be-reviewed). If so, when the changes are ready, [complete the pre-review checklist and request an API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#mechanics). Status of requested reviews is tracked in the [API Review project](https://github.com/orgs/kubernetes/projects/169).",,,,,,Anecdotal,comment,,,,,,,,2024-03-06,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/123737#issuecomment-1979862675,repo: kubernetes/kubernetes | issue: Require email_verified to be used when email is set as username via CEL | keyword: gotcha
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/123737#"" title=""Author self-approved"">enj</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/123737#issuecomment-1986263539"" title=""Approved"">liggitt</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https://git.k8s.io/c…",,,,,,Anecdotal,comment,,,,,,,,2024-03-08,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/123737#issuecomment-1986269823,repo: kubernetes/kubernetes | issue: Require email_verified to be used when email is set as username via CEL | keyword: gotcha
"Try to clarify EndpointSlice semantics some more #### What this PR does / why we need it: Jordan's review of #129837 pointed out some places where I was assuming the EndpointSlice semantics which are agreed upon by the EndpointSlice controller and kube-proxy, but which are not actually required by the API. This is sort of a followup to #106267, trying to clarify the difference between ""everything the EndpointSlice API lets you do"" vs ""how the EndpointSliceController and kube-proxy use the Endpo…",,,,,,Anecdotal,issue,,,,,,,,2025-01-29,github/danwinship,https://github.com/kubernetes/kubernetes/pull/129886,repo: kubernetes/kubernetes | keyword: gotcha | state: closed
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-01-29,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/129886#issuecomment-2621781902,repo: kubernetes/kubernetes | issue: Try to clarify EndpointSlice semantics some more | keyword: gotcha
"This PR [may require API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#what-apis-need-to-be-reviewed). If so, when the changes are ready, [complete the pre-review checklist and request an API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#mechanics). Status of requested reviews is tracked in the [API Review project](https://github.com/orgs/kubernetes/projects/169).",,,,,,Anecdotal,comment,,,,,,,,2025-01-29,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/129886#issuecomment-2621835543,repo: kubernetes/kubernetes | issue: Try to clarify EndpointSlice semantics some more | keyword: gotcha
"The majority of people writing EndpointSlices are writing them for them to be consumed by the service proxy, so they need to know how the service proxy will interpret them. On the flip side, if you are only used to how the EndpointSlice controller generates slices and how kube-proxy processes them, you might miss out on the fact that API validation allows for slices that would be invalid by those rules (e.g., I didn't realize when porting the aggregator code that I had to check for `EndpointPor…",,,,,,Anecdotal,comment,,,,,,,,2025-01-30,github/danwinship,https://github.com/kubernetes/kubernetes/pull/129886#issuecomment-2624517678,repo: kubernetes/kubernetes | issue: Try to clarify EndpointSlice semantics some more | keyword: gotcha
"/remove-sig api-machinery --- off-topic: TODO: the OWNERS files should not be configured to tag api-only changes with this, see: https://www.kubernetes.dev/docs/guide/owners/#filters @Jefftree @jpbetz @fedebongio if we can identify which files I can send a PR, or one of the api-machinery leads could, the config format is very simple and documented above, but we need some confidence which files are and aren't. re-moving the label twice a week in the meeting seems pretty pointless. I would do it …",,,,,,Anecdotal,comment,,,,,,,,2025-01-30,github/BenTheElder,https://github.com/kubernetes/kubernetes/pull/129886#issuecomment-2625610895,repo: kubernetes/kubernetes | issue: Try to clarify EndpointSlice semantics some more | keyword: gotcha
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/129886#"" title=""Author self-approved"">danwinship</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/129886#pullrequestreview-2676438864"" title=""LGTM"">thockin</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https://git.…",,,,,,Anecdotal,comment,,,,,,,,2025-03-11,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/129886#issuecomment-2715976694,repo: kubernetes/kubernetes | issue: Try to clarify EndpointSlice semantics some more | keyword: gotcha
"> This is nice. Were you hoping to get it into 33? Sure; it's not really version-specific at all (though the claim that the EndpointSlice controller writes out IP addresses ""in canonical form"" is not 100% true for older releases. /hold cancel",,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/danwinship,https://github.com/kubernetes/kubernetes/pull/129886#issuecomment-2717826499,repo: kubernetes/kubernetes | issue: Try to clarify EndpointSlice semantics some more | keyword: gotcha
"Add retries to some netlink calls #### What type of PR is this? /kind bug /kind flake #### What this PR does / why we need it: Some netlink calls fail with partial results. These can be retried. #### Which issue(s) this PR fixes: Fixes https://github.com/kubernetes/kubernetes/issues/129562 #### Special notes for your reviewer: #### Does this PR introduce a user-facing change? <!-- If no, just write ""NONE"" in the release-note block below. If yes, a release note is required: Enter your extended r…",,,,,,Anecdotal,issue,,,,,,,,2025-01-20,github/adrianmoisey,https://github.com/kubernetes/kubernetes/pull/129704,repo: kubernetes/kubernetes | keyword: gotcha | state: closed
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-01-20,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/129704#issuecomment-2602160697,repo: kubernetes/kubernetes | issue: Add retries to some netlink calls | keyword: gotcha
"> Please squash your fixup commits. I plan to do this at the end, I'm finding the history quite useful at the moment as I figure this one out",,,,,,Anecdotal,comment,,,,,,,,2025-01-25,github/adrianmoisey,https://github.com/kubernetes/kubernetes/pull/129704#issuecomment-2613839898,repo: kubernetes/kubernetes | issue: Add retries to some netlink calls | keyword: gotcha
"Hey @aojea @danwinship @aroradaman I've made a bunch of changes, I think it's getting close to be actually ready. I have a few open questions that I need help with",,,,,,Anecdotal,comment,,,,,,,,2025-01-26,github/adrianmoisey,https://github.com/kubernetes/kubernetes/pull/129704#issuecomment-2614518543,repo: kubernetes/kubernetes | issue: Add retries to some netlink calls | keyword: gotcha
"I've pushed my latest work. It's been rebased and split up into a commit per addition of retry, along with its test. I've also refactored the tests according to @aroradaman's suggestions. ~~I still need to add a test for runner.List() though~~ EDIT: I've added tests for runner.List()",,,,,,Anecdotal,comment,,,,,,,,2025-02-04,github/adrianmoisey,https://github.com/kubernetes/kubernetes/pull/129704#issuecomment-2632975701,repo: kubernetes/kubernetes | issue: Add retries to some netlink calls | keyword: gotcha
> Also bumping netlink ([f8c7f3a](https://github.com/kubernetes/kubernetes/commit/f8c7f3a6a67f408b49a79db9b7784aac6ddfce4b)) should be the last commit of the PR instead of first. Done!,,,,,,Anecdotal,comment,,,,,,,,2025-02-07,github/adrianmoisey,https://github.com/kubernetes/kubernetes/pull/129704#issuecomment-2643103678,repo: kubernetes/kubernetes | issue: Add retries to some netlink calls | keyword: gotcha
"@adrianmoisey: The following test **failed**, say `/retest` to rerun all failed tests or `/retest-required` to rerun all mandatory failed tests: Test name | Commit | Details | Required | Rerun command --- | --- | --- | --- | --- check-dependency-stats | 6f4166ab62e7ae75f9e51936033bed57d3b30786 | [link](https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/129704/check-dependency-stats/1887871394658652160) | false | `/test check-dependency-stats` [Full PR test history](https://prow.k8s.io/…",,,,,,Anecdotal,comment,,,,,,,,2025-02-08,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/129704#issuecomment-2645772445,repo: kubernetes/kubernetes | issue: Add retries to some netlink calls | keyword: gotcha
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/129704#"" title=""Author self-approved"">adrianmoisey</a>* **Once this PR has been reviewed and has the lgtm label**, please ask for approval from [aojea](https://github.com/aojea). For more information see [the Code Review Process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process). The full list of commands accepted by this…",,,,,,Anecdotal,comment,,,,,,,,2025-02-17,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/129704#issuecomment-2663648503,repo: kubernetes/kubernetes | issue: Add retries to some netlink calls | keyword: gotcha
"@adrianmoisey: Closed this PR. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/129704#issuecomment-2670723317): >/close > >Handled in https://github.com/kubernetes/kubernetes/pull/130256 Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kub…",,,,,,Anecdotal,comment,,,,,,,,2025-02-20,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/129704#issuecomment-2670723501,repo: kubernetes/kubernetes | issue: Add retries to some netlink calls | keyword: gotcha
"[DRA] DeviceAllocationModeExactCount mode should also take the pool.inComplete field into account ### What happened? I have a question about the `pool.inComplete` field: Why is it necessary to check if `pool.inComplete` is `true` only for the `DeviceAllocationModeAll` allocation mode, but not for `DeviceAllocationModeExactCount`? For example, if the pool is in the middle of an update such as adjusting resource capacity. Shouldn’t allocation operations (whether using DeviceAllocationModeAll or D…",,,,,,Anecdotal,issue,,,,,,,,2025-02-08,github/cccccc0226,https://github.com/kubernetes/kubernetes/issues/130043,repo: kubernetes/kubernetes | keyword: gotcha | state: closed
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-02-08,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/130043#issuecomment-2644417587,repo: kubernetes/kubernetes | issue: [DRA] DeviceAllocationModeExactCount mode should also take the pool.inComplete field into account | keyword: gotcha
"IIUC, I think this is unnecessary. In `DeviceAllocationModeAll` mode, all pool devices are allocated, so this is considered an early return. If it is `DeviceAllocationModeExactCount`, we will make a more detailed selection in `alloc.allocateOne(deviceIndices{})`. 🤔",,,,,,Anecdotal,comment,,,,,,,,2025-02-08,github/googs1025,https://github.com/kubernetes/kubernetes/issues/130043#issuecomment-2644490570,repo: kubernetes/kubernetes | issue: [DRA] DeviceAllocationModeExactCount mode should also take the pool.inComplete field into account | keyword: gotcha
"But after a preliminary look at the code (without going into it in depth), I'm a little curious about a few points: Do we need to return an error when `if pool.IsInvalid ` ? Or should we just continue ? https://github.com/kubernetes/kubernetes/blob/69ab91a5c59617872c9f48737c64409a9dec2957/staging/src/k8s.io/dynamic-resource-allocation/structured/allocator.go#L618-L624 In addition, it seems that we can continue here to ensure that the completed pool avoids entering the loop. 🤔 @pohly https://git…",,,,,,Anecdotal,comment,,,,,,,,2025-02-08,github/googs1025,https://github.com/kubernetes/kubernetes/issues/130043#issuecomment-2644496245,repo: kubernetes/kubernetes | issue: [DRA] DeviceAllocationModeExactCount mode should also take the pool.inComplete field into account | keyword: gotcha
"> But after a preliminary look at the code (without going into it in depth), I'm a little curious about a few points: Do we need to return an error when `if pool.IsInvalid ` ? Or should we just continue ? > > [kubernetes/staging/src/k8s.io/dynamic-resource-allocation/structured/allocator.go](https://github.com/kubernetes/kubernetes/blob/69ab91a5c59617872c9f48737c64409a9dec2957/staging/src/k8s.io/dynamic-resource-allocation/structured/allocator.go#L618-L624) > > Lines 618 to 624 in [69ab91a](/ku…",,,,,,Anecdotal,comment,,,,,,,,2025-02-08,github/cccccc0226,https://github.com/kubernetes/kubernetes/issues/130043#issuecomment-2644500989,repo: kubernetes/kubernetes | issue: [DRA] DeviceAllocationModeExactCount mode should also take the pool.inComplete field into account | keyword: gotcha
"> For example, if the pool is in the middle of an update such as adjusting resource capacity. Shouldn’t allocation operations (whether using DeviceAllocationModeAll or DeviceAllocationModeExactCount mode) in scheduler be halted to prevent concurrency issues? What bad outcome do you have in mind? An incomplete pool is exactly that: not complete yet. If there are enough known devices to satisfy the request, what benefit does it have to wait? Note that this is based on not having scoring. As soon …",,,,,,Anecdotal,comment,,,,,,,,2025-02-09,github/pohly,https://github.com/kubernetes/kubernetes/issues/130043#issuecomment-2646547425,repo: kubernetes/kubernetes | issue: [DRA] DeviceAllocationModeExactCount mode should also take the pool.inComplete field into account | keyword: gotcha
The downside of waiting is that it slows down the scheduler. We have to throw an error to abort scheduling of the pod and try again later once we have more information.,,,,,,Anecdotal,comment,,,,,,,,2025-02-09,github/pohly,https://github.com/kubernetes/kubernetes/issues/130043#issuecomment-2646547807,repo: kubernetes/kubernetes | issue: [DRA] DeviceAllocationModeExactCount mode should also take the pool.inComplete field into account | keyword: gotcha
"> > For example, if the pool is in the middle of an update such as adjusting resource capacity. Shouldn’t allocation operations (whether using DeviceAllocationModeAll or DeviceAllocationModeExactCount mode) in scheduler be halted to prevent concurrency issues? > > What bad outcome do you have in mind? > > An incomplete pool is exactly that: not complete yet. If there are enough known devices to satisfy the request, what benefit does it have to wait? > > Note that this is based on not having sco…",,,,,,Anecdotal,comment,,,,,,,,2025-02-10,github/cccccc0226,https://github.com/kubernetes/kubernetes/issues/130043#issuecomment-2646745997,repo: kubernetes/kubernetes | issue: [DRA] DeviceAllocationModeExactCount mode should also take the pool.inComplete field into account | keyword: gotcha
"> An incomplete pool is exactly that: not complete yet. If there are enough known devices to satisfy the request, what benefit does it have to wait? As mentioned here, there is no scoring now. We don't need to filter whether the pool is complete, and even if it is incomplete, it doesn't seem to matter. We just need to make sure that the devices in the pool can be matched correctly. 🤔",,,,,,Anecdotal,comment,,,,,,,,2025-02-10,github/googs1025,https://github.com/kubernetes/kubernetes/issues/130043#issuecomment-2646982609,repo: kubernetes/kubernetes | issue: [DRA] DeviceAllocationModeExactCount mode should also take the pool.inComplete field into account | keyword: gotcha
"> > An incomplete pool is exactly that: not complete yet. If there are enough known devices to satisfy the request, what benefit does it have to wait? > > As mentioned here, there is no scoring now. We don't need to filter whether the pool is complete, and even if it is incomplete, it doesn't seem to matter. We just need to make sure that the devices in the pool can be matched correctly. 🤔 @googs1025 So is the scenario I mentioned above actually a valid concern or not?",,,,,,Anecdotal,comment,,,,,,,,2025-02-10,github/cccccc0226,https://github.com/kubernetes/kubernetes/issues/130043#issuecomment-2646999370,repo: kubernetes/kubernetes | issue: [DRA] DeviceAllocationModeExactCount mode should also take the pool.inComplete field into account | keyword: gotcha
"> If I have a GPU pool containing two resourceSlices, and initially resourceslice.device[*].capacity[""memory""] was mistakenly set to 100GB, but in reality, it is only 80GB, thus requiring modification to these resourceSlices. > > However, during the process of updating resourceslices, if scheduler.Filter method is invoked to allocate devices for exactCount mode + resourceClaim.Spec.devices.requests[*].selector[*]='need 100GB memory', without considering the incomplete pool, it might incorrectly…",,,,,,Anecdotal,comment,,,,,,,,2025-02-10,github/pohly,https://github.com/kubernetes/kubernetes/issues/130043#issuecomment-2647125069,repo: kubernetes/kubernetes | issue: [DRA] DeviceAllocationModeExactCount mode should also take the pool.inComplete field into account | keyword: gotcha
"> > If I have a GPU pool containing two resourceSlices, and initially resourceslice.device[*].capacity[""memory""] was mistakenly set to 100GB, but in reality, it is only 80GB, thus requiring modification to these resourceSlices. > > However, during the process of updating resourceslices, if scheduler.Filter method is invoked to allocate devices for exactCount mode + resourceClaim.Spec.devices.requests[_].selector[_]='need 100GB memory', without considering the incomplete pool, it might incorrect…",,,,,,Anecdotal,comment,,,,,,,,2025-02-10,github/cccccc0226,https://github.com/kubernetes/kubernetes/issues/130043#issuecomment-2647141103,repo: kubernetes/kubernetes | issue: [DRA] DeviceAllocationModeExactCount mode should also take the pool.inComplete field into account | keyword: gotcha
"MutatingAdmissionPolicy mutation ordering issue ### What happened? This works: ``` - patchType: ""JSONPatch"" jsonPatch: expression: > [ JSONPatch{ op: ""add"", path: ""/spec/initContainers"", value: [] }, JSONPatch{ op: ""add"", path: ""/spec/initContainers/-"", value: Object.spec.initContainers{ name: ""mesh-proxy"", image: ""mesh-proxy/v1.0.0"", restartPolicy: ""Always"" } } ] ``` But, this fails for a pod: ``` - patchType: ""JSONPatch"" jsonPatch: expression: > [ JSONPatch{ op: ""add"", path: ""/spec/initContai…",,,,,,Anecdotal,issue,,,,,,,,2024-12-19,github/kfox1111,https://github.com/kubernetes/kubernetes/issues/129309,repo: kubernetes/kubernetes | keyword: gotcha | state: closed
"FYI this issue is related to the serialization of the object to JSON before applying the second mutation. The first mutation will set the value to an empty list, however during serialization of the object for the second mutation the value of `initContainers` will be omitted as it is marked as `omitempty` in the `PodSpec` type. An alternative would be to use an `ApplyConfiguration` with something similar to the following expression ``` Object { spec: Object.spec { template: Object.spec.template …",,,,,,Anecdotal,comment,,,,,,,,2025-01-09,github/knrc,https://github.com/kubernetes/kubernetes/issues/129309#issuecomment-2580757291,repo: kubernetes/kubernetes | issue: MutatingAdmissionPolicy mutation ordering issue | keyword: gotcha
@knrc Thanks for reporting this. Are you interested in proposing a PR to fix or would you prefer someone else resolve this one?,,,,,,Anecdotal,comment,,,,,,,,2025-01-14,github/jpbetz,https://github.com/kubernetes/kubernetes/issues/129309#issuecomment-2590896683,repo: kubernetes/kubernetes | issue: MutatingAdmissionPolicy mutation ordering issue | keyword: gotcha
"Looking at this more closely, it's less clear to me that this is the wrong behavior. The reason the first case works is that we apply the mutations in series without interleaving in any other operations. This allows the `value: []` to be preserved between operations. The reason the second case fails is that we reserialize the object (and apply defaulting, I believe) and so the `value: []` is omitted, causing the second patch operation to fail. This behavior is consistent with how completely ind…",,,,,,Anecdotal,comment,,,,,,,,2025-01-14,github/jpbetz,https://github.com/kubernetes/kubernetes/issues/129309#issuecomment-2591168341,repo: kubernetes/kubernetes | issue: MutatingAdmissionPolicy mutation ordering issue | keyword: gotcha
"@jpbetz I'm not the reporter, that's @kfox1111, but I did take a quick look at the issue last week. I suggested he keep this open so that it could be discussed, at the very least to document this is how it behaves. It's the serialization between the mutations that's causing the issue, and this is likely unexpected.",,,,,,Anecdotal,comment,,,,,,,,2025-01-14,github/knrc,https://github.com/kubernetes/kubernetes/issues/129309#issuecomment-2591170333,repo: kubernetes/kubernetes | issue: MutatingAdmissionPolicy mutation ordering issue | keyword: gotcha
"Yeah. If its working through all the patches in a row, actually allowing changes to flow from one to the next, just that the omit functionality happens to be undoing things in this example, I think that is probably okish, and there is no bug here.",,,,,,Anecdotal,comment,,,,,,,,2025-01-14,github/kfox1111,https://github.com/kubernetes/kubernetes/issues/129309#issuecomment-2591171980,repo: kubernetes/kubernetes | issue: MutatingAdmissionPolicy mutation ordering issue | keyword: gotcha
"/close As discussed above, we would improve the documentation for MAP ordering later: https://github.com/kubernetes/website/issues/49641 Thanks for raising it!",,,,,,Anecdotal,comment,,,,,,,,2025-02-04,github/cici37,https://github.com/kubernetes/kubernetes/issues/129309#issuecomment-2634602007,repo: kubernetes/kubernetes | issue: MutatingAdmissionPolicy mutation ordering issue | keyword: gotcha
"@cici37: Closing this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/129309#issuecomment-2634602007): >/close > >As discussed above, we would improve the documentation for MAP ordering later: https://github.com/kubernetes/website/issues/49641 >Thanks for raising it! Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to m…",,,,,,Anecdotal,comment,,,,,,,,2025-02-04,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/129309#issuecomment-2634602188,repo: kubernetes/kubernetes | issue: MutatingAdmissionPolicy mutation ordering issue | keyword: gotcha
Why do we need to remove the status field when creating a CustomResource ### What happened? **status field content lost in customresource** 1. create crd ```yaml apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: annotations: controller-gen.kubebuilder.io/version: v0.16.4 name: testcrds.foo.test.com spec: group: foo.test.com names: kind: TestCRD listKind: TestCRDList plural: testcrds singular: testcrd scope: Namespaced versions: - name: v1alpha1 schema: openAPIV3Schema…,,,,,,Anecdotal,issue,,,,,,,,2025-01-02,github/Chaunceyctx,https://github.com/kubernetes/kubernetes/issues/129451,repo: kubernetes/kubernetes | keyword: gotcha | state: closed
"@Chaunceyctx: The label(s) `sig/apimachinery` cannot be applied, because the repository doesn't have them. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/129451): >### What happened? > >1. create crd >```yaml >apiVersion: apiextensions.k8s.io/v1 >kind: CustomResourceDefinition >metadata: > annotations: > controller-gen.kubebuilder.io/version: v0.16.4 > name: testcrds.foo.test.com >spec: > group: foo.test.com > names: > kind: TestCRD > listKind: TestCRDList > plu…",,,,,,Anecdotal,comment,,,,,,,,2025-01-02,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/129451#issuecomment-2567475942,repo: kubernetes/kubernetes | issue: Why do we need to remove the status field when creating a CustomResource | keyword: gotcha
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2025-01-02,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/129451#issuecomment-2567476059,repo: kubernetes/kubernetes | issue: Why do we need to remove the status field when creating a CustomResource | keyword: gotcha
/remove-kind bug /kind feature I don’t read this like a bug report at the moment. I’m not familiar with this area though.,,,,,,Anecdotal,comment,,,,,,,,2025-01-02,github/kannon92,https://github.com/kubernetes/kubernetes/issues/129451#issuecomment-2567846125,repo: kubernetes/kubernetes | issue: Why do we need to remove the status field when creating a CustomResource | keyword: gotcha
"> /remove-kind bug /kind feature > > I don’t read this like a bug report at the moment. I’m not familiar with this area though. gotcha. To some extent, I don't consider this like a bug either :) Let's see what other contributors think about this.",,,,,,Anecdotal,comment,,,,,,,,2025-01-03,github/Chaunceyctx,https://github.com/kubernetes/kubernetes/issues/129451#issuecomment-2568632374,repo: kubernetes/kubernetes | issue: Why do we need to remove the status field when creating a CustomResource | keyword: gotcha
"Nothing in the `status` portion of the object is settable via the regular resource endpoint, only the `/status` subresource endpoint. The *intent* of status subresources is that they are only settable by API clients via the `/status` endpoint, which is authorized differently. If you want to default the status stanza if missing, you can specify that directly in the CRD schema: ```diff ... status: + default: {} description: TestCRDStatus defines the observed state of TestCRD. properties: powerSta…",,,,,,Anecdotal,comment,,,,,,,,2025-01-06,github/liggitt,https://github.com/kubernetes/kubernetes/issues/129451#issuecomment-2573447014,repo: kubernetes/kubernetes | issue: Why do we need to remove the status field when creating a CustomResource | keyword: gotcha
"> Nothing in the `status` portion of the object is settable via the regular resource endpoint, only the `/status` subresource endpoint. > > The _intent_ of status subresources is that they are only settable by API clients via the `/status` endpoint, which is authorized differently. > > If you want to default the status stanza if missing, you can specify that directly in the CRD schema: > > ```diff > ... > status: > + default: {} > description: TestCRDStatus defines the observed state of TestCRD…",,,,,,Anecdotal,comment,,,,,,,,2025-01-07,github/Chaunceyctx,https://github.com/kubernetes/kubernetes/issues/129451#issuecomment-2574208280,repo: kubernetes/kubernetes | issue: Why do we need to remove the status field when creating a CustomResource | keyword: gotcha
[FG:InPlacePodVerticalScaling] Introduce /resize subresource to request pod resource resizing #### What type of PR is this? /kind feature /kind api-change #### What this PR does / why we need it: This PR is a continuation to @iholder101's PR (https://github.com/kubernetes/kubernetes/pull/127320) that originally introduced /resize subresource and @mouuii's PR (https://github.com/kubernetes/kubernetes/pull/124887) that added support for pod resize in LimitRanger admission plugin. Introduce /resiz…,,,,,,Anecdotal,issue,,,,,,,,2024-10-22,github/AnishShah,https://github.com/kubernetes/kubernetes/pull/128266,repo: kubernetes/kubernetes | keyword: gotcha | state: closed
"This PR [may require API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#what-apis-need-to-be-reviewed). If so, when the changes are ready, [complete the pre-review checklist and request an API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#mechanics). Status of requested reviews is tracked in the [API Review project](https://github.com/orgs/kubernetes/projects/169).",,,,,,Anecdotal,comment,,,,,,,,2024-10-22,github/k8s-triage-robot,https://github.com/kubernetes/kubernetes/pull/128266#issuecomment-2429979826,repo: kubernetes/kubernetes | issue: [FG:InPlacePodVerticalScaling] Introduce  /resize subresource to request pod resource resizing | keyword: gotcha
First note: the release not could be a bit more precise about what it means for end-users. Do they need to change how they do something?,,,,,,Anecdotal,comment,,,,,,,,2024-10-25,github/thockin,https://github.com/kubernetes/kubernetes/pull/128266#issuecomment-2438635343,repo: kubernetes/kubernetes | issue: [FG:InPlacePodVerticalScaling] Introduce  /resize subresource to request pod resource resizing | keyword: gotcha
> First note: the release not could be a bit more precise about what it means for end-users. Do they need to change how they do something? I updated the release note. They basically need to update their k8s client code to use the new subresource.,,,,,,Anecdotal,comment,,,,,,,,2024-10-25,github/AnishShah,https://github.com/kubernetes/kubernetes/pull/128266#issuecomment-2438700551,repo: kubernetes/kubernetes | issue: [FG:InPlacePodVerticalScaling] Introduce  /resize subresource to request pod resource resizing | keyword: gotcha
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/128266#"" title=""Author self-approved"">AnishShah</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/128266#issuecomment-2455713542"" title=""Approved"">thockin</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https://git.k8…",,,,,,Anecdotal,comment,,,,,,,,2024-11-04,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/128266#issuecomment-2455713742,repo: kubernetes/kubernetes | issue: [FG:InPlacePodVerticalScaling] Introduce  /resize subresource to request pod resource resizing | keyword: gotcha
"/lgtm This is blocking a lot of follow-up work, so I'm going to remove the hold. Please clean up the redundant validation in a follow-up. /hold cancel",,,,,,Anecdotal,comment,,,,,,,,2024-11-05,github/tallclair,https://github.com/kubernetes/kubernetes/pull/128266#issuecomment-2458238885,repo: kubernetes/kubernetes | issue: [FG:InPlacePodVerticalScaling] Introduce  /resize subresource to request pod resource resizing | keyword: gotcha
Retrospective feedback on the changelog; this could have been ```diff -A new /resize subresource was added to request pod resource resizing. Update your k8s client code to utilize the /resize subresource for Pod resizing operations. +Added a new `resize` subresource to Pods; you can use this to request changes to resource requests or limits. ```,,,,,,Anecdotal,comment,,,,,,,,2025-01-12,github/sftim,https://github.com/kubernetes/kubernetes/pull/128266#issuecomment-2585810485,repo: kubernetes/kubernetes | issue: [FG:InPlacePodVerticalScaling] Introduce  /resize subresource to request pod resource resizing | keyword: gotcha
"[kubelet]: fixed container restart due to pod spec field changes <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if …",,,,,,Anecdotal,issue,,,,,,,,2024-04-07,github/HirazawaUi,https://github.com/kubernetes/kubernetes/pull/124220,repo: kubernetes/kubernetes | keyword: gotcha | state: closed
"Please note that we're already in [Test Freeze](https://github.com/kubernetes/sig-release/blob/master/releases/release_phases.md#test-freeze) for the `release-1.30` branch. This means every merged PR will be automatically fast-forwarded via the periodic [ci-fast-forward](https://testgrid.k8s.io/sig-release-releng-blocking#git-repo-kubernetes-fast-forward) job to the release branch of the upcoming v1.30.0 release. Fast forwards are scheduled to happen every 6 hours, whereas the most recent run w…",,,,,,Anecdotal,comment,,,,,,,,2024-04-07,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/124220#issuecomment-2041494890,repo: kubernetes/kubernetes | issue: [kubelet]: fixed container restart due to pod spec field changes | keyword: gotcha
"Does this warrant a gate? Following @wojtek-t discussion on gates, I could MAYBE see this with a starts-beta, on-by-default gate? Or not, I don't feel too strongly about it. It's clearly a bugfix, but non-zero risk.",,,,,,Anecdotal,comment,,,,,,,,2024-04-24,github/thockin,https://github.com/kubernetes/kubernetes/pull/124220#issuecomment-2073685905,repo: kubernetes/kubernetes | issue: [kubelet]: fixed container restart due to pod spec field changes | keyword: gotcha
"> Am I the only one who thinks this might need a gate, so if something unforeseen goes wrong, a user can disable this? yes. We had many discussions when we were changing this for InPlace VPA. This change seems to be safe, but I need to refresh on what was the discussion before. @HirazawaUi did you check what was the discussion when this was changed for inplace VPA?",,,,,,Anecdotal,comment,,,,,,,,2024-04-26,github/SergeyKanzhelev,https://github.com/kubernetes/kubernetes/pull/124220#issuecomment-2080238668,repo: kubernetes/kubernetes | issue: [kubelet]: fixed container restart due to pod spec field changes | keyword: gotcha
"> yes. We had many discussions when we were changing this for InPlace VPA. This change seems to be safe, but I need to refresh on what was the discussion before. @HirazawaUi did you check what was the discussion when this was changed for inplace VPA? After taking a closer look at the comments in https://github.com/kubernetes/enhancements/pull/686 and https://github.com/kubernetes/kubernetes/pull/102884, I see that we didn't cover this area in the comments, but we still had a lot of discussion i…",,,,,,Anecdotal,comment,,,,,,,,2024-04-28,github/HirazawaUi,https://github.com/kubernetes/kubernetes/pull/124220#issuecomment-2081558256,repo: kubernetes/kubernetes | issue: [kubelet]: fixed container restart due to pod spec field changes | keyword: gotcha
"Thanks for adding me to the review. At first glance, this looks right. I vaguely recall arguing in a node meeting for similar design where fields that need an action should be included in the hash, and exclude mutable fields that don't ... but I have to dig into the discussion threads to jog my memory. I do want to explicitly ask the question whether we should grandfather the currently covered fields (excluding Resources, ResizePolicy ofcourse)? (I don't think it is needed given that upgrade re…",,,,,,Anecdotal,comment,,,,,,,,2024-05-05,github/vinaykul,https://github.com/kubernetes/kubernetes/pull/124220#issuecomment-2094990121,repo: kubernetes/kubernetes | issue: [kubelet]: fixed container restart due to pod spec field changes | keyword: gotcha
"I am eager to see this go in, and I think it has ripple effects on other KEPs (which can't set default values for new fields without this). Who owns the final approval on it?",,,,,,Anecdotal,comment,,,,,,,,2024-05-15,github/thockin,https://github.com/kubernetes/kubernetes/pull/124220#issuecomment-2113451925,repo: kubernetes/kubernetes | issue: [kubelet]: fixed container restart due to pod spec field changes | keyword: gotcha
"> /lgtm Thanks for the review @haircommander Imho, the contents in the hash are a contract between kubelet and runtime that help the kubelet identify when it must restart the container. As @liggitt mentioned, this change includes the minimum set of fields in the Container struct that, when changed, indicate that container needs to be restarted. I just wanted to make sure we didn't miss something buried in the container struct somewhere. /lgtm",,,,,,Anecdotal,comment,,,,,,,,2024-05-20,github/vinaykul,https://github.com/kubernetes/kubernetes/pull/124220#issuecomment-2120739055,repo: kubernetes/kubernetes | issue: [kubelet]: fixed container restart due to pod spec field changes | keyword: gotcha
/cc @AkihiroSuda @mrunalp @dchen1107 @SergeyKanzhelev for approval And do we need to discuss adding gates to control risk proposed by @thockin?,,,,,,Anecdotal,comment,,,,,,,,2024-05-20,github/HirazawaUi,https://github.com/kubernetes/kubernetes/pull/124220#issuecomment-2120760610,repo: kubernetes/kubernetes | issue: [kubelet]: fixed container restart due to pod spec field changes | keyword: gotcha
"@HirazawaUi: GitHub didn't allow me to request PR reviews from the following users: for, approve. Note that only [kubernetes members](https://github.com/orgs/kubernetes/people) and repo collaborators can review this PR, and authors cannot review their own PRs. <details> In response to [this](https://github.com/kubernetes/kubernetes/pull/124220#issuecomment-2120760610): >/cc @AkihiroSuda @mrunalp @dchen1107 @SergeyKanzhelev for approve > >And do we need to discuss adding gates to control risk pr…",,,,,,Anecdotal,comment,,,,,,,,2024-05-20,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/124220#issuecomment-2120760812,repo: kubernetes/kubernetes | issue: [kubelet]: fixed container restart due to pod spec field changes | keyword: gotcha
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/124220#"" title=""Author self-approved"">HirazawaUi</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/124220#pullrequestreview-2072271631"" title=""Approved"">mrunalp</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/124220#pullrequestreview-2072360702"" title=""LGTM"">vinaykul</a>* The full list of commands accepted by this bot can be found […",,,,,,Anecdotal,comment,,,,,,,,2024-05-22,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/124220#issuecomment-2125735721,repo: kubernetes/kubernetes | issue: [kubelet]: fixed container restart due to pod spec field changes | keyword: gotcha
DRA: integration with cluster autoscaler The problem that we need to solve for cluster autoscaling when the cluster uses dynamic resource allocation is to simulate how adding new nodes will affect pod scheduling. This matters when some DRA driver depends on resources on such new nodes. One possible solution is outlined in https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/3063-dynamic-resource-allocation#cluster-autoscaler Another idea was to enable loading and calling WASM pl…,,,,,,Anecdotal,issue,,,,,,,,2023-06-12,github/pohly,https://github.com/kubernetes/kubernetes/issues/118612,repo: kubernetes/kubernetes | keyword: gotcha | state: closed
"My full branch with all in-tree changes is https://github.com/kubernetes/kubernetes/compare/master...pohly:kubernetes:dra-cluster-autoscaler The corresponding WIP PR for CA is https://github.com/kubernetes/autoscaler/pull/6163 But this is just for reference. In the Batch WG meeting from 2023-12-07 we agreed to focus on ""numeric parameters"" as a compromise between flexibility and ease-of-use with regards to autoscaling. I'm currently doing some prototyping and will publish a KEP soon.",,,,,,Anecdotal,comment,,,,,,,,2023-12-12,github/pohly,https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-1851803927,repo: kubernetes/kubernetes | issue: DRA: integration with cluster autoscaler | keyword: gotcha
"See https://github.com/kubernetes/enhancements/issues/4381 for the ""structured parameters"" KEP. /assign @towca See https://github.com/kubernetes/kubernetes/pull/123516#discussion_r1506495568 for a discussion how the current code for ""structured parameters"" could fit into Cluster Autoscaler.",,,,,,Anecdotal,comment,,,,,,,,2024-03-11,github/pohly,https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-1987834312,repo: kubernetes/kubernetes | issue: DRA: integration with cluster autoscaler | keyword: gotcha
"Hey, I started looking into the scheduler side of the required changes and could really use some scheduler expertise and discussion here. @kubernetes/sig-scheduling-leads could you take a look when you get a chance? ### Quick summary of the current state: * The [DRA plugin](https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go) currently uses real listers for 6 DRA-related objects. * Some of the real listers are necessary for t…",,,,,,Anecdotal,comment,,,,,,,,2024-04-12,github/towca,https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-2052397181,repo: kubernetes/kubernetes | issue: DRA: integration with cluster autoscaler | keyword: gotcha
"Overall, the proposal SGTM. Of course, the devil lives in the details, and some details we will only know once you start prototyping :) My only immediate question is regarding: > In the DRA plugin keep the real listers for eventHandler logic, warn about using them in the filtering/reservation logic Can you remind me which event handlers? Ideally, there shouldn't be any event handlers in the plugins. > I thought the claims were supposed to be assumed in Reserve, but it seems that it actually hap…",,,,,,Anecdotal,comment,,,,,,,,2024-04-12,github/alculquicondor,https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-2052437912,repo: kubernetes/kubernetes | issue: DRA: integration with cluster autoscaler | keyword: gotcha
"> > I thought the claims were supposed to be assumed in Reserve, but it seems that it actually happens in PreBind. > > @pohly do you recall the reasoning? Storing a locally-modified object in the assume cache (= deep-copy, change some status field, assume) is affected by a race: if that same object gets modified in the API server, the informer receives a more recent copy and the local modification gets dropped. I know that the volume binding plugin does that and pointed it out [here](https://gi…",,,,,,Anecdotal,comment,,,,,,,,2024-04-13,github/pohly,https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-2053570591,repo: kubernetes/kubernetes | issue: DRA: integration with cluster autoscaler | keyword: gotcha
"@towca: have you seen https://github.com/kubernetes/kubernetes/pull/124102 and the follow-up in https://github.com/kubernetes/kubernetes/compare/master...pohly:kubernetes:dra-scheduler-assume-cache-eventhandlers? I think I mentioned those already, but I am no longer sure where :sweat_smile: That changes how the assume cache gets managed. My expectation is that this is a prerequisite of your work.",,,,,,Anecdotal,comment,,,,,,,,2024-04-13,github/pohly,https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-2053571719,repo: kubernetes/kubernetes | issue: DRA: integration with cluster autoscaler | keyword: gotcha
"> Can you remind me which event handlers? Ideally, there shouldn't be any event handlers in the plugins. @alculquicondor The event handlers registered by `EnqueueExtensions.EventsToRegister`: https://github.com/kubernetes/kubernetes/blob/cae35dba5a3060711a2a3f958537003bc74a59c0/pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go#L381C29-L381C45. This seems like a valid thing to do in a plugin, at least judging by the framework docs. > @towca: have you seen #124102 and the follo…",,,,,,Anecdotal,comment,,,,,,,,2024-04-15,github/towca,https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-2057242874,repo: kubernetes/kubernetes | issue: DRA: integration with cluster autoscaler | keyword: gotcha
"> The event handlers registered by `EnqueueExtensions.EventsToRegister` Ah gotcha. But that code doesn't depend on the listers. CA doesn't use this function, so it shouldn't be a problem.",,,,,,Anecdotal,comment,,,,,,,,2024-04-15,github/alculquicondor,https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-2057305312,repo: kubernetes/kubernetes | issue: DRA: integration with cluster autoscaler | keyword: gotcha
"> If I understand the race correctly, I think my proposal would still take care of it, while also making integration with CA straightforward. WDYT? We can wrap the assume cache API as you suggested. But we still need to move the assume cache implementation (https://github.com/kubernetes/kubernetes/pull/124102) and then make it support driving cluster events (pkg/scheduler/eventhandlers.go in https://github.com/kubernetes/kubernetes/compare/master...pohly:kubernetes:dra-scheduler-assume-cache-ev…",,,,,,Anecdotal,comment,,,,,,,,2024-04-15,github/pohly,https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-2057437925,repo: kubernetes/kubernetes | issue: DRA: integration with cluster autoscaler | keyword: gotcha
"> > The event handlers registered by `EnqueueExtensions.EventsToRegister` > > Ah gotcha. But that code doesn't depend on the listers. CA doesn't use this function, so it shouldn't be a problem. @alculquicondor IIUC the event handlers are only used for moving the pod between scheduler queues, so definitely shouldn't impact CA. However, they do actually use listers, see e.g. [here](https://github.com/kubernetes/kubernetes/blob/cae35dba5a3060711a2a3f958537003bc74a59c0/pkg/scheduler/framework/plugi…",,,,,,Anecdotal,comment,,,,,,,,2024-04-16,github/towca,https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-2059445909,repo: kubernetes/kubernetes | issue: DRA: integration with cluster autoscaler | keyword: gotcha
"> Do I understand correctly that after your changes are merged, the claim Assume call can be moved to Reserve? No, it has to stay in PreBind. PreBind is where the plugin gets the updated ResourceClaim back from the API server and that object then gets stored in the assume cache.",,,,,,Anecdotal,comment,,,,,,,,2024-04-16,github/pohly,https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-2059517231,repo: kubernetes/kubernetes | issue: DRA: integration with cluster autoscaler | keyword: gotcha
"> So it seems that the plugin will need a lister for this regardless of the changes I proposed, right? I see. Yes, I think so. > But you're right that to solve the race we'd then also need to add event handling (event being either Add or Assume) functionality to the cache, which probably doesn't make sense. The cache is already updated by event handlers, so I don't see why this wouldn't be fine. > Instead of updating the NodeInfo/PodInfo snapshot (at the beginning of a new scheduling cycle) fro…",,,,,,Anecdotal,comment,,,,,,,,2024-04-16,github/alculquicondor,https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-2059578544,repo: kubernetes/kubernetes | issue: DRA: integration with cluster autoscaler | keyword: gotcha
"> > But you're right that to solve the race we'd then also need to add event handling (event being either Add or Assume) functionality to the cache, which probably doesn't make sense. > > The cache is already updated by event handlers, so I don't see why this wouldn't be fine. @alculquicondor IIUC the cache is updated from the informers by event handlers, or by the `schedule_one.go` logic calling `Assume()` without going through the API/informers. My understanding was that in order to solve the…",,,,,,Anecdotal,comment,,,,,,,,2024-04-17,github/towca,https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-2061801397,repo: kubernetes/kubernetes | issue: DRA: integration with cluster autoscaler | keyword: gotcha
"> > No, it has to stay in PreBind. PreBind is where the plugin gets the updated ResourceClaim back from the API server and that object then gets stored in the assume cache. > > Is it not enough that the plugin's claim event handler gets the Assume notification? Both the event handlers in `pkg/scheduler/eventhandlers.go` and in `pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go` must be driven by real changes in the API server. They also should better use the same source. In h…",,,,,,Anecdotal,comment,,,,,,,,2024-04-17,github/pohly,https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-2062028294,repo: kubernetes/kubernetes | issue: DRA: integration with cluster autoscaler | keyword: gotcha
"Suppose Reserve were to use AssumeCache.Assume with a ResourceClaim where the status has been updated to document an allocation *before* actually writing that status to the apiserver. The following then can happen: - pod A causes resource X to be used for claim A, which gets recorded in the assume cache during Reserve - something modifies claim A in the apiserver - the apiserver sends an update of claim A with no allocation - the assume cache compares revisions and drops the locally modified co…",,,,,,Anecdotal,comment,,,,,,,,2024-04-17,github/pohly,https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-2062039161,repo: kubernetes/kubernetes | issue: DRA: integration with cluster autoscaler | keyword: gotcha
"@pohly Thanks for the detailed answers! Sorry for more questions, but I'm still not clear on many things: 1. Does the race you just described have anything to do with making the AssumeCache global? Or is that to fix a different race (the one mentioned [here](https://github.com/kubernetes/kubernetes/blob/cae35dba5a3060711a2a3f958537003bc74a59c0/pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go#L601))? I think we might've been talking about 2 different things. 2. If we have inF…",,,,,,Anecdotal,comment,,,,,,,,2024-04-19,github/towca,https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-2066885513,repo: kubernetes/kubernetes | issue: DRA: integration with cluster autoscaler | keyword: gotcha
"The race in https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-2062039161 does not affect the dynamicresource plugin because I am avoiding the root cause (storing a locally modified copy of the object in the assume cache). I believe the volumebinding plugin is affected, but perhaps it doesn't matter there. In the dynamicresource plugin, I am handling the local modification with the ""in-flight claim"" map instead of the assume cache. The other race which does affect the dynamicre…",,,,,,Anecdotal,comment,,,,,,,,2024-04-20,github/pohly,https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-2067619788,repo: kubernetes/kubernetes | issue: DRA: integration with cluster autoscaler | keyword: gotcha
"> ). https://github.com/kubernetes/kubernetes/pull/124102 doesn't fix that (it's just moving code around, without functional changes) but then my [follow-up](https://github.com/kubernetes/kubernetes/compare/master...pohly:kubernetes:dra-scheduler-assume-cache-eventhandlers) fixes it. @pohly Yup, that's what I meant. Ok, so I think I have the full picture now: * There is a race condition affecting `AssumeCache` if a locally-modified object is assumed before doing the API call with the actual cha…",,,,,,Anecdotal,comment,,,,,,,,2024-04-22,github/towca,https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-2069978242,repo: kubernetes/kubernetes | issue: DRA: integration with cluster autoscaler | keyword: gotcha
"`port-forward` API call does not support websockets, SPDY is still default. While the `exec` and `logs` calls have been updated to be able to use websockets, the `port-forward` call is still stuck on SPDY. We should enable port-forward over websockets. PR that made the original switch for exec and logs I believe is: https://github.com/kubernetes/kubernetes/pull/13885 and the work to be done should be in `pkg/kubelet/server/remotecommand` probably using `pk/util/wsstream`",,,,,,Anecdotal,issue,,,,,,,,2016-09-16,github/sebgoa,https://github.com/kubernetes/kubernetes/issues/32880,repo: kubernetes/kubernetes | keyword: gotcha | state: closed
@fraenkel since you fixed the headless service issue so fast :) do you want to have a look at that one ? That would be a big help.,,,,,,Anecdotal,comment,,,,,,,,2016-09-16,github/sebgoa,https://github.com/kubernetes/kubernetes/issues/32880#issuecomment-247656965,"repo: kubernetes/kubernetes | issue: `port-forward` API call does not support websockets, SPDY is still default. | keyword: gotcha"
"@sebgoa I see where we need to switch port-forward to use websockets on the backend. However, It seems port-forward, exec and attach still use SPDY on the front-end. Is this supposed to be fixed in this PR as well?",,,,,,Anecdotal,comment,,,,,,,,2016-09-16,github/fraenkel,https://github.com/kubernetes/kubernetes/issues/32880#issuecomment-247719270,"repo: kubernetes/kubernetes | issue: `port-forward` API call does not support websockets, SPDY is still default. | keyword: gotcha"
"Hi, I think we can do both with exec, no ? kubectl can probably stay as is and we allow for both spy and websockets server side ? @smarterclayton probably knows best.",,,,,,Anecdotal,comment,,,,,,,,2016-09-17,github/sebgoa,https://github.com/kubernetes/kubernetes/issues/32880#issuecomment-247756130,"repo: kubernetes/kubernetes | issue: `port-forward` API call does not support websockets, SPDY is still default. | keyword: gotcha"
"It would be best to fix port forward to support websockets first. I don't think that we need to switch kubectl away from SPDY at this time. Websocket is a little less efficient than our spdy protocol last I checked and head of line blocking may be relevant (hard to say without a lot more testing), so we'd want a really concrete reason.",,,,,,Anecdotal,comment,,,,,,,,2016-09-17,github/smarterclayton,https://github.com/kubernetes/kubernetes/issues/32880#issuecomment-247808613,"repo: kubernetes/kubernetes | issue: `port-forward` API call does not support websockets, SPDY is still default. | keyword: gotcha"
@sebgoa It turns out putting it in remotecommand won't work or do well. It is best to lift code over to the portforward package and re-implement. port forwarding has a different request pattern and uses different streams. I will give it a go and then we can discuss options.,,,,,,Anecdotal,comment,,,,,,,,2016-09-17,github/fraenkel,https://github.com/kubernetes/kubernetes/issues/32880#issuecomment-247811419,"repo: kubernetes/kubernetes | issue: `port-forward` API call does not support websockets, SPDY is still default. | keyword: gotcha"
"@sebgoa Sorry, I am on my 5th try. Port forward using SPDY vs websockets is basically two different implementations. It would have been far better to go from SPDY to HTTP/2 given a much closer model. Once I get a complete websockets implementation with corresponding test cases I can begin to look at what if anything can be merged. It will be very very little.",,,,,,Anecdotal,comment,,,,,,,,2016-09-28,github/fraenkel,https://github.com/kubernetes/kubernetes/issues/32880#issuecomment-250196740,"repo: kubernetes/kubernetes | issue: `port-forward` API call does not support websockets, SPDY is still default. | keyword: gotcha"
"thanks. @ngtuna or @cab105 might be able to help. we work together, but out of cycles to make this one work. Though we need it :( for Cabin, the iOS app for Kubernetes. Maybe you can make a PR and folks like @smarterclayton or @sttts can comment and help out.",,,,,,Anecdotal,comment,,,,,,,,2016-09-28,github/sebgoa,https://github.com/kubernetes/kubernetes/issues/32880#issuecomment-250197561,"repo: kubernetes/kubernetes | issue: `port-forward` API call does not support websockets, SPDY is still default. | keyword: gotcha"
"@sebgoa I just began to write the tests for this, and uncovered the next gotcha that I haven't handled. Since websockets doesn't have streams, I already created a request structure for the PortForward but didn't realize that I must also package the responses to distinguish the streams. This also means clients must know the internal request/response for PortForwarding. Just want to make sure that we are all on the same page before I go add even more code.",,,,,,Anecdotal,comment,,,,,,,,2016-09-30,github/fraenkel,https://github.com/kubernetes/kubernetes/issues/32880#issuecomment-250753665,"repo: kubernetes/kubernetes | issue: `port-forward` API call does not support websockets, SPDY is still default. | keyword: gotcha"
"Definitely port forward, if it has a protocol, has to be versioned like the others. On Fri, Sep 30, 2016 at 10:08 AM, Michael Fraenkel <notifications@github.com > wrote: > > @sebgoa https://github.com/sebgoa I just began to write the tests for > this, and uncovered the next gotcha that I haven't handled. Since > websockets doesn't have streams, I already created a request structure for > the PortForward but didn't realize that I must also package the responses > to distinguish the streams. This…",,,,,,Anecdotal,comment,,,,,,,,2016-09-30,github/smarterclayton,https://github.com/kubernetes/kubernetes/issues/32880#issuecomment-250765816,"repo: kubernetes/kubernetes | issue: `port-forward` API call does not support websockets, SPDY is still default. | keyword: gotcha"
"So I have come up with a new way which should make the response aka forwarding much much easier. The request side is still a mess since I either use a fake stdin to retrieve the port information or it gets encoded as part of the URL. While I could use the HTTP headers to transfer the information, that is not possible if we are looking at a javascript client. Let me see where I end up with these bits of changes and then we can discuss how we want the request portion to work.",,,,,,Anecdotal,comment,,,,,,,,2016-09-30,github/fraenkel,https://github.com/kubernetes/kubernetes/issues/32880#issuecomment-250818899,"repo: kubernetes/kubernetes | issue: `port-forward` API call does not support websockets, SPDY is still default. | keyword: gotcha"
"All of these APIs if they need to return structured data need to have a mechanism for API versioning. For web sockets we use websocket protocols, so you would pick protocol one and go from there (see how exec and others use it). On Fri, Sep 30, 2016 at 2:31 PM, Michael Fraenkel notifications@github.com wrote: > So I have come up with a new way which should make the response aka > forwarding much much easier. The request side is still a mess since I > either use a fake stdin to retrieve the port…",,,,,,Anecdotal,comment,,,,,,,,2016-09-30,github/smarterclayton,https://github.com/kubernetes/kubernetes/issues/32880#issuecomment-250859512,"repo: kubernetes/kubernetes | issue: `port-forward` API call does not support websockets, SPDY is still default. | keyword: gotcha"
I was originally trying to match the multiple port forwards on a connection which is what I ended up giving up on. If I only allow a single port forward then it is doable.,,,,,,Anecdotal,comment,,,,,,,,2016-09-30,github/fraenkel,https://github.com/kubernetes/kubernetes/issues/32880#issuecomment-250861343,"repo: kubernetes/kubernetes | issue: `port-forward` API call does not support websockets, SPDY is still default. | keyword: gotcha"
Got a version I am happy with. Please go ahead and review it. We noticed that the websocket receive error handling is a bit chatty. We always get the following: ``` E1003 14:57:18.559563 32603 conn.go:251] Error on socket receive: read tcp 127.0.0.1:61882->127.0.0.1:61883: use of closed network connection ```,,,,,,Anecdotal,comment,,,,,,,,2016-10-03,github/fraenkel,https://github.com/kubernetes/kubernetes/issues/32880#issuecomment-251194323,"repo: kubernetes/kubernetes | issue: `port-forward` API call does not support websockets, SPDY is still default. | keyword: gotcha"
> I was originally trying to match the multiple port forwards on a connection which is what I ended up giving up on. If I only allow a single port forward then it is doable. Has connection multiplexing ever been implemented since? I know we have port multiplexing but it would be nice if we could use the same websocket for multiple client connections.,,,,,,Anecdotal,comment,,,,,,,,2024-12-16,github/mruoss,https://github.com/kubernetes/kubernetes/issues/32880#issuecomment-2544789690,"repo: kubernetes/kubernetes | issue: `port-forward` API call does not support websockets, SPDY is still default. | keyword: gotcha"
"KEP-3962: MutatingAdmissionPolicy Alpha #### What type of PR is this? /kind feature #### What this PR does / why we need it: Implements alpha of https://github.com/kubernetes/enhancements/issues/3962 This supersedes https://github.com/kubernetes/kubernetes/pull/126497 to include: - [x] API Updated to support JSON Patch and Apply Configuration mutations - [x] Remove message and messageExpression since they have no obvious relevance for mutation - [x] JSON Patch implementation + ""jsonpatch.escape…",,,,,,Anecdotal,issue,,,,,,,,2024-09-05,github/jpbetz,https://github.com/kubernetes/kubernetes/pull/127134,repo: kubernetes/kubernetes | keyword: gotcha | state: closed
"@deads2k I've changed JSON Patch to be declared entirely in CEL, moved reinvocationPolicy up to the spec level and added a `jsonpatch.esacpeKey()` utility function. This is ready for another API review pass.",,,,,,Anecdotal,comment,,,,,,,,2024-09-13,github/jpbetz,https://github.com/kubernetes/kubernetes/pull/127134#issuecomment-2347435194,repo: kubernetes/kubernetes | issue: KEP-3962: MutatingAdmissionPolicy Alpha | keyword: gotcha
"> I went looking to see how we'd done the end to end testing for this, and I didn't see any. If that's because it's not done yet: is it useful to have some for alpha? It's mostly because this feature does not require any nodes or contollers to fully test, so we were able to write the tests against an apiserver started in an integration test. We will need to add e2e tests eventually, primary for conformance, but that's not super urgent, the ""end to end"" coverage we get with the integration tests…",,,,,,Anecdotal,comment,,,,,,,,2024-10-08,github/jpbetz,https://github.com/kubernetes/kubernetes/pull/127134#issuecomment-2400956805,repo: kubernetes/kubernetes | issue: KEP-3962: MutatingAdmissionPolicy Alpha | keyword: gotcha
> 4. Construct a struct as an item inside the map/list is not supported currently? This is supported and tested. See https://github.com/jpbetz/kubernetes/blob/b8c1f3fb7d0c44ae885c9709ba8263c895db334d/staging/src/k8s.io/apiserver/pkg/admission/plugin/policy/mutating/compilation_test.go#L538-L540 > 5. Modify list items/associated list remain unsupported for ApplyConfiguration(might remain unsupported). This is supported. See above mentioned test. The key fields of the list item provide the associ…,,,,,,Anecdotal,comment,,,,,,,,2024-10-25,github/jpbetz,https://github.com/kubernetes/kubernetes/pull/127134#issuecomment-2437941218,repo: kubernetes/kubernetes | issue: KEP-3962: MutatingAdmissionPolicy Alpha | keyword: gotcha
"> > 4. Construct a struct as an item inside the map/list is not supported currently? > > This is supported and tested. See https://github.com/jpbetz/kubernetes/blob/b8c1f3fb7d0c44ae885c9709ba8263c895db334d/staging/src/k8s.io/apiserver/pkg/admission/plugin/policy/mutating/compilation_test.go#L538-L540 > For example, to set `imagePullPolicy` to `always` to all containers in the pod: ``` expression: 'Object{ spec: Object.spec{ containers: [object.spec.containers.map(c, Object.spec.containers{ name…",,,,,,Anecdotal,comment,,,,,,,,2024-10-25,github/cici37,https://github.com/kubernetes/kubernetes/pull/127134#issuecomment-2437976009,repo: kubernetes/kubernetes | issue: KEP-3962: MutatingAdmissionPolicy Alpha | keyword: gotcha
"> returns err: `error applying patch: failed to convert patch object to typed object: .spec.containers: element 0: associative list with keys may not have non-map elements`. Is the expression not in expected form? `containers: [object.spec.containers.map(c, ...)]` attempts to set containers to a list of lists. This should instead be `containers: object.spec.containers.map(c, ...)`. The error received in cases like this will be improved when we do a full type check against actual runtime types, …",,,,,,Anecdotal,comment,,,,,,,,2024-10-25,github/jpbetz,https://github.com/kubernetes/kubernetes/pull/127134#issuecomment-2438006740,repo: kubernetes/kubernetes | issue: KEP-3962: MutatingAdmissionPolicy Alpha | keyword: gotcha
"Thanks! The extra brackets did the trick. It works with ``` expression: 'Object{ spec: Object.spec{ containers: object.spec.containers.map(c, Object.spec.containers{ name: c.name, imagePullPolicy: ""Always"" } )}}' ``` Agree that the current type checking is messy and err message is not helping much.. Will add it into the TODO list before Beta. /lgtm Thank you!",,,,,,Anecdotal,comment,,,,,,,,2024-10-25,github/cici37,https://github.com/kubernetes/kubernetes/pull/127134#issuecomment-2438012761,repo: kubernetes/kubernetes | issue: KEP-3962: MutatingAdmissionPolicy Alpha | keyword: gotcha
Thanks @sttts! I've merged in the last round of feedback and then rebased and squashed all feedback commits into the main commits.,,,,,,Anecdotal,comment,,,,,,,,2024-10-25,github/jpbetz,https://github.com/kubernetes/kubernetes/pull/127134#issuecomment-2438571615,repo: kubernetes/kubernetes | issue: KEP-3962: MutatingAdmissionPolicy Alpha | keyword: gotcha
"/retest Flake was: ``` Kubernetes e2e suite: [It] [sig-node] Lifecycle Sleep Hook [NodeConformance] when create a pod with lifecycle hook using sleep action reduce GracePeriodSeconds during runtime expand_less 39s { failed [FAILED] unexpected delay duration before killing the pod, cost = 20.523691046s In [It] at: k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:610 @ 11/05/24 13:29:02.553 } ```",,,,,,Anecdotal,comment,,,,,,,,2024-11-05,github/jpbetz,https://github.com/kubernetes/kubernetes/pull/127134#issuecomment-2457403335,repo: kubernetes/kubernetes | issue: KEP-3962: MutatingAdmissionPolicy Alpha | keyword: gotcha
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/127134#issuecomment-2457623328"" title=""Approved"">ardaguclu</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/127134#issuecomment-2457480769"" title=""Approved"">deads2k</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/127134#"" title=""Author self-approved"">jpbetz</a>*, *<a href=""https://github.com/kubernetes/kubernetes/pull/127134#issuec…",,,,,,Anecdotal,comment,,,,,,,,2024-11-05,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/127134#issuecomment-2457624079,repo: kubernetes/kubernetes | issue: KEP-3962: MutatingAdmissionPolicy Alpha | keyword: gotcha
"Revert ""kubectl: remove subresource restrictions from all commands"" #### What type of PR is this? /kind regression /sig cli #### What this PR does / why we need it: This reverts commit e1ca63489f2b788f893ab37a27242ce319e1eaf6 (https://github.com/kubernetes/kubernetes/pull/128296), which unexpectadly droped validation for `--subresource` flag, rather than expanding the available list. #### Which issue(s) this PR fixes: None #### Special notes for your reviewer: /assign @ardaguclu @AnishShah ####…",,,,,,Anecdotal,issue,,,,,,,,2024-11-06,github/soltysh,https://github.com/kubernetes/kubernetes/pull/128608,repo: kubernetes/kubernetes | keyword: gotcha | state: closed
"[APPROVALNOTIFIER] This PR is **APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/128608#"" title=""Author self-approved"">soltysh</a>* The full list of commands accepted by this bot can be found [here](https://go.k8s.io/bot-commands?repo=kubernetes%2Fkubernetes). The pull request process is described [here](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process) <details > Needs approval from an approver in each o…",,,,,,Anecdotal,comment,,,,,,,,2024-11-06,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/128608#issuecomment-2459593098,"repo: kubernetes/kubernetes | issue: Revert ""kubectl: remove subresource restrictions from all commands"" | keyword: gotcha"
We discussed the issue in #128296 that old kubectl binaries do not work due to this allowlist. There are no side-effects from removing this allowlist either as the server returns resource NotFound error when the scheme doesn't match,,,,,,Anecdotal,comment,,,,,,,,2024-11-06,github/AnishShah,https://github.com/kubernetes/kubernetes/pull/128608#issuecomment-2460100939,"repo: kubernetes/kubernetes | issue: Revert ""kubectl: remove subresource restrictions from all commands"" | keyword: gotcha"
"> We discussed the issue in #128296 that old kubectl binaries do not work due to this allowlist. > > There are no side-effects from removing this allowlist either as the server returns resource NotFound error when the scheme doesn't match Gotcha, I missed somehow these comments when looking through the PR the first time. This can be closed.",,,,,,Anecdotal,comment,,,,,,,,2024-11-06,github/soltysh,https://github.com/kubernetes/kubernetes/pull/128608#issuecomment-2460185341,"repo: kubernetes/kubernetes | issue: Revert ""kubectl: remove subresource restrictions from all commands"" | keyword: gotcha"
"kube-proxy pods continuously CrashLoopBackOff ### What happened? I'm very new to k8s and having problem with kube-proxy pods. After I join worker node to control plane with command ""kubeadm join ~"" the worker node status has continuously restarting kube-proxy, weave-net, nvidia-device-plugin pods with worker node status ready. `kubectl get pods -n kube-system -o wide` ``` NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-proxy-9v788 1/1 Running 0 3d3h 192.168.136.42 ogr…",,,,,,Anecdotal,issue,,,,,,,,2023-06-05,github/spoiler0,https://github.com/kubernetes/kubernetes/issues/118461,repo: kubernetes/kubernetes | keyword: pro tip | state: closed
"This issue is currently awaiting triage. If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance. The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment. <details> Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md). If you have questions or suggestions related to my beha…",,,,,,Anecdotal,comment,,,,,,,,2023-06-05,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-1576781524,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
> kubectl logs -n kube-system pods/kube-proxy-... that doesn't look like logs from a failed kube proxy pod. there should be a Fxxxx error (fatal). ...unless the container runtime is killing the kube-proxy container. /kind support,,,,,,Anecdotal,comment,,,,,,,,2023-06-05,github/neolit123,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-1577198010,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
you have these unusual logs > Warning FailedCreatePodSandBox 33m kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to create containerd task: failed to create shim task: OCI runtime create failed: unable to retrieve OCI runtime error (open /run/containerd/io.containerd.runtime.v2.task/k8s.io/e03e11b36be09c08482c1c614df85e1aa0565755e17e8d25f3bd6639f87d77fa/log.json: no such file or directory): fork/exec /usr/bin/nvidia-container-runtime: no such file or directory: unk…,,,,,,Anecdotal,comment,,,,,,,,2023-06-05,github/aojea,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-1577695845,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
> Vm with same hostname might be the cause of this issue. Confirm all of your VMs have different hostname @tamilselvan1102 Every machine has different hostname.,,,,,,Anecdotal,comment,,,,,,,,2023-06-07,github/spoiler0,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-1580107363,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
> you have these unusual logs > > > Warning FailedCreatePodSandBox 33m kubelet Failed to create pod sandbox: rpc error: code = Unknown desc = failed to create containerd task: failed to create shim task: OCI runtime create failed: unable to retrieve OCI runtime error (open /run/containerd/io.containerd.runtime.v2.task/k8s.io/e03e11b36be09c08482c1c614df85e1aa0565755e17e8d25f3bd6639f87d77fa/log.json: no such file or directory): fork/exec /usr/bin/nvidia-container-runtime: no such file or director…,,,,,,Anecdotal,comment,,,,,,,,2023-06-07,github/spoiler0,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-1580112285,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
> I guess this might also solve your problem ` sudo apt install nvidia-container-runtime` /close Thanks for reporting but this seems related to the specific environment and not to a bug in the kubernetes code,,,,,,Anecdotal,comment,,,,,,,,2023-06-22,github/aojea,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-1602758237,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
@aojea: Closing this issue. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-1602758237): >> I guess this might also solve your problem ` sudo apt install nvidia-container-runtime` > >/close > >Thanks for reporting but this seems related to the specific environment and not to a bug in the kubernetes code Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).…,,,,,,Anecdotal,comment,,,,,,,,2023-06-22,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-1602758516,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
What's the solution for this issue ? I meet the same issue when join the worker to kubernetes. the Calico node and kube-proxy keep crashing. ![image](https://github.com/kubernetes/kubernetes/assets/130526722/ec55e4cc-2ab0-46ea-8737-2f5272bfb8fc),,,,,,Anecdotal,comment,,,,,,,,2023-09-05,github/xiaomixin,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-1706040271,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
"Same problem here, I have a k8s created with vagrant, kubeadm, with gpu being passthrough inside and other vms without. The one with gpu, node-1, is having a bad time with flannel, kube-proxy and metallb pods - they keep restarting however there're no sensible errors other than ""exit code 2"" This problem appears on k8s v1.26 however it was absent on v1.23 <img width=""1318"" alt=""Screenshot 2023-12-10 at 19 00 58"" src=""https://github.com/kubernetes/kubernetes/assets/11798853/f5e9816b-57ae-4363-bb…",,,,,,Anecdotal,comment,,,,,,,,2023-12-10,github/mikefrostov,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-1849008720,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
"**TLDR @1qzxc Still present on version 1.27 .... I tried a lot of things. The only thing that worked was to revert back to the version you suggested. For future reference, here's everything I tried.** I managed to install kubernetes following the steps documented in [_readme/vps-kubernetes.md](). I was attempting to get nginx and certbot started in k8s. However there was a lingering issue with kube-proxy restarting. I tried checking the events log and with the help of GPT and StackOverflow foun…",,,,,,Anecdotal,comment,,,,,,,,2024-03-02,github/adrian-moisa,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-1974924360,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
I am having the exact same issue. Which repos did you use for install this? kubelet=1.23.17 kubeadm=1.23.17 kubectl=1.23.17,,,,,,Anecdotal,comment,,,,,,,,2024-03-05,github/bsdero,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-1979409905,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
"> I am having the exact same issue. Which repos did you use for install this? kubelet=1.23.17 kubeadm=1.23.17 kubectl=1.23.17 I had this Nameserver limits exceeded with more than three DNS servers when I was using ubuntu 22.04, I used instead Ubuntu 18.04.6 LTS and all worked fine. However after installing containerd, when you need to set systemd cgroup driver, you need to create the directory and file as I found it didn't exist /etc/containerd/config.toml and set it to the content shared on [h…",,,,,,Anecdotal,comment,,,,,,,,2024-03-22,github/Omar-Bensalah,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-2016090391,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
"I had a similar problem, coredns and flannel were constantly restarting on the node where the kubeadm join was performed on the node worker with kubernetes v1.29. The problem disappeared after I copied the /etc/containerd/config.toml file from the master node deployed on ubuntu 22.04 and kubernetes version 1.28 to the worker node and restarted containerd. After this, coredns and flannel reboots stopped. Later, I tested another case - connecting a worker node (ubuntu 22.04 kubernetes 1.29) to a …",,,,,,Anecdotal,comment,,,,,,,,2024-03-24,github/lulkek,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-2016663513,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
"I also fix my issues, by using crictl commands. One of the reasons I got this messages, was because old, dead pods from other versions were there, running in a zombie state, or the like. To manually remove everything ( pods, pod images) by using crictl, helped a lot to keep the systems clean, before attempt a reinstall.",,,,,,Anecdotal,comment,,,,,,,,2024-04-09,github/bsdero,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-2046154559,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
"> I had a similar problem, coredns and flannel were constantly restarting on the node where the kubeadm join was performed on the node worker with kubernetes v1.29. The problem disappeared after I copied the /etc/containerd/config.toml file from the master node deployed on ubuntu 22.04 and kubernetes version 1.28 to the worker node and restarted containerd. After this, coredns and flannel reboots stopped. Later, I tested another case - connecting a worker node (ubuntu 22.04 kubernetes 1.29) to …",,,,,,Anecdotal,comment,,,,,,,,2024-04-14,github/yctsoi0,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-2053837187,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
"Hi everyone, I've had a similar problem with my configuration. I'm working with a React JS app that I built and then shared in the nginx html folder. I created a `yaml` file with a reverse proxy because I need to create a proxy pass to a backend. In this file, I've: ```json location /api { proxy_pass https://domain-backend.com:8443/directory/services; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_b…",,,,,,Anecdotal,comment,,,,,,,,2024-06-26,github/tomasmalio,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-2191029979,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
@tomasmalio this does not looks related and it seems you can better use some ingress or gateway project to solve that problem,,,,,,Anecdotal,comment,,,,,,,,2024-06-26,github/aojea,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-2191905372,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
"@aojea, the problem is that I've tried using different configurations in the ingress but always the pod crashed because sometimes the server which contains the endpoints died, so the DNS hosts doesn't answer the request.",,,,,,Anecdotal,comment,,,,,,,,2024-06-26,github/tomasmalio,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-2191939868,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
"whatever it is , the original issue is not related and this is becoming a magnet for unrelated things, if the pod crashes because some nginx configuration is an nginx related problem and you may find better support on other forums, these issues are for kubernetes bugs",,,,,,Anecdotal,comment,,,,,,,,2024-06-26,github/aojea,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-2191982950,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
"> > I had a similar problem, coredns and flannel were constantly restarting on the node where the kubeadm join was performed on the node worker with kubernetes v1.29. The problem disappeared after I copied the /etc/containerd/config.toml file from the master node deployed on ubuntu 22.04 and kubernetes version 1.28 to the worker node and restarted containerd. After this, coredns and flannel reboots stopped. Later, I tested another case - connecting a worker node (ubuntu 22.04 kubernetes 1.29) t…",,,,,,Anecdotal,comment,,,,,,,,2024-11-28,github/wkjun,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-2505603085,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
"i having 2 issues when I'm using minikube. When it was installed, the Kubeproxy and storage provisioned continuously crashing. ![Image](https://github.com/user-attachments/assets/5b04b2af-cd6d-4ce1-a9fa-b90320fb1d44) Then I increase the limit for reading files by ulimit -n 100000 and changed in the file /etc/security/limits.conf then again, it's kept crashing. ![Image](https://github.com/user-attachments/assets/93eaaa41-bbd0-46fa-8f69-04655296eb31) This is the error showing in there? anyone kno…",,,,,,Anecdotal,comment,,,,,,,,2025-03-05,github/jobin-sirpi,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-2700050543,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
"> i having 2 issues when I'm using minikube. When it was installed, the Kubeproxy and storage provisioned continuously crashing.我在使用 minikube 时遇到了两个问题。安装完成后，Kubeproxy 和预配置的存储不断崩溃。 > > ![Image](https://github.com/user-attachments/assets/5b04b2af-cd6d-4ce1-a9fa-b90320fb1d44) > > Then I increase the limit for reading files by ulimit -n 100000 and changed in the file /etc/security/limits.conf然后我通过 ulimit -n 100000 增加了读取文件的限制，并在文件 /etc/security/limits.conf 中进行了更改 then again, it's kept crashing.然后它又不…",,,,,,Anecdotal,comment,,,,,,,,2025-05-28,github/whosefriendA,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-2916115449,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
"> i having 2 issues when I'm using minikube. When it was installed, the Kubeproxy and storage provisioned continuously crashing.我在使用 minikube 时遇到了两个问题。安装完成后，Kubeproxy 和预配置的存储不断崩溃。 > > ![Image](https://github.com/user-attachments/assets/5b04b2af-cd6d-4ce1-a9fa-b90320fb1d44) > > Then I increase the limit for reading files by ulimit -n 100000 and changed in the file /etc/security/limits.conf然后我通过 ulimit -n 100000 增加了读取文件的限制，并在文件 /etc/security/limits.conf 中进行了更改 then again, it's kept crashing.然后它又不…",,,,,,Anecdotal,comment,,,,,,,,2025-05-28,github/whosefriendA,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-2916351942,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
"On-Prem Kubernetes Cluster Setup – My Contribution & Troubleshooting Experience Hey everyone, I recently configured a Kubernetes cluster on-premises in my personal datacenter, and I wanted to share my experience and the challenges I faced — hoping this helps others who are doing the same! Infrastructure Overview 1 Master Server (Running KVM hypervisor) 2 Bare-metal Worker Nodes 1 Dedicated server for Object/NAS Storage (MinIO/NFS) KVM-based Virtual Machines: 3 Master VMs (Master1 - Active, Mast…",,,,,,Anecdotal,comment,,,,,,,,2025-06-07,github/surajsj17,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-2952700740,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
AWS: Friendly ELB Names `a44e18e4c552b11e683bb02fff13e176` is not a very friendly ELB name. The AWS console does not allow for searching/sorting by tags for every resource. We use something like this: ``` [environment]-[app] staging-my-app ``` It would be nice if we could pick a template to use for naming. A combination of cluster name and pod name might work well.,,,,,,Anecdotal,issue,,,,,,,,2016-07-29,github/ProTip,https://github.com/kubernetes/kubernetes/issues/29789,repo: kubernetes/kubernetes | keyword: pro tip | state: closed
"The whole env name might be too long, as the whole name should be <= 20 characters. By default, I use `[app]-[env-short-name]` and env-short-name is a terraform map that is ""stag"" for staging and prod for ""production"". In any case, doing just that doesn't seem enough as we need to guarantee the ELB name is unique. So some randomness or something might be needed",,,,,,Anecdotal,comment,,,,,,,,2016-07-29,github/rata,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-236265248,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
"What about prefixing it with something. Pod, petset, etc. GCE volumes are prefixed and are super easy to find because of the prefixing.",,,,,,Anecdotal,comment,,,,,,,,2016-07-29,github/chrislovecnm,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-236288104,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
"On Fri, Jul 29, 2016 at 01:42:50PM -0700, Chris Love wrote: > What about prefixing it with something. Pod, petset, etc. GCE volumes are prefixed and are super easy to find because of the prefixing. Remember the max is 20 characters for an ELB name",,,,,,Anecdotal,comment,,,,,,,,2016-07-29,github/rata,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-236289377,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
ELB names can be up to 32 characters long: - [Docs](http://docs.aws.amazon.com/ElasticLoadBalancing/latest/APIReference/API_CreateLoadBalancer.html) - The 32 char name I posted that was generate by kubernetes,,,,,,Anecdotal,comment,,,,,,,,2016-07-29,github/ProTip,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-236290864,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
:+1: we are also interested in this feature :smile: We are currently implementing our own adapter with custom annotation to automatically assign Route 53 DNS records (Alias) for the ELB.,,,,,,Anecdotal,comment,,,,,,,,2016-10-19,github/hjacobs,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-254919743,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
"We have yet to come up with a strategy that doesn't have a high chance of collisions, given ELB naming limitations and also the potential for hostile tenants. We do tag the ELB though with the name of the service though, in the tag kubernetes.io/service-name.",,,,,,Anecdotal,comment,,,,,,,,2017-06-25,github/justinsb,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-310877920,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
@ProTip @justinsb Would something like this be possible using the following template: ``` k8s-{namespace:8}-{name:8}-{uuid} ``` Here is an example of an ELB: ``` k8s-default-myservic-307abb8b71a ```,,,,,,Anecdotal,comment,,,,,,,,2017-07-26,github/lpabon,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-318093511,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
"Issues go stale after 90d of inactivity. Mark the issue as fresh with `/remove-lifecycle stale`. Stale issues rot after an additional 30d of inactivity and eventually close. Prevent issues from auto-closing with an `/lifecycle frozen` comment. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or `@fejta`. /lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2018-01-01,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-354672208,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
"Is this issue fixed? I tried in kubernetes 1.8.4 with these additional annotation from existing ELB: - service.beta.kubernetes.io/aws-load-balancer-name: ""some-name"" - kubernetes.io/aws-load-balancer-name: ""some-name"" - kubernetes.io/loadbalancer-name: ""some-name"" and nothing changed",,,,,,Anecdotal,comment,,,,,,,,2018-02-02,github/akhmadfld,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-362500422,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
Why just not add something like `service.beta.kubernetes.io/aws-load-balancer-name` [to annotations](https://github.com/kubernetes/kubernetes/blob/master/pkg/cloudprovider/providers/aws/aws.go#L99) and allow users to manage this?,,,,,,Anecdotal,comment,,,,,,,,2018-03-08,github/KIVagant,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-371596837,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
"Thumbs up to that, although is there any scenario in which Kubernetes needs to recreate the ELB in order to apply a change to a service? If so, this would then force a delete-before-create scenario whereas before the replacement ELB could be brought up independent of deleting the existing to-be-replaced ELB.",,,,,,Anecdotal,comment,,,,,,,,2018-03-08,github/greg-jaunt,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-371603284,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
I think this is a must have when it comes to managing the cluster. It is quite annoying to have to dig through random string ELBs when attempting to find the one you are looking for. Especially if you have multiple coming from kubernetes.,,,,,,Anecdotal,comment,,,,,,,,2018-04-19,github/BobbyJohansen,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-382779165,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
"For large corporations where there is a Tier 1 devops support team, they need to be able to look at an ELB, EC2 instance, AutoScaling group, etc. and, at a glance, be able to tell what the component belongs to. Currently kubernetes assumes that all human interaction will be done through some kube interface, but if the DevOps/Ops team doesn't have access to this, they are hard pressed to figure out what some UUID named components belong to and who to call. Please keep in mind that the components…",,,,,,Anecdotal,comment,,,,,,,,2018-04-19,github/corby,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-382781297,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
"To be fair, the information IS present in the tags, it's just incredibly tedious to find stuff since AWS doesn't expose the ability to see tag columns on the ELB listing like they do for ec2 instances. (Why?!?)",,,,,,Anecdotal,comment,,,,,,,,2018-04-19,github/nneul,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-382786344,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
"We've had some attempts at solving this before, but the problems are around: * Collisions if users can specify names (though this can be fixed with a random suffix) * What happens when e.g. an annotation is changed, or what happens to existing ELBs if we change the prefix As @nneul pointed out, we do add tags so the information is visible, though the AWS console doesn't surface them as nicely as it could. I'd welcome any attempt at fixing this, but it is non-trivial.",,,,,,Anecdotal,comment,,,,,,,,2018-04-19,github/justinsb,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-382788384,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
"Though, as mentioned, the presentation on AWS's part leaves something to be desired (by default), I think the tags satisfy our needs for cataloging what ELB comes from which cluster. The names _could_ be somewhat more friendly, but is it fair to consider this mostly cosmetic (due to the tags)? Or is there something else we could add to the tags to satisfy the other concerns here?",,,,,,Anecdotal,comment,,,,,,,,2018-04-19,github/gtaylor,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-382797498,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
Would something like k8s-{servicename}-{uuid} not suffice within the given elb character limit? Truncate service name to a limit as well.,,,,,,Anecdotal,comment,,,,,,,,2018-04-19,github/BobbyJohansen,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-382801334,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
"@nneul : Pro Tip for finding your service's ELB in AWS. In the search box on the Load Balancers page in the EC2 console type ""tag:k"", then select ""kubernetes.io/service-name"" from the dropdown, then select your service name from the list.",,,,,,Anecdotal,comment,,,,,,,,2018-04-19,github/greg-jaunt,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-382808453,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
"@BobbyJohansen I believe the ELB name is limited to 32 characters... While certainly you could chop off the uuid, that raises issues about either potential collisions and needing to handle those additional failure modes. Does seem like it would be useful as an option though if set on an annotation for the loadbalancer.",,,,,,Anecdotal,comment,,,,,,,,2018-04-19,github/nneul,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-382823058,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
Why not invert the tags? Make the Name tag human readable and add a kube-id tag to contain the kubernetes UUID? Do that for all aws metadata tags across all objects.,,,,,,Anecdotal,comment,,,,,,,,2018-04-19,github/corby,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-382846162,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
ELBs don't use the 'Name' tag since the name can only be set once during provisioning and can't be changed. That is unlike EC2 instances and other resources where the 'Name' is just an abitrary label.,,,,,,Anecdotal,comment,,,,,,,,2018-04-19,github/nneul,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-382850045,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
"Update on this issue (applies to all cloud provider LBs, not just ELB): we've merged in https://github.com/kubernetes/kubernetes/pull/66589 which gives us a mechanism to customize the LB names per provider, previously this was done by 1 global method. In upcoming releases, we want to start pushing changes to actually give LBs more meaningful names (based on what works best for that provider). The biggest challenge so far is how to handle existing LBs (should they be dynamically renamed? not tou…",,,,,,Anecdotal,comment,,,,,,,,2018-09-05,github/andrewsykim,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-418839061,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
"@andrewsykim > should they be dynamically renamed No, for god's sake. It's a huge pain to fix all Route53 records that point to ELBs and it could cause big production downtime for some projects. > not touched at all Of course!",,,,,,Anecdotal,comment,,,,,,,,2018-10-09,github/KIVagant,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-428125741,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
"Issues go stale after 90d of inactivity. Mark the issue as fresh with `/remove-lifecycle stale`. Stale issues rot after an additional 30d of inactivity and eventually close. If this issue is safe to close now please do so with `/close`. Send feedback to sig-testing, kubernetes/test-infra and/or [fejta](https://github.com/fejta). /lifecycle stale",,,,,,Anecdotal,comment,,,,,,,,2019-01-07,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/29789#issuecomment-451891021,repo: kubernetes/kubernetes | issue: AWS: Friendly ELB Names | keyword: pro tip
"Promote PodDisruptionBudget to GA <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For referenc…",,,,,,Anecdotal,issue,,,,,,,,2019-08-18,github/mortent,https://github.com/kubernetes/kubernetes/pull/81571,repo: kubernetes/kubernetes | keyword: pro tip | state: closed
"This PR [may require API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#what-apis-need-to-be-reviewed). If so, when the changes are ready, [complete the pre-review checklist and request an API review](https://git.k8s.io/community/sig-architecture/api-review-process.md#mechanics). Status of requested reviews is tracked in the [API Review project](https://github.com/orgs/kubernetes/projects/13).",,,,,,Anecdotal,comment,,,,,,,,2019-08-26,github/fejta-bot,https://github.com/kubernetes/kubernetes/pull/81571#issuecomment-525059960,repo: kubernetes/kubernetes | issue: Promote PodDisruptionBudget to GA | keyword: pro tip
the policyv1 group needs to be added to `staging/src/k8s.io/api/roundtrip_test.go` and compatibility fixtures generated (see https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/api/testdata/README.md#current-version or run `hack/update-generated-api-compatibility-data.sh`),,,,,,Anecdotal,comment,,,,,,,,2019-08-28,github/liggitt,https://github.com/kubernetes/kubernetes/pull/81571#issuecomment-525573957,repo: kubernetes/kubernetes | issue: Promote PodDisruptionBudget to GA | keyword: pro tip
"typically, the beta version of an API is marked as deprecated (in godoc and in the release note) once the v1 API is available.",,,,,,Anecdotal,comment,,,,,,,,2019-08-28,github/liggitt,https://github.com/kubernetes/kubernetes/pull/81571#issuecomment-525574902,repo: kubernetes/kubernetes | issue: Promote PodDisruptionBudget to GA | keyword: pro tip
"@liggitt PDBs already have e2e tests in `test/e2e/apps/disruption.go`, but these are obviously using the v1beta1 API. Should I have those over to use the v1 API or will that create problems with the skew tests? Or should I create new e2e tests that use the new v1 API?",,,,,,Anecdotal,comment,,,,,,,,2019-08-28,github/mortent,https://github.com/kubernetes/kubernetes/pull/81571#issuecomment-525854534,repo: kubernetes/kubernetes | issue: Promote PodDisruptionBudget to GA | keyword: pro tip
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/81571#"" title=""Author self-approved"">mortent</a>* To complete the [pull request process](https://git.k8s.io/community/contributors/guide/owners.md#the-code-review-process), please assign **lavalamp** You can assign the PR to them by writing `/assign @lavalamp` in a comment when ready. The full list of commands accepted by this bot can be found [here](htt…",,,,,,Anecdotal,comment,,,,,,,,2019-08-29,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/81571#issuecomment-526382970,repo: kubernetes/kubernetes | issue: Promote PodDisruptionBudget to GA | keyword: pro tip
> @liggitt I have looked at #77383 and it does not affect the effort to move PDBs to GA. you mention providing better feedback in that scenario... does the API provide a mechanism for doing that (e.g. status conditions)?,,,,,,Anecdotal,comment,,,,,,,,2019-10-23,github/liggitt,https://github.com/kubernetes/kubernetes/pull/81571#issuecomment-545421506,repo: kubernetes/kubernetes | issue: Promote PodDisruptionBudget to GA | keyword: pro tip
"Is this considered an optional or required API? If required, this will need e2e tests exercising the v1 APIs being introduced that can be promoted to conformance after soaking. See https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/kubernetes-sig-leads/3lrI2lCzxPk/6hUhIK3HDwAJ for details See https://apisnoop.cncf.io/?bucket=apisnoop%2Fdev%2Fci-kubernetes-e2e-gci-gce%2F1183553991464718336&zoomed=category-beta-policy for current e2e coverage of the beta APIs (though it's un…",,,,,,Anecdotal,comment,,,,,,,,2019-10-23,github/liggitt,https://github.com/kubernetes/kubernetes/pull/81571#issuecomment-545442112,repo: kubernetes/kubernetes | issue: Promote PodDisruptionBudget to GA | keyword: pro tip
"I need to look at ways to improve feedback. Status conditions would be my guess on how we might do it. PDBs currently does not have status conditions, so we need to figure out which conditions make sense for PDBs and try to keep them consistent with the conditions in other resources. Would it be a problem to add this after GA? We already have e2e tests for PDBs, although they are under apps rather than policy: https://github.com/kubernetes/kubernetes/blob/master/test/e2e/apps/disruption.go",,,,,,Anecdotal,comment,,,,,,,,2019-10-23,github/mortent,https://github.com/kubernetes/kubernetes/pull/81571#issuecomment-545659038,repo: kubernetes/kubernetes | issue: Promote PodDisruptionBudget to GA | keyword: pro tip
"> We already have e2e tests for PDBs, although they are under apps rather than policy: https://github.com/kubernetes/kubernetes/blob/master/test/e2e/apps/disruption.go I see 12 API calls for PDBs under test/e2e: ``` $ grep -R .PodDisruptionBudgets test/e2e test/e2e/framework/service/jig.go: newPdb, err := j.Client.PolicyV1beta1().PodDisruptionBudgets(j.Namespace).Create(pdb) test/e2e/framework/service/jig.go: pdb, err := j.Client.PolicyV1beta1().PodDisruptionBudgets(j.Namespace).Get(j.Name, met…",,,,,,Anecdotal,comment,,,,,,,,2019-10-25,github/liggitt,https://github.com/kubernetes/kubernetes/pull/81571#issuecomment-546449107,repo: kubernetes/kubernetes | issue: Promote PodDisruptionBudget to GA | keyword: pro tip
"The tests should be against v1. The fact that the beta APIs are no longer tested affects overall coverage but does not affect conformance coverage since those aren't subject to conformance. We do have the problem though that if we promote the API to GA prior to promoting the tests to conformance, then we see a drop in conformance coverage during the gap. If we have a blocking check for that a la https://github.com/kubernetes/enhancements/pull/1306, we have a problem. The cleanest solution would…",,,,,,Anecdotal,comment,,,,,,,,2019-10-25,github/johnbelamaric,https://github.com/kubernetes/kubernetes/pull/81571#issuecomment-546514199,repo: kubernetes/kubernetes | issue: Promote PodDisruptionBudget to GA | keyword: pro tip
Bug triage for 1.17 here with a gentle reminder that code freeze for this release is on November 14. Is this issue still intended for 1.17?,,,,,,Anecdotal,comment,,,,,,,,2019-10-26,github/markjacksonfishing,https://github.com/kubernetes/kubernetes/pull/81571#issuecomment-546598055,repo: kubernetes/kubernetes | issue: Promote PodDisruptionBudget to GA | keyword: pro tip
@liggitt I had missed some of the e2e-tests that used PDBs. I have updated the PR. @johnbelamaric I will look at creating a PR to promote PDB e2e tests to conformance tests.,,,,,,Anecdotal,comment,,,,,,,,2019-10-29,github/mortent,https://github.com/kubernetes/kubernetes/pull/81571#issuecomment-547510161,repo: kubernetes/kubernetes | issue: Promote PodDisruptionBudget to GA | keyword: pro tip
@liggitt @lavalamp I think this should be ready now. Promotion of e2e-tests to conformance is done in #84740. I would like to look into adding conditions separately from this as it will require a deeper look at the controller to identify the correct conditions. @markyjackson-taulia I hope to get this into 1.17.,,,,,,Anecdotal,comment,,,,,,,,2019-11-05,github/mortent,https://github.com/kubernetes/kubernetes/pull/81571#issuecomment-549912320,repo: kubernetes/kubernetes | issue: Promote PodDisruptionBudget to GA | keyword: pro tip
"Merge pull request #1 from kubernetes/master update from k8s pro <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, read our contributor guidelines https://git.k8s.io/community/contributors/devel/pull-requests.md#the-pr-submit-process and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide 2. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/devel/pull-requests.md#best…",,,,,,Anecdotal,issue,,,,,,,,2017-09-27,github/jsonkey,https://github.com/kubernetes/kubernetes/pull/53131,repo: kubernetes/kubernetes | keyword: pro tip | state: closed
"Hi @jsonkey. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `/ok-to-test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step. I understand the commands that are listed [here](https://github.com/kuberne…",,,,,,Anecdotal,comment,,,,,,,,2017-09-27,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/53131#issuecomment-332497156,repo: kubernetes/kubernetes | issue: Merge pull request #1 from kubernetes/master | keyword: pro tip
"[APPROVALNOTIFIER] This PR is **NOT APPROVED** This pull-request has been approved by: *<a href=""https://github.com/kubernetes/kubernetes/pull/53131#"" title=""Author self-approved"">jsonkey</a>* *No associated issue*. Update pull-request body to add a reference to an issue, or get approval with `/approve no-issue` The full list of commands accepted by this bot can be found [here](https://github.com/kubernetes/test-infra/blob/master/commands.md). <details > Needs approval from an approver in each …",,,,,,Anecdotal,comment,,,,,,,,2017-09-27,github/k8s-github-robot,https://github.com/kubernetes/kubernetes/pull/53131#issuecomment-332497343,repo: kubernetes/kubernetes | issue: Merge pull request #1 from kubernetes/master | keyword: pro tip
"Adding do-not-merge/release-note-label-needed because the release note process has not been followed. One of the following labels is required ""release-note"", ""release-note-action-required"", or ""release-note-none"". Please see: https://github.com/kubernetes/community/blob/master/contributors/devel/pull-requests.md#write-release-notes-if-needed.",,,,,,Anecdotal,comment,,,,,,,,2017-09-27,github/k8s-github-robot,https://github.com/kubernetes/kubernetes/pull/53131#issuecomment-332497344,repo: kubernetes/kubernetes | issue: Merge pull request #1 from kubernetes/master | keyword: pro tip
@jsonkey Looks like you opened a PR against the `kubernetes/kubernetes` repo accidently? I think you meant this one: https://github.com/ipdcode/kubernetes/pull/1.,,,,,,Anecdotal,comment,,,,,,,,2017-09-27,github/nikhita,https://github.com/kubernetes/kubernetes/pull/53131#issuecomment-332611860,repo: kubernetes/kubernetes | issue: Merge pull request #1 from kubernetes/master | keyword: pro tip
"Failure cluster [f37f41...] failed 55 builds, 5 jobs, and 7 tests over 1 days ### Failure cluster [f37f4141e4673a71e190](https://go.k8s.io/triage#f37f4141e4673a71e190) ##### Error text: ``` /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:348 Expected <bool>: false to be true /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:347 ``` ##### Failure cluster statistics: 7 tests failed, 5 jobs failed, 55 builds failed. …",,,,,,Anecdotal,issue,,,,,,,,2017-09-22,github/fejta-bot,https://github.com/kubernetes/kubernetes/issues/52927,repo: kubernetes/kubernetes | keyword: pro tip | state: closed
@fejta-bot: GitHub didn't allow me to assign the following users: rkouj. Note that only [kubernetes members](https://github.com/orgs/kubernetes/people) can be assigned. <details> In response to [this](https://github.com/kubernetes/kubernetes/issues/52927): >### Failure cluster [f37f4141e4673a71e190](https://go.k8s.io/triage#f37f4141e4673a71e190) >##### Error text: >``` >/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl.go:348 >Expected > <bool>: false >to be…,,,,,,Anecdotal,comment,,,,,,,,2017-09-22,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/issues/52927#issuecomment-331576334,"repo: kubernetes/kubernetes | issue: Failure cluster [f37f41...] failed 55 builds, 5 jobs, and 7 tests over 1 days | keyword: pro tip"
"@fejta-bot There are no sig labels on this issue. Please [add a sig label](https://github.com/kubernetes/test-infra/blob/master/commands.md) by: 1. mentioning a sig: `@kubernetes/sig-<group-name>-<group-suffix>` e.g., `@kubernetes/sig-contributor-experience-<group-suffix>` to notify the contributor experience sig, OR 2. specifying the label manually: `/sig <label>` e.g., `/sig scalability` to apply the `sig/scalability` label Note: Method 1 will trigger an email to the group. You can find the g…",,,,,,Anecdotal,comment,,,,,,,,2017-09-22,github/k8s-github-robot,https://github.com/kubernetes/kubernetes/issues/52927#issuecomment-331576457,"repo: kubernetes/kubernetes | issue: Failure cluster [f37f41...] failed 55 builds, 5 jobs, and 7 tests over 1 days | keyword: pro tip"
"ouch - okay seems I was following up an out-dated doc, did not aware of your fix for the previous job - thanks for pointing out!",,,,,,Anecdotal,comment,,,,,,,,2017-09-25,github/krzyzacy,https://github.com/kubernetes/kubernetes/issues/52927#issuecomment-331942277,"repo: kubernetes/kubernetes | issue: Failure cluster [f37f41...] failed 55 builds, 5 jobs, and 7 tests over 1 days | keyword: pro tip"
"np, i saw the log showing the pod ""disappeared"" after about a minute, and i thought ""wait a minute, i've seen this before!"" 😄",,,,,,Anecdotal,comment,,,,,,,,2017-09-25,github/ncdc,https://github.com/kubernetes/kubernetes/issues/52927#issuecomment-331943061,"repo: kubernetes/kubernetes | issue: Failure cluster [f37f41...] failed 55 builds, 5 jobs, and 7 tests over 1 days | keyword: pro tip"
"Merge pull request #1 from kubernetes/master update from k8s pro <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, read our contributor guidelines https://github.com/kubernetes/kubernetes/blob/master/CONTRIBUTING.md and developer guide https://github.com/kubernetes/kubernetes/blob/master/docs/devel/development.md 2. If you want *faster* PR reviews, read how: https://github.com/kubernetes/kubernetes/blob/master/docs/devel/faster_reviews.md 3. Foll…",,,,,,Anecdotal,issue,,,,,,,,2017-03-10,github/jsonkey,https://github.com/kubernetes/kubernetes/pull/42863,repo: kubernetes/kubernetes | keyword: pro tip | state: closed
"Hi @jsonkey. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `@k8s-bot ok to test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step. <details> Instructions for interacting with me using PR comments ar…",,,,,,Anecdotal,comment,,,,,,,,2017-03-10,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/42863#issuecomment-285547546,repo: kubernetes/kubernetes | issue: Merge pull request #1 from kubernetes/master | keyword: pro tip
"<!-- Reviewable:start --> This change is [<img src=""https://reviewable.kubernetes.io/review_button.png"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kubernetes/42863) <!-- Reviewable:end -->",,,,,,Anecdotal,comment,,,,,,,,2017-03-10,github/k8s-reviewable,https://github.com/kubernetes/kubernetes/pull/42863#issuecomment-285547566,repo: kubernetes/kubernetes | issue: Merge pull request #1 from kubernetes/master | keyword: pro tip
"[APPROVALNOTIFIER] This PR is **APPROVED** The following people have approved this PR: *jsonkey* Needs approval from an approver in each of these OWNERS Files: We suggest the following people: cc You can indicate your approval by writing `/approve` in a comment You can cancel your approval by writing `/approve cancel` in a comment <!-- META={""approvers"":[]} -->",,,,,,Anecdotal,comment,,,,,,,,2017-03-10,github/k8s-github-robot,https://github.com/kubernetes/kubernetes/pull/42863#issuecomment-285547730,repo: kubernetes/kubernetes | issue: Merge pull request #1 from kubernetes/master | keyword: pro tip
"Merge pull request #1 from kubernetes/master update from k8s pro <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, read our contributor guidelines https://github.com/kubernetes/kubernetes/blob/master/CONTRIBUTING.md and developer guide https://github.com/kubernetes/kubernetes/blob/master/docs/devel/development.md 2. If you want *faster* PR reviews, read how: https://github.com/kubernetes/kubernetes/blob/master/docs/devel/faster_reviews.md 3. Foll…",,,,,,Anecdotal,issue,,,,,,,,2017-02-22,github/jsonkey,https://github.com/kubernetes/kubernetes/pull/41869,repo: kubernetes/kubernetes | keyword: pro tip | state: closed
"Hi @jsonkey. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `@k8s-bot ok to test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step. <details> Instructions for interacting with me using PR comments ar…",,,,,,Anecdotal,comment,,,,,,,,2017-02-22,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/41869#issuecomment-281579583,repo: kubernetes/kubernetes | issue: Merge pull request #1 from kubernetes/master | keyword: pro tip
"<!-- Reviewable:start --> This change is [<img src=""https://reviewable.kubernetes.io/review_button.png"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kubernetes/41869) <!-- Reviewable:end -->",,,,,,Anecdotal,comment,,,,,,,,2017-02-22,github/k8s-reviewable,https://github.com/kubernetes/kubernetes/pull/41869#issuecomment-281579595,repo: kubernetes/kubernetes | issue: Merge pull request #1 from kubernetes/master | keyword: pro tip
"[APPROVALNOTIFIER] This PR is **APPROVED** The following people have approved this PR: *jsonkey* Needs approval from an approver in each of these OWNERS Files: You can indicate your approval by writing `/approve` in a comment You can cancel your approval by writing `/approve cancel` in a comment <!-- META={""approvers"":[]} -->",,,,,,Anecdotal,comment,,,,,,,,2017-02-22,github/k8s-github-robot,https://github.com/kubernetes/kubernetes/pull/41869#issuecomment-281579632,repo: kubernetes/kubernetes | issue: Merge pull request #1 from kubernetes/master | keyword: pro tip
"Merge pull request #1 from kubernetes/master update from k8s pro <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, read our contributor guidelines https://github.com/kubernetes/kubernetes/blob/master/CONTRIBUTING.md and developer guide https://github.com/kubernetes/kubernetes/blob/master/docs/devel/development.md 2. If you want *faster* PR reviews, read how: https://github.com/kubernetes/kubernetes/blob/master/docs/devel/faster_reviews.md 3. Foll…",,,,,,Anecdotal,issue,,,,,,,,2017-01-19,github/jsonkey,https://github.com/kubernetes/kubernetes/pull/40138,repo: kubernetes/kubernetes | keyword: pro tip | state: closed
"Hi @jsonkey. Thanks for your PR. I'm waiting for a [kubernetes](https://github.com/orgs/kubernetes/people) member to verify that this patch is reasonable to test. If it is, they should reply with `@k8s-bot ok to test` on its own line. Until that is done, I will not automatically test new commits in this PR, but the usual testing commands by org members will still work. Regular contributors should join the org to skip this step. <details> Instructions for interacting with me using PR comments ar…",,,,,,Anecdotal,comment,,,,,,,,2017-01-19,github/k8s-ci-robot,https://github.com/kubernetes/kubernetes/pull/40138#issuecomment-273742419,repo: kubernetes/kubernetes | issue: Merge pull request #1 from kubernetes/master | keyword: pro tip
"<!-- Reviewable:start --> This change is [<img src=""https://reviewable.kubernetes.io/review_button.png"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kubernetes/40138) <!-- Reviewable:end -->",,,,,,Anecdotal,comment,,,,,,,,2017-01-19,github/k8s-reviewable,https://github.com/kubernetes/kubernetes/pull/40138#issuecomment-273742465,repo: kubernetes/kubernetes | issue: Merge pull request #1 from kubernetes/master | keyword: pro tip
[APPROVALNOTIFIER] Needs approval from an approver in each of these OWNERS Files: You can indicate your approval by writing `/approve` in a comment You can cancel your approval by writing `/approve cancel` in a comment,,,,,,Anecdotal,comment,,,,,,,,2017-01-24,github/k8s-github-robot,https://github.com/kubernetes/kubernetes/pull/40138#issuecomment-274687339,repo: kubernetes/kubernetes | issue: Merge pull request #1 from kubernetes/master | keyword: pro tip
"Add GUBERNATOR flag which produces g8r link for node e2e tests When you run 'make tests-e2e-node REMOTE=true GUBERNATOR=true' outputs a URL to view the test results on Gubernator. ~~Should work after my PR for Gubernator is merged.~~ @timstclair <!-- Reviewable:start --> --- This change is [<img src=""https://reviewable.kubernetes.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.kubernetes.io/reviews/kubernetes/kubernetes/30414) <!-- Reviewable:end -->",,,,,,Anecdotal,issue,,,,,,,,2016-08-11,github/mnshaw,https://github.com/kubernetes/kubernetes/pull/30414,repo: kubernetes/kubernetes | keyword: pro tip | state: closed
Still need to: - ~~get user approval for using buckets~~ - ~~make json files first in _artifacts folder before moving into GCS?~~ Known issue: The timestamps seem to be in a different timezone,,,,,,Anecdotal,comment,,,,,,,,2016-08-12,github/mnshaw,https://github.com/kubernetes/kubernetes/pull/30414#issuecomment-239507350,repo: kubernetes/kubernetes | issue: Add GUBERNATOR flag which produces g8r link for node e2e tests | keyword: pro tip
"LGTM, after you fix this: ``` Verifying hack/make-rules/../../hack/verify-gofmt.sh !!! 'gofmt -s -w' needs to be run on the following files: ./test/e2e_node/runner/run_e2e.go FAILED hack/make-rules/../../hack/verify-gofmt.sh 12s ```",,,,,,Anecdotal,comment,,,,,,,,2016-08-15,github/timstclair,https://github.com/kubernetes/kubernetes/pull/30414#issuecomment-239933799,repo: kubernetes/kubernetes | issue: Add GUBERNATOR flag which produces g8r link for node e2e tests | keyword: pro tip
GCE e2e build/test **passed** for commit d69252cbc2f813dde70635ee01c7b78bc835f5d8. - [Test Results](https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/pr-logs/pull/30414/kubernetes-pull-build-test-e2e-gce/53941) - [Build Log](http://pr-test.k8s.io/30414/kubernetes-pull-build-test-e2e-gce/53941/build-log.txt) - [Test Artifacts](https://console.developers.google.com/storage/browser/kubernetes-jenkins/pr-logs/pull/30414/kubernetes-pull-build-test-e2e-gce/53941/artifacts/) - [Internal Jenk…,,,,,,Anecdotal,comment,,,,,,,,2016-08-17,github/k8s-bot,https://github.com/kubernetes/kubernetes/pull/30414#issuecomment-240283896,repo: kubernetes/kubernetes | issue: Add GUBERNATOR flag which produces g8r link for node e2e tests | keyword: pro tip
GCE e2e build/test **passed** for commit d69252cbc2f813dde70635ee01c7b78bc835f5d8. - [Test Results](https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/pr-logs/pull/30414/kubernetes-pull-build-test-e2e-gce/54500) - [Build Log](http://pr-test.k8s.io/30414/kubernetes-pull-build-test-e2e-gce/54500/build-log.txt) - [Test Artifacts](https://console.developers.google.com/storage/browser/kubernetes-jenkins/pr-logs/pull/30414/kubernetes-pull-build-test-e2e-gce/54500/artifacts/) - [Internal Jenk…,,,,,,Anecdotal,comment,,,,,,,,2016-08-18,github/k8s-bot,https://github.com/kubernetes/kubernetes/pull/30414#issuecomment-240865650,repo: kubernetes/kubernetes | issue: Add GUBERNATOR flag which produces g8r link for node e2e tests | keyword: pro tip
"Update HumanResourcePrinter signature w single PrintOptions param release-note-none - Makes [HumanReadablePrinter options field non-exported again](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubectl/resource_printer.go#L346-349) - Adds test-case for HumanReadablePrinter resource printing with aliases. - Better formatting for saving resource ""kind"" aliases [![Analytics](https://kubernetes-site.appspot.com/UA-36037335-10/GitHub/.github/PULL_REQUEST_TEMPLATE.md?pixel)]() <!-- Reviewa…",,,,,,Anecdotal,issue,,,,,,,,2016-07-05,github/juanvallejo,https://github.com/kubernetes/kubernetes/pull/28509,repo: kubernetes/kubernetes | keyword: pro tip | state: closed
"Can one of the admins verify that this patch is reasonable to test? If so, please reply ""ok to test"". (Note: ""add to whitelist"" is no longer supported. Please update configurations in [kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull](https://github.com/kubernetes/test-infra/tree/master/jenkins/job-configs/kubernetes-jenkins-pull) instead.) This message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry. Otherwise, if t…",,,,,,Anecdotal,comment,,,,,,,,2016-07-05,github/k8s-bot,https://github.com/kubernetes/kubernetes/pull/28509#issuecomment-230594429,repo: kubernetes/kubernetes | issue: Update HumanResourcePrinter signature w single PrintOptions param | keyword: pro tip
"Can one of the admins verify that this patch is reasonable to test? If so, please reply ""ok to test"". (Note: ""add to whitelist"" is no longer supported. Please update configurations in [kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull](https://github.com/kubernetes/test-infra/tree/master/jenkins/job-configs/kubernetes-jenkins-pull) instead.) This message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry. Otherwise, if t…",,,,,,Anecdotal,comment,,,,,,,,2016-07-05,github/k8s-bot,https://github.com/kubernetes/kubernetes/pull/28509#issuecomment-230594585,repo: kubernetes/kubernetes | issue: Update HumanResourcePrinter signature w single PrintOptions param | keyword: pro tip
"Can one of the admins verify that this patch is reasonable to test? If so, please reply ""ok to test"". (Note: ""add to whitelist"" is no longer supported. Please update configurations in [kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull](https://github.com/kubernetes/test-infra/tree/master/jenkins/job-configs/kubernetes-jenkins-pull) instead.) This message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry. Otherwise, if t…",,,,,,Anecdotal,comment,,,,,,,,2016-07-05,github/k8s-bot,https://github.com/kubernetes/kubernetes/pull/28509#issuecomment-230594663,repo: kubernetes/kubernetes | issue: Update HumanResourcePrinter signature w single PrintOptions param | keyword: pro tip
"Can one of the admins verify that this patch is reasonable to test? If so, please reply ""ok to test"". (Note: ""add to whitelist"" is no longer supported. Please update configurations in [kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull](https://github.com/kubernetes/test-infra/tree/master/jenkins/job-configs/kubernetes-jenkins-pull) instead.) This message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry. Otherwise, if t…",,,,,,Anecdotal,comment,,,,,,,,2016-07-05,github/k8s-bot,https://github.com/kubernetes/kubernetes/pull/28509#issuecomment-230594762,repo: kubernetes/kubernetes | issue: Update HumanResourcePrinter signature w single PrintOptions param | keyword: pro tip
"Can one of the admins verify that this patch is reasonable to test? If so, please reply ""ok to test"". (Note: ""add to whitelist"" is no longer supported. Please update configurations in [kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull](https://github.com/kubernetes/test-infra/tree/master/jenkins/job-configs/kubernetes-jenkins-pull) instead.) This message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry. Otherwise, if t…",,,,,,Anecdotal,comment,,,,,,,,2016-07-05,github/k8s-bot,https://github.com/kubernetes/kubernetes/pull/28509#issuecomment-230594862,repo: kubernetes/kubernetes | issue: Update HumanResourcePrinter signature w single PrintOptions param | keyword: pro tip
"Can one of the admins verify that this patch is reasonable to test? If so, please reply ""ok to test"". (Note: ""add to whitelist"" is no longer supported. Please update configurations in [kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull](https://github.com/kubernetes/test-infra/tree/master/jenkins/job-configs/kubernetes-jenkins-pull) instead.) This message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry. Otherwise, if t…",,,,,,Anecdotal,comment,,,,,,,,2016-07-07,github/k8s-bot,https://github.com/kubernetes/kubernetes/pull/28509#issuecomment-230994143,repo: kubernetes/kubernetes | issue: Update HumanResourcePrinter signature w single PrintOptions param | keyword: pro tip
Looks like some code verification failed. Could you run these commands locally and fix the failures? ``` console hack/verify-generated-docs.sh hack/verify-govet.sh hack/verify-symbols.sh ```,,,,,,Anecdotal,comment,,,,,,,,2016-07-13,github/janetkuo,https://github.com/kubernetes/kubernetes/pull/28509#issuecomment-232482217,repo: kubernetes/kubernetes | issue: Update HumanResourcePrinter signature w single PrintOptions param | keyword: pro tip
"<img class=""emoji"" title="":lgtm:"" alt="":lgtm:"" align=""absmiddle"" src=""https://reviewable.kubernetes.io/lgtm.png"" height=""20"" width=""61""/> --- Review status: 0 of 4 files reviewed at latest revision, all discussions resolved. --- _Comments from [Reviewable](https://reviewable.kubernetes.io:443/reviews/kubernetes/kubernetes/28509#-:-KOqC6UMOv_Ug51UF2wB:bnfp4nl)_ <!-- Sent from Reviewable.io -->",,,,,,Anecdotal,comment,,,,,,,,2016-08-10,github/janetkuo,https://github.com/kubernetes/kubernetes/pull/28509#issuecomment-238998622,repo: kubernetes/kubernetes | issue: Update HumanResourcePrinter signature w single PrintOptions param | keyword: pro tip
"Reviewed 4 of 4 files at r1. Review status: all files reviewed at latest revision, all discussions resolved. --- _Comments from [Reviewable](https://reviewable.kubernetes.io:443/reviews/kubernetes/kubernetes/28509)_ <!-- Sent from Reviewable.io -->",,,,,,Anecdotal,comment,,,,,,,,2016-08-10,github/janetkuo,https://github.com/kubernetes/kubernetes/pull/28509#issuecomment-238998668,repo: kubernetes/kubernetes | issue: Update HumanResourcePrinter signature w single PrintOptions param | keyword: pro tip
GCE e2e build/test **passed** for commit 620758c7e965b91a6965db1a538457c82a7df5d9. - [Test Results](https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/pr-logs/pull/28509/kubernetes-pull-build-test-e2e-gce/53091) - [Build Log](http://pr-test.k8s.io/28509/kubernetes-pull-build-test-e2e-gce/53091/build-log.txt) - [Test Artifacts](https://console.developers.google.com/storage/browser/kubernetes-jenkins/pr-logs/pull/28509/kubernetes-pull-build-test-e2e-gce/53091/artifacts/) - [Internal Jenk…,,,,,,Anecdotal,comment,,,,,,,,2016-08-11,github/k8s-bot,https://github.com/kubernetes/kubernetes/pull/28509#issuecomment-239313788,repo: kubernetes/kubernetes | issue: Update HumanResourcePrinter signature w single PrintOptions param | keyword: pro tip
GCE e2e build/test **passed** for commit 620758c7e965b91a6965db1a538457c82a7df5d9. - [Test Results](https://k8s-gubernator.appspot.com/build/kubernetes-jenkins/pr-logs/pull/28509/kubernetes-pull-build-test-e2e-gce/53184) - [Build Log](http://pr-test.k8s.io/28509/kubernetes-pull-build-test-e2e-gce/53184/build-log.txt) - [Test Artifacts](https://console.developers.google.com/storage/browser/kubernetes-jenkins/pr-logs/pull/28509/kubernetes-pull-build-test-e2e-gce/53184/artifacts/) - [Internal Jenk…,,,,,,Anecdotal,comment,,,,,,,,2016-08-12,github/k8s-bot,https://github.com/kubernetes/kubernetes/pull/28509#issuecomment-239461143,repo: kubernetes/kubernetes | issue: Update HumanResourcePrinter signature w single PrintOptions param | keyword: pro tip
"Use data-testid instead of aria-label for test selectors Grafana [currently recommends](https://github.com/grafana/grafana/blob/main/contribute/style-guides/e2e.md) using the aria-label attribute as a way to both describe elements in the UI and create selectors for using in unit and E2E tests. As the Grafana team has scaled, this is no longer a good recommendation because it ends up hampering accessibility by using _too many_ label that might not be that useful, or may be out of date. It also m…",,,,,,Anecdotal,issue,,,,,,,,2021-07-07,github/joshhunt,https://github.com/grafana/grafana/issues/36523,repo: grafana/grafana | keyword: best practice | state: open
"### To migrate from aria-label selectors to data-testid Check out this PR for this in practice https://github.com/grafana/grafana/pull/78399. 1. Replace any aria-label props using values from e2e-selectors with data-testid ```diff <section - aria-label={selectors.components.TimeZonePicker.container} + data-testid={selectors.components.TimeZonePicker.container} className={cx(style.timeZoneContainer, style.timeSettingContainer)} > ``` 2. Update the selector value, prefixing it with `data-testid`.…",,,,,,Anecdotal,comment,,,,,,,,2023-11-20,github/joshhunt,https://github.com/grafana/grafana/issues/36523#issuecomment-1819160512,repo: grafana/grafana | issue: Use data-testid instead of aria-label for test selectors | keyword: best practice
Will close when we move lint rule from betterer to eslint proper. Will probably exempt all of old dashboards stuff that will be removed soon.,,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/joshhunt,https://github.com/grafana/grafana/issues/36523#issuecomment-3179330920,repo: grafana/grafana | issue: Use data-testid instead of aria-label for test selectors | keyword: best practice
"Auth: `Secure` flag not enabled on cookies ### What happened? <img width=""906"" height=""183"" alt=""Image"" src=""https://github.com/user-attachments/assets/e723aada-8237-46ff-a2e5-73677ac43132"" /> 1. Load page while logged in 2. Go to storage tab in dev tools 3. `Secure` flag isn't enabled on any cookies ### Environment (with versions)? Grafana: 12.0.2 OS: Any Browser: Any",,,,,,Anecdotal,issue,,,,,,,,2025-07-28,github/snowdrop4,https://github.com/grafana/grafana/issues/108779,repo: grafana/grafana | keyword: best practice | state: open
"Hi @snowdrop4, there is a configuration option for it. See the docs: - https://grafana.com/docs/grafana/latest/setup-grafana/configure-security/configure-security-hardening/#add-a-secure-attribute-to-cookies",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/mkutlak,https://github.com/grafana/grafana/issues/108779#issuecomment-3127566697,repo: grafana/grafana | issue: Auth: `Secure` flag not enabled on cookies | keyword: best practice
"Oh! Wups. Thank you. Shouldn't this be on by default though? I know it'd make it not work if you don't have a TLS certificate, but it's generally best-practices to enable security features by default, since it prevents problems like this, where the user doesn't notice.",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/snowdrop4,https://github.com/grafana/grafana/issues/108779#issuecomment-3131730029,repo: grafana/grafana | issue: Auth: `Secure` flag not enabled on cookies | keyword: best practice
"Users: Changing a user password does not invalidate login sessions ### Reproduction 1. Login with user X in Browser A. 2. Login with user X in Browser B. 3. Change the password for user X, in Browser A. 4. Refresh the page in Browser B. Observe that the user session is still valid in Browser B. Security best-practices have the user session get invalidated upon password change. This ensures that, e.g., an attacker is locked out of a compromised account, if the user changes their password. ### En…",,,,,,Anecdotal,issue,,,,,,,,2025-07-28,github/snowdrop4,https://github.com/grafana/grafana/issues/108772,repo: grafana/grafana | keyword: best practice | state: open
"Docs: Deep Dive on DataFrames, Queries, Visualizations for Datasource Plugin Developers I was working on a datasource plugin recently and realized that while we have some [really great documentation on dataframes](https://grafana.com/developers/plugin-tools/key-concepts/data-frames) that explains them conceptually (the columnar structure, long vs wide formats etc) I still had a few questions on best practices when using them, that I think other devs might have as well. Some possible areas of fu…",,,,,,Anecdotal,issue,,,,,,,,2023-04-19,github/sarahzinger,https://github.com/grafana/grafana/issues/66892,repo: grafana/grafana | keyword: best practice | state: open
"@sympatheticmoose - Which developer would you suggest as the go-to person for building the content needed for this enhancement? Once they are identified, they can draft answers to these questions or I can interview them and draft it.",,,,,,Anecdotal,comment,,,,,,,,2023-09-25,github/josmperez,https://github.com/grafana/grafana/issues/66892#issuecomment-1734548239,"repo: grafana/grafana | issue: Docs: Deep Dive on DataFrames, Queries, Visualizations for Datasource Plugin Developers | keyword: best practice"
@josmperez some channels for your to get started - dataframes #wg-dataplane or #grafana-metrics but also in plugins platform you will get most of the answers you need - Datalinks -> #grafana-explore,,,,,,Anecdotal,comment,,,,,,,,2023-09-26,github/tolzhabayev,https://github.com/grafana/grafana/issues/66892#issuecomment-1735488059,"repo: grafana/grafana | issue: Docs: Deep Dive on DataFrames, Queries, Visualizations for Datasource Plugin Developers | keyword: best practice"
"i've been wanting to do this for a while. we have evolved and deprecated various practices, helper funcs, and frame types in the past year. i can help out with filling in any knowledge gaps, at least from a frontend/json dataframe perspective. i'm on PTO for the remainder of this year, but happy to start chipping away at this in Jan/Feb.",,,,,,Anecdotal,comment,,,,,,,,2023-12-16,github/leeoniya,https://github.com/grafana/grafana/issues/66892#issuecomment-1858812102,"repo: grafana/grafana | issue: Docs: Deep Dive on DataFrames, Queries, Visualizations for Datasource Plugin Developers | keyword: best practice"
"Thanks, @leeoniya - I'm looking forward to your contribution after New Year's. If possible, please aim for January so that we can resolve this issue in Q4.",,,,,,Anecdotal,comment,,,,,,,,2023-12-18,github/josmperez,https://github.com/grafana/grafana/issues/66892#issuecomment-1861828858,"repo: grafana/grafana | issue: Docs: Deep Dive on DataFrames, Queries, Visualizations for Datasource Plugin Developers | keyword: best practice"
"this might slip to next quarter for me. we're currently focused on rolling out new tooltips, wrapping up XYChart GA, and deprecations of Angular panels.",,,,,,Anecdotal,comment,,,,,,,,2024-03-05,github/leeoniya,https://github.com/grafana/grafana/issues/66892#issuecomment-1977701721,"repo: grafana/grafana | issue: Docs: Deep Dive on DataFrames, Queries, Visualizations for Datasource Plugin Developers | keyword: best practice"
"just a heads up that since writing this issue last year, a good amount on this has changed I think. For example, I see now that `MutableDataFrame` is deprecated. So whenever this is does get worked on, we shouldn't use the list that was made here as a comprehensive list, but look at the exported methods and prioritize which we want users to be most aware of (and maybe just add good inline comments to the rest). Also I think also https://grafana.com/developers/plugin-tools/create-a-plugin/develo…",,,,,,Anecdotal,comment,,,,,,,,2024-03-07,github/sarahzinger,https://github.com/grafana/grafana/issues/66892#issuecomment-1983623603,"repo: grafana/grafana | issue: Docs: Deep Dive on DataFrames, Queries, Visualizations for Datasource Plugin Developers | keyword: best practice"
"@natellium docs would help with making datasources better work with different dataviz features, might be worth considering to dedicate some time to this.",,,,,,Anecdotal,comment,,,,,,,,2025-05-09,github/tolzhabayev,https://github.com/grafana/grafana/issues/66892#issuecomment-2867311326,"repo: grafana/grafana | issue: Docs: Deep Dive on DataFrames, Queries, Visualizations for Datasource Plugin Developers | keyword: best practice"
"Security Vulnerability: Prometheus 0.52.0 - End of Life Notification (EOL) ### What happened? Hi, We have observed that the latest grafana/grafana:11.5.1 image includes Prometheus version 0.52.0, which has reached its end of life. Please upgrade the Prometheus package to the latest supported version to ensure security and stability. Best Regards, Anjani. ### What did you expect to happen? We expected the latest grafana/grafana image to include a supported version of Prometheus, to align with se…",,,,,,Anecdotal,issue,,,,,,,,2025-02-11,github/anjaniprayaga,https://github.com/grafana/grafana/issues/100422,repo: grafana/grafana | keyword: best practice | state: open
"Currently the module version of `github.com/prometheus/prometheus` is defined as version v0.301.0: https://github.com/grafana/grafana/blob/ea788975e054b32de436085e5ae24122696323c7/go.mod#L138 This update from version `v0.52.0` to `v0.301.0` was done via PR https://github.com/grafana/grafana/pull/98848 (You can also see that the pinning had been removed) - I've checked the branch release-11.5.1 as well - Here the version v0.301.0 is defined as well. Could you please inform here in the issue, whi…",,,,,,Anecdotal,comment,,,,,,,,2025-02-15,github/rgoltz,https://github.com/grafana/grafana/issues/100422#issuecomment-2661020212,repo: grafana/grafana | issue: Security Vulnerability: Prometheus 0.52.0 - End of Life Notification (EOL) | keyword: best practice
"Hi @rgoltz, Thank you for your response! I was using the Docker tag 11.5.1. I also tried using the main tag and noticed that the latest prometheus:0.301.0 is being used. Thanks, Anjani",,,,,,Anecdotal,comment,,,,,,,,2025-02-17,github/anjaniprayaga,https://github.com/grafana/grafana/issues/100422#issuecomment-2661931884,repo: grafana/grafana | issue: Security Vulnerability: Prometheus 0.52.0 - End of Life Notification (EOL) | keyword: best practice
"Hi Anjani. There was an other PR https://github.com/grafana/grafana/pull/100816, which dropped the old version-reference for `github.com/prometheus/prometheus` in `go.work.sum`. It's already merged. In case you like to stay with the version-based tag (e.g. `11.5.1`), you just need to wait for the next build = tag-update or upcoming minor of 11.5.x. If you need more detailed information, we need to wait for Grafana-team member giving an update (I'm just a user as you are.) Cheers, Robert",,,,,,Anecdotal,comment,,,,,,,,2025-02-18,github/rgoltz,https://github.com/grafana/grafana/issues/100422#issuecomment-2664710122,repo: grafana/grafana | issue: Security Vulnerability: Prometheus 0.52.0 - End of Life Notification (EOL) | keyword: best practice
"InfluxDB Query Builder Title Mapping for fields/measurements/retention_policy **Why is this needed**: Often the names of measurements/fields/retention_policies are choosen highly artificially like UUIDs or URNs as they are perceived as beeing application internal like primary keys of relational tables. From a developer perspective this is even best practice (don't use meaningful ids). If users now interact with Grafana with the database and use the InfluxDB Query Builder Form, they are confront…",,,,,,Anecdotal,issue,,,,,,,,2025-01-29,github/lukaslentner,https://github.com/grafana/grafana/issues/99755,repo: grafana/grafana | keyword: best practice | state: open
"Prometheus: unable to use multiple ad hoc filters with `.*` as default value ### What happened? It is not possible to use multiple Prometheus ad hoc filters initialized to `.*` via a single ad hoc variable as a dashboard's default (or via URL deep link). Specifically, it is not possible to start filtering from any of these ad hoc filters arbitrarily. We would like to support a dashboard where filters are mutually exclusive; e.g., it is possible that label A with a value of X is not seen on any …",,,,,,Anecdotal,issue,,,,,,,,2024-10-02,github/jessermejia,https://github.com/grafana/grafana/issues/94176,repo: grafana/grafana | keyword: best practice | state: open
"Summarizing here for my understanding, thank you @jessermejia for all the references, they were very helpful. Adhoc filters in the Grafana Prometheus data source use the Prometheus labels api and label values api and each accept a match[] param in the form of a series. e.g. {label:""value""} It is mandatory to provide at least one non-empty label value as selector in the match param. {label:""value""} => non-empty {label:""""} => definitely empty {label:"".*""} => empty [Prometheus documentation](https…",,,,,,Anecdotal,comment,,,,,,,,2024-10-03,github/bohandley,https://github.com/grafana/grafana/issues/94176#issuecomment-2391617189,repo: grafana/grafana | issue: Prometheus: unable to use multiple ad hoc filters with `.*` as default value | keyword: best practice
">How does this sound @jessermejia, that we avoid the error by filtering out when all series matchers label values evaluate to empty? @bohandley Sounds great! Allowing users to combine ad hoc filters in such a way that returns no data is definitely undesirable and likely confusing. We agree it's just the initial call / error that is the issue. Thanks for taking a look so quickly.",,,,,,Anecdotal,comment,,,,,,,,2024-10-03,github/jessermejia,https://github.com/grafana/grafana/issues/94176#issuecomment-2391999256,repo: grafana/grafana | issue: Prometheus: unable to use multiple ad hoc filters with `.*` as default value | keyword: best practice
"For prioritization, we should have some time to look at it in Q4. I will update this issue and add it to our work in Q4. Thank you @jessermejia",,,,,,Anecdotal,comment,,,,,,,,2024-10-08,github/bohandley,https://github.com/grafana/grafana/issues/94176#issuecomment-2400405452,repo: grafana/grafana | issue: Prometheus: unable to use multiple ad hoc filters with `.*` as default value | keyword: best practice
The Prometheus data source has been handed off to the @grafana/oss-big-tent squad. The current timeline will need to be adjusted as the squad decides priorities. Thank you for your patience!,,,,,,Anecdotal,comment,,,,,,,,2025-01-24,github/bohandley,https://github.com/grafana/grafana/issues/94176#issuecomment-2613076962,repo: grafana/grafana | issue: Prometheus: unable to use multiple ad hoc filters with `.*` as default value | keyword: best practice
"Hi everyone @grafana/oss-big-tent is here, I’d like to provide some context regarding the recent changes introduced in [this PR](https://github.com/grafana/grafana/pull/74962). Goal of the change: The primary objective was to enhance adhoc filters by ensuring they consider existing filters when suggesting keys and values. Previously, this wasn't the case—we were always returning the full list, which contradicted the intended behavior outlined in the official Grafana documentation: [Adhoc Filter…",,,,,,Anecdotal,comment,,,,,,,,2025-01-24,github/itsmylife,https://github.com/grafana/grafana/issues/94176#issuecomment-2613492280,repo: grafana/grafana | issue: Prometheus: unable to use multiple ad hoc filters with `.*` as default value | keyword: best practice
"SQL Expressions: LLM plugin integration - SQL suggestions and SQL explanations ### What does this PR do? 📓 > [!TIP] > Dashboards team please feel free to _just_ review changes within the `/dashboard/components/GenAI` directory, thank you!! 😄 This PR adds Grafana LLM plugin support for SQL Expressions. The SQL Expressions feature is currently tailored to powerusers, but this LLM implementation will make it more accessible (at least that's the hope)! 😄 #### Review tracking | Item / Topic | Dashbo…",,,,,,Anecdotal,issue,,,,,,,,2025-07-02,github/alexjonspencer1,https://github.com/grafana/grafana/pull/107545,repo: grafana/grafana | keyword: best practice | state: closed
"Main issue I'm hitting getting going is the timeout. Had to hit the button several times (15s), but quite nice when I got the response. - Maybe we can make this time out configurable setting? - What is the timeout in this case? Is it the total lifetime of the stream? Generally with OpenAI it is pretty slow and seems faster because the text is streaming in, I wonder is the a timeout to start getting the response, or total lifetime? Feels like total lifetime and I'm wondering if we can go longer …",,,,,,Anecdotal,comment,,,,,,,,2025-07-18,github/kylebrandt,https://github.com/grafana/grafana/pull/107545#issuecomment-3089496704,repo: grafana/grafana | issue: SQL Expressions: LLM plugin integration - SQL suggestions and SQL explanations | keyword: best practice
"> Main issue I'm hitting getting going is the timeout. Had to hit the button several times (15s), but quite nice when I got the response. > > * Maybe we can make this time out configurable setting? > * What is the timeout in this case? Is it the total lifetime of the stream? Generally with OpenAI it is pretty slow and seems faster because the text is streaming in, I wonder is the a timeout to start getting the response, or total lifetime? Feels like total lifetime and I'm wondering if we can go…",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/alexjonspencer1,https://github.com/grafana/grafana/pull/107545#issuecomment-3098679567,repo: grafana/grafana | issue: SQL Expressions: LLM plugin integration - SQL suggestions and SQL explanations | keyword: best practice
"Hey, you asked me to try this out with some more real examples. ## I'm Excited - Great stuff I continue to be excited about this feature. The ""Explain"" button in particular seems to be doing a great job - nice work on this 🎉 ## Improve button I had a panel I had built which was incredibly slow. With the help of Cursor/Claude I was able to speed it up by removing the cross-join, and adding an additional CTE at the top of the query (the `all_namespaces` CTE), although it did take Claude maybe 2 f…",,,,,,Anecdotal,comment,,,,,,,,2025-07-23,github/samjewell,https://github.com/grafana/grafana/pull/107545#issuecomment-3108564153,repo: grafana/grafana | issue: SQL Expressions: LLM plugin integration - SQL suggestions and SQL explanations | keyword: best practice
"DataSourcePicker: Refactor as function component and tests <!-- Thank you for sending a pull request! Here are some tips: 1. If this is your first time, please read our contribution guide at https://github.com/grafana/grafana/blob/main/CONTRIBUTING.md 2. Ensure you include and run the appropriate tests as part of your Pull Request. 3. In a new feature or configuration option, an update to the documentation is necessary. Everything related to the documentation is under the docs folder in the roo…",,,,,,Anecdotal,issue,,,,,,,,2025-08-06,github/andresmgot,https://github.com/grafana/grafana/pull/109254,repo: grafana/grafana | keyword: best practice | state: closed
"⚠️ &nbsp;&nbsp;**Possible breaking changes (md version)**&nbsp;&nbsp; ⚠️ <h3>grafana-runtime</h3><h4>Removals</h4><b>DataSourcePicker.dataSourceSrv</b><br> <sub>/home/runner/work/grafana/grafana/base/grafana-runtime/dist/types/components/DataSourcePicker.d.ts</sub><br> <pre lang=""typescript""> dataSourceSrv: import(""../services/dataSourceSrv"").DataSourceSrv; </pre><br> <b>DataSourcePicker.state</b><br> <sub>/home/runner/work/grafana/grafana/base/grafana-runtime/dist/types/components/DataSourcePi…",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/grafana-pr-automation[bot],https://github.com/grafana/grafana/pull/109254#issuecomment-3159854392,repo: grafana/grafana | issue: DataSourcePicker: Refactor as function component and tests | keyword: best practice
Chatted offline with @joshhunt and discussed that mid-term we want the internal [DataSourcePicker](https://github.com/grafana/grafana/blob/main/public/app/features/datasources/components/picker/DataSourcePicker.tsx) to replace the one exposed in the UI so we won't need this.,,,,,,Anecdotal,comment,,,,,,,,2025-08-11,github/andresmgot,https://github.com/grafana/grafana/pull/109254#issuecomment-3174018385,repo: grafana/grafana | issue: DataSourcePicker: Refactor as function component and tests | keyword: best practice
[WIP] Canvas: Add elements developer guide Work in progress for a developer guide for Canvas elements. Intention here is to capture best practices for community members to follow who want to contribute to Canvas' development.,,,,,,Anecdotal,issue,,,,,,,,2025-06-27,github/drew08t,https://github.com/grafana/grafana/pull/107283,repo: grafana/grafana | keyword: best practice | state: closed
This pull request has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in 2 weeks if no further activity occurs. Please feel free to give a status update or ping for review. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-07-27,github/github-actions[bot],https://github.com/grafana/grafana/pull/107283#issuecomment-3123779182,repo: grafana/grafana | issue: [WIP] Canvas: Add elements developer guide | keyword: best practice
This pull request has been automatically closed because it has not had any further activity in the last 2 weeks. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-08-11,github/github-actions[bot],https://github.com/grafana/grafana/pull/107283#issuecomment-3173103770,repo: grafana/grafana | issue: [WIP] Canvas: Add elements developer guide | keyword: best practice
"chore: use native histogram for search shadow traffic match percentage <!-- Thank you for sending a pull request! Here are some tips: 1. If this is your first time, please read our contribution guide at https://github.com/grafana/grafana/blob/main/CONTRIBUTING.md 2. Ensure you include and run the appropriate tests as part of your Pull Request. 3. In a new feature or configuration option, an update to the documentation is necessary. Everything related to the documentation is under the docs folde…",,,,,,Anecdotal,issue,,,,,,,,2025-08-05,github/inf0rmer,https://github.com/grafana/grafana/pull/109189,repo: grafana/grafana | keyword: best practice | state: closed
@cursoragent Review this PR please. Is it possible to change a metric type if the base metric name doesn't change in prometheus?,,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/JohnnyQQQQ,https://github.com/grafana/grafana/pull/109189#issuecomment-3156455161,repo: grafana/grafana | issue: chore: use native histogram for search shadow traffic match percentage | keyword: best practice
"I've looked into the Prometheus documentation and community discussions regarding changing metric types. Generally, it's not recommended to change the type of an existing metric that shares the same name in Prometheus. Prometheus expects a consistent metric type for a given metric name. If you change a metric from, say, a counter to a gauge, or in this PR's case, from a standard histogram to a native histogram, it can lead to issues. This is because Prometheus stores and queries different metri…",,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/cursor[bot],https://github.com/grafana/grafana/pull/109189#issuecomment-3156455471,repo: grafana/grafana | issue: chore: use native histogram for search shadow traffic match percentage | keyword: best practice
"Prometheus: Performance improvements for high cardinality metrics When dealing with high-cardinality metrics (ever-growing metrics, labels, or label values), the autocomplete feature is not as responsive as expected. Although users should avoid high-cardinality cases whenever possible, they sometimes end up with them regardless. From Prometheus ""[Best practices](https://prometheus.io/docs/practices/naming/)"" page: >CAUTION: Remember that every unique combination of key-value label pairs represe…",,,,,,Anecdotal,issue,,,,,,,,2025-02-10,github/itsmylife,https://github.com/grafana/grafana/issues/100376,repo: grafana/grafana | keyword: best practice | state: closed
"> with a ""Load More"" option I suggest if you limit to 1,000 then you don't need to load more. Nobody has any business scrolling past 1,000 names then demanding more; they can add more characters and hence narrow down the set. * Note https://github.com/grafana/grafana/issues/85444",,,,,,Anecdotal,comment,,,,,,,,2025-02-10,github/bboreham,https://github.com/grafana/grafana/issues/100376#issuecomment-2648660495,repo: grafana/grafana | issue: Prometheus: Performance improvements for high cardinality metrics | keyword: best practice
"With respect to: > Require at least three characters before triggering metric name autocomplete (should this be configurable?). This makes sense. But I believe we should make an exception - when the user presses `ctrl+space` (the Monaco shortcut to explicitly reveal autocomplete suggestions), we should not care how many characters the user has input.",,,,,,Anecdotal,comment,,,,,,,,2025-02-12,github/NWRichmond,https://github.com/grafana/grafana/issues/100376#issuecomment-2654178384,repo: grafana/grafana | issue: Prometheus: Performance improvements for high cardinality metrics | keyword: best practice
"Other possible optimizations, for parts of the autocomplete system that tend to become [long tasks](https://web.dev/articles/optimize-long-tasks): - run expensive parts (like [`filterMetricNames`](https://github.com/grafana/grafana/blob/2b054d4154ad174d630dadec49fa26a97d151232/packages/grafana-prometheus/src/components/monaco-query-field/monaco-completion-provider/completions.ts#L51-L63), sometimes) in a web worker - there may be some opportunities to [yield](https://web.dev/articles/optimize-l…",,,,,,Anecdotal,comment,,,,,,,,2025-02-12,github/NWRichmond,https://github.com/grafana/grafana/issues/100376#issuecomment-2654211422,repo: grafana/grafana | issue: Prometheus: Performance improvements for high cardinality metrics | keyword: best practice
"@NWRichmond I think you do have to consider limits somewhere. There are instances out there with over 5 million metrics that takes 22 seconds for Prometheus to return them to the browser. The request is 22mb over the wire, and then 400 mb of JSON for the browser to pause. I have a fast computer with 128GB of RAM, and the entire browser hangs for a few seconds. I don't think that's a great experience for anyone.",,,,,,Anecdotal,comment,,,,,,,,2025-02-28,github/joshhunt,https://github.com/grafana/grafana/issues/100376#issuecomment-2690786862,repo: grafana/grafana | issue: Prometheus: Performance improvements for high cardinality metrics | keyword: best practice
"@joshhunt I wholeheartedly agree - that's where https://github.com/grafana/grafana/issues/85444 comes in. To clarify my point above, I'm proposing that: - when the user enters at least three characters, autocomplete is enabled - when the user enters zero or more characters and hits `ctrl+space`, autocomplete is enabled - in both cases, the data source should limit the payloads that power autocomplete so that we're not loading massive, unbounded lists that degrade the UX",,,,,,Anecdotal,comment,,,,,,,,2025-02-28,github/NWRichmond,https://github.com/grafana/grafana/issues/100376#issuecomment-2690830787,repo: grafana/grafana | issue: Prometheus: Performance improvements for high cardinality metrics | keyword: best practice
"**Project Update** Working on a PR, to be ready this week **Trending** <!-- Keep one, delete the rest--> <!-- data key=""trending"" start --> 🟢 on track <!-- data end --> **Next** Cover edge cases and prepare a PR",,,,,,Anecdotal,comment,,,,,,,,2025-07-14,github/szabalza,https://github.com/grafana/grafana/issues/100376#issuecomment-3069556162,repo: grafana/grafana | issue: Prometheus: Performance improvements for high cardinality metrics | keyword: best practice
"**Project Update** Code editor PR merged. Needs review for the visual query editor. **Trending** <!-- Keep one, delete the rest--> <!-- data key=""trending"" start --> 🟢 on track <!-- data end --> **Next** Merge remaining PR and gather feedback.",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/zoltanbedi,https://github.com/grafana/grafana/issues/100376#issuecomment-3127113859,repo: grafana/grafana | issue: Prometheus: Performance improvements for high cardinality metrics | keyword: best practice
"Alerting: Fix contact points scoped permission check This pull request introduces updates to improve the handling of permissions and UI interactions for contact points in the alerting feature. The key changes include refining edit and delete permission logic, updating test coverage to align with these changes, and improving the readability of certain test utilities. ### Updates to permission logic and UI behavior: * [`ContactPointHeader.tsx`](diffhunk://#diff-bd80acfd65ce0ebbe81500cff314e3c9078…",,,,,,Anecdotal,issue,,,,,,,,2025-07-29,github/konrad147,https://github.com/grafana/grafana/pull/108851,repo: grafana/grafana | keyword: best practice | state: closed
"Improve indexing observability This PR does several things to improve observability of indexing: * Introduces ""reason"" parameter when building an index -- used for logging and metrics * Introduces metrics to track indexing builds, failures and skipped indexing: * `grafana_index_server_index_build_total` (per reason) * `grafana_index_server_index_build_failures_total` * `grafana_index_server_index_build_skipped_total` * Changes previous `grafana_index_server_index_creation_time_seconds` into `gr…",,,,,,Anecdotal,issue,,,,,,,,2025-07-30,github/pstibrany,https://github.com/grafana/grafana/pull/108901,repo: grafana/grafana | keyword: best practice | state: closed
Variables: Improve structure and conceptual information The conceptual content of this page is not as clear as it could be and the explanations that are here need to be more robust. This page doesn't meet beginners where they're at. ### Tasks **Intro** - [x] Move definition of a variable to the first thing on the page - [x] Clarify definition and how it pertains to panel titles - [x] Add more use cases in Grafana to list - [x] Remove or improve reference to templates in intro (link to Templates…,,,,,,Anecdotal,issue,,,,,,,,2024-01-09,github/imatwawana,https://github.com/grafana/grafana/issues/80252,repo: grafana/grafana | keyword: best practice | state: closed
This issue has been automatically marked as stale because it has not had activity in the last year. It will be closed in 30 days if no further activity occurs. Please feel free to leave a comment if you believe the issue is still relevant. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-01-10,github/github-actions[bot],https://github.com/grafana/grafana/issues/80252#issuecomment-2581592347,repo: grafana/grafana | issue: Variables: Improve structure and conceptual information | keyword: best practice
/variables/#variable-best-practices -> /variables/add-template-variables/#variable-best-practices - datasources/influxdb/template-variables /variables/#templates -> /variables/#template-variables - 93 instances - alias the heading,,,,,,Anecdotal,comment,,,,,,,,2025-07-18,github/imatwawana,https://github.com/grafana/grafana/issues/80252#issuecomment-3090394307,repo: grafana/grafana | issue: Variables: Improve structure and conceptual information | keyword: best practice
"Update _index.md This PR makes a minor formatting correction to the documentation by changing the capitalization of ""Best Practices"" to ""Best practices"" in the menu title for the Grafana Alerting best practices documentation. Updates menuTitle field to use lowercase ""practices"" for consistency with standard title formatting",,,,,,Anecdotal,issue,,,,,,,,2025-07-16,github/JohnnyK-Grafana,https://github.com/grafana/grafana/pull/108192,repo: grafana/grafana | keyword: best practice | state: closed
Sharing: Export dashboard as image Enable `sharingDashboardImage` feature toggle to test locally. You will also need image renderer running and set up locally as well to test (can be ran in http mode) Feature Implementation: - Added a new feature to export dashboards as images (PNG) - Implemented in the new dashboard scene architecture - Uses the Grafana image renderer plugin for generating images Key Components: - ExportAsImage.tsx: Main component for the export UI - utils.ts: Core functionali…,,,,,,Anecdotal,issue,,,,,,,,2025-04-19,github/nmarrs,https://github.com/grafana/grafana/pull/104207,repo: grafana/grafana | keyword: best practice | state: closed
"Since you've added the `Add to what's new` label, consider drafting a [What's new note](https://admin.grafana.com/content-admin/#/collections/whats-new/new) for this feature. <!-- Sticky Pull Request Comment -->",,,,,,Anecdotal,comment,,,,,,,,2025-04-19,github/github-actions[bot],https://github.com/grafana/grafana/pull/104207#issuecomment-2816531766,repo: grafana/grafana | issue: Sharing: Export dashboard as image | keyword: best practice
"Since you've added the `Add to what's new` label, consider drafting a [What's new note](https://admin.grafana.com/content-admin/#/collections/whats-new/new) for this feature. <!-- Sticky Pull Request Comment -->",,,,,,Anecdotal,comment,,,,,,,,2025-04-19,github/github-actions[bot],https://github.com/grafana/grafana/pull/104207#issuecomment-2816531770,repo: grafana/grafana | issue: Sharing: Export dashboard as image | keyword: best practice
:cry: zizmor failed with exit code 13. <details> <summary>Expand for full output</summary> ``` warning[excessive-permissions]: overly broad permissions --> ./.github/workflows/alerting-swagger-gen.yml:7:3 | 7 | / gen-swagger: 8 | | name: Alerting Swagger spec generation cron job ... | 36 | | team-reviewers: 'grafana/alerting-backend' 37 | | draft: false | | - | |_______________________| | this job | default permissions used due to no permissions: block | = note: audit confidence → Medium warnin…,,,,,,Anecdotal,comment,,,,,,,,2025-04-30,github/github-actions[bot],https://github.com/grafana/grafana/pull/104207#issuecomment-2843290979,repo: grafana/grafana | issue: Sharing: Export dashboard as image | keyword: best practice
:cry: zizmor failed with exit code 13. <details> <summary>Expand for full output</summary> ``` warning[excessive-permissions]: overly broad permissions --> ./.github/workflows/alerting-swagger-gen.yml:7:3 | 7 | / gen-swagger: 8 | | name: Alerting Swagger spec generation cron job ... | 36 | | team-reviewers: 'grafana/alerting-backend' 37 | | draft: false | | - | |_______________________| | this job | default permissions used due to no permissions: block | = note: audit confidence → Medium warnin…,,,,,,Anecdotal,comment,,,,,,,,2025-04-30,github/github-actions[bot],https://github.com/grafana/grafana/pull/104207#issuecomment-2843322010,repo: grafana/grafana | issue: Sharing: Export dashboard as image | keyword: best practice
:cry: zizmor failed with exit code 13. <details> <summary>Expand for full output</summary> ``` warning[excessive-permissions]: overly broad permissions --> ./.github/workflows/alerting-swagger-gen.yml:7:3 | 7 | / gen-swagger: 8 | | name: Alerting Swagger spec generation cron job ... | 36 | | team-reviewers: 'grafana/alerting-backend' 37 | | draft: false | | - | |_______________________| | this job | default permissions used due to no permissions: block | = note: audit confidence → Medium warnin…,,,,,,Anecdotal,comment,,,,,,,,2025-04-30,github/github-actions[bot],https://github.com/grafana/grafana/pull/104207#issuecomment-2843346271,repo: grafana/grafana | issue: Sharing: Export dashboard as image | keyword: best practice
:cry: zizmor failed with exit code 13. <details> <summary>Expand for full output</summary> ``` warning[excessive-permissions]: overly broad permissions --> ./.github/workflows/alerting-swagger-gen.yml:7:3 | 7 | / gen-swagger: 8 | | name: Alerting Swagger spec generation cron job ... | 36 | | team-reviewers: 'grafana/alerting-backend' 37 | | draft: false | | - | |_______________________| | this job | default permissions used due to no permissions: block | = note: audit confidence → Medium warnin…,,,,,,Anecdotal,comment,,,,,,,,2025-04-30,github/github-actions[bot],https://github.com/grafana/grafana/pull/104207#issuecomment-2843393800,repo: grafana/grafana | issue: Sharing: Export dashboard as image | keyword: best practice
:cry: zizmor failed with exit code 13. <details> <summary>Expand for full output</summary> ``` warning[excessive-permissions]: overly broad permissions --> ./.github/workflows/alerting-swagger-gen.yml:7:3 | 7 | / gen-swagger: 8 | | name: Alerting Swagger spec generation cron job ... | 36 | | team-reviewers: 'grafana/alerting-backend' 37 | | draft: false | | - | |_______________________| | this job | default permissions used due to no permissions: block | = note: audit confidence → Medium warnin…,,,,,,Anecdotal,comment,,,,,,,,2025-04-30,github/github-actions[bot],https://github.com/grafana/grafana/pull/104207#issuecomment-2843516645,repo: grafana/grafana | issue: Sharing: Export dashboard as image | keyword: best practice
:cry: zizmor failed with exit code 13. <details> <summary>Expand for full output</summary> ``` warning[excessive-permissions]: overly broad permissions --> ./.github/workflows/alerting-swagger-gen.yml:7:3 | 7 | / gen-swagger: 8 | | name: Alerting Swagger spec generation cron job ... | 36 | | team-reviewers: 'grafana/alerting-backend' 37 | | draft: false | | - | |_______________________| | this job | default permissions used due to no permissions: block | = note: audit confidence → Medium warnin…,,,,,,Anecdotal,comment,,,,,,,,2025-05-01,github/github-actions[bot],https://github.com/grafana/grafana/pull/104207#issuecomment-2843837138,repo: grafana/grafana | issue: Sharing: Export dashboard as image | keyword: best practice
:cry: zizmor failed with exit code 13. <details> <summary>Expand for full output</summary> ``` warning[excessive-permissions]: overly broad permissions --> ./.github/workflows/alerting-swagger-gen.yml:7:3 | 7 | / gen-swagger: 8 | | name: Alerting Swagger spec generation cron job ... | 36 | | team-reviewers: 'grafana/alerting-backend' 37 | | draft: false | | - | |_______________________| | this job | default permissions used due to no permissions: block | = note: audit confidence → Medium warnin…,,,,,,Anecdotal,comment,,,,,,,,2025-05-01,github/github-actions[bot],https://github.com/grafana/grafana/pull/104207#issuecomment-2843880084,repo: grafana/grafana | issue: Sharing: Export dashboard as image | keyword: best practice
:cry: zizmor failed with exit code 13. <details> <summary>Expand for full output</summary> ``` warning[excessive-permissions]: overly broad permissions --> ./.github/workflows/alerting-swagger-gen.yml:7:3 | 7 | / gen-swagger: 8 | | name: Alerting Swagger spec generation cron job ... | 36 | | team-reviewers: 'grafana/alerting-backend' 37 | | draft: false | | - | |_______________________| | this job | default permissions used due to no permissions: block | = note: audit confidence → Medium warnin…,,,,,,Anecdotal,comment,,,,,,,,2025-05-01,github/github-actions[bot],https://github.com/grafana/grafana/pull/104207#issuecomment-2843880170,repo: grafana/grafana | issue: Sharing: Export dashboard as image | keyword: best practice
:cry: zizmor failed with exit code 13. <details> <summary>Expand for full output</summary> ``` warning[excessive-permissions]: overly broad permissions --> ./.github/workflows/alerting-swagger-gen.yml:7:3 | 7 | / gen-swagger: 8 | | name: Alerting Swagger spec generation cron job ... | 36 | | team-reviewers: 'grafana/alerting-backend' 37 | | draft: false | | - | |_______________________| | this job | default permissions used due to no permissions: block | = note: audit confidence → Medium warnin…,,,,,,Anecdotal,comment,,,,,,,,2025-05-01,github/github-actions[bot],https://github.com/grafana/grafana/pull/104207#issuecomment-2843882746,repo: grafana/grafana | issue: Sharing: Export dashboard as image | keyword: best practice
:cry: zizmor failed with exit code 13. <details> <summary>Expand for full output</summary> ``` warning[excessive-permissions]: overly broad permissions --> ./.github/workflows/alerting-swagger-gen.yml:7:3 | 7 | / gen-swagger: 8 | | name: Alerting Swagger spec generation cron job ... | 36 | | team-reviewers: 'grafana/alerting-backend' 37 | | draft: false | | - | |_______________________| | this job | default permissions used due to no permissions: block | = note: audit confidence → Medium warnin…,,,,,,Anecdotal,comment,,,,,,,,2025-05-01,github/github-actions[bot],https://github.com/grafana/grafana/pull/104207#issuecomment-2843892863,repo: grafana/grafana | issue: Sharing: Export dashboard as image | keyword: best practice
:cry: zizmor failed with exit code 13. <details> <summary>Expand for full output</summary> ``` warning[excessive-permissions]: overly broad permissions --> ./.github/workflows/alerting-swagger-gen.yml:7:3 | 7 | / gen-swagger: 8 | | name: Alerting Swagger spec generation cron job ... | 36 | | team-reviewers: 'grafana/alerting-backend' 37 | | draft: false | | - | |_______________________| | this job | default permissions used due to no permissions: block | = note: audit confidence → Medium warnin…,,,,,,Anecdotal,comment,,,,,,,,2025-05-01,github/github-actions[bot],https://github.com/grafana/grafana/pull/104207#issuecomment-2843962067,repo: grafana/grafana | issue: Sharing: Export dashboard as image | keyword: best practice
⚠️ E2E workflow for dashboardNewLayouts has failed. [Click here to view the run](https://github.com/grafana/grafana/actions/runs/15276883605).,,,,,,Anecdotal,comment,,,,,,,,2025-05-27,github/github-actions[bot],https://github.com/grafana/grafana/pull/104207#issuecomment-2912597408,repo: grafana/grafana | issue: Sharing: Export dashboard as image | keyword: best practice
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/15694769126) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **add-option-to-export-dashboard-or-panel-as-image** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/E…,,,,,,Anecdotal,comment,,,,,,,,2025-06-17,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/104207#issuecomment-2978525646,repo: grafana/grafana | issue: Sharing: Export dashboard as image | keyword: best practice
- Your instance can be accessed at: https://ephemeral15111821104207nmarrs.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-06-17,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/104207#issuecomment-2978560181,repo: grafana/grafana | issue: Sharing: Export dashboard as image | keyword: best practice
"Hey @nmarrs - I'm prepping for the 12.1 release, do you plan to release this for then? If so, could you submit the [what's new note](https://admin.grafana.com/content-admin/#/collections/whats-new/new) for it?",,,,,,Anecdotal,comment,,,,,,,,2025-07-08,github/marybaldwin,https://github.com/grafana/grafana/pull/104207#issuecomment-3049485305,repo: grafana/grafana | issue: Sharing: Export dashboard as image | keyword: best practice
"> I gave a local test, and it looking great! Awesome work @nmarrs > > ### Codewise: > * I see a few files generated, is this PR changing alerting? 🤔 `apps/alerting/notifications/pkg/apis/alerting/v0alpha1/templategroup_schema_gen.go` > > ### UX feedback: > #### Export in Panel View only export panel > I am not sure if it is intentional, but from panel view, the export dashboard only exports the panel in view. > > We can disable the export image from there, or change the copy to ""export panel "" …",,,,,,Anecdotal,comment,,,,,,,,2025-07-15,github/nmarrs,https://github.com/grafana/grafana/pull/104207#issuecomment-3074487592,repo: grafana/grafana | issue: Sharing: Export dashboard as image | keyword: best practice
"Annotations: Allow `target=""_blank""` This PR allows sanitizing anchor elements with `target=""_blank""`. Issue was reported in annotations. Based on the best practices outlined [here](https://developer.chrome.com/docs/lighthouse/best-practices/external-anchors-use-rel-noopener). Fixes https://github.com/grafana/grafana/issues/85772 Test panel: [debug-Annotations bug-2025-06-03 16_19_18.json.txt](https://github.com/user-attachments/files/20580984/debug-Annotations.bug-2025-06-03.16_19_18.json.txt)…",,,,,,Anecdotal,issue,,,,,,,,2025-06-03,github/adela-almasan,https://github.com/grafana/grafana/pull/106301,repo: grafana/grafana | keyword: best practice | state: closed
":cry: zizmor failed with exit code 14. <details> <summary>Expand for full output</summary> ``` error[excessive-permissions]: overly broad permissions --> ./.github/workflows/release-pr.yml:42:3 | 42 | id-token: write | ^^^^^^^^^^^^^^^ id-token: write is overly broad at the workflow level | = note: audit confidence → High 93 findings (45 ignored, 47 suppressed): 0 unknown, 0 informational, 0 low, 0 medium, 1 high ``` </details> <!-- comment-action/zizmor GitHub Actions static analysis/analysis -…",,,,,,Anecdotal,comment,,,,,,,,2025-06-03,github/github-actions[bot],https://github.com/grafana/grafana/pull/106301#issuecomment-2937236753,"repo: grafana/grafana | issue: Annotations: Allow `target=""_blank""` | keyword: best practice"
":cry: zizmor failed with exit code 14. <details> <summary>Expand for full output</summary> ``` error[excessive-permissions]: overly broad permissions --> ./.github/workflows/release-pr.yml:42:3 | 42 | id-token: write | ^^^^^^^^^^^^^^^ id-token: write is overly broad at the workflow level | = note: audit confidence → High 93 findings (45 ignored, 47 suppressed): 0 unknown, 0 informational, 0 low, 0 medium, 1 high ``` </details> <!-- comment-action/zizmor GitHub Actions static analysis/analysis -…",,,,,,Anecdotal,comment,,,,,,,,2025-06-03,github/github-actions[bot],https://github.com/grafana/grafana/pull/106301#issuecomment-2937256323,"repo: grafana/grafana | issue: Annotations: Allow `target=""_blank""` | keyword: best practice"
"@adela-almasan Hope you don't mind me pushing a change, see https://github.com/grafana/grafana/pull/106301/commits/f8328a5466532ff16ca78ff31a871fad0c63099f TS compiles without errors and added back the hook as we can just add more hooks if we want custom behavior in the future.",,,,,,Anecdotal,comment,,,,,,,,2025-07-02,github/KristianGrafana,https://github.com/grafana/grafana/pull/106301#issuecomment-3026899485,"repo: grafana/grafana | issue: Annotations: Allow `target=""_blank""` | keyword: best practice"
"Thank you, @KristianGrafana! Sorry, I misunderstood your initial comment, I thought you were suggesting to remove the hook.. Adding `node.setAttribute('rel', 'noopener noreferrer');` simplifies things but will override the existing `rel` for the node and we might not want that? [This test](https://github.com/grafana/grafana/actions/runs/16019635610/job/45193292703?pr=106301) is catching that use case.",,,,,,Anecdotal,comment,,,,,,,,2025-07-02,github/adela-almasan,https://github.com/grafana/grafana/pull/106301#issuecomment-3027694530,"repo: grafana/grafana | issue: Annotations: Allow `target=""_blank""` | keyword: best practice"
"@adela-almasan Since all other values in `rel` attributes are informational only, it's safe to overwrite them as it won't impact functionality (`external` does nothing for instance). I modified the test in https://github.com/grafana/grafana/pull/106301/commits/0f1f1487cfd9aa0467e9138e5691bd7b95f78be1 - LMK if you're happy so I can approve :)",,,,,,Anecdotal,comment,,,,,,,,2025-07-02,github/KristianGrafana,https://github.com/grafana/grafana/pull/106301#issuecomment-3027834511,"repo: grafana/grafana | issue: Annotations: Allow `target=""_blank""` | keyword: best practice"
"@KristianGrafana ok, sounds good, I'm just trying to avoid future complains. Should we also bring back `removeHook` or it's ok as-is?",,,,,,Anecdotal,comment,,,,,,,,2025-07-02,github/adela-almasan,https://github.com/grafana/grafana/pull/106301#issuecomment-3027864489,"repo: grafana/grafana | issue: Annotations: Allow `target=""_blank""` | keyword: best practice"
"@adela-almasan Sorry for late reply, I was on PTO. > I'm just trying to avoid future complains. Valid concern. However, the purpose of a sanitizing function is to be trustworthy where untrusted data goes in, and trusted goes out. Avoiding complexity is key here. For instance, if we have `<a href=x rel=<script>alert()</script>` - should we keep the `rel` value here even if it's clearly malicious and might end up in the DOM? We should only keep valid values, but then we need to keep a list of app…",,,,,,Anecdotal,comment,,,,,,,,2025-07-08,github/KristianGrafana,https://github.com/grafana/grafana/pull/106301#issuecomment-3047728889,"repo: grafana/grafana | issue: Annotations: Allow `target=""_blank""` | keyword: best practice"
"Hey @KristianGrafana, sorry I was ooo. I think maybe adding back the `finally` would work better? In case `sanitize` throws an error the hook won't be removed, if you add it in the same block. I'll push, lmk if you think it's ok.",,,,,,Anecdotal,comment,,,,,,,,2025-07-11,github/adela-almasan,https://github.com/grafana/grafana/pull/106301#issuecomment-3062603960,"repo: grafana/grafana | issue: Annotations: Allow `target=""_blank""` | keyword: best practice"
"Add Relyance github action <!-- Thank you for sending a pull request! Here are some tips: 1. If this is your first time, please read our contribution guide at https://github.com/grafana/grafana/blob/main/CONTRIBUTING.md 2. Ensure you include and run the appropriate tests as part of your Pull Request. 3. In a new feature or configuration option, an update to the documentation is necessary. Everything related to the documentation is under the docs folder in the root of the repository. 4. If the P…",,,,,,Anecdotal,issue,,,,,,,,2025-06-19,github/rhorvath,https://github.com/grafana/grafana/pull/107012,repo: grafana/grafana | keyword: best practice | state: closed
"Enforce lowercase usernames and email addresses for all new accounts Goal: As an administrator I can enable the enforcement of lowercase usernames and email addresses for all new accounts. ---- Enforcing lowercase usernames and email addresses, as outlined in #41831, while a best practice, is a breaking change and should be reserved for a major release of Grafana such as v9. **In the interim, we need to start by offering the ability to enforce lowercase usernames and email addresses for new acc…",,,,,,Anecdotal,issue,,,,,,,,2021-11-17,github/pkolyvas,https://github.com/grafana/grafana/issues/41833,repo: grafana/grafana | keyword: best practice | state: closed
"Note that we have an AzureAD provider, which gives us `Camel.Case@email.address`. Should these then get automatically mapped insided of Grafana?",,,,,,Anecdotal,comment,,,,,,,,2023-06-26,github/AndreasBergmeier6176,https://github.com/grafana/grafana/issues/41833#issuecomment-1607066303,repo: grafana/grafana | issue: Enforce lowercase usernames and email addresses for all new accounts | keyword: best practice
This issue has been automatically marked as stale because it has not had activity in the last year. It will be closed in 30 days if no further activity occurs. Please feel free to leave a comment if you believe the issue is still relevant. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-06-11,github/github-actions[bot],https://github.com/grafana/grafana/issues/41833#issuecomment-2961021027,repo: grafana/grafana | issue: Enforce lowercase usernames and email addresses for all new accounts | keyword: best practice
This issue has been automatically closed because it has not had any further activity in the last 30 days. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-07-11,github/github-actions[bot],https://github.com/grafana/grafana/issues/41833#issuecomment-3060001471,repo: grafana/grafana | issue: Enforce lowercase usernames and email addresses for all new accounts | keyword: best practice
"Alerting: Wrap all `AlertingPageWrapper`s in `StrictMode` **What is this feature?** There have been some issues within alerting that _might_ have been made more visible by using [StrictMode](https://react.dev/reference/react/StrictMode). Rather than opt the entire Grafana UI into this, we can wrap any (or, most) alerting UI in StrictMode when running locally, so we're more likely to spot issues moving forward **Why do we need this feature?** 🧹, 🐛 and best practices (we previously had StrictMode…",,,,,,Anecdotal,issue,,,,,,,,2024-10-25,github/tomratcliffe,https://github.com/grafana/grafana/pull/95390,repo: grafana/grafana | keyword: best practice | state: closed
This pull request has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in 2 weeks if no further activity occurs. Please feel free to give a status update or ping for review. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2024-11-25,github/github-actions[bot],https://github.com/grafana/grafana/pull/95390#issuecomment-2496530112,repo: grafana/grafana | issue: Alerting: Wrap all `AlertingPageWrapper`s in `StrictMode` | keyword: best practice
This pull request has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in 2 weeks if no further activity occurs. Please feel free to give a status update or ping for review. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-01-11,github/github-actions[bot],https://github.com/grafana/grafana/pull/95390#issuecomment-2585001470,repo: grafana/grafana | issue: Alerting: Wrap all `AlertingPageWrapper`s in `StrictMode` | keyword: best practice
This pull request has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in 2 weeks if no further activity occurs. Please feel free to give a status update or ping for review. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-02-17,github/github-actions[bot],https://github.com/grafana/grafana/pull/95390#issuecomment-2661792507,repo: grafana/grafana | issue: Alerting: Wrap all `AlertingPageWrapper`s in `StrictMode` | keyword: best practice
This pull request has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in 2 weeks if no further activity occurs. Please feel free to give a status update or ping for review. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-03-22,github/github-actions[bot],https://github.com/grafana/grafana/pull/95390#issuecomment-2744893676,repo: grafana/grafana | issue: Alerting: Wrap all `AlertingPageWrapper`s in `StrictMode` | keyword: best practice
This pull request has been automatically closed because it has not had any further activity in the last 2 weeks. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-04-05,github/github-actions[bot],https://github.com/grafana/grafana/pull/95390#issuecomment-2780137633,repo: grafana/grafana | issue: Alerting: Wrap all `AlertingPageWrapper`s in `StrictMode` | keyword: best practice
This pull request has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in 2 weeks if no further activity occurs. Please feel free to give a status update or ping for review. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-05-15,github/github-actions[bot],https://github.com/grafana/grafana/pull/95390#issuecomment-2882013514,repo: grafana/grafana | issue: Alerting: Wrap all `AlertingPageWrapper`s in `StrictMode` | keyword: best practice
"This should still be done, but there is a test/some functionality within the edit contact points flow that is indeed broken. The initialisation of a template value is incorrect when we have strict mode enabled",,,,,,Anecdotal,comment,,,,,,,,2025-05-22,github/tomratcliffe,https://github.com/grafana/grafana/pull/95390#issuecomment-2901817332,repo: grafana/grafana | issue: Alerting: Wrap all `AlertingPageWrapper`s in `StrictMode` | keyword: best practice
This pull request has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in 2 weeks if no further activity occurs. Please feel free to give a status update or ping for review. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-06-22,github/github-actions[bot],https://github.com/grafana/grafana/pull/95390#issuecomment-2993883352,repo: grafana/grafana | issue: Alerting: Wrap all `AlertingPageWrapper`s in `StrictMode` | keyword: best practice
This pull request has been automatically closed because it has not had any further activity in the last 2 weeks. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-07-06,github/github-actions[bot],https://github.com/grafana/grafana/pull/95390#issuecomment-3040549246,repo: grafana/grafana | issue: Alerting: Wrap all `AlertingPageWrapper`s in `StrictMode` | keyword: best practice
Error 'Permissions needed: dashboards:read' when switching from 8.X to 9.X for anonymous access **What happened**: I was trying to update Grafana to newest for [CNCF DevStats](https://cncf.teststats.cncf.io) project (8.5.11 -> 9.1.3) and: - This is [the version](https://envoy.devstats.cncf.io) that still uses 8.5.11 - everything work OK. - This is [the version](https://cncf.teststats.cncf.io) where I'm attempting to use 9.1.3 - it fails for every single dashboard with: ``` Error You'll need add…,,,,,,Anecdotal,issue,,,,,,,,2022-09-09,github/lukaszgryglicki,https://github.com/grafana/grafana/issues/54974,repo: grafana/grafana | keyword: best practice | state: closed
Hey. Thanks for reporting this :) I may have a suspicion on what the issue is but I need a little more info to confirm if it is the same problem. 1. Are you using provisioning to create your dashboards? 2. If you look at the permissions for one dashboard is only admin listed there?,,,,,,Anecdotal,comment,,,,,,,,2022-09-17,github/kalleep,https://github.com/grafana/grafana/issues/54974#issuecomment-1250028478,repo: grafana/grafana | issue: Error 'Permissions needed: dashboards:read' when switching from 8.X to 9.X for anonymous access | keyword: best practice
"1. I'm not using provisioning. 2. I'm not adding any permissions - all those dashboards are public - should be available for all users - there is no sign in or anything like that in this project. 3. When I log in as admin (this is the only user who can edit them) I can see permissions only for admin - but I didn't add/change them <img width=""2233"" alt=""Zrzut ekranu 2022-09-19 o 07 33 49"" src=""https://user-images.githubusercontent.com/2469783/190955085-7202646a-3140-4b24-8486-6fcd364fd144.png""> …",,,,,,Anecdotal,comment,,,,,,,,2022-09-19,github/lukaszgryglicki,https://github.com/grafana/grafana/issues/54974#issuecomment-1250590090,repo: grafana/grafana | issue: Error 'Permissions needed: dashboards:read' when switching from 8.X to 9.X for anonymous access | keyword: best practice
"@lukaszgryglicki thanks for getting back with more details. We are still investigating the issue, and have a hunch where the root cause is, however can't verify completely. With Grafana 9+ we have turned on RBAC (role-based access control) as an underlying system for access control in Grafana. When RBAC is being turned on, there are data migrations happening and we suspect that you are either facing a bug which we had or something went wrong with the migrations. To verify this assumption, I can…",,,,,,Anecdotal,comment,,,,,,,,2022-09-20,github/vtorosyan,https://github.com/grafana/grafana/issues/54974#issuecomment-1252179049,repo: grafana/grafana | issue: Error 'Permissions needed: dashboards:read' when switching from 8.X to 9.X for anonymous access | keyword: best practice
"Hi, don't have time for it, working 12+ hours everyday, I'll just wait when this is resolved and switch to `9.X` only then. Also Grafana runs in K8s so I would have to exec bash inbto container then install sqlite there to do queries - no time blocks me.",,,,,,Anecdotal,comment,,,,,,,,2022-09-23,github/lukaszgryglicki,https://github.com/grafana/grafana/issues/54974#issuecomment-1256294101,repo: grafana/grafana | issue: Error 'Permissions needed: dashboards:read' when switching from 8.X to 9.X for anonymous access | keyword: best practice
"If this is the only way, I will try to add this to my init scripts for grafana pods. But not earlier than on Friday or next Monday.",,,,,,Anecdotal,comment,,,,,,,,2022-10-03,github/lukaszgryglicki,https://github.com/grafana/grafana/issues/54974#issuecomment-1265559302,repo: grafana/grafana | issue: Error 'Permissions needed: dashboards:read' when switching from 8.X to 9.X for anonymous access | keyword: best practice
"Hi @lukaszgryglicki, sorry about late response. Have resetting the migrations as described [here](https://github.com/grafana/grafana/issues/54974#issuecomment-1252179049) helped? You would need to run that SQL only once and after that everything should be stable. If the SQL above does not fix the issue - I'd recommend to bump to the latest minor (if possible), as we had many other fixes which might be the root cause of what you experience. We are also [updating](https://github.com/grafana/grafa…",,,,,,Anecdotal,comment,,,,,,,,2022-12-02,github/vtorosyan,https://github.com/grafana/grafana/issues/54974#issuecomment-1334991262,repo: grafana/grafana | issue: Error 'Permissions needed: dashboards:read' when switching from 8.X to 9.X for anonymous access | keyword: best practice
"As I said, I can run it inside a grafana pod, but it will be lost when pod gets recreated. I can add this to pod startup script and see if that helps. But I will try newest 9.X first, I'll LYK.",,,,,,Anecdotal,comment,,,,,,,,2022-12-02,github/lukaszgryglicki,https://github.com/grafana/grafana/issues/54974#issuecomment-1335032595,repo: grafana/grafana | issue: Error 'Permissions needed: dashboards:read' when switching from 8.X to 9.X for anonymous access | keyword: best practice
"So updating to the newest `9.1.3` is `NOT` solving the issue: <img width=""2359"" alt=""Zrzut ekranu 2022-12-2 o 12 01 17"" src=""https://user-images.githubusercontent.com/2469783/205278310-5a36ed06-97e5-4fc7-9c8e-32077d310acd.png""> Trying SQL approach...",,,,,,Anecdotal,comment,,,,,,,,2022-12-02,github/lukaszgryglicki,https://github.com/grafana/grafana/issues/54974#issuecomment-1335079402,repo: grafana/grafana | issue: Error 'Permissions needed: dashboards:read' when switching from 8.X to 9.X for anonymous access | keyword: best practice
OK I've `bash`'ed to Grafana pod running 9.1.3 and did: ``` vim sql.sql cat sql.sql DELETE FROM builtin_role where role_id IN (SELECT id FROM role WHERE name LIKE 'managed:%'); DELETE FROM team_role where role_id IN (SELECT id FROM role WHERE name LIKE 'managed:%'); DELETE FROM user_role where role_id IN (SELECT id FROM role WHERE name LIKE 'managed:%'); DELETE FROM permission where role_id IN (SELECT id FROM role WHERE name LIKE 'managed:%'); DELETE FROM role WHERE name LIKE 'managed:%'; DELET…,,,,,,Anecdotal,comment,,,,,,,,2022-12-02,github/lukaszgryglicki,https://github.com/grafana/grafana/issues/54974#issuecomment-1335100175,repo: grafana/grafana | issue: Error 'Permissions needed: dashboards:read' when switching from 8.X to 9.X for anonymous access | keyword: best practice
"Reverted back to 8.5.15 (but kept that SQL in my init scripts, it seems to make no harm on 8.X and doesn't help in 9.X), still cannot use 9.x :(",,,,,,Anecdotal,comment,,,,,,,,2022-12-02,github/lukaszgryglicki,https://github.com/grafana/grafana/issues/54974#issuecomment-1335122820,repo: grafana/grafana | issue: Error 'Permissions needed: dashboards:read' when switching from 8.X to 9.X for anonymous access | keyword: best practice
"@lukaszgryglicki can you please share how is the provisioning of dashboards and folders are done? If the provisioning is not done through the interfaces that Grafana provides (e.g. API/provisioning files/terraform/UI), then the permissions that new access control system generates won't be generated. For example, if the provisioning is done by manual data ingestion, the permissions system won't work.",,,,,,Anecdotal,comment,,,,,,,,2022-12-05,github/vtorosyan,https://github.com/grafana/grafana/issues/54974#issuecomment-1337609801,repo: grafana/grafana | issue: Error 'Permissions needed: dashboards:read' when switching from 8.X to 9.X for anonymous access | keyword: best practice
"It is done manually, so the question is how can I fix permissions after they are done manually: - https://github.com/cncf/devstats/blob/master/grafana/shared/grafana_start.sh - https://github.com/cncf/devstats/blob/master/grafana/shared/update_sqlite.sql",,,,,,Anecdotal,comment,,,,,,,,2022-12-06,github/lukaszgryglicki,https://github.com/grafana/grafana/issues/54974#issuecomment-1338815222,repo: grafana/grafana | issue: Error 'Permissions needed: dashboards:read' when switching from 8.X to 9.X for anonymous access | keyword: best practice
Any updates? This issue is open for 3 months and 5 days already and I provided all info that I was asked for and also tried suggested solution and its not working? Any hope on this?,,,,,,Anecdotal,comment,,,,,,,,2022-12-14,github/lukaszgryglicki,https://github.com/grafana/grafana/issues/54974#issuecomment-1351864910,repo: grafana/grafana | issue: Error 'Permissions needed: dashboards:read' when switching from 8.X to 9.X for anonymous access | keyword: best practice
"@lukaszgryglicki thank you for the patience. At the moment I am afraid we don't have a good strategy to provide or promise any solution when the data is manipulated manually. The access control system has grown with it's complexity quite a bit with the 9.x release, and we are aiming for standardized experience of provisioning as supporting all possible scenarios is not feasible anymore. I understand this is a show stopper for upgrade, sorry about that. Our recommendation would be to use one of …",,,,,,Anecdotal,comment,,,,,,,,2023-01-02,github/vtorosyan,https://github.com/grafana/grafana/issues/54974#issuecomment-1368877318,repo: grafana/grafana | issue: Error 'Permissions needed: dashboards:read' when switching from 8.X to 9.X for anonymous access | keyword: best practice
"Hi, I can't, I have a custom deployment. Is there a way of fixing this via some SQL script? Myabe just another one? Not deleting but maybe adding some default permissions/entries in tables(s)?",,,,,,Anecdotal,comment,,,,,,,,2023-01-02,github/lukaszgryglicki,https://github.com/grafana/grafana/issues/54974#issuecomment-1368881555,repo: grafana/grafana | issue: Error 'Permissions needed: dashboards:read' when switching from 8.X to 9.X for anonymous access | keyword: best practice
"Hey, I'm afraid supporting custom SQL script or hacking around the way is not something we could commit to do and support long term, as the underlying complexity is not trivial to handle outside of the system boundaries. Even if we come up something, it might stop working in near future as there is a lot of improvements and changes happening in Grafana. I understand that effort of using the supported methods is high, however that would bring stability long term and we could support better on ou…",,,,,,Anecdotal,comment,,,,,,,,2023-01-19,github/vtorosyan,https://github.com/grafana/grafana/issues/54974#issuecomment-1396632101,repo: grafana/grafana | issue: Error 'Permissions needed: dashboards:read' when switching from 8.X to 9.X for anonymous access | keyword: best practice
i don't want custom SQL or anything like this - I just want to know how to enable any user to access dashboards - just the way it worked in 8.x.,,,,,,Anecdotal,comment,,,,,,,,2023-01-19,github/lukaszgryglicki,https://github.com/grafana/grafana/issues/54974#issuecomment-1396691419,repo: grafana/grafana | issue: Error 'Permissions needed: dashboards:read' when switching from 8.X to 9.X for anonymous access | keyword: best practice
"@lukaszgryglicki Unfortunately with the OSS and without using available public interfaces for provisioning resources, I can't offer any workaround that would make it work with the Grafana >9.",,,,,,Anecdotal,comment,,,,,,,,2023-02-20,github/vtorosyan,https://github.com/grafana/grafana/issues/54974#issuecomment-1436860568,repo: grafana/grafana | issue: Error 'Permissions needed: dashboards:read' when switching from 8.X to 9.X for anonymous access | keyword: best practice
I face the same issue with grafana 10.2.0 Dashboards are imported via: ``` apiVersion: 1 providers: - name: 'ServiceDescription' orgId: 1 folder: '' type: file disableDeletion: false updateIntervalSeconds: 60 #how often Grafana will scan for changed dashboards options: path: /var/lib/my-dashboards foldersFromFilesStructure: true ``` My config (grafana.ini): ``` [security] allow_embedding = true x_xss_protection = false [auth] disable_login_form = true oauth_auto_login = true oauth_allow_insecur…,,,,,,Anecdotal,comment,,,,,,,,2023-10-25,github/GreenRover,https://github.com/grafana/grafana/issues/54974#issuecomment-1779204820,repo: grafana/grafana | issue: Error 'Permissions needed: dashboards:read' when switching from 8.X to 9.X for anonymous access | keyword: best practice
docs(alerting): Add `Detect missing series in Prometheus` section to the MissingData guide Add `Detect missing series in Prometheus` section to the MissingData guide ⭐ [Preview](https://deploy-preview-grafana-107329-zb444pucvq-vp.a.run.app/docs/grafana/latest/alerting/best-practices/missing-data/),,,,,,Anecdotal,issue,,,,,,,,2025-06-27,github/ppcano,https://github.com/grafana/grafana/pull/107329,repo: grafana/grafana | keyword: best practice | state: closed
The backport to `release-12.0.3` failed: ``` error cherry-picking: error running git cherry-pick: error running command 'git cherry-pick -x 2f1a6ae1712b4ab22560fc9a1e7067ded43cfb7d' error: exit status 1 stdout: Auto-merging docs/sources/alerting/best-practices/missing-data.md CONFLICT (content): Merge conflict in docs/sources/alerting/best-practices/missing-data.md Auto-merging docs/sources/alerting/fundamentals/alert-rule-evaluation/stale-alert-instances.md stderr: error: could not apply 2f1a6…,,,,,,Anecdotal,comment,,,,,,,,2025-06-27,github/grafana-delivery-bot[bot],https://github.com/grafana/grafana/pull/107329#issuecomment-3014130932,repo: grafana/grafana | issue: docs(alerting): Add `Detect missing series in Prometheus` section to the MissingData guide | keyword: best practice
"WIP: Semantic conventions for Grafana's log/span attributes **What is this feature?** Not sure about this yet, but something I've been thinking about for a while inspired by the [otel semantic conventions](https://opentelemetry.io/docs/specs/semconv/). Open for feedback, 👍 or 👎 . The main idea with o11ysomconv package is to provide all the common attributes that we're using all around the code base. For specific attributes to certain feature etc I imagine that each feature implements attributes…",,,,,,Anecdotal,issue,,,,,,,,2024-06-05,github/marefr,https://github.com/grafana/grafana/pull/88815,repo: grafana/grafana | keyword: best practice | state: closed
"Hi! Greetings from the Release Guild. Just letting you know I'm updating your PR's milestone to [11.2.x](https://github.com/grafana/grafana/milestone/529). If you need to see your changes in a previous version, please consider adding a backport label. Thank you!",,,,,,Anecdotal,comment,,,,,,,,2024-06-14,github/diegommm,https://github.com/grafana/grafana/pull/88815#issuecomment-2168797520,repo: grafana/grafana | issue: WIP: Semantic conventions for Grafana's log/span attributes | keyword: best practice
This pull request has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in 2 weeks if no further activity occurs. Please feel free to give a status update or ping for review. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2024-07-15,github/github-actions[bot],https://github.com/grafana/grafana/pull/88815#issuecomment-2227589207,repo: grafana/grafana | issue: WIP: Semantic conventions for Grafana's log/span attributes | keyword: best practice
This pull request has been automatically closed because it has not had any further activity in the last 2 weeks. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2024-07-30,github/github-actions[bot],https://github.com/grafana/grafana/pull/88815#issuecomment-2257310686,repo: grafana/grafana | issue: WIP: Semantic conventions for Grafana's log/span attributes | keyword: best practice
"Saga: POC to connect tokens and themes to Token Studio <!-- Thank you for sending a pull request! Here are some tips: 1. If this is your first time, please read our contribution guide at https://github.com/grafana/grafana/blob/main/CONTRIBUTING.md 2. Ensure you include and run the appropriate tests as part of your Pull Request. 3. In a new feature or configuration option, an update to the documentation is necessary. Everything related to the documentation is under the docs folder in the root of…",,,,,,Anecdotal,issue,,,,,,,,2024-02-06,github/JoaoSilvaGrafana,https://github.com/grafana/grafana/pull/81977,repo: grafana/grafana | keyword: best practice | state: closed
[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/grafana/grafana?pullRequest=81977) <br/>All committers have signed the CLA.,,,,,,Anecdotal,comment,,,,,,,,2024-02-06,github/CLAassistant,https://github.com/grafana/grafana/pull/81977#issuecomment-1930203854,repo: grafana/grafana | issue: Saga: POC to connect tokens and themes to Token Studio | keyword: best practice
"> I might be missing smth, but I don't see any logic in this PR for actually connecting the tokens? Is that done outside of grafana? Token Studio just has to point at a folder to be able to sync the tokens, so that's where the connection takes place, now with styled-dictionary we will read those json files and convert them to usable tokens 👍",,,,,,Anecdotal,comment,,,,,,,,2024-02-14,github/JoaoSilvaGrafana,https://github.com/grafana/grafana/pull/81977#issuecomment-1943649290,repo: grafana/grafana | issue: Saga: POC to connect tokens and themes to Token Studio | keyword: best practice
"> Is the thinking to use themes/dark.js and themes/light.js and map that to the input of createTheme? The goal at the start is just to try to use those theme objects and see how it would work if we used them directly. Then at the end of this POC we want to think how would this interact with the current themes, can we make it work with the current createTheme/useTheme or would it require us to create new things and have a migration process.",,,,,,Anecdotal,comment,,,,,,,,2024-03-01,github/JoaoSilvaGrafana,https://github.com/grafana/grafana/pull/81977#issuecomment-1972965793,repo: grafana/grafana | issue: Saga: POC to connect tokens and themes to Token Studio | keyword: best practice
"@JoaoSilvaGrafana another big thing missing in this POC is a translation where we build GrafanaThemeV2 from the new v3 theme, I strongly recommend against a new theme type & architecture, but if it's a road we want to explore we need to be able to build the old theme from new theme so old components that use v2 theme will get the latest color values. This is how the migration between v1 to v2 worked, the v1 theme was built from the new authoritative v2 theme so any color change in the new theme…",,,,,,Anecdotal,comment,,,,,,,,2024-04-15,github/torkelo,https://github.com/grafana/grafana/pull/81977#issuecomment-2055204673,repo: grafana/grafana | issue: Saga: POC to connect tokens and themes to Token Studio | keyword: best practice
"> @JoaoSilvaGrafana another big thing missing in this POC is a translation where we build GrafanaThemeV2 from the new v3 theme, > > I strongly recommend against a new theme type & architecture, but if it's a road we want to explore we need to be able to build the old theme from new theme so old components that use v2 theme will get the latest color values. > > This is how the migration between v1 to v2 worked, the v1 theme was built from the new authoritative v2 theme so any color change in the…",,,,,,Anecdotal,comment,,,,,,,,2024-04-15,github/JoaoSilvaGrafana,https://github.com/grafana/grafana/pull/81977#issuecomment-2056753255,repo: grafana/grafana | issue: Saga: POC to connect tokens and themes to Token Studio | keyword: best practice
This pull request has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in 2 weeks if no further activity occurs. Please feel free to give a status update or ping for review. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2024-05-23,github/github-actions[bot],https://github.com/grafana/grafana/pull/81977#issuecomment-2126045268,repo: grafana/grafana | issue: Saga: POC to connect tokens and themes to Token Studio | keyword: best practice
"Logs Panel: Remove infoThresh parameter from ufuzzy call **What is this feature?** The current settings for ufuzzy are unoptimized for our use-case, we are not utilizing highlighting or using the ufuzzy rank. As such we should be using the default settings, and omit this parameter. **Why do we need this feature?** Potential optimization, better adherence to ufuzzy best practices **Who is this feature for?** @leeoniya 😆 Fixes N/A **Special notes for your reviewer:** This does not appear to have …",,,,,,Anecdotal,issue,,,,,,,,2023-12-11,github/gtk-grafana,https://github.com/grafana/grafana/pull/79356,repo: grafana/grafana | keyword: best practice | state: closed
"if you're using `order` you are using ranking. lemme check this out before you merge. if all you want is matching, the threshold should be 0 and you should just use `idxs`",,,,,,Anecdotal,comment,,,,,,,,2023-12-11,github/leeoniya,https://github.com/grafana/grafana/pull/79356#issuecomment-1850812912,repo: grafana/grafana | issue: Logs Panel: Remove infoThresh parameter from ufuzzy call | keyword: best practice
"POC/K8s: grafana apiserver (with custom storage) This POC builds from lessons learned in https://github.com/grafana/grafana/pull/66218 and https://github.com/grafana/grafana/pull/66219 In this approach, we are using a kubernetes extension apiserver https://github.com/grafana/grafana-apiserver and then implementing storage directly (currently SQL) ### Run with the following feature toggles ``` [feature_toggles] grafanaAPIServer = true entityStore = true storage = true ``` ### For kubectl support…",,,,,,Anecdotal,issue,,,,,,,,2023-05-05,github/toddtreece,https://github.com/grafana/grafana/pull/67945,repo: grafana/grafana | keyword: lesson learned | state: closed
"This pull request has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in 2 weeks if no further activity occurs. Please feel free to give a status update now, ping for review, or re-open when it's ready. Thank you for your contributions!",,,,,,Anecdotal,comment,,,,,,,,2023-10-23,github/github-actions[bot],https://github.com/grafana/grafana/pull/67945#issuecomment-1774306837,repo: grafana/grafana | issue: POC/K8s: grafana apiserver (with custom storage) | keyword: lesson learned
"This pull request has been automatically closed because it has not had activity in the last 2 weeks. Please feel free to give a status update now, ping for review, or re-open when it's ready. Thank you for your contributions!",,,,,,Anecdotal,comment,,,,,,,,2023-11-06,github/github-actions[bot],https://github.com/grafana/grafana/pull/67945#issuecomment-1793956989,repo: grafana/grafana | issue: POC/K8s: grafana apiserver (with custom storage) | keyword: lesson learned
"SQL Expressions: Add more functions to the allowlist This PR adds more functions (and more nodes) to the allowlist of safe functions and nodes. This PR replaces https://github.com/grafana/grafana/pull/102026 When writing this PR, I asked the Cursor Agent (Claude 3.7) to - Add all the functions present in #102026, but - Omit those that got added in #103157 I made sure that the tests cases would also run in SQL Fiddle, and ran them there, to check that the SQL is valid and the results can be eyeb…",,,,,,Anecdotal,issue,,,,,,,,2025-04-07,github/samjewell,https://github.com/grafana/grafana/pull/103546,repo: grafana/grafana | keyword: lesson learned | state: closed
`NTILE` is not supported yet in Dolt itself - see https://docs.dolthub.com/sql-reference/sql-support/expressions-functions-operators,,,,,,Anecdotal,comment,,,,,,,,2025-04-08,github/samjewell,https://github.com/grafana/grafana/pull/103546#issuecomment-2785554664,repo: grafana/grafana | issue: SQL Expressions: Add more functions to the allowlist | keyword: lesson learned
"Alerting: message templates are dropped/reset for unknown reasons ### What happened? We've got Grafana Alerting configured using the `Slack` contact point to send webhooks to Mattermost. We configured our own notification templates and are using them in the notifier via the ""Title"" and ""Text Body"" optional slack settings. This works well for some time until at one point the templates are dropped and the alerts are sent using the default template. To get back our own template we have to edit the…",,,,,,Anecdotal,issue,,,,,,,,2024-05-29,github/Z3po,https://github.com/grafana/grafana/issues/88405,repo: grafana/grafana | keyword: lesson learned | state: closed
I just recognized that this issue was caused by wrong usage on my side :facepalm: .... This whole topic of alert message templating absolutely needs more documentation. To create my own template I just went ahead copied the default template from the github repository and modified it to my needs. I changed the names from `default.*` to `mattermost.title` and `mattermost.body` but I missed the fact that I was overwriting the `__text_alert_list` template. Now this caused my template to be used whe…,,,,,,Anecdotal,comment,,,,,,,,2024-06-07,github/Z3po,https://github.com/grafana/grafana/issues/88405#issuecomment-2154508640,repo: grafana/grafana | issue: Alerting: message templates are dropped/reset for unknown reasons | keyword: lesson learned
"Hey @Z3po, Looks like I added that label by mistake—thanks for catching that! And big thanks for sharing your solution to this issue—it’s super helpful. By the way, we’ve recently updated the documentation on templating, so it’s worth checking out: [Templating Notifications Docs](https://grafana.com/docs/grafana/latest/alerting/configure-notifications/template-notifications/). Hint: It would be very nice if the default templates would just be visible in the Grafana Frontend. There's no place to…",,,,,,Anecdotal,comment,,,,,,,,2025-01-08,github/tonypowa,https://github.com/grafana/grafana/issues/88405#issuecomment-2577017134,repo: grafana/grafana | issue: Alerting: message templates are dropped/reset for unknown reasons | keyword: lesson learned
"Chore: uFuzzy 1.0.6 Fixes some minor search bugs, plus an additional fix for debounce in Prometheus Metric Encyclopedia",,,,,,Anecdotal,issue,,,,,,,,2023-03-09,github/leeoniya,https://github.com/grafana/grafana/pull/64575,repo: grafana/grafana | keyword: lesson learned | state: closed
"The backport to `v9.4.x` failed: ``` The process '/usr/bin/git' failed with exit code 1 ``` To backport manually, run these commands in your terminal: ```bash # Fetch latest updates from GitHub git fetch # Create a new branch git switch --create backport-64575-to-v9.4.x origin/v9.4.x # Cherry-pick the merged commit of this pull request and resolve the conflicts git cherry-pick -x 14251db9bad9846f88a6816f04681751a639431e # Push it to GitHub git push --set-upstream origin backport-64575-to-v9.4.x…",,,,,,Anecdotal,comment,,,,,,,,2023-03-10,github/grafanabot,https://github.com/grafana/grafana/pull/64575#issuecomment-1463692351,repo: grafana/grafana | issue: Chore: uFuzzy 1.0.6 | keyword: lesson learned
there's too much diff to backport here. moving to 9.5.0. @bohandley we should manually backport [MetricEncyclopediaModal.tsx](https://github.com/grafana/grafana/pull/64575/files#diff-3eee20a8edbaf82dbaac97cabbfb6a6823d30905021f2d46aa47a3fe7fb20551) as part of #64577. there's a lesson here for me. let's see if i've learned anything for next time :sweat_smile: .,,,,,,Anecdotal,comment,,,,,,,,2023-03-10,github/leeoniya,https://github.com/grafana/grafana/pull/64575#issuecomment-1463897695,repo: grafana/grafana | issue: Chore: uFuzzy 1.0.6 | keyword: lesson learned
"Docker image :latest is missing ARM variants <!-- Please use this template to create your bug report. By providing as much info as possible you help us understand the issue, reproduce it and resolve it for you quicker. Therefore take a couple of extra minutes to make sure you have provided all info needed. PROTIP: record your screen and attach it as a gif to showcase the issue. - Questions should be posted to: https://community.grafana.com - Use query inspector to troubleshoot issues: https://b…",,,,,,Anecdotal,issue,,,,,,,,2022-04-13,github/p-schneider,https://github.com/grafana/grafana/issues/47676,repo: grafana/grafana | keyword: lesson learned | state: closed
"Same problem, and if use `:latest`, then in docker logs: `standard_init_linux.go:228: exec user process caused: exec format error`",,,,,,Anecdotal,comment,,,,,,,,2022-04-13,github/S474N,https://github.com/grafana/grafana/issues/47676#issuecomment-1098340258,repo: grafana/grafana | issue: Docker image :latest is missing ARM variants | keyword: lesson learned
"I noticed that builds uploaded by the @grafanaci user have arm64, only the upload by @dsotirakis two days ago has the missing builds.",,,,,,Anecdotal,comment,,,,,,,,2022-04-14,github/dneary,https://github.com/grafana/grafana/issues/47676#issuecomment-1099323790,repo: grafana/grafana | issue: Docker image :latest is missing ARM variants | keyword: lesson learned
Just came here to say this. My raspberry pi cluster has been complaining since a few days ago. I guess this is a valid reason to stop using “latest” on images. Lesson learned I think. :),,,,,,Anecdotal,comment,,,,,,,,2022-04-15,github/tomthetommy,https://github.com/grafana/grafana/issues/47676#issuecomment-1100240591,repo: grafana/grafana | issue: Docker image :latest is missing ARM variants | keyword: lesson learned
"Hello all! Thanks for raising this. We accidentally tagged latest with the wrong version, then we force-pushed the correct `latest` image and the archs were messed up. Normally, our `latest` images include all architectures. Will fix asap. 🙏",,,,,,Anecdotal,comment,,,,,,,,2022-04-18,github/dsotirakis,https://github.com/grafana/grafana/issues/47676#issuecomment-1101188512,repo: grafana/grafana | issue: Docker image :latest is missing ARM variants | keyword: lesson learned
"Hello again! Should be fixed by now: ![image](https://user-images.githubusercontent.com/15115078/163782720-e5588aea-6c4c-4c3d-9686-c7c24c083eff.png) Also: * Changed the user from my personal one `dimsotirakis` (had docker hub permissions issues that are resolved now), to `grafanaci`. * `latest` tags point to `8.4.6` now. Please let me know if something still is/looks wrong! Thanks!",,,,,,Anecdotal,comment,,,,,,,,2022-04-18,github/dsotirakis,https://github.com/grafana/grafana/issues/47676#issuecomment-1101228777,repo: grafana/grafana | issue: Docker image :latest is missing ARM variants | keyword: lesson learned
"> Now are :main and :latest same version 8.4.6? `main` tag will change on the next merge against the OSS main branch. > Where is 9.0? Hang in there, not ready yet! 😉",,,,,,Anecdotal,comment,,,,,,,,2022-04-18,github/dsotirakis,https://github.com/grafana/grafana/issues/47676#issuecomment-1101248566,repo: grafana/grafana | issue: Docker image :latest is missing ARM variants | keyword: lesson learned
"`:latest` and `:8.4.6` (incl. `-ubuntu` in both oss and enterprise) looks good to me now. `:main[ubuntu]` currenty points to the same images as `:latest` / `:8.4.6`, I assume that will change with the next commit to the main branch. Thank you!",,,,,,Anecdotal,comment,,,,,,,,2022-04-18,github/p-schneider,https://github.com/grafana/grafana/issues/47676#issuecomment-1101248931,repo: grafana/grafana | issue: Docker image :latest is missing ARM variants | keyword: lesson learned
"> @dsotirakis yesterday was 9.0.0 on :latest, look: That was on the `main` tag. Will change again soon as mentioned above.",,,,,,Anecdotal,comment,,,,,,,,2022-04-18,github/dsotirakis,https://github.com/grafana/grafana/issues/47676#issuecomment-1101256192,repo: grafana/grafana | issue: Docker image :latest is missing ARM variants | keyword: lesson learned
Karma to Jest: history_ctrl Ref #12224 Lessons learned: do not call async code in constructor. - Updated `ts-jest` version - Added .gitignore - Converted history_ctrl to Jest,,,,,,Anecdotal,issue,,,,,,,,2018-06-21,github/tskarhed,https://github.com/grafana/grafana/pull/12367,repo: grafana/grafana | keyword: lesson learned | state: closed
"Feature Request (Grafana 2.0): LDAP / Active Directory authentication and import groups I need to authenticate user against an Active Directory Backend. We need also import all ""memberOf"" groups as main groups to organize dashboards. Could be possible that every user can see / edit (depending on the group permissions ) the dashboards contained on the groups where he belongs ?",,,,,,Anecdotal,issue,,,,,,,,2015-02-10,github/toni-moreno,https://github.com/grafana/grafana/issues/1450,repo: grafana/grafana | keyword: lesson learned | state: closed
"This is planned for Grafana 2.0 release but I am not 100% certain that it will make it, might be pushed to 2.1 release.",,,,,,Anecdotal,comment,,,,,,,,2015-02-10,github/torkelo,https://github.com/grafana/grafana/issues/1450#issuecomment-73709899,repo: grafana/grafana | issue: Feature Request (Grafana 2.0): LDAP / Active Directory authentication and import groups | keyword: lesson learned
":+1: very interested to see LDAP supported A question: Would it be possible at some point to run grafana 2.0 without any user authentication, similar to grafana 1.0? For simple setups the whole user/organization layer adds unnecessary maintenance complexity.",,,,,,Anecdotal,comment,,,,,,,,2015-03-16,github/costimuraru,https://github.com/grafana/grafana/issues/1450#issuecomment-81632513,repo: grafana/grafana | issue: Feature Request (Grafana 2.0): LDAP / Active Directory authentication and import groups | keyword: lesson learned
@costimuraru You also need to specify org_name and org_role for anonymous users: ``` [auth.anonymous] ; enable anonymous access enabled = false ; specify organization name that should be used for unauthenticated users org_name = Main Org. ; specify role for unauthenticated users org_role = Viewer ```,,,,,,Anecdotal,comment,,,,,,,,2015-03-17,github/torkelo,https://github.com/grafana/grafana/issues/1450#issuecomment-82395058,repo: grafana/grafana | issue: Feature Request (Grafana 2.0): LDAP / Active Directory authentication and import groups | keyword: lesson learned
"@torkelo yup, I've specified the other params as well. I eventually managed to get the anonymous mode working. I created https://github.com/grafana/grafana/issues/1610 with my findings.",,,,,,Anecdotal,comment,,,,,,,,2015-03-17,github/costimuraru,https://github.com/grafana/grafana/issues/1450#issuecomment-82618152,repo: grafana/grafana | issue: Feature Request (Grafana 2.0): LDAP / Active Directory authentication and import groups | keyword: lesson learned
@torkelo This feature for delegated access control would be very useful for AD and LDAP. Is this being set for the 2.1 milestone?,,,,,,Anecdotal,comment,,,,,,,,2015-03-21,github/jmelfi,https://github.com/grafana/grafana/issues/1450#issuecomment-84455116,repo: grafana/grafana | issue: Feature Request (Grafana 2.0): LDAP / Active Directory authentication and import groups | keyword: lesson learned
"I m not sure what you mean? no it is not, there are no plans for per dashboard permissions, or plans for dashboard groups. Might come, but are a thousand other issues. Dashboard folders is something that might be added in 2.1, permissions on dashboard folder might come in 2.2, but remember permissions on dashboard has nothing to do with what metrics can be viewed it is only to filter the dashboard search, and to limit some users to not be able to view/edit some dashboard, not sure how useful th…",,,,,,Anecdotal,comment,,,,,,,,2015-03-21,github/torkelo,https://github.com/grafana/grafana/issues/1450#issuecomment-84460556,repo: grafana/grafana | issue: Feature Request (Grafana 2.0): LDAP / Active Directory authentication and import groups | keyword: lesson learned
"@jmelfi just to clarify, LDAP auth would be awesome, and hope to get it done in 2.1 but not sure how long / complex it is to implement",,,,,,Anecdotal,comment,,,,,,,,2015-03-21,github/torkelo,https://github.com/grafana/grafana/issues/1450#issuecomment-84460853,repo: grafana/grafana | issue: Feature Request (Grafana 2.0): LDAP / Active Directory authentication and import groups | keyword: lesson learned
"I was unsure of the complexity required to add this feature. The ability for dashboard level control would be nice for restricting edits to groups of users. Metrics would be read only by default. I am not sure the difficulty of having it filter metrics by groups would be beneficial overall. I don't really see too big of an issue if a person can see a dashboard, as long as they would be restricted from being able to alter/save changes to it if they are not authorized. Thanks for the quick reply …",,,,,,Anecdotal,comment,,,,,,,,2015-03-21,github/jmelfi,https://github.com/grafana/grafana/issues/1450#issuecomment-84464133,repo: grafana/grafana | issue: Feature Request (Grafana 2.0): LDAP / Active Directory authentication and import groups | keyword: lesson learned
"Is this still planned for version 2.1? LDAP auth would be useful since having users create new, separate, accounts is an unnecessary pita.",,,,,,Anecdotal,comment,,,,,,,,2015-04-02,github/rreadman,https://github.com/grafana/grafana/issues/1450#issuecomment-88834755,repo: grafana/grafana | issue: Feature Request (Grafana 2.0): LDAP / Active Directory authentication and import groups | keyword: lesson learned
+1 Can this be added to the 2.1 milestone so that it pops in on the filter if it is in the pipeline for the 2.1 release? We greatly desire the LDAP integration in our authentication method.,,,,,,Anecdotal,comment,,,,,,,,2015-04-15,github/jmelfi,https://github.com/grafana/grafana/issues/1450#issuecomment-93492611,repo: grafana/grafana | issue: Feature Request (Grafana 2.0): LDAP / Active Directory authentication and import groups | keyword: lesson learned
"singlestat coloring doesn't like '0' value norms? We have a number of queue metrics that are usually 0. The singlestat panel is grumpy about this, and displays its irritation by refusing to allow the 'coloring' feature to work. ![image](https://cloud.githubusercontent.com/assets/8292406/6002972/c071e7b2-aac5-11e4-9637-a0f34e273980.png) The settings for that white singlestat... ![image](https://cloud.githubusercontent.com/assets/8292406/6002979/d3313ba0-aac5-11e4-9910-5d2b73d67ae1.png) So it sho…",,,,,,Anecdotal,issue,,,,,,,,2015-02-02,github/christrotter,https://github.com/grafana/grafana/issues/1414,repo: grafana/grafana | keyword: lesson learned | state: closed
Container: latest tag on docker hub is an unreleased version of grafana ### What happened? I pulled `grafana/grafana:latest` and ran `grafana --version` in the container. Got `grafana version 12.2.0-16711121739` The version I got appears to correspond to this commit https://github.com/grafana/grafana/commit/8ac74e24df1ff9e5de0c1d0f78dcd8be12305b4e ### What did you expect to happen? I expected it to be the latest released version like `grafana version 12.1.0` ### Did this work before? I don't be…,,,,,,Anecdotal,issue,,,,,,,,2025-08-04,github/Starttoaster,https://github.com/grafana/grafana/issues/109161,repo: grafana/grafana | keyword: workaround | state: open
"If I may add to the topic: the tag 12.2.0 (without any -build number) is missing on Docker Hub, probably for this specific reason 😉 (and I miss it!!! 😅)",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/EHRETic,https://github.com/grafana/grafana/issues/109161#issuecomment-3178872041,repo: grafana/grafana | issue: Container: latest tag on docker hub is an unreleased version of grafana | keyword: workaround
"Strangely enough, the `:latest` tag got just updated and it's now an alias of `v11.4.8`. That is also an unreleased version?",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/Niek,https://github.com/grafana/grafana/issues/109161#issuecomment-3180149193,repo: grafana/grafana | issue: Container: latest tag on docker hub is an unreleased version of grafana | keyword: workaround
"> Strangely enough, the `:latest` tag got just updated and it's now an alias of `v11.4.8`. That is also an unreleased version? We just accidentally updated to this too, and it broke a lot of our alerts we provision with the provisioning API :joy: The previous image tag publishing strategy actually made sense, why did this change in such a broken way?",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/Starttoaster,https://github.com/grafana/grafana/issues/109161#issuecomment-3180225056,repo: grafana/grafana | issue: Container: latest tag on docker hub is an unreleased version of grafana | keyword: workaround
"Maybe it's not related, but why `grafana/grafana:latest` points to `12.1.1` and `grafana/grafana-oss:latest` points to `12.1.0`? 🤔",,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/Madh93,https://github.com/grafana/grafana/issues/109161#issuecomment-3185131105,repo: grafana/grafana | issue: Container: latest tag on docker hub is an unreleased version of grafana | keyword: workaround
"> Seems to be resolved now, `:latest` now point to 12.1.1. Check again, it's probably just because 12.1.1 was the last version they had released at the time you had written. They released another 11.x.x release since, it probably overwrote 12.1.1 again.",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/Starttoaster,https://github.com/grafana/grafana/issues/109161#issuecomment-3186830494,repo: grafana/grafana | issue: Container: latest tag on docker hub is an unreleased version of grafana | keyword: workaround
"You are right @Starttoaster... It updated again, now to an unreleased `v12.2.0-16952534486 (7d7721e30a)`. As a workaround, should we switch to the `grafana/grafana-oss:latest` image or does it have the same issue?",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/Niek,https://github.com/grafana/grafana/issues/109161#issuecomment-3186838831,repo: grafana/grafana | issue: Container: latest tag on docker hub is an unreleased version of grafana | keyword: workaround
"Right now it points to `10.1.1`. This is annoying, to say the least. Current hash is `sha256:1b9ca4bbc4a2eebda7de79834d61106efe57402a7720b3fb95f27b2b6793374f`",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/PatrickBaus,https://github.com/grafana/grafana/issues/109161#issuecomment-3197623685,repo: grafana/grafana | issue: Container: latest tag on docker hub is an unreleased version of grafana | keyword: workaround
"The `:latest` tag is basically unusable now, yes. Grafana is a fairly low-consequence part of our infrastructure. It can go down for a bit and it doesn't break our monitoring or alerting pipelines, and I can fix it with a simple helm rollback (we build our own more specifically versioned image off of `grafana/grafana:latest` so helm rollbacks are actually useful to us here.) This is why we pin to `:latest`, for ease of automatic updates.",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/Starttoaster,https://github.com/grafana/grafana/issues/109161#issuecomment-3198143190,repo: grafana/grafana | issue: Container: latest tag on docker hub is an unreleased version of grafana | keyword: workaround
"Grafana 12.1 upgrade issue - db migration ### What happened? We are running Grafana in Kubernetes with a PostgreSQL 14.5 database since Grafana 10.4. While attempting to upgrade from **Grafana 12.0 to 12.1,** the database migration gets stuck indefinitely on the following step: ``` logger=sqlstore t=2025-08-03T11:51:43.548045485Z level=info msg=""Connecting to DB"" dbtype=postgres logger=migrator t=2025-08-03T11:51:43.564028186Z level=info msg=""Locking database"" logger=migrator t=2025-08-03T11:51…",,,,,,Anecdotal,issue,,,,,,,,2025-08-03,github/edgarkz,https://github.com/grafana/grafana/issues/109101,repo: grafana/grafana | keyword: workaround | state: open
I hit the same issue and have cleaned up the annotations table with this config: ```yaml grafana.ini: annotations.api: max_age: 1w max_annotations_to_keep: 5000 annotations.dashboard: max_age: 1w max_annotations_to_keep: 5000 unified_alerting.state_history.annotations: max_age: 1w max_annotations_to_keep: 5000 ```,,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/mogopz,https://github.com/grafana/grafana/issues/109101#issuecomment-3149144887,repo: grafana/grafana | issue: Grafana 12.1 upgrade issue - db migration | keyword: workaround
"I tested this upgrade on my small dev environment and everything went smoothly. And then when i was deploying this on production this migration basically consumed almost 20GB of my database storage in 10min crashing my database. Even though the database had around 7GB of data to begin with. Such changes should really be more visible in release notes, if they can have such an impact",,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/Vimao,https://github.com/grafana/grafana/issues/109101#issuecomment-3150198376,repo: grafana/grafana | issue: Grafana 12.1 upgrade issue - db migration | keyword: workaround
"Same problem here. As an attempt to resolve the issue, we cleaned up the table, leaving only the notes with dates compatible with our metric storage time (3M) and activated the database's autoscale, but an error occurs during the migration.",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/diogenesblip,https://github.com/grafana/grafana/issues/109101#issuecomment-3167781434,repo: grafana/grafana | issue: Grafana 12.1 upgrade issue - db migration | keyword: workaround
"Same problem here, these sorts of changes to the internal table layout should be clearly stated with a big warning. In my case, I could perhaps drop the items from the `annotations` table.",,,,,,Anecdotal,comment,,,,,,,,2025-08-09,github/kitos9112,https://github.com/grafana/grafana/issues/109101#issuecomment-3172136333,repo: grafana/grafana | issue: Grafana 12.1 upgrade issue - db migration | keyword: workaround
I tried below grafana.ini config: but it seems ignored or not activated yet ``` [analytics] check_for_updates = false [annotations] max_age = 6M max_annotations_to_keep = 10000 [annotations.api] max_age = 6M max_annotations_to_keep = 10000 [annotations.dashboard] max_age = 6M max_annotations_to_keep = 10000 ``` `annotations` table has 19M records - the internal cleanup should reduce it but it seems nothing happens until now,,,,,,Anecdotal,comment,,,,,,,,2025-08-10,github/edgarkz,https://github.com/grafana/grafana/issues/109101#issuecomment-3172622605,repo: grafana/grafana | issue: Grafana 12.1 upgrade issue - db migration | keyword: workaround
> I tried below grafana.ini config: but it seems ignored or not activated yet > > ``` > > [analytics] > check_for_updates = false > [annotations] > max_age = 6M > max_annotations_to_keep = 10000 > [annotations.api] > max_age = 6M > max_annotations_to_keep = 10000 > [annotations.dashboard] > max_age = 6M > max_annotations_to_keep = 10000 > ``` > > `annotations` table has 19M records - the internal cleanup should reduce it but it seems nothing happens until now Check your grafana logs. When i cha…,,,,,,Anecdotal,comment,,,,,,,,2025-08-10,github/Vimao,https://github.com/grafana/grafana/issues/109101#issuecomment-3172703901,repo: grafana/grafana | issue: Grafana 12.1 upgrade issue - db migration | keyword: workaround
"I wonder if it is an expected behaviour? I mean, why populating a single column with uid intreases disk size consumption so much? I'm not sure if grafana executes rollback migrations, but I when I rolled back the version (stopping the migration), I got the disk space back, which might indicate that the disk usage was not from the table itself, but to execute the migration.",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/makarov-roman,https://github.com/grafana/grafana/issues/109101#issuecomment-3196910638,repo: grafana/grafana | issue: Grafana 12.1 upgrade issue - db migration | keyword: workaround
"Does anyone have a working Helm chart configuration for annotation cleanups? I tried the suggestions above, but none of them worked.",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/edgarkz,https://github.com/grafana/grafana/issues/109101#issuecomment-3196977576,repo: grafana/grafana | issue: Grafana 12.1 upgrade issue - db migration | keyword: workaround
"HA Grafana & Plugin updates: App Crashing Everytime There is a New Release originally posted by [algo7](https://github.com/algo7) **Describe the bug** Every time there is a new release of the logs-drill down apps (even a patch version), the existing version of the app will crash without any changes of Grafana itself. This pattern has been observed since at least half a year already. The most recent observation is from version 1.0.24 to 1.0.25 **To Reproduce** Steps to reproduce the behavior: 1.…",,,,,,Anecdotal,issue,,,,,,,,2025-08-06,github/gtk-grafana,https://github.com/grafana/grafana/issues/109269,repo: grafana/grafana | keyword: workaround | state: open
"I screwed up and transferred this to a private repo, here's the rest of the comment history that got stripped: <hr /> From: @gtk-grafana @algo7 How do you deploy Grafana? Helm? Do you have any [HA](https://grafana.com/docs/grafana/latest/setup-grafana/set-up-for-high-availability/) settings configured? If you open the app in a private browser window do you still see the error, does it go away when you clear the browser cache and refresh? Are you seeing this error in all web browsers? Multiple u…",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/gtk-grafana,https://github.com/grafana/grafana/issues/109269#issuecomment-3160493214,repo: grafana/grafana | issue: HA Grafana & Plugin updates: App Crashing Everytime There is a New Release | keyword: workaround
"@gtk-grafana is there any way we could reproduce the issue? If not it's difficult to debug. As a general comment, if your plugin is in [the list of default preinstalled plugins](https://github.com/grafana/grafana/blob/main/pkg/setting/setting_plugins.go#L37) you don't need / should not install it with any other method (e.g. GF_INSTALL_PLUGINS), and if you do, you should disable the preinstall feature (using `preinstall_disabled` as was mentioned before. This is what's being done in Cloud). Usin…",,,,,,Anecdotal,comment,,,,,,,,2025-08-11,github/andresmgot,https://github.com/grafana/grafana/issues/109269#issuecomment-3174442146,repo: grafana/grafana | issue: HA Grafana & Plugin updates: App Crashing Everytime There is a New Release | keyword: workaround
"@andresmgot No clue how to reproduce this locally as of now, but we got [another report](https://github.com/grafana/logs-drilldown/issues/1502) of this a few days ago",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/gtk-grafana,https://github.com/grafana/grafana/issues/109269#issuecomment-3197524867,repo: grafana/grafana | issue: HA Grafana & Plugin updates: App Crashing Everytime There is a New Release | keyword: workaround
"Support ntfy.sh as an alerting integration <!-- Please only use this template for submitting feature requests --> **Why is this needed**: [ntfy.sh](https://github.com/binwiederhier/ntfy) is a popular service used to send notifications to a variety of systems (browser, mobile, etc.). The notifications can be sent via HTTP with a title, topic (i.e. channel), priority and body. The body can be in Markdown so that it is nicely viewed by the ntfy.sh clients. **What would you like to be added**: I wo…",,,,,,Anecdotal,issue,,,,,,,,2024-07-21,github/davidfrickert,https://github.com/grafana/grafana/issues/90714,repo: grafana/grafana | keyword: workaround | state: open
This issue has been automatically marked as stale because it has not had activity in the last year. It will be closed in 30 days if no further activity occurs. Please feel free to leave a comment if you believe the issue is still relevant. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/github-actions[bot],https://github.com/grafana/grafana/issues/90714#issuecomment-3194061407,repo: grafana/grafana | issue: Support ntfy.sh as an alerting integration | keyword: workaround
"Regex Value Mapping for numbers <!-- Please only use this template for submitting feature requests --> **Why is this needed**: Hi, i tried to use the regex value mapping with a number field and was frustrated that nothing worked. Until i took a look at the code ([valueMappings.ts#L55](https://github.com/grafana/grafana/blob/beb3a8f02d2681e867cfab845661d81c7ea3ebaa/packages/grafana-data/src/utils/valueMappings.ts#L55)) and saw that only values of type string are supported. My use case is the fol…",,,,,,Anecdotal,issue,,,,,,,,2024-03-16,github/modularTaco,https://github.com/grafana/grafana/issues/84622,repo: grafana/grafana | keyword: workaround | state: open
"Had the same issue. We are using ""Convert field type"" transformation as a workaround to have the field converted into String to later apply regex mapping.",,,,,,Anecdotal,comment,,,,,,,,2024-08-13,github/gesteban,https://github.com/grafana/grafana/issues/84622#issuecomment-2285839680,repo: grafana/grafana | issue: Regex Value Mapping for numbers | keyword: workaround
This issue has been automatically marked as stale because it has not had activity in the last year. It will be closed in 30 days if no further activity occurs. Please feel free to leave a comment if you believe the issue is still relevant. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/github-actions[bot],https://github.com/grafana/grafana/issues/84622#issuecomment-3186518187,repo: grafana/grafana | issue: Regex Value Mapping for numbers | keyword: workaround
"Alerting: Discord alert message is above the title ### What happened? When sending alert warning message to discord, the message text is above the title, the title is inside the attachment below the text. I am providing a picture that shows the current behavior. ![alert_actual_behavior](https://github.com/grafana/grafana/assets/114157744/3d61125e-4e5b-44d2-9c09-4290f34b7b24) ### What did you expect to happen? The embed message should contain text under the title or there should be an option to …",,,,,,Anecdotal,issue,,,,,,,,2024-04-19,github/npcgeorge,https://github.com/grafana/grafana/issues/86565,repo: grafana/grafana | keyword: workaround | state: open
"Hi @npcgeorge Thanks for reporting this issue I reproduced the issue from my end troubleshooting notes for reviewer: ![image](https://github.com/grafana/grafana/assets/45235678/b46855bc-c168-4f9c-81b0-264711ee71fb) Grafana server logs (alert notification sent to Discord) ``` 2024-07-09 09:32:20 logger=notifications t=2024-07-09T07:32:20.014100704Z level=debug msg=""Sending webhook"" url=https://discordapp.com/api/webhooks/... httpmethod=POST 2024-07-09 09:32:20 logger=notifications t=2024-07-09T0…",,,,,,Anecdotal,comment,,,,,,,,2024-07-09,github/tonypowa,https://github.com/grafana/grafana/issues/86565#issuecomment-2216867524,repo: grafana/grafana | issue: Alerting: Discord alert message is above the title | keyword: workaround
"Workaround which I discovered: Discord is compatible with Slack Webhooks, so you can: 1. In Grafana, configure notification point for slack 2. Use the Discord webhook url and append `/slack` to it This will post a slack-style message to the discord webhook which it accepts. The slack-style message is formatted correctly. Best Regards Mydayyy",,,,,,Anecdotal,comment,,,,,,,,2024-10-08,github/Mydayyy,https://github.com/grafana/grafana/issues/86565#issuecomment-2400599992,repo: grafana/grafana | issue: Alerting: Discord alert message is above the title | keyword: workaround
This happens because message content is passed to content field in message. If Grafana would like to include it in embed instead it should be passed in description of the embed. Easiest solution to fix this problem temporarily is to disable it completely. Although I would prefer not using embed at all and instead use message content :smile: Above mentioned @Mydayyy solution wouldn't work when message includes Discord markdown. https://discord.com/developers/docs/resources/webhook#execute-webhook,,,,,,Anecdotal,comment,,,,,,,,2024-10-19,github/cwchristerw,https://github.com/grafana/grafana/issues/86565#issuecomment-2423648786,repo: grafana/grafana | issue: Alerting: Discord alert message is above the title | keyword: workaround
My workaround for this and the limitations of the Slack approach is to use a webhook contact point to send the default.message to an Express server and forward it on to Discord after applying logic and formatting as a valid Discord Embed JSON blob. It also lets me handle alert groups in a more granular way than Grafana allows me to (or at least I personally find it easier in JavaScript).,,,,,,Anecdotal,comment,,,,,,,,2024-11-29,github/timothybennette8,https://github.com/grafana/grafana/issues/86565#issuecomment-2508694512,repo: grafana/grafana | issue: Alerting: Discord alert message is above the title | keyword: workaround
"> Workaround which I discovered: Discord is compatible with Slack Webhooks, so you can: > > 1. In Grafana, configure notification point for slack > > 2. Use the Discord webhook url and append `/slack` to it > > > This will post a slack-style message to the discord webhook which it accepts. The slack-style message is formatted correctly. > > Best Regards Mydayyy Thanks for this!",,,,,,Anecdotal,comment,,,,,,,,2025-01-23,github/erkston,https://github.com/grafana/grafana/issues/86565#issuecomment-2611087544,repo: grafana/grafana | issue: Alerting: Discord alert message is above the title | keyword: workaround
"Hi\! I'd like to work on this issue. From what I understand, the problem is that Discord alert messages appear above the title because the message content is being passed to the 'content' field instead of the embed's 'description' field. I'll investigate the Discord notification implementation and submit a PR to fix this.",,,,,,Anecdotal,comment,,,,,,,,2025-08-16,github/khj68,https://github.com/grafana/grafana/issues/86565#issuecomment-3193703861,repo: grafana/grafana | issue: Alerting: Discord alert message is above the title | keyword: workaround
"I've submitted a PR to fix this issue: https://github.com/grafana/alerting/pull/368 The fix moves the alert message content from the Discord message's main `content` field to the embed's `description` field, which ensures the message appears below the title as expected.",,,,,,Anecdotal,comment,,,,,,,,2025-08-16,github/khj68,https://github.com/grafana/grafana/issues/86565#issuecomment-3193710891,repo: grafana/grafana | issue: Alerting: Discord alert message is above the title | keyword: workaround
"Canvas: Revisit ""No data"" when trying to visualize metric values from multiple queries ### What happened? When creating multiple data queries, the second value has no data, but still shows up in the drop down. [#73588](https://github.com/grafana/grafana/issues/73588). ### What did you expect to happen? Similar to other panels, the ability to display the data still, have an A or B. ### Did this work before? I don't think so ### How do we reproduce it? 1. Create a canvas 2. Create two queries ###…",,,,,,Anecdotal,issue,,,,,,,,2025-07-01,github/EStork09,https://github.com/grafana/grafana/issues/107452,repo: grafana/grafana | keyword: workaround | state: open
Hi @EStork09 thanks for surfacing this issue. Can you please start by trying the workaround proposed in the GH issue you linked above (https://github.com/grafana/grafana/issues/73588) and let us know if that _doesn't_ work for you? - https://github.com/grafana/grafana/issues/73588#issuecomment-1690232129,,,,,,Anecdotal,comment,,,,,,,,2025-07-03,github/jesdavpet,https://github.com/grafana/grafana/issues/107452#issuecomment-3032815112,"repo: grafana/grafana | issue: Canvas: Revisit ""No data"" when trying to visualize metric values from multiple queries | keyword: workaround"
"Hey @jesdavpet , yes I tried it, I my labels didn’t share a common label to properly merge with so it either was all or the other. I even did a promql or statement to get it into one query but it would still return incorrectly or no Data. The work around is a little clunky when doing multiple queries as arguably I want to have many more queries to fill the data with that won’t cleanly merge.",,,,,,Anecdotal,comment,,,,,,,,2025-07-03,github/EStork09,https://github.com/grafana/grafana/issues/107452#issuecomment-3032922588,"repo: grafana/grafana | issue: Canvas: Revisit ""No data"" when trying to visualize metric values from multiple queries | keyword: workaround"
Installing & enabling plugins through provisioning It's currently possible to [provision](http://docs.grafana.org/administration/provisioning/) datasources and dashboards but not plugins. When using the Docker container this is solved to some extent by being able to provision/install plugins using an env variable. This leaves app plugins without provisioning as they need to be enabled after they have been installed. It also leaves installs without docker without provisioning support for plugins…,,,,,,Anecdotal,issue,,,,,,,,2018-03-28,github/xlson,https://github.com/grafana/grafana/issues/11409,repo: grafana/grafana | keyword: workaround | state: open
"I think the `grafana-cli` needs an 'plugins enable' command equivalent to `plugins install` that it now has, then the docker `run.sh` can have this functionality.",,,,,,Anecdotal,comment,,,,,,,,2018-05-28,github/iMartyn,https://github.com/grafana/grafana/issues/11409#issuecomment-392510880,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
"@kethahel99 not yet, but we're definitely keeping this in mind. The more attention this gets, the likelier it is that we implement it.",,,,,,Anecdotal,comment,,,,,,,,2018-11-26,github/xlson,https://github.com/grafana/grafana/issues/11409#issuecomment-441629174,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
"@xlson Hi. Have you tried `GF_INSTALL_PLUGINS: grafana-clock-panel,grafana-simple-json-datasource` this for plugin provisioning? I think it's solving all possible issues with plugins",,,,,,Anecdotal,comment,,,,,,,,2018-11-27,github/onyxet,https://github.com/grafana/grafana/issues/11409#issuecomment-442004100,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
@onyxet Unfortunately that only solves the problem for users running Grafana with Docker. This issue is aimed at developing a solution for non-docker users.,,,,,,Anecdotal,comment,,,,,,,,2018-11-27,github/xlson,https://github.com/grafana/grafana/issues/11409#issuecomment-442004847,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
@onyxet This variable is mentioned in the original post and doesn't provision plugins. It only installs them in a disabled state. If you're automating your Grafana install you still have to use something like curl to enable and configure each plugin after the container is started.,,,,,,Anecdotal,comment,,,,,,,,2018-11-27,github/PaulGallon,https://github.com/grafana/grafana/issues/11409#issuecomment-442006199,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
@PaulGallon Sorry but can't agree with you. I just checked `GF_INSTALL_PLUGINS: camptocamp-prometheus-alertmanager-datasource` this env with `grafana/grafana:5.3.4` image and datasource is ready out the box. So I believe that for docker users it's working as mentioned,,,,,,Anecdotal,comment,,,,,,,,2018-11-27,github/onyxet,https://github.com/grafana/grafana/issues/11409#issuecomment-442044864,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
@xlson I see. I guess that systemd configuration with env configuration like: `Environment=GF_INSTALL_PLUGINS: camptocamp-prometheus-alertmanager-datasource` might be working solution. Here is some doc about that: http://docs.grafana.org/installation/configuration/#using-environment-variables Or I didn't understand the issue?,,,,,,Anecdotal,comment,,,,,,,,2018-11-27,github/onyxet,https://github.com/grafana/grafana/issues/11409#issuecomment-442045865,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
"@onyxet Some plugins are ready right out of the box, some aren't and require extra configuration (which requires provisioning). I understand that you are trying to be helpful but I'm afraid the issue documented here is not solved by linking to Grafana docs as the issue is that Grafana itself does not do provisioning of plugins. `GF_INSTALL_PLUGINS` only works in Docker (it's parsed by the run script in the container), there is nothing in Grafana itself to implement this functionality. Kindly, L…",,,,,,Anecdotal,comment,,,,,,,,2018-11-27,github/xlson,https://github.com/grafana/grafana/issues/11409#issuecomment-442048200,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
"@xlson Well, I haven't faced any issues with extra configuration yet. And main issue wasn't clear in description that's why I'm a bit confused and try to figure out what you're facing. Now I got it. Thanks, Rodion (Just an engineer)",,,,,,Anecdotal,comment,,,,,,,,2018-11-27,github/onyxet,https://github.com/grafana/grafana/issues/11409#issuecomment-442050909,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
"If you want something that comes with a difficult to automate setup, check the grafana kubernetes app - https://grafana.com/plugins/grafana-kubernetes-app It's completely unmaintained, the authors are MIA and it's incredibly frustrating, but it's a good thing to see what people are wanting to automate.",,,,,,Anecdotal,comment,,,,,,,,2018-12-03,github/iMartyn,https://github.com/grafana/grafana/issues/11409#issuecomment-443666318,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
Some implementation hints (was looking into fixing this but have other priorities): - `pkg/api/plugins.go` (`UpdatePluginSetting`) - `services/sqlstore/plugin_setting.go` (`UpdatePluginSetting`) - `cmd/grafana-cli/commands/commands.go`,,,,,,Anecdotal,comment,,,,,,,,2018-12-13,github/EronWright,https://github.com/grafana/grafana/issues/11409#issuecomment-447071623,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
"+1 Would be great to provision everything in this manor. Currently deploying in k8s and getting persistent dashboards, datasources etc nicely but it's getting a little messy with plugins, users, teams, etc unless I'm missing something",,,,,,Anecdotal,comment,,,,,,,,2019-07-11,github/B34stInXile,https://github.com/grafana/grafana/issues/11409#issuecomment-510471241,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
"> > > +1 > Would be great to provision everything in this manor. > Currently deploying in k8s and getting persistent dashboards, datasources etc nicely but it's getting a little messy with plugins, users, teams, etc unless I'm missing something I believe the recommended method to deal with users is to use external authentication. We use github auth in our environment. That being said I came for the plugins. I want to keep our grafana instance fully stateless so provisioning plugins would be hug…",,,,,,Anecdotal,comment,,,,,,,,2019-10-17,github/tpettit,https://github.com/grafana/grafana/issues/11409#issuecomment-543350995,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
"I actually used the pharos production repo for grafana in the end storing all configs in PVC which makes more sense in kubernetes: https://github.com/PharosProduction/tutorial-grafana-prometheus-k8s You exec into a pod and run the plugin install command as you normally would but they store in pvc and provision the plugins, users etc ready for when a pod recycles or a new one is spun up.",,,,,,Anecdotal,comment,,,,,,,,2019-10-18,github/B34stInXile,https://github.com/grafana/grafana/issues/11409#issuecomment-543635490,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
"Is anyone aware of any workarounds for this? We're deploying Grafana via Docker and K8s across multiple nodes, so we need some automated way of enabling app plugins.",,,,,,Anecdotal,comment,,,,,,,,2020-04-17,github/robatron,https://github.com/grafana/grafana/issues/11409#issuecomment-615458521,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
"+1 Reviewing, maybe this issue can be closed. Plugins enablement could be done with provisioning yaml files: https://grafana.com/docs/grafana/latest/administration/provisioning/#plugins",,,,,,Anecdotal,comment,,,,,,,,2022-06-24,github/vicmunoz,https://github.com/grafana/grafana/issues/11409#issuecomment-1165655859,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
"@vicmunoz as far as I know provisioning plugins does not install them in `8.5.1`. I had to use `GF_INSTALL_PLUGINS=one-plugin,another-plugin`",,,,,,Anecdotal,comment,,,,,,,,2022-07-10,github/MatteoGioioso,https://github.com/grafana/grafana/issues/11409#issuecomment-1179676733,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
"> Is anyone aware of any workarounds for this? We're deploying Grafana via Docker and K8s across multiple nodes, so we need some automated way of enabling app plugins. I modified the grafana.db to include an ""insert or replace"" query to insert a line into plugin_setting table which enables the required app plugin. installation itself is done by copying the plugin files into the grafana image. All this is achieved using a helm chart + dockerfile + entrypoint script.",,,,,,Anecdotal,comment,,,,,,,,2023-03-26,github/Exzizer,https://github.com/grafana/grafana/issues/11409#issuecomment-1484094105,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
"I'm deploying Grafana on embedded devices, and installation is fairly easy: just grab the plugin zip from the marketplace, dump the archive contents in the correct place, done. Enabling and configuration seems to be easy, as mentioned earlier in the issues: just [follow the docs](https://grafana.com/docs/grafana/latest/administration/provisioning/#plugins). What I'm currently struggling with is finding information on how to find information on what to put in `jsonData` and `secureJsonData`. (I'…",,,,,,Anecdotal,comment,,,,,,,,2023-05-22,github/jaskij,https://github.com/grafana/grafana/issues/11409#issuecomment-1557910612,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
@jaskij see https://grafana.com/docs/grafana/latest/administration/provisioning/#data-sources and the tables below for `jsonData` and `secureJsonData` where data source column=HTTP*. Alternatively use Chrome Developer Tools or similar and the network tab to inspect the HTTP request payload when creating a datasource - you would see `jsonData` and `secureJsonData` and some other fields such as url etc.,,,,,,Anecdotal,comment,,,,,,,,2023-05-22,github/marefr,https://github.com/grafana/grafana/issues/11409#issuecomment-1558073906,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
"@marefr thank you, that sounds like it will work. I think that together, these steps should actually cover everything when it comes to installing and enabling plugins? If so, all that would be left is describing this process in your documentation in a single place before this issue can be closed? I believe adding - a link to [local plugin installation](https://grafana.com/docs/grafana/latest/administration/plugin-management/#install-plugin-on-local-grafana) - a paragraph or a link explaining ho…",,,,,,Anecdotal,comment,,,,,,,,2023-05-22,github/jaskij,https://github.com/grafana/grafana/issues/11409#issuecomment-1558089039,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
This issue has been automatically marked as stale because it has not had activity in the last year. It will be closed in 30 days if no further activity occurs. Please feel free to leave a comment if you believe the issue is still relevant. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2024-05-22,github/github-actions[bot],https://github.com/grafana/grafana/issues/11409#issuecomment-2123724458,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
"> Any updates with this? This issue is been resolved. 1. need to install plugin In Dockerfile . We can use below directive to install plugin RUN grafana cli plugins install alexanderzobnin-zabbix-app 2. enable plugin ,through this step, the plugin zabbix will be enabled. #plugins COPY provisioning/plugins/zabbix.yaml /etc/grafana/provisioning/plugins/zabbix.yaml zabbix plugin yaml like this: <img width=""344"" height=""124"" alt=""Image"" src=""https://github.com/user-attachments/assets/1a80f8e3-3398-…",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/chlinwei,https://github.com/grafana/grafana/issues/11409#issuecomment-3190949220,repo: grafana/grafana | issue: Installing & enabling plugins through provisioning | keyword: workaround
"Auto-save option I think it would be useful to have an auto-save option for dashboards such that any time a row or panel is created/modified, it would be updated in Elasticsearch.",,,,,,Anecdotal,issue,,,,,,,,2014-03-06,github/Ziaunys,https://github.com/grafana/grafana/issues/174,repo: grafana/grafana | keyword: workaround | state: open
"Good idea, but could be tricky. I regularly use dashboards that I do not want to save. But a start would be to warn when you change dashboard / close window if the dashboard has been modified and not saved. Another option would be an auto save, but with a count down so you could cancel it.",,,,,,Anecdotal,comment,,,,,,,,2014-03-07,github/torkelo,https://github.com/grafana/grafana/issues/174#issuecomment-36969944,repo: grafana/grafana | issue: Auto-save option | keyword: workaround
Agreed. A pop-up dialogue seems like the right solution. The most common complaint I hear from my co-workers is that they forgot to save a dashboard.,,,,,,Anecdotal,comment,,,,,,,,2014-03-07,github/Ziaunys,https://github.com/grafana/grafana/issues/174#issuecomment-37051556,repo: grafana/grafana | issue: Auto-save option | keyword: workaround
Grafana now have a warning popup if you try to leave the page before saving. Closing this since I think that's good enough of a solution for this problem.,,,,,,Anecdotal,comment,,,,,,,,2015-12-23,github/bergquist,https://github.com/grafana/grafana/issues/174#issuecomment-166840960,repo: grafana/grafana | issue: Auto-save option | keyword: workaround
Just lost tons of changes I've made since Chrome tab crashed! So auto-save at least into draft so you can come back and continue to work exactly where you left off.,,,,,,Anecdotal,comment,,,,,,,,2016-06-19,github/asafm,https://github.com/grafana/grafana/issues/174#issuecomment-226981069,repo: grafana/grafana | issue: Auto-save option | keyword: workaround
Any chance this may be implemented? Or at least some sort of workaround. I am really tired of losing hours of works because my browser crashed and I forget to save the panel MANUALLY every 2-3 minutes.,,,,,,Anecdotal,comment,,,,,,,,2017-12-19,github/gusutabopb,https://github.com/grafana/grafana/issues/174#issuecomment-352663222,repo: grafana/grafana | issue: Auto-save option | keyword: workaround
"+1. I just lost a bunch of changes in my prod dashboard just because I did not explicitly click on ""Save"" floppy button.",,,,,,Anecdotal,comment,,,,,,,,2019-10-25,github/ankitmalhotra,https://github.com/grafana/grafana/issues/174#issuecomment-546422986,repo: grafana/grafana | issue: Auto-save option | keyword: workaround
"I would love this one too, maybe to chose per dashboard.. I either forget to save, or chrome crashes, and I personally never revert back to previous versions, I just make experimental dashboards for testing if needed",,,,,,Anecdotal,comment,,,,,,,,2020-02-09,github/dakipro,https://github.com/grafana/grafana/issues/174#issuecomment-583909268,repo: grafana/grafana | issue: Auto-save option | keyword: workaround
Any update on this? It really seems like one of the most basic requirements. is there a workaround I can do without manually exporting the json each time?,,,,,,Anecdotal,comment,,,,,,,,2020-07-02,github/BenCoughlan15,https://github.com/grafana/grafana/issues/174#issuecomment-652970961,repo: grafana/grafana | issue: Auto-save option | keyword: workaround
"Seriously, 6 years to implement an auto-save feature?! I'm completely dumbfounded that why is there no response from any Engineer of the Grafana community? And people continue to loose their hours of work..",,,,,,Anecdotal,comment,,,,,,,,2020-07-16,github/ankitmalhotra,https://github.com/grafana/grafana/issues/174#issuecomment-659718478,repo: grafana/grafana | issue: Auto-save option | keyword: workaround
"> So auto-save at least into draft so you can come back and continue to work exactly where you left off. This would be probably the best, auto-saved drafts, so you don't break existing dashboards, but you don't lose your work in progress either.",,,,,,Anecdotal,comment,,,,,,,,2020-12-14,github/mihalyr,https://github.com/grafana/grafana/issues/174#issuecomment-744537835,repo: grafana/grafana | issue: Auto-save option | keyword: workaround
"Would very much like to see this feature- myself and others have lost quite a few Dashboard changes due to mixing up tabs or crashes or otherwise not manually saving a dashboard, at the right time. The versioning system is already inplace, so i think this feature would be a nice complement to it. (+ an improvement upon this feature request, would be for the Dashboard's ""Last Modified and Saved"" date/time to be displayed somewhere, ie maybe at the top of the grafana gui, ie under the ""Folder/Das…",,,,,,Anecdotal,comment,,,,,,,,2021-01-20,github/bob454522,https://github.com/grafana/grafana/issues/174#issuecomment-763893613,repo: grafana/grafana | issue: Auto-save option | keyword: workaround
This issue just turned 7 years old. Is it implemented yet? I just lost a lot of modifications... I ALWAYS want it to autosave immediately when I change something!,,,,,,Anecdotal,comment,,,,,,,,2021-03-18,github/IT-Economy,https://github.com/grafana/grafana/issues/174#issuecomment-801975397,repo: grafana/grafana | issue: Auto-save option | keyword: workaround
of course not. last time i had to restore from disk backup /var/lib/grafana or something. it's really easy to delete something in grafana.,,,,,,Anecdotal,comment,,,,,,,,2021-03-18,github/DocMAX,https://github.com/grafana/grafana/issues/174#issuecomment-802018707,repo: grafana/grafana | issue: Auto-save option | keyword: workaround
Alerting: ini setting to disable notification channels <!-- Please only use this template for submitting feature requests --> **Why is this needed**: We don't support certain alerting notification channels and don't want them from appearing in the contact point integration dropdown. We also don't want them to be created through APIs. Our current workaround is to delete them from [available_channels](https://github.com/grafana/grafana/blob/main/pkg/services/ngalert/notifier/channels_config/avail…,,,,,,Anecdotal,issue,,,,,,,,2025-08-15,github/chriscerie,https://github.com/grafana/grafana/issues/109719,repo: grafana/grafana | keyword: workaround | state: open
"Upgrading from 11.6.0 to 11.6.1 breaks `[database]`'s `url` format ### What happened? I recently (over a month ago) upgraded from 11.6.0 to 11.6.1 using the official APT repositories. It apparently went into a crash loop failing to start because it couldn't connect to the database (it really should have been able to, as other services connecting to the same postgresql cluster worked just fine) with a completely unrelated error message. Logs attached. Initially, my `[database]` config had `type`…",,,,,,Anecdotal,issue,,,,,,,,2025-05-06,github/slonkazoid,https://github.com/grafana/grafana/issues/105008,repo: grafana/grafana | keyword: workaround | state: open
"reported the same in https://github.com/grafana/grafana/issues/104844. problem also exists in v12.0.0. :( are you saying that this only happens if you configure the database through url? i.e. configuring through host, port etc variables still works?",,,,,,Anecdotal,comment,,,,,,,,2025-05-07,github/clambin,https://github.com/grafana/grafana/issues/105008#issuecomment-2858121594,repo: grafana/grafana | issue: Upgrading from 11.6.0 to 11.6.1 breaks `[database]`'s `url` format | keyword: workaround
"yes, it's up on https://gf.slonk.ing/ On May 7, 2025 1:59:28 PM GMT+03:00, Christophe Lambin ***@***.***> wrote: >clambin left a comment (grafana/grafana#105008) > >reported the same in https://github.com/grafana/grafana/issues/104844. problem also exists in v12.0.0. :( > >are you saying that this only happens if you configure the database through url? i.e. configuring through host, port etc variables still works? > >-- >Reply to this email directly or view it on GitHub: >https://github.com/gra…",,,,,,Anecdotal,comment,,,,,,,,2025-05-07,github/slonkazoid,https://github.com/grafana/grafana/issues/105008#issuecomment-2858166145,repo: grafana/grafana | issue: Upgrading from 11.6.0 to 11.6.1 breaks `[database]`'s `url` format | keyword: workaround
"and i can confirm that the issue persists on v12.x On May 7, 2025 1:59:28 PM GMT+03:00, Christophe Lambin ***@***.***> wrote: >clambin left a comment (grafana/grafana#105008) > >reported the same in https://github.com/grafana/grafana/issues/104844. problem also exists in v12.0.0. :( > >are you saying that this only happens if you configure the database through url? i.e. configuring through host, port etc variables still works? > >-- >Reply to this email directly or view it on GitHub: >https://g…",,,,,,Anecdotal,comment,,,,,,,,2025-05-07,github/slonkazoid,https://github.com/grafana/grafana/issues/105008#issuecomment-2858171526,repo: grafana/grafana | issue: Upgrading from 11.6.0 to 11.6.1 breaks `[database]`'s `url` format | keyword: workaround
"You're right: specifying the url is broken. If you define host, name, user, password etc individually, Grafana starts up correctly. Thanks for the workaround!",,,,,,Anecdotal,comment,,,,,,,,2025-05-07,github/clambin,https://github.com/grafana/grafana/issues/105008#issuecomment-2858194414,repo: grafana/grafana | issue: Upgrading from 11.6.0 to 11.6.1 breaks `[database]`'s `url` format | keyword: workaround
"Same issue here after we went from 11.5 to 12.0 directly. Sad that there was no reaction from the team so far as this is kind of critical. Especially since you can set stuff in the URL that you can't in the remaining options (like `?search_path=grafana`) Edit: Ah, didn't see that the issue is fixed in the upcoming releases: https://github.com/grafana/grafana/issues/104844#issuecomment-3032110063",,,,,,Anecdotal,comment,,,,,,,,2025-07-04,github/der-eismann,https://github.com/grafana/grafana/issues/105008#issuecomment-3036116245,repo: grafana/grafana | issue: Upgrading from 11.6.0 to 11.6.1 breaks `[database]`'s `url` format | keyword: workaround
"Is this still an issues for you folks in the Grafana 12.1, 12.0.3 and 11.6.4 versions? We fixed a few things in those related to this problem. Let me know 👍",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/JohnnyQQQQ,https://github.com/grafana/grafana/issues/105008#issuecomment-3187478936,repo: grafana/grafana | issue: Upgrading from 11.6.0 to 11.6.1 breaks `[database]`'s `url` format | keyword: workaround
"SQL expression: configuration sql_expression_cell_output_limit doesn't works ### What happened? With my grafana.ini with configuration : ``` [expressions] # Enable or disable the expressions functionality. enabled = true sql_expression_cell_limit = 250000 sql_expression_cell_output_limit = 250000 sql_expression_timeout = 20s ``` When using expression there are a warning with limitation to 100000 instead 250000 with big query : <img width=""889"" height=""163"" alt=""Image"" src=""https://github.com/us…",,,,,,Anecdotal,issue,,,,,,,,2025-08-14,github/ogeny,https://github.com/grafana/grafana/issues/109652,repo: grafana/grafana | keyword: workaround | state: open
"Cant get rid of ""items dropped from union"" icon when using Math expression ### What happened? Updated to latest Grafana and now there are warning icons in the title of each panel where I use a Math expression. ### What did you expect to happen? After digging into the warning message I expected to find something broken with my query that I could fix, or at least acknowledge and suppress the warning. ### Did this work before? Yes as in there was never any warning before, as it was introduced in h…",,,,,,Anecdotal,issue,,,,,,,,2024-03-09,github/bobemoe,https://github.com/grafana/grafana/issues/84133,repo: grafana/grafana | keyword: workaround | state: open
"Actually, I can recreate on play.grafana. It seems if I provide different names to the Random Walk (as you normally would?) the warning appears. I don't think the original differences I mention may be to do with tags so much now. ![Screenshot_2024-03-09_13-07-09](https://github.com/grafana/grafana/assets/2304970/b6d5add9-5910-4e24-bf21-b831d361d546) I don't know why randomWalk takes a name? Maybe this wasn't the best function to recreate with. I'll try and recreate with some actual metrics on p…",,,,,,Anecdotal,comment,,,,,,,,2024-03-09,github/bobemoe,https://github.com/grafana/grafana/issues/84133#issuecomment-1986855251,"repo: grafana/grafana | issue: Cant get rid of ""items dropped from union"" icon when using Math expression | keyword: workaround"
"Yep, easy recreate: 1. Go to play.grafana.org 2. Edit the logins panel 3. Add a math expression for total: $A+$B ![Screenshot_2024-03-09_13-21-16](https://github.com/grafana/grafana/assets/2304970/9e26ac7a-08b0-4bbc-932a-ec65398fe3db) 4. If you set the alias so A and B both have the same alias then the warning disappears. ![Screenshot_2024-03-09_13-25-34](https://github.com/grafana/grafana/assets/2304970/3fcc876a-18a2-4f55-b0d5-0287adc6fde5)",,,,,,Anecdotal,comment,,,,,,,,2024-03-09,github/bobemoe,https://github.com/grafana/grafana/issues/84133#issuecomment-1986856431,"repo: grafana/grafana | issue: Cant get rid of ""items dropped from union"" icon when using Math expression | keyword: workaround"
"I've got the same thing but with a prometheus query, can't use math to work out the percentage of two values, same error appears.",,,,,,Anecdotal,comment,,,,,,,,2024-03-12,github/lg-d,https://github.com/grafana/grafana/issues/84133#issuecomment-1991321328,"repo: grafana/grafana | issue: Cant get rid of ""items dropped from union"" icon when using Math expression | keyword: workaround"
"Thank you for this post, I have the exact same issue. I already [posted this](https://community.grafana.com/t/2-items-dropped-from-union-s-after-updating-to-v10-2/111979) to the Grafana forums a while ago but didn't receive any satisfying responses. I've seen the pull request but to be honest I understand maybe half of it. I am currently sitting on Grafana v10.1.5 and can't upgrade because of how faulty my dashboards would look with all the warning signs. I've set up Prometheus and Grafana ""out…",,,,,,Anecdotal,comment,,,,,,,,2024-03-18,github/Max-E-Million,https://github.com/grafana/grafana/issues/84133#issuecomment-2003339893,"repo: grafana/grafana | issue: Cant get rid of ""items dropped from union"" icon when using Math expression | keyword: workaround"
"OK, So I made a dirty hack. You need to be using nginx reverse proxy for grafana and be able to host a js file somewhere. nginx config: ``` location / { proxy_pass http://grafana; sub_filter '<link rel=""stylesheet"" href=""public/build/grafana.dark.' '<script src=""https://WHERE_YOU_HOSTED_IT/grafana_warning_hack.js""></script> <link rel=""stylesheet"" href=""public/build/grafana.dark.'; sub_filter_once on; } ``` you may need to change `.dark.` to `.light.` (in 2 places) depending on your theme and de…",,,,,,Anecdotal,comment,,,,,,,,2024-04-10,github/bobemoe,https://github.com/grafana/grafana/issues/84133#issuecomment-2047250369,"repo: grafana/grafana | issue: Cant get rid of ""items dropped from union"" icon when using Math expression | keyword: workaround"
"Quick fix for anybody experiencing this issue, take the Math expression with the warning, hide the response, then use another expression to multiply the hidden expression by 1 and show the response for that expression. This removes the warning symbol and makes the graphs look cleaner. Example: Query A: $A Query B: $B Expression C: $A + $B (Hide Response) Expression D: $C * 1",,,,,,Anecdotal,comment,,,,,,,,2024-07-28,github/AlienCrafter,https://github.com/grafana/grafana/issues/84133#issuecomment-2254639896,"repo: grafana/grafana | issue: Cant get rid of ""items dropped from union"" icon when using Math expression | keyword: workaround"
This issue has been automatically marked as stale because it has not had activity in the last year. It will be closed in 30 days if no further activity occurs. Please feel free to leave a comment if you believe the issue is still relevant. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/github-actions[bot],https://github.com/grafana/grafana/issues/84133#issuecomment-3130335733,"repo: grafana/grafana | issue: Cant get rid of ""items dropped from union"" icon when using Math expression | keyword: workaround"
"``` Expression warning 2 items dropped from union(s): [""$D / $C"": ($C: {__name__=With Website}) ($D: {__name__=Missing Links})] ``` I have same problem here, any progress for this?",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/encoreshao,https://github.com/grafana/grafana/issues/84133#issuecomment-3178409805,"repo: grafana/grafana | issue: Cant get rid of ""items dropped from union"" icon when using Math expression | keyword: workaround"
"@encoreshao The hidden expression method above is the best workaround so far. What version grafana are you on? I just tried to recreate the issue on https://play.grafana.org but its a bit tricky since I don't seem to be able to edit a panel, although I can create one. I still see the warning in the expression builder in create mode but it doesn't seem to be appearing in the title bar of the graph as per the original issue any more. I think maybe this is solved in the latest version yet the warn…",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/bobemoe,https://github.com/grafana/grafana/issues/84133#issuecomment-3179718417,"repo: grafana/grafana | issue: Cant get rid of ""items dropped from union"" icon when using Math expression | keyword: workaround"
"Got it, thanks @bobemoe for the checking! We are using v12.0.2 in our server. Rendering is correct, just warning in my view.",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/encoreshao,https://github.com/grafana/grafana/issues/84133#issuecomment-3187086789,"repo: grafana/grafana | issue: Cant get rid of ""items dropped from union"" icon when using Math expression | keyword: workaround"
"Live Measurements: Measurement tag information seems to get lost ### What happened? A dashboard with a number of ""Stat"" panels using query type ""Live Measurements"" is running in a browser for a while (say an hour or even more). Without any interaction, the panels switch to ""No data"", while they were showing measurements all fine before. The measurements are coming from Telegraf via Grafana Websocket. The measurements are filtered within the ""Transform"" area of the panel by tag as I want just sp…",,,,,,Anecdotal,issue,,,,,,,,2023-07-27,github/peerfiet,https://github.com/grafana/grafana/issues/72461,repo: grafana/grafana | keyword: workaround | state: open
"thank you for creating this issue @peerfiet do you see any missing data when inspecting the panel? ![image](https://github.com/grafana/grafana/assets/45235678/f35f8c54-5ec3-4e58-a606-274137e4502a) >Is the bug inside a dashboard panel? f I copy to clipboard and paste here I get: ""There was an error posting your comment: Body is too long"" Try going to the dashboard --> selecting the panel options--> more --> get help, download the dashboard and upload it here ![image](https://github.com/grafana/g…",,,,,,Anecdotal,comment,,,,,,,,2023-08-02,github/tonypowa,https://github.com/grafana/grafana/issues/72461#issuecomment-1661730953,repo: grafana/grafana | issue: Live Measurements: Measurement tag information seems to get lost | keyword: workaround
Here is a snapshot of one of the panels while it's working fine. I'll add another snapshot as soon as the issue will appear again. [debug-Steckdosen-02-08-2023 11_17_45.json.txt](https://github.com/grafana/grafana/files/12238435/debug-Steckdosen-02-08-2023.11_17_45.json.txt),,,,,,Anecdotal,comment,,,,,,,,2023-08-02,github/peerfiet,https://github.com/grafana/grafana/issues/72461#issuecomment-1661847657,repo: grafana/grafana | issue: Live Measurements: Measurement tag information seems to get lost | keyword: workaround
Here is the snapshot of the same panel as above when it fails. [debug-Steckdosen-02-08-2023 13_43_06.json.txt](https://github.com/grafana/grafana/files/12239621/debug-Steckdosen-02-08-2023.13_43_06.json.txt),,,,,,Anecdotal,comment,,,,,,,,2023-08-02,github/peerfiet,https://github.com/grafana/grafana/issues/72461#issuecomment-1662059977,repo: grafana/grafana | issue: Live Measurements: Measurement tag information seems to get lost | keyword: workaround
"I just tried the following out of curiosity: stop grafana-server, wait a few seconds, start grafana-server. Observations: After restart of grafana-server data starts coming again (as expected), but is still showing up with the ""wrong"" measurement names in transform area (power 1, power 2, ...). As such filtering is still failing and panel is still showing ""No Data"".",,,,,,Anecdotal,comment,,,,,,,,2023-08-02,github/peerfiet,https://github.com/grafana/grafana/issues/72461#issuecomment-1662076854,repo: grafana/grafana | issue: Live Measurements: Measurement tag information seems to get lost | keyword: workaround
"One more observation: If you have multiple stat panels on the dashboard (even) with different measurements, they all fail at the same time.",,,,,,Anecdotal,comment,,,,,,,,2023-08-03,github/peerfiet,https://github.com/grafana/grafana/issues/72461#issuecomment-1663323279,repo: grafana/grafana | issue: Live Measurements: Measurement tag information seems to get lost | keyword: workaround
"OK, just one more thing as I think it could help: I saw that the ""support snapshot"" size is ever increasing over time (don't know if that's normal) and a lot of measurement data seems to me to be part of the snapshot: ![image](https://github.com/grafana/grafana/assets/102018536/3550dbaa-12e8-4d70-92d1-1b8064f0e1ac) The dashboard itself is set to show the ""Last 24 hours"". As I don't need history for the Stat panels (I use them without the ""graph"" part) I had the idea to adjust the ""Buffer size"" …",,,,,,Anecdotal,comment,,,,,,,,2023-08-03,github/peerfiet,https://github.com/grafana/grafana/issues/72461#issuecomment-1664178522,repo: grafana/grafana | issue: Live Measurements: Measurement tag information seems to get lost | keyword: workaround
"Update: while in the screenshot of the initial post 2 tags are visible, I've now reduced this (on the sending telegraf side) to 1 tag per measurement (the ""name"" in above example). Now it's working stable.",,,,,,Anecdotal,comment,,,,,,,,2024-06-17,github/peerfiet,https://github.com/grafana/grafana/issues/72461#issuecomment-2172976708,repo: grafana/grafana | issue: Live Measurements: Measurement tag information seems to get lost | keyword: workaround
"> Update: while in the screenshot of the initial post 2 tags are visible, I've now reduced this (on the sending telegraf side) to 1 tag per measurement (the ""name"" in above example). Now it's working stable. Thanks for coming back and updating. I use a Grafana live measurements extensively. Like yourself I've found that it can be very finicky to initially set up, but (usually) once you have a visualization showing and as long as your data stays consistent/exact , it usually continues to work (a…",,,,,,Anecdotal,comment,,,,,,,,2024-07-21,github/bob454522,https://github.com/grafana/grafana/issues/72461#issuecomment-2241769077,repo: grafana/grafana | issue: Live Measurements: Measurement tag information seems to get lost | keyword: workaround
"@bob454522: What you describe is pretty much what I also ended up with :) While this is working if I use ONE live-measurement in a Stat panel, I can't get this to work stable if I use MULTIPLE live-measurements in a Stat panel. What happens is that after a short time the updating stops in the panel where I use MULTIPLE live-measurements, while the same live-measurements split across multiple panels (in the same dashboard) keep updating. Well, I can live with that.",,,,,,Anecdotal,comment,,,,,,,,2024-08-13,github/peerfiet,https://github.com/grafana/grafana/issues/72461#issuecomment-2286431086,repo: grafana/grafana | issue: Live Measurements: Measurement tag information seems to get lost | keyword: workaround
This issue has been automatically marked as stale because it has not had activity in the last year. It will be closed in 30 days if no further activity occurs. Please feel free to leave a comment if you believe the issue is still relevant. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/github-actions[bot],https://github.com/grafana/grafana/issues/72461#issuecomment-3186517004,repo: grafana/grafana | issue: Live Measurements: Measurement tag information seems to get lost | keyword: workaround
Reduce tranformations shows incorrect values **What happened**: This issue is the same as described in #24950 Reduction transformations show incorrect values in table in 'Series to rows mode' **What you expected to happen**: Reduction transformations show correct values in table in 'Series to rows mode' In editing mode values are correct: ![grafana_editing](https://user-images.githubusercontent.com/1822286/152203354-a18b960b-77ca-4f46-a845-7ff2ab74344a.png) Back to dashboard: ![grafana_dashboar…,,,,,,Anecdotal,issue,,,,,,,,2022-02-02,github/dmarteau,https://github.com/grafana/grafana/issues/44791,repo: grafana/grafana | keyword: workaround | state: open
"Thanks for creating this issue, @dmarteau I'm actually having trouble reproducing this. Here is a dashboard I made with dummy data. The reduce -> max / mean appear to work, and if I save the dashboard and refresh, the values remain. https://play.grafana.org/d/qOAczV-7z/reduce-transform-test?orgId=1 Maybe you can provide more details on how this is breaking on your side? 👍",,,,,,Anecdotal,comment,,,,,,,,2022-02-04,github/zuchka,https://github.com/grafana/grafana/issues/44791#issuecomment-1030372251,repo: grafana/grafana | issue: Reduce tranformations shows incorrect values | keyword: workaround
"Thanks for creating this issue! We think it's missing some basic information. Follow the issue template and add additional information that will help us replicate the problem. For data visualization issues: - Query results from the inspect drawer (data tab & query inspector) - Panel settings can be extracted in the panel inspect drawer JSON tab For dashboard related issues: - Dashboard JSON can be found in the dashboard settings JSON model view For authentication, provisioning and alerting issu…",,,,,,Anecdotal,comment,,,,,,,,2022-09-06,github/grafanabot,https://github.com/grafana/grafana/issues/44791#issuecomment-1238098197,repo: grafana/grafana | issue: Reduce tranformations shows incorrect values | keyword: workaround
"@zuchka I an into this same exact issue with a customer. They sent me a dataframe (with some data cleaned up to protect privacy) which they agree I could attach to this public issue: [DataFrame JSON (Public).txt](https://github.com/grafana/grafana/files/9590380/DataFrame.JSON.Public.txt) As with the OP, the issue happens when using ""Series to Rows"" transform, with Calculation=""Mean"". Here is my summary of the issue as I saw it when testing with the customer's dataframe: > I was able to use the …",,,,,,Anecdotal,comment,,,,,,,,2022-09-16,github/ethirolle,https://github.com/grafana/grafana/issues/44791#issuecomment-1249917042,repo: grafana/grafana | issue: Reduce tranformations shows incorrect values | keyword: workaround
The `Outer join` before the `Reduce` is a working workaround for us as well. For us we're using `Last*` instead of `mean`. Very weird.,,,,,,Anecdotal,comment,,,,,,,,2024-07-26,github/atsai1220,https://github.com/grafana/grafana/issues/44791#issuecomment-2253176909,repo: grafana/grafana | issue: Reduce tranformations shows incorrect values | keyword: workaround
"Trigger dashboard refresh from custom panel plugins **Why is this needed**: Grafana does not currently support triggering a dashboard refresh from within a panel. This limitation becomes apparent in scenarios where interactive behavior across multiple panels is desired. For example, consider a dashboard built using custom data source and panel plugins, where the panel uses Plotly for rendering charts. When a user zooms into one panel (x-axis contains numeric values), the selected numeric range …",,,,,,Anecdotal,issue,,,,,,,,2025-08-13,github/kartheeswaran-ni,https://github.com/grafana/grafana/issues/109587,repo: grafana/grafana | keyword: workaround | state: open
"Transformation: Grouping to matrix header incorrect ### What happened? When using the ""Grouping to matrix"" transformation on table data, the name of the Cell value field appears in the header instead of the value referred by the Column name. ![Screenshot 2024-05-03 at 9 09 04 AM](https://github.com/grafana/grafana/assets/22201598/0c12475d-af6e-4de8-8c2a-7440dfaafeaa) ![Screenshot 2024-05-03 at 9 08 55 AM](https://github.com/grafana/grafana/assets/22201598/21c9fe0c-55ce-4724-b209-9d95a5e6136e) E…",,,,,,Anecdotal,issue,,,,,,,,2024-05-03,github/danialre,https://github.com/grafana/grafana/issues/87332,repo: grafana/grafana | keyword: workaround | state: open
One more note: `influxdbBackendMigration` is set to false but enabling it doesn't change this behavior. Also tested with the newest release (10.4.2) and this bug is still occurring.,,,,,,Anecdotal,comment,,,,,,,,2024-05-03,github/danialre,https://github.com/grafana/grafana/issues/87332#issuecomment-2093270980,repo: grafana/grafana | issue: Transformation: Grouping to matrix header incorrect | keyword: workaround
"I have the same issue here. Glad I am not the only one: I want to get fields for each year: <img width=""1176"" alt=""image"" src=""https://github.com/grafana/grafana/assets/894150/0c54138e-f733-482b-b582-a9bc6d260231""> However it always sets the Cell Value field name for every column: <img width=""1176"" alt=""image"" src=""https://github.com/grafana/grafana/assets/894150/9f4bab67-4848-4496-93cf-dd69443602a7""> Also using InfluxDB as data source <img width=""653"" alt=""image"" src=""https://github.com/grafan…",,,,,,Anecdotal,comment,,,,,,,,2024-05-06,github/cschlipf,https://github.com/grafana/grafana/issues/87332#issuecomment-2095735466,repo: grafana/grafana | issue: Transformation: Grouping to matrix header incorrect | keyword: workaround
"I am also facing the same issue here, ![image](https://github.com/grafana/grafana/assets/119028518/d67d8673-e1bd-4230-b304-156db23e6eb7) Grafana version - v10.3.3 Database used - InfluxDB Datbase version - v2.4.0 When using the same query in the Grafana - v9.0.7, everything works fine, ![image](https://github.com/grafana/grafana/assets/119028518/66845fce-86ac-499c-92ac-7cd47d865199) While trying to check further noticed a difference in the transform debug that the field.config has no properties…",,,,,,Anecdotal,comment,,,,,,,,2024-05-22,github/Sathyan5,https://github.com/grafana/grafana/issues/87332#issuecomment-2124539259,repo: grafana/grafana | issue: Transformation: Grouping to matrix header incorrect | keyword: workaround
We also have this issue with version v11.1.0 (5b85c4c2fc). Also Influx as Datasource. ![grafik](https://github.com/user-attachments/assets/6cbca222-7b2f-4744-8fc4-20dbf2d817bf),,,,,,Anecdotal,comment,,,,,,,,2024-08-21,github/Letsamsiyu,https://github.com/grafana/grafana/issues/87332#issuecomment-2301627523,repo: grafana/grafana | issue: Transformation: Grouping to matrix header incorrect | keyword: workaround
"I found a workaround for the issue in my case, I could use the ""Fields with name"" override on the table, and add the override property for display name. In my case the ""name"" field was correct when looking in the debugger just the config.displayName was wrong, so the override could still select by name and change the display",,,,,,Anecdotal,comment,,,,,,,,2024-09-26,github/jlangy,https://github.com/grafana/grafana/issues/87332#issuecomment-2377906053,repo: grafana/grafana | issue: Transformation: Grouping to matrix header incorrect | keyword: workaround
"Thank you very much for the Workaround @jlangy . Works well enough if the fields are not constantly changing. In my case it's the year, so I have to add a new override every year, which is good enough. However there might be more use cases, where this workaround is less feasible.",,,,,,Anecdotal,comment,,,,,,,,2024-10-06,github/cschlipf,https://github.com/grafana/grafana/issues/87332#issuecomment-2395518686,repo: grafana/grafana | issue: Transformation: Grouping to matrix header incorrect | keyword: workaround
"Hi, Poking around I may have found a working workaround (influxdb + grafana 11.3.1) The transformation ""transpose"" seems to fix the field name for some reason. So, inverting the column & row then applying a transpose fix the issue. Then you just need to change the type for the Time field, which seem to require two cast (number then time). ![Image](https://github.com/user-attachments/assets/1284b181-a3d2-4704-8739-2a15720df8e3)",,,,,,Anecdotal,comment,,,,,,,,2024-12-04,github/Stormshield-robinc,https://github.com/grafana/grafana/issues/87332#issuecomment-2516922882,repo: grafana/grafana | issue: Transformation: Grouping to matrix header incorrect | keyword: workaround
I think this may have been fixed as I no longer needed the Transpose workaround (and indeed using it restores the old broken behaviour) as of v12.1.0.,,,,,,Anecdotal,comment,,,,,,,,2025-08-11,github/NetcraftPm,https://github.com/grafana/grafana/issues/87332#issuecomment-3175108008,repo: grafana/grafana | issue: Transformation: Grouping to matrix header incorrect | keyword: workaround
"Docker: the latest (v11.6.0) Docker image for ARM64 is broken ### What happened? I've just updated to the latest Grafana image on my QNAP NAS (ARM64-based processor) and got the following error: ``` SIGILL: illegal instruction PC=0xaaf8050 m=0 sigcode=1 instruction bytes: 0x0 0x6 0x38 0xd5 0xe0 0x7 0x0 0xf9 0xc0 0x3 0x5f 0xd6 0x0 0x0 0x0 0x0 goroutine 1 gp=0x40000021c0 m=0 mp=0x11eb9be0 [running, locked to thread]: github.com/tetratelabs/wazero/internal/platform.getisar0() github.com/tetratelab…",,,,,,Anecdotal,issue,,,,,,,,2025-03-26,github/Mystic-Mirage,https://github.com/grafana/grafana/issues/102848,repo: grafana/grafana | keyword: workaround | state: open
"Hello, same problem here with new TESLAMATE version that uses new Grafana docker 11.6. I have the following error in the log of grafana docker runing on Synology DS218 NAS: SIGILL: illegal instruction PC=0xaaf8050 m=0 sigcode=1 instruction bytes: 0x0 0x6 0x38 0xd5 0xe0 0x7 0x0 0xf9 0xc0 0x3 0x5f 0xd6 0x0 0x0 0x0 0x0 goroutine 1 gp=0x4000002380 m=0 mp=0x11eb9be0 [running, locked to thread]: github.com/tetratelabs/wazero/internal/platform.getisar0() github.com/tetratelabs/wazero@v1.8.2/internal/p…",,,,,,Anecdotal,comment,,,,,,,,2025-04-21,github/JGC-HA,https://github.com/grafana/grafana/issues/102848#issuecomment-2819240614,repo: grafana/grafana | issue: Docker: the latest (v11.6.0) Docker image for ARM64 is broken | keyword: workaround
"Actually, I think this is https://github.com/tetratelabs/wazero/issues/2362 . tl;dr: Wazero have dropped support for ""old"" Linux kernels, anything prior to 4.11. Would it be possible to lock Grafana 11.6/12 to an earlier wazero version? Kernel upgrades on weird arm64 hardware devices aren't trivial.",,,,,,Anecdotal,comment,,,,,,,,2025-05-30,github/jmason,https://github.com/grafana/grafana/issues/102848#issuecomment-2921855074,repo: grafana/grafana | issue: Docker: the latest (v11.6.0) Docker image for ARM64 is broken | keyword: workaround
"Hi, any action to fix this issue? Currently it is impossible to run Grafana docker on Synology NAS since 11.6 version :( It is possible to avoid the use of wazero in Grafana docker? This is the root cause of the issue. Thanks",,,,,,Anecdotal,comment,,,,,,,,2025-08-09,github/JGC-HA,https://github.com/grafana/grafana/issues/102848#issuecomment-3172235693,repo: grafana/grafana | issue: Docker: the latest (v11.6.0) Docker image for ARM64 is broken | keyword: workaround
"Dashboard list is not doing OR between tags ### What happened? Dashboard List - Im trying to select more dashboards with different tags. But it always display only one tag. https://github.com/user-attachments/assets/0aec5932-8b48-4fdb-9fa8-bea078eb8459 ### What did you expect to happen? According to dashboard-list help, we should be able to list more dashboards with different tags: ![Image](https://github.com/user-attachments/assets/fe6db28c-3941-49d7-a20e-18d35074e8c4) https://grafana.com/docs…",,,,,,Anecdotal,issue,,,,,,,,2025-03-28,github/robinpecha,https://github.com/grafana/grafana/issues/103045,repo: grafana/grafana | keyword: workaround | state: open
"Or there is a way to use multiple queries? Something like ""AAA & BBBBB"" or with any other separeator (nothing works for me) ; , /",,,,,,Anecdotal,comment,,,,,,,,2025-03-28,github/robinpecha,https://github.com/grafana/grafana/issues/103045#issuecomment-2760735166,repo: grafana/grafana | issue: Dashboard list is not doing OR between tags | keyword: workaround
I'm on Grafana **v12.0.2** and I'm experiencing the same bug with the Dashboard List panel. The panel doesn't correctly filter by multiple tags. The query field also failed to list multiple dashboards with different names. I've ended up using a single tag for my dashboards as a workaround,,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/Fouz,https://github.com/grafana/grafana/issues/103045#issuecomment-3168760456,repo: grafana/grafana | issue: Dashboard list is not doing OR between tags | keyword: workaround
"[feature-request] Labels/metadata on tooltips **Why is this needed**: When showing data on state timelines, I need to be able to see some clarifying info on hover to e.g. understand the cause of the state or how a potential issue was resolved. **What would you like to be added**: I want to be able to specify fields to show in the tooltip on hover. Example from the community panel call Gannt, where this feature is indicated: ![image](https://github.com/grafana/grafana/assets/88662219/554502bc-c3…",,,,,,Anecdotal,issue,,,,,,,,2023-08-29,github/Bjarke-Svane,https://github.com/grafana/grafana/issues/73989,repo: grafana/grafana | keyword: workaround | state: open
"I have a table with 4 columns: date, name, num_value, comment. I can plot Grafana time series using this SQL: select date, name, num_value as "" "" from T and applying transformation: Prepare Time Series → Format → Multi-frame time series. The tooltip shows (date, name, value). I would like to add to tooltip the content of column “comment” in my table T. How to do it?",,,,,,Anecdotal,comment,,,,,,,,2024-02-12,github/mlubinsky,https://github.com/grafana/grafana/issues/73989#issuecomment-1939386917,repo: grafana/grafana | issue: [feature-request] Labels/metadata on tooltips | keyword: workaround
It is being requested by one of our Premier customers as well. https://grafana.zendesk.com/agent/tickets/128661 Note from customer: _This is important to us as we are currently looking into building our own panel as a workaround. It would be a much cleaner integration to do in your panel. It's preventing us from making use of all our data._,,,,,,Anecdotal,comment,,,,,,,,2024-03-08,github/grvsoni,https://github.com/grafana/grafana/issues/73989#issuecomment-1985651876,repo: grafana/grafana | issue: [feature-request] Labels/metadata on tooltips | keyword: workaround
"+1 This would also be extremely useful for us. Our use-case: We monitor the up-time of physical PCs that control microscopes. Users need to be able to briefly turn PCs off, but the PCs should never stay off. So we monitor this. I have a panel with a state time line that shows when a PC is off. I would be extremely useful to show the location of the PC in the tooltip (i.e. the room it in), so that I can go turn it on without having to check in our wiki, where it is located (currently I only get …",,,,,,Anecdotal,comment,,,,,,,,2024-03-12,github/michaelmell,https://github.com/grafana/grafana/issues/73989#issuecomment-1991061373,repo: grafana/grafana | issue: [feature-request] Labels/metadata on tooltips | keyword: workaround
"update: this is getting closer to reality. we're nearing the end of a long, winding road of a major rewrite and unification of our viz tooltip architecture. we're starting to roll out the `newVizTooltips` feature flag to production. once we're at 100% deployment for a couple weeks we will remove ~4,000 lines of previous tooltip code that is scattered across 8-10 different panels. then we'll have some hope of tackling this issue in a sane, maintainable manner. i would estimate coming back to thi…",,,,,,Anecdotal,comment,,,,,,,,2024-03-12,github/leeoniya,https://github.com/grafana/grafana/issues/73989#issuecomment-1991301435,repo: grafana/grafana | issue: [feature-request] Labels/metadata on tooltips | keyword: workaround
"+1 This feature is critical for us because we have been useing 3-rd party chart StatusMap which probably will not rewrite from Angular JS to React JS (there is no reaction on issue by this company). These our charts without additional informations in tooltip are not usable very well. There are some examples of chart usage: ![image](https://github.com/grafana/grafana/assets/163393938/e0b3e9ac-e952-44a2-a035-36799aa0ada4) (tooltip is customized there (last line below the value ""Priority 3"")) ![im…",,,,,,Anecdotal,comment,,,,,,,,2024-03-14,github/ghost,https://github.com/grafana/grafana/issues/73989#issuecomment-1996818672,repo: grafana/grafana | issue: [feature-request] Labels/metadata on tooltips | keyword: workaround
+1 on this. Complex hover tooltips which allow to show data without the need to go to another dashboard are a must to make Grafana more BI front-end compatible.,,,,,,Anecdotal,comment,,,,,,,,2024-04-01,github/collectivetrader,https://github.com/grafana/grafana/issues/73989#issuecomment-2030649258,repo: grafana/grafana | issue: [feature-request] Labels/metadata on tooltips | keyword: workaround
"quick status update we now have the new tooltip architecture migration completed :tada: interestingly, we missed the fact that the behavior requested here was actually the default tooltip behavior in BarChart, which is now regressed and we're working on a fix. this rabbit-holed into a tricky-but-necessary BarChart refactor: https://github.com/grafana/grafana/pull/87160 i've added a util function to deal with part of the `hideFrom` situation described above, so we're in a better place now. https…",,,,,,Anecdotal,comment,,,,,,,,2024-05-07,github/leeoniya,https://github.com/grafana/grafana/issues/73989#issuecomment-2098984568,repo: grafana/grafana | issue: [feature-request] Labels/metadata on tooltips | keyword: workaround
"As a thought. If tooltips could be just like a filtered dashboard Dashboards can be reused and assigned as tooltips, filtered by variables. That way a tooltip could be fully customizable. Consider this. If tooltips could actually be a thumbnail view of a linked dashboard, it's done. You could design your tooltip as you like, even with embedded graphs, and a click would take you to THAT dashboard to drill down in the data. On the panel, the option on tooltip to use ""Classic"" or ""Dashboard as Too…",,,,,,Anecdotal,comment,,,,,,,,2024-05-15,github/ctroncoso,https://github.com/grafana/grafana/issues/73989#issuecomment-2113013249,repo: grafana/grafana | issue: [feature-request] Labels/metadata on tooltips | keyword: workaround
"This request has also come up in the context of the heatmap visualization as a replacement for the deprecated status map visualization. <img width=""226"" alt=""status map example"" src=""https://github.com/user-attachments/assets/b16eaa5a-1712-43f2-87d4-d8a660d8f5fd""> See https://github.com/grafana/support-escalations/issues/11693",,,,,,Anecdotal,comment,,,,,,,,2024-07-26,github/nmarrs,https://github.com/grafana/grafana/issues/73989#issuecomment-2253355782,repo: grafana/grafana | issue: [feature-request] Labels/metadata on tooltips | keyword: workaround
"I thought I had a way, but the tooltip refuses to show the value if it's not visible in the chart: ![image](https://github.com/user-attachments/assets/35c3851a-dc26-4a0f-b7bb-a5732969c008)",,,,,,Anecdotal,comment,,,,,,,,2024-08-10,github/uncaught,https://github.com/grafana/grafana/issues/73989#issuecomment-2282177830,repo: grafana/grafana | issue: [feature-request] Labels/metadata on tooltips | keyword: workaround
Hi All I have implemented this function using the Grafana plane plugin. I added this function based on Timeseries. ![Snipast](https://github.com/user-attachments/assets/74ebba76-c085-4f93-afec-129da22949cb),,,,,,Anecdotal,comment,,,,,,,,2024-08-15,github/a270443177,https://github.com/grafana/grafana/issues/73989#issuecomment-2290759371,repo: grafana/grafana | issue: [feature-request] Labels/metadata on tooltips | keyword: workaround
"With much thanks to @a270443177 I can confirm that his plugin works great. Using a mysql source, the following were needed to get it to work: * Ensure grafana version is > 10.3 (tested on 10.3, 10.4, 11.4) * Enable the unsigned plugin: * using an env var: GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS=vxiaobai-timeseries-panel * or by adding 'allow_loading_unsigned_plugins = vxiaobai-timeseries-panel' in the ini * enable NewVizTooltips * using an env var: GF_FEATURE_TOGGLES_ENABLE=newVizTooltips * o…",,,,,,Anecdotal,comment,,,,,,,,2024-12-06,github/ghaushe-ampere,https://github.com/grafana/grafana/issues/73989#issuecomment-2523555093,repo: grafana/grafana | issue: [feature-request] Labels/metadata on tooltips | keyword: workaround
"Datasource: AWS Cloudwatch Datasources fail after upgrading to 12.1.0 from 12.0.2. ### What happened? Upgraded grafana to 12.1.0 from 12.0.2. AWS Cloudwatch datasources were failing with following error, `error in DimensionValuesHandler: unable to call AWS API: operation error CloudWatch: ListMetrics, https response error StatusCode: 400, RequestID: 5e1ec98f-81b0-4ce9-a3a0-291c2caf3068, api error InvalidAction: Could not find operation ListMetrics for version 2010-08-01 error getting accounts f…",,,,,,Anecdotal,issue,,,,,,,,2025-08-01,github/santhoshza,https://github.com/grafana/grafana/issues/109032,repo: grafana/grafana | keyword: workaround | state: closed
"> Hi [@grafana-support](https://github.com/grafana-support) we already have a ticket in incoming that's the same problem, here's the reply and the workaround: https://github.com/grafana/support-escalations/issues/17671#issuecomment-3141408480 The link doesn't open up for me.",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/santhoshza,https://github.com/grafana/grafana/issues/109032#issuecomment-3145117638,repo: grafana/grafana | issue: Datasource: AWS Cloudwatch Datasources fail after upgrading to 12.1.0 from 12.0.2. | keyword: workaround
"@santhoshza I'm sorry about that, Friday afternoon brain. Let me copy paste it here: > it's not necessary to enter the regional sts endpoint, as, if the region is specified, [aws sdk will infer the endpoint](https://docs.aws.amazon.com/sdkref/latest/guide/feature-sts-regionalized-endpoints.html) and send the request to the correct regional one (if not opt in). We've already identified the bug and have https://github.com/grafana/grafana-aws-sdk/pull/283 to fix it, but the solution is just to cle…",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/idastambuk,https://github.com/grafana/grafana/issues/109032#issuecomment-3145136493,repo: grafana/grafana | issue: Datasource: AWS Cloudwatch Datasources fail after upgrading to 12.1.0 from 12.0.2. | keyword: workaround
"Thank you, that workaround is something we already figured out :) But the problem is, by the time we do the upgrade and update the config, it does trigger ""**DatasourceNoData**"" alerts. We cannot update the config in advance of upgrade as it stops datasource working in 12.0.2. We have good number of users using the system and don't want to spam them with these false emails. Hence we need proper fix in order to proceed with upgrade. Hope that make sense.",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/santhoshza,https://github.com/grafana/grafana/issues/109032#issuecomment-3145168627,repo: grafana/grafana | issue: Datasource: AWS Cloudwatch Datasources fail after upgrading to 12.1.0 from 12.0.2. | keyword: workaround
"Hi again @santhoshza, I just released the sdk with the bugfix. We will try to get the update to cloudwatch released with the next 12.1.x patch release",,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/idastambuk,https://github.com/grafana/grafana/issues/109032#issuecomment-3150557713,repo: grafana/grafana | issue: Datasource: AWS Cloudwatch Datasources fail after upgrading to 12.1.0 from 12.0.2. | keyword: workaround
"12.1.1 packages is not available from the yum repo, still waiting. And direct download links are all seems to have changed. this breaks our automation pipelines.",,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/santhoshza,https://github.com/grafana/grafana/issues/109032#issuecomment-3191311909,repo: grafana/grafana | issue: Datasource: AWS Cloudwatch Datasources fail after upgrading to 12.1.0 from 12.0.2. | keyword: workaround
"Hi @santhoshza, I was going to suggest opening a ticket in the main Grafana repo about the installation problems, but I see you've already done that. Once it's resolved, let us know if any further problems arise with Cloudwatch querying.",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/idastambuk,https://github.com/grafana/grafana/issues/109032#issuecomment-3195951100,repo: grafana/grafana | issue: Datasource: AWS Cloudwatch Datasources fail after upgrading to 12.1.0 from 12.0.2. | keyword: workaround
"Allow a custom prefix for OpsGenie notifications <!-- Please only use this template for submitting feature requests --> **What would you like to be added**: When the OpsGenie notification plugin creates an alert in opsgenie it sets to alias file to an ID, for example ""alertID-1"". The ""alertID"" part is hardcoded and the number at the end increments. I would like to be able to replace ""alertID"" with a custom string, or prepend a string to the existing alert ID. **Why is this needed**: In this sce…",,,,,,Anecdotal,issue,,,,,,,,2021-01-06,github/nigel4321,https://github.com/grafana/grafana/issues/30080,repo: grafana/grafana | keyword: workaround | state: closed
I'm also facing the exact same problem. It's very problematic as it forbids the integration of multiple grafana instances to a single Opsgenie.,,,,,,Anecdotal,comment,,,,,,,,2022-01-25,github/jblezoray,https://github.com/grafana/grafana/issues/30080#issuecomment-1021201414,repo: grafana/grafana | issue: Allow a custom prefix for OpsGenie notifications | keyword: workaround
"I think fixing this issue would be pretty simple if the prefix of this 'alias' identifier was configurable, with a default value to the current ""AlertId"" string : https://github.com/grafana/grafana/blob/main/pkg/services/alerting/notifiers/opsgenie.go#L162 I think I can work on this fix.",,,,,,Anecdotal,comment,,,,,,,,2022-02-07,github/jblezoray,https://github.com/grafana/grafana/issues/30080#issuecomment-1031750697,repo: grafana/grafana | issue: Allow a custom prefix for OpsGenie notifications | keyword: workaround
As far as I can see this is still a problem in Grafana.. Different alerts from different grafana instances using the same opsgenie API key indeeding opening only one Opsgenie ticket for all the alerts due to same alias.. Any solution? even workaround,,,,,,Anecdotal,comment,,,,,,,,2022-08-23,github/ZoharZrihen,https://github.com/grafana/grafana/issues/30080#issuecomment-1224058630,repo: grafana/grafana | issue: Allow a custom prefix for OpsGenie notifications | keyword: workaround
"I would very much like to have this new feature implemented, as the moment, we cannot use auto-close of alerts because we have a custom alias, which is derived from the alert name (prepended with the grafana location's name) @jblezoray : any luck with implementing this ?",,,,,,Anecdotal,comment,,,,,,,,2024-07-16,github/jgournet,https://github.com/grafana/grafana/issues/30080#issuecomment-2230041469,repo: grafana/grafana | issue: Allow a custom prefix for OpsGenie notifications | keyword: workaround
"@jgournet I had this fix : https://github.com/grafana/grafana/pull/45016 But it was never merged. And since it was already about some legacy code 2 years ago, it is probably largely outdated now.",,,,,,Anecdotal,comment,,,,,,,,2024-07-16,github/jblezoray,https://github.com/grafana/grafana/issues/30080#issuecomment-2230539790,repo: grafana/grafana | issue: Allow a custom prefix for OpsGenie notifications | keyword: workaround
"Too bad - that would be have been great ! Just thinking: 1) what about making it simpler, and change from sending ""alertId"" as the alias, we could send the alert UUID ? this is auto-generated by grafana to a long unique sequence, but can also be set by users when creating a new alert. So this should not break any existing setup, and enhance the setup for people who have multi-grafana setups (without any configuration change needed). 2) or if you feel like it, just re-creating a new PR with the …",,,,,,Anecdotal,comment,,,,,,,,2024-07-17,github/jgournet,https://github.com/grafana/grafana/issues/30080#issuecomment-2234388581,repo: grafana/grafana | issue: Allow a custom prefix for OpsGenie notifications | keyword: workaround
This issue has been automatically marked as stale because it has not had activity in the last year. It will be closed in 30 days if no further activity occurs. Please feel free to leave a comment if you believe the issue is still relevant. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-07-19,github/github-actions[bot],https://github.com/grafana/grafana/issues/30080#issuecomment-3091396704,repo: grafana/grafana | issue: Allow a custom prefix for OpsGenie notifications | keyword: workaround
This issue has been automatically closed because it has not had any further activity in the last 30 days. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/github-actions[bot],https://github.com/grafana/grafana/issues/30080#issuecomment-3194886710,repo: grafana/grafana | issue: Allow a custom prefix for OpsGenie notifications | keyword: workaround
"Cannot install grafana rpm in SLES **What happened**: Grafana requires freetype but that is not provided by OpenSUSE. **What you expected to happen**: It should require based on libraries, not package name (that might differ) **How to reproduce it (as minimally and precisely as possible)**: ``` rpm --import https://packages.grafana.com/gpg.key zypper ar https://packages.grafana.com/oss/rpm grafana zypper ref grafana zypper install grafana ``` **Anything else we need to know?**: I would expect t…",,,,,,Anecdotal,issue,,,,,,,,2022-08-18,github/luizluca,https://github.com/grafana/grafana/issues/53909,repo: grafana/grafana | keyword: workaround | state: closed
"Thanks for creating this issue! We think it's missing some basic information. Follow the issue template and add additional information that will help us replicate the problem. For data visualization issues: - Query results from the inspect drawer (data tab & query inspector) - Panel settings can be extracted in the panel inspect drawer JSON tab For dashboard related issues: - Dashboard JSON can be found in the dashboard settings JSON model view For authentication, provisioning and alerting issu…",,,,,,Anecdotal,comment,,,,,,,,2022-08-18,github/grafanabot,https://github.com/grafana/grafana/issues/53909#issuecomment-1219960573,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
"We've closed this issue since it needs more information and hasn't had any activity recently. We can re-open it after you you add more information. To avoid having your issue closed in the future, please read our [CONTRIBUTING](https://github.com/grafana/grafana/blob/main/CONTRIBUTING.md) guidelines. Happy graphing!",,,,,,Anecdotal,comment,,,,,,,,2022-11-23,github/grafanabot,https://github.com/grafana/grafana/issues/53909#issuecomment-1325497686,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
@luizluca can you please confirm that you cannot run the newest version of Grafana (9.2.5) on SLES 15SP3? thanks a bunch 👍,,,,,,Anecdotal,comment,,,,,,,,2022-11-24,github/zuchka,https://github.com/grafana/grafana/issues/53909#issuecomment-1325874521,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
"We've closed this issue since it needs more information and hasn't had any activity recently. We can re-open it after you you add more information. To avoid having your issue closed in the future, please read our [CONTRIBUTING](https://github.com/grafana/grafana/blob/main/CONTRIBUTING.md) guidelines. Happy graphing!",,,,,,Anecdotal,comment,,,,,,,,2022-12-21,github/grafanabot,https://github.com/grafana/grafana/issues/53909#issuecomment-1361905528,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
"> @luizluca can you please confirm that you cannot run the newest version of Grafana (9.2.5) on SLES 15SP3? thanks a bunch 👍 Yes, the issue is still there. Please reopen the bug.",,,,,,Anecdotal,comment,,,,,,,,2022-12-21,github/luizluca,https://github.com/grafana/grafana/issues/53909#issuecomment-1362104282,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
"We've closed this issue since it needs more information and hasn't had any activity recently. We can re-open it after you you add more information. To avoid having your issue closed in the future, please read our [CONTRIBUTING](https://github.com/grafana/grafana/blob/main/CONTRIBUTING.md) guidelines. Happy graphing!",,,,,,Anecdotal,comment,,,,,,,,2023-03-08,github/grafanabot,https://github.com/grafana/grafana/issues/53909#issuecomment-1459652387,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
"Hi @luizluca I did not find in our documentation as how to install it via zypper. What I found is this [one](https://grafana.com/docs/grafana/latest/setup-grafana/installation/rpm/#install-with-rpm) and just scroll down, you will find a heading of ""**On OpenSUSE or SUSE**"" Please try that and let us know if that works. Regarding the zipper package manager, I am not sure if that is possible or not and will need to ask the team. But please try that and then let us know.",,,,,,Anecdotal,comment,,,,,,,,2023-03-08,github/usmangt,https://github.com/grafana/grafana/issues/53909#issuecomment-1459656724,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
"> Hi @luizluca > > I did not find in our documentation as how to install it via zypper. What I found is this [one](https://grafana.com/docs/grafana/latest/setup-grafana/installation/rpm/#install-with-rpm) and just scroll down, you will find a heading of ""**On OpenSUSE or SUSE**"" > > Please try that and let us know if that works. > > Regarding the zipper package manager, I am not sure if that is possible or not and will need to ask the team. But please try that and then let us know. Zypper uses …",,,,,,Anecdotal,comment,,,,,,,,2023-03-08,github/luizluca,https://github.com/grafana/grafana/issues/53909#issuecomment-1460069208,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
"Grafana have a que unusual packaging system based on go. depends: []string{""/sbin/service"", ""chkconfig"", ""fontconfig"", ""freetype"", ""urw-fonts""}, In this line, ""freetype"" should be something both RHEL and SLES can satisfy. I can't tell why garrafa requires those packages and, without that info, I can't suggest what you could use instead. Btw, it still ships initrd scripts, uses chkconfig,b and those things are quite dead for some time now.",,,,,,Anecdotal,comment,,,,,,,,2023-03-08,github/luizluca,https://github.com/grafana/grafana/issues/53909#issuecomment-1460087453,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
"In RHEL7, freeetype provides: ``` # rpm -q freetype --provides freetype = 2.8-14.el7_9.1 freetype(x86-64) = 2.8-14.el7_9.1 freetype-bytecode libfreetype.so.6()(64bit) ``` This is RHEL91: ``` # rpm -q freetype --provides freetype = 2.10.4-9.el9 freetype(x86-64) = 2.10.4-9.el9 freetype-bytecode freetype-subpixel libfreetype.so.6()(64bit) ``` In SLE12, it has a freetype package, but it looks like a different freetype from RHEL as it does not contain libraries, only programs: ``` # rpm -q freetype-…",,,,,,Anecdotal,comment,,,,,,,,2023-04-03,github/luizluca,https://github.com/grafana/grafana/issues/53909#issuecomment-1494728308,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
"@luizluca mentioned that ""chkconfig"" is still a dependency: > Btw, it still ships initrd scripts, uses chkconfig,b and those things are quite dead for some time now. I had similar problems with SLES 15 from SP1 to SP4: ""chkconfig"" was already installed in /sbin/chkconfig (provided by ""aaa_base | openSUSE Base Package"") but grafana-rpm (9.3.13-1) only checks for chkconfig without the correct path. I did a ""rpmrebuild"" and set ""Requires: /sbin/chkconfig"" -> installation successful",,,,,,Anecdotal,comment,,,,,,,,2023-05-09,github/tectumopticum,https://github.com/grafana/grafana/issues/53909#issuecomment-1539769549,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
"We'll be distributing a new rpm installer for Grafana 10 and I'll look at reducing the amount of dependencies and removing sysvinit scripts, which should solve this problem as `chkconfig` is only used in those scripts.",,,,,,Anecdotal,comment,,,,,,,,2023-05-10,github/kminehart,https://github.com/grafana/grafana/issues/53909#issuecomment-1542312446,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
"> We'll be distributing a new rpm installer for Grafana 10 and I'll look at reducing the amount of dependencies and removing sysvinit scripts, which should solve this problem as `chkconfig` is only used in those scripts. @kminehart perfect! Thanks for your fast reply and your efforts.",,,,,,Anecdotal,comment,,,,,,,,2023-05-11,github/tectumopticum,https://github.com/grafana/grafana/issues/53909#issuecomment-1543635677,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
"> We'll be distributing a new rpm installer for Grafana 10 and I'll look at reducing the amount of dependencies and removing sysvinit scripts, which should solve this problem as `chkconfig` is only used in those scripts. The freetype dependency issue is still there. ``` Problem: nothing provides 'freetype' needed by the to be installed grafana-10.0.0-1.x86_64 Solution 1: do not install grafana-10.0.0-1.x86_64 Solution 2: break grafana-10.0.0-1.x86_64 by ignoring some of its dependencies ```",,,,,,Anecdotal,comment,,,,,,,,2023-06-16,github/luizluca,https://github.com/grafana/grafana/issues/53909#issuecomment-1593864372,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
I see that this issue has been opened for quite a while now. Has anyone gotten to a solution/workaround? I just encountered the same issue on SLES Thumbleweed: `Problem: nothing provides 'freetype' needed by the to be installed grafana-10.3.1-1.x86_64`,,,,,,Anecdotal,comment,,,,,,,,2024-02-07,github/KaiserDMC,https://github.com/grafana/grafana/issues/53909#issuecomment-1932891605,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
"SuSE's build of grafana (assuming it is like OpenSuse at https://download.opensuse.org/repositories/openSUSE:/Factory/standard/src/grafana-10.3.5-1.1.src.rpm) only calls out ``` Requires: group(grafana) Requires: user(grafana) ``` The build process does not find a dependency on either fontconfig or freetype ``` rpm -qp --requires grafana-10.3.5-1.1.x86_64.rpm warning: grafana-10.3.5-1.1.x86_64.rpm: Header V3 RSA/SHA512 Signature, key ID 29b700a4: NOKEY /bin/sh /bin/sh /bin/sh /bin/sh /usr/bin/e…",,,,,,Anecdotal,comment,,,,,,,,2024-05-01,github/jhansonhpe,https://github.com/grafana/grafana/issues/53909#issuecomment-2088404510,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
To follow up my last comment (mostly because I was prodded by this Grafana youtube video - https://www.youtube.com/watch?v=2MWsu0xy5Xc If freetype is changed to libfreetype.so.6()(64bit) (I used rpmrebuild for this) the resulting rpm does install on sles. Also to @kminehart comment from last year it would be good if /sbin/service could be removed as that is legacy system v as well.,,,,,,Anecdotal,comment,,,,,,,,2024-06-05,github/jhansonhpe,https://github.com/grafana/grafana/issues/53909#issuecomment-2150751759,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
The only thing stopping us from removing the freetype dependency from our RPMs is that it might break the image rendering plugin. I'll ask around and see if we can remove it.,,,,,,Anecdotal,comment,,,,,,,,2024-08-09,github/kminehart,https://github.com/grafana/grafana/issues/53909#issuecomment-2277973796,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
> The only thing stopping us from removing the freetype dependency from our RPMs is that it might break the image rendering plugin. > > I'll ask around and see if we can remove it. So don't remove it. You just need to specify something that the free type package provides in both systems. Probably you just need libfreetype6(x86-64),,,,,,Anecdotal,comment,,,,,,,,2024-08-09,github/luizluca,https://github.com/grafana/grafana/issues/53909#issuecomment-2278747725,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
This issue has been automatically marked as stale because it has not had activity in the last year. It will be closed in 30 days if no further activity occurs. Please feel free to leave a comment if you believe the issue is still relevant. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-08-10,github/github-actions[bot],https://github.com/grafana/grafana/issues/53909#issuecomment-3172305455,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
The only dependency we provide to `fpm` via `--depends` is `/sbin/service`. ``` [root@95304546e1ff src]# rpm -qp grafana_12.1.1_16903967602_linux_amd64.rpm --requires /bin/sh /bin/sh /sbin/service rpmlib(CompressedFileNames) <= 3.0.4-1 rpmlib(FileDigests) <= 4.6.0-1 rpmlib(PayloadFilesHavePrefix) <= 4.0-1 ```,,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/kminehart,https://github.com/grafana/grafana/issues/53909#issuecomment-3194685015,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
"Docker image: Grafana 12.1.1 incorrectly built It looks like Grafana 12.1.1 image is not properly built, it only has an AMD64 version ### What happened? When building for arm64, I get the following message ``` InvalidBaseImagePlatform: Base image docker.io/grafana/grafana-oss:12.1.1 was pulled with platform ""linux/amd64"", expected ""linux/arm64"" for current build (line 1) ``` Grafana fails with ”exec format error"" ### What did you expect to happen? Grafana to start like 12.1.0 ### Did this work …",,,,,,Anecdotal,issue,,,,,,,,2025-08-16,github/rickardp,https://github.com/grafana/grafana/issues/109767,repo: grafana/grafana | keyword: workaround | state: closed
"I am also affected by this, running on RaspberryOS bookworm and k3s. Version 12.1.0 on the other hand works fine. Incidentally, version 11.6.5 is affected by the exact same issue, while version 11.6.4 also works fine.",,,,,,Anecdotal,comment,,,,,,,,2025-08-16,github/tvenieris,https://github.com/grafana/grafana/issues/109767#issuecomment-3193886204,repo: grafana/grafana | issue: Docker image: Grafana 12.1.1 incorrectly built | keyword: workaround
I'm currently facing the same issue. It appears that the following Docker image tags are also affected: - [11.4.8-ubuntu](https://hub.docker.com/layers/grafana/grafana-oss/11.4.8-ubuntu/images/sha256-82460545fe6049fd9e3a95b4fff8063bf1bce3a2668bd61756fa65a793cd7380) - [11.5.8-ubuntu](https://hub.docker.com/layers/grafana/grafana-oss/11.5.8-ubuntu/images/sha256-ee52968567cbf26531f46234e8d718078e427d626485fb1fb50cae6593b3431e) - [11.6.5-ubuntu](https://hub.docker.com/layers/grafana/grafana-oss/11.…,,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/yanorei32,https://github.com/grafana/grafana/issues/109767#issuecomment-3194261433,repo: grafana/grafana | issue: Docker image: Grafana 12.1.1 incorrectly built | keyword: workaround
"Hi @kminehart, Would you happen to know anything about the recent Docker image pushes that seem to differ from the usual release flow? Any insights would be greatly appreciated.",,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/yanorei32,https://github.com/grafana/grafana/issues/109767#issuecomment-3194263036,repo: grafana/grafana | issue: Docker image: Grafana 12.1.1 incorrectly built | keyword: workaround
"It seems like this issue is relevant to this: https://github.com/grafana/grafana/issues/109721#issuecomment-3191693484 As a workaround, switching the image to [grafana/grafana](https://hub.docker.com/r/grafana/grafana) should work.",,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/Ry0taK,https://github.com/grafana/grafana/issues/109767#issuecomment-3194300786,repo: grafana/grafana | issue: Docker image: Grafana 12.1.1 incorrectly built | keyword: workaround
"I just tagged the same image as grafana/grafana for those versions but I guess since it's a manifest it doesn't work like that. I'll fix it this afternoon. Like others are saying, just use the grafana/grafana repo. In the near future we'll most likely be deprecating the `grafana/grafana-oss` repo since it's redundant. it should still continue to work for at least a couple versions after the announcement.",,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/kminehart,https://github.com/grafana/grafana/issues/109767#issuecomment-3194435270,repo: grafana/grafana | issue: Docker image: Grafana 12.1.1 incorrectly built | keyword: workaround
"Canvas: Color set by field not working ### What happened? Hi, Using Canvas plugin if you set a color by field (for example the background), the color isn't not set and it seems it's always overide by ""color scheme"". ![image](https://github.com/grafana/grafana/assets/5184702/0ca6e5ca-75dc-41c4-b90c-3d080fb5333d) ![image](https://github.com/grafana/grafana/assets/5184702/f9a55419-b468-46f7-afd9-cd03d209f902) I tried all color schemes, nothing works. I tried with HEX color code, same result. The o…",,,,,,Anecdotal,issue,,,,,,,,2023-08-03,github/battosai30,https://github.com/grafana/grafana/issues/72787,repo: grafana/grafana | keyword: workaround | state: closed
Hi @battosai30 - thank you for submitting this issue! It would be super helpful if you could [provide a debug panel dashboard](https://grafana.com/docs/grafana/latest/troubleshooting/send-panel-to-grafana-support/) so we can replicate the issue you are experiencing with your data. Thanks!,,,,,,Anecdotal,comment,,,,,,,,2023-08-23,github/nmarrs,https://github.com/grafana/grafana/issues/72787#issuecomment-1690714520,repo: grafana/grafana | issue: Canvas: Color set by field not working | keyword: workaround
"Of course :) | Key | Value | |--|--| | Panel | canvas @ 9.4.3 | | Grafana | 9.4.3 (cf0a135595) // Open Source | <details><summary>Panel debug snapshot dashboard</summary> ```json { ""panels"": [ { ""datasource"": { ""type"": ""grafana"", ""uid"": ""grafana"" }, ""fieldConfig"": { ""defaults"": { ""mappings"": [], ""thresholds"": { ""mode"": ""absolute"", ""steps"": [ { ""color"": ""green"", ""value"": null }, { ""color"": ""red"", ""value"": 80 } ] }, ""color"": { ""mode"": ""fixed"" } }, ""overrides"": [] }, ""gridPos"": { ""h"": 13, ""w"": 15,…",,,,,,Anecdotal,comment,,,,,,,,2023-08-25,github/battosai30,https://github.com/grafana/grafana/issues/72787#issuecomment-1692937646,repo: grafana/grafana | issue: Canvas: Color set by field not working | keyword: workaround
This issue has been automatically marked as stale because it has not had activity in the last year. It will be closed in 30 days if no further activity occurs. Please feel free to leave a comment if you believe the issue is still relevant. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-07-17,github/github-actions[bot],https://github.com/grafana/grafana/issues/72787#issuecomment-3082203307,repo: grafana/grafana | issue: Canvas: Color set by field not working | keyword: workaround
This issue has been automatically closed because it has not had any further activity in the last 30 days. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-08-17,github/github-actions[bot],https://github.com/grafana/grafana/issues/72787#issuecomment-3194059824,repo: grafana/grafana | issue: Canvas: Color set by field not working | keyword: workaround
"Enviroment Provider Fails on Special Characters ### What happened? When using the env provider (`$__env{EXAMPLE}`) the resulting value is incorrectly parsed when some special characters are in use, namely: `#` and `;`. This is highlighted in the database configuration as causing potential problems here: ``` # If the password contains # or ; you have to wrap it with triple quotes. Ex """"""#password;"""""" ``` I attempted with both: ``` password: $__env{ENV_PASSWORD} ``` and a triple quote: ``` passwo…",,,,,,Anecdotal,issue,,,,,,,,2024-05-14,github/iveelsm,https://github.com/grafana/grafana/issues/87858,repo: grafana/grafana | keyword: workaround | state: closed
Hi @iveelsm Thank you for creating this issue Can you try to reproduce it in a more recent version of Grafana that we can support (last two mayor versions) ? Thank you,,,,,,Anecdotal,comment,,,,,,,,2024-06-03,github/tonypowa,https://github.com/grafana/grafana/issues/87858#issuecomment-2144731817,repo: grafana/grafana | issue: Enviroment Provider Fails on Special Characters | keyword: workaround
"@tonypowa ~~Absolutely, but I am on vacation for the next week, so I will get back to you~~ In hindsight, I am realizing I actually posted the wrong version. I was looking at the helm chart version. The Grafana version is `10.4.1` My apologies.",,,,,,Anecdotal,comment,,,,,,,,2024-06-04,github/iveelsm,https://github.com/grafana/grafana/issues/87858#issuecomment-2148143090,repo: grafana/grafana | issue: Enviroment Provider Fails on Special Characters | keyword: workaround
"Thanks for your reply One last question, are you using Grafana's default db or are you using a different one (e.g. postgresql ) The issue is being routed to a engineering team for review",,,,,,Anecdotal,comment,,,,,,,,2024-06-05,github/tonypowa,https://github.com/grafana/grafana/issues/87858#issuecomment-2149170806,repo: grafana/grafana | issue: Enviroment Provider Fails on Special Characters | keyword: workaround
"I added this to my helm values to work around the issue for `database.password` ```yaml grafana.ini: database: type: mysql name: grafana host: ${DB_HOSTNAME} user: ${DB_USERNAME} envFromSecrets: - name: grafana-db-secret env: DB_HOSTNAME: <my not so secret db hostname> GF_PATHS_CONFIG: /etc/grafana.patched/grafana.ini extraInitContainers: - name: inject-database-password image: busybox:musl command: [""sh"", ""-c""] args: - | sed ""/^\[database\]$/a\password = $DB_PASSWORD"" \ /etc/grafana/grafana.in…",,,,,,Anecdotal,comment,,,,,,,,2024-07-14,github/kasimeka,https://github.com/grafana/grafana/issues/87858#issuecomment-2227305402,repo: grafana/grafana | issue: Enviroment Provider Fails on Special Characters | keyword: workaround
This issue has been automatically marked as stale because it has not had activity in the last year. It will be closed in 30 days if no further activity occurs. Please feel free to leave a comment if you believe the issue is still relevant. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-07-15,github/github-actions[bot],https://github.com/grafana/grafana/issues/87858#issuecomment-3071668155,repo: grafana/grafana | issue: Enviroment Provider Fails on Special Characters | keyword: workaround
This issue has been automatically closed because it has not had any further activity in the last 30 days. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-08-15,github/github-actions[bot],https://github.com/grafana/grafana/issues/87858#issuecomment-3190415370,repo: grafana/grafana | issue: Enviroment Provider Fails on Special Characters | keyword: workaround
"Transformations: Group by does not change transformation when operation is cleared first ### What happened? When using ""group by"", there are two options that can be set per field. An operation will determine if the field will be calculated, or grouped. You can choose to count per group, or select an aggregation for the calculation. If a field is set to calculate with an aggregation, and the operation field is cleared, the visualization will not reflect that change. However, selecting calculate …",,,,,,Anecdotal,issue,,,,,,,,2025-08-06,github/gelicia,https://github.com/grafana/grafana/issues/109305,repo: grafana/grafana | keyword: workaround | state: closed
Failed to load dashboard Unknown layout element kind: GridLayoutRow - after Update 12.0.2 to 12.1.0 for new Layout Dashboards ### What happened? In Grafana 12.0.2 I created or converted Dashboards to the new layout. In Grafana 12.1.0 the dashboards with the new layout cannot be opened anymore with this error: ``` Failed to load dashboard Unknown layout element kind: GridLayoutRow ``` Dashboards with the old layout work. ### What did you expect to happen? Dashboards should be compatible through …,,,,,,Anecdotal,issue,,,,,,,,2025-07-27,github/Nachtfalkeaw,https://github.com/grafana/grafana/issues/108746,repo: grafana/grafana | keyword: workaround | state: closed
"I can confirm. I have the same issue. And as far as I see, the featureToggle for the new layout is not available anymore. (https://grafana.com/docs/grafana/latest/setup-grafana/configure-grafana/feature-toggles/) I also checked the news article and it doesn't say anything of dashboardNewLayouts feature toggle anymore. Using wayback machine, it was definitvely listed on the site: old version: <img width=""1246"" height=""469"" alt=""Image"" src=""https://github.com/user-attachments/assets/5b5b0929-e412…",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/GunFood,https://github.com/grafana/grafana/issues/108746#issuecomment-3132162955,repo: grafana/grafana | issue: Failed to load dashboard Unknown layout element kind: GridLayoutRow - after Update 12.0.2 to 12.1.0 for new Layout Dashboards | keyword: workaround
"Hello, I tried to export a ""NewLayoutDashboard"" in 12.0.2 and tried to import in on 12.1.0 and I got this error message when I tried to import it. maybe it helpts to identify the issue: `Dashboard.dashboard.grafana.app """" is invalid: [DashboardSpec.layout.kind: Invalid value: conflicting values ""AutoGridLayout"" and ""GridLayout"", DashboardSpec.layout.kind: Invalid value: conflicting values ""RowsLayout"" and ""GridLayout"", DashboardSpec.layout.kind: Invalid value: conflicting values ""TabsLayout"" an…",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/Nachtfalkeaw,https://github.com/grafana/grafana/issues/108746#issuecomment-3145794466,repo: grafana/grafana | issue: Failed to load dashboard Unknown layout element kind: GridLayoutRow - after Update 12.0.2 to 12.1.0 for new Layout Dashboards | keyword: workaround
"I think that 12.0.x converted dashboards into the new format incorrectly (at least somewhat). When the dashboard has rows, the editor doesn't recognize them as ""groups"" at first - you have to click ""Group Panels > Group into row"" at the bottom, and it seems to convert the rows into the new ""groups"" format. <img width=""288"" height=""129"" alt=""Image"" src=""https://github.com/user-attachments/assets/8474968e-67cf-4d3d-b25a-55b5e3986a26"" /> Once I did that on a 12.0.x server, I was able to export / i…",,,,,,Anecdotal,comment,,,,,,,,2025-08-02,github/thetoothpick,https://github.com/grafana/grafana/issues/108746#issuecomment-3146195610,repo: grafana/grafana | issue: Failed to load dashboard Unknown layout element kind: GridLayoutRow - after Update 12.0.2 to 12.1.0 for new Layout Dashboards | keyword: workaround
"@thetoothpick that worked. I could at least migrate the dashboards from 12.0.3 to 12.1 after doing your trick. However then I wanted to convert from v2 back to v1 (because I want to revert some by accident converted dashboards). So I open the dashboard in 12.1 click on ""Export"" --> export as code --> v1 and then I get this error: I selected all rows/groups I had and used ""Auto Grid"" but this does not do the trick :-/ Maybe any ideas ? ``` { ""error"": ""Failed to convert dashboard to v1. Error: Ca…",,,,,,Anecdotal,comment,,,,,,,,2025-08-02,github/Nachtfalkeaw,https://github.com/grafana/grafana/issues/108746#issuecomment-3146703219,repo: grafana/grafana | issue: Failed to load dashboard Unknown layout element kind: GridLayoutRow - after Update 12.0.2 to 12.1.0 for new Layout Dashboards | keyword: workaround
I followed this and grouped everything as row and used Custom grid (and Auto grid) but Not able to Export. Based on this doc https://grafana.com/docs/grafana/latest/observability-as-code/schema-v2/layout-schema/ @axelavargas,,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/Nachtfalkeaw,https://github.com/grafana/grafana/issues/108746#issuecomment-3149976849,repo: grafana/grafana | issue: Failed to load dashboard Unknown layout element kind: GridLayoutRow - after Update 12.0.2 to 12.1.0 for new Layout Dashboards | keyword: workaround
Hi @Nachtfalkeaw! You are using the experimental `dashboardNewLayouts` feature toggle and we had to do some crucial refactoring around unifying different row types into one. This refactoring resulted in previously created rows with `dashboardNewLayouts` feature toggle enabled invalid and therefore the errors. This is experimental feature toggle and destructive changes can happen more info on our release life cycle [here](https://grafana.com/docs/release-life-cycle/). Workaround: Since this refa…,,,,,,Anecdotal,comment,,,,,,,,2025-08-11,github/Sergej-Vlasov,https://github.com/grafana/grafana/issues/108746#issuecomment-3174206456,repo: grafana/grafana | issue: Failed to load dashboard Unknown layout element kind: GridLayoutRow - after Update 12.0.2 to 12.1.0 for new Layout Dashboards | keyword: workaround
"@Sergej-Vlasov Thank you for your reply. I understand that this is experimental. I was able to migrate a dashboard from 12.0.x to 12.1.x. however I can not export this dashboard in 12.1.x as v1 Dashboard. If I want to export this in 12.1.x then I still get this error: ``` { ""error"": ""Failed to convert dashboard to v1. Error: Cannot convert non-GridLayout layout to v1"" } ``` Based on this added feature in 12.1.x I assumed I could export and existing dashboard as v1 schema however it does not wor…",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/Nachtfalkeaw,https://github.com/grafana/grafana/issues/108746#issuecomment-3181105059,repo: grafana/grafana | issue: Failed to load dashboard Unknown layout element kind: GridLayoutRow - after Update 12.0.2 to 12.1.0 for new Layout Dashboards | keyword: workaround
@Nachtfalkeaw in order to export a dashboard as V1 you first need to remove all V2 features like tabs and rows grouping as well as change auto grids to custom grids.,,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/Sergej-Vlasov,https://github.com/grafana/grafana/issues/108746#issuecomment-3183222827,repo: grafana/grafana | issue: Failed to load dashboard Unknown layout element kind: GridLayoutRow - after Update 12.0.2 to 12.1.0 for new Layout Dashboards | keyword: workaround
"Table: Hiding first row in override hides count rows footer ### What happened? In the table panel, there is the option to show a footer, andto display the single count for all data rows. When this is toggled, an extra row is appended to the bottom of the table, and the count is added to the leftmost cell. When that column is hidden with an override, the count is hidden along with the rest of the column, even though there is valid data for the count. The workaround for this is using the ""Organiz…",,,,,,Anecdotal,issue,,,,,,,,2025-08-12,github/gelicia,https://github.com/grafana/grafana/issues/109533,repo: grafana/grafana | keyword: workaround | state: closed
Investigations: Move Grafana core packages to pkg/registry/apps **What is this feature?** Moves the packages from Investigations that import from `github.com/grafana/grafana/pkg/... `into `pkg/registry/apps` to follow our convention that `apps/` shouldn't be importing from Grafana core. Otherwise this should be a no-op. **Why do we need this feature?** Removes the workaround since `apps/` shouldn't be importing from Grafana core at all. **Who is this feature for?** Developers **Which issue(s) d…,,,,,,Anecdotal,issue,,,,,,,,2025-08-11,github/macabu,https://github.com/grafana/grafana/pull/109477,repo: grafana/grafana | keyword: workaround | state: closed
"BulkDeleteProvisionedResource: Use resource names to perform bulk deletion job **What is this feature?** - Update `BulkDeleteProvisionedResource` to use delete job - Update `BulkMoveProvisionedResource` to use move job - Clean up all previous utils/hook that was created for a quick workaround, they are no longer needed. Bulk delete: https://github.com/user-attachments/assets/059daa05-9533-41fe-917f-d9a422856948 Bulk move: https://github.com/user-attachments/assets/5dac5078-d4d9-4855-8164-5fa7ff…",,,,,,Anecdotal,issue,,,,,,,,2025-08-06,github/ywzheng1,https://github.com/grafana/grafana/pull/109281,repo: grafana/grafana | keyword: workaround | state: closed
"@ywzheng1 I love the summary and the effect ❤️ I think we should offer simply a ""Close"" button and ""Open Pull Request"" button if they push to the branch and replace the ""Delete/Move + Cancel"" buttons.",,,,,,Anecdotal,comment,,,,,,,,2025-08-07,github/MissingRoberto,https://github.com/grafana/grafana/pull/109281#issuecomment-3162554153,repo: grafana/grafana | issue: BulkDeleteProvisionedResource: Use resource names to perform bulk deletion job | keyword: workaround
Advisor: Move Grafana core packages to pkg/registry/apps **What is this feature?** Moves the packages from Advisor that import from `github.com/grafana/grafana/pkg/...` into `pkg/registry/apps` to follow the convention. Otherwise this should be a no-op. **Why do we need this feature?** Removes the workaround since `apps/` shouldn't be importing from Grafana core at all. **Who is this feature for?** Developers **Which issue(s) does this PR fix?**: N/A **Special notes for your reviewer:** Please …,,,,,,Anecdotal,issue,,,,,,,,2025-08-11,github/macabu,https://github.com/grafana/grafana/pull/109475,repo: grafana/grafana | keyword: workaround | state: closed
"Alerting: Notification policies - Contact Points not available for selections after upgrade to 12.1 ### What happened? After upgrade to Grafana 12.1 contact points are not available for filtering and adding child policy. ### What did you expect to happen? Contact points should be available for selection, both filtering and adding child policy. ### Did this work before? Yes, in Grafana 12.0.2 ### How do we reproduce it? 1. Serve Grafana from sub path (eg. /grafana) 2. Add Contact Point 3. In /gr…",,,,,,Anecdotal,issue,,,,,,,,2025-08-04,github/agebhar1,https://github.com/grafana/grafana/issues/109104,repo: grafana/grafana | keyword: workaround | state: closed
"+1 on this issue same problem on Grafana v12.1.0 Grafana running on a VM with Debian stable, package from the Grafana OSS repo Client on safari on macOS 15.5",,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/recioalex,https://github.com/grafana/grafana/issues/109104#issuecomment-3151172527,repo: grafana/grafana | issue: Alerting: Notification policies - Contact Points not available for selections after upgrade to 12.1 | keyword: workaround
👍 observed this in our infra as well. we're running with kubernetes behind an ingress handling prometheus and grafana on separate subpaths.,,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/hjpotter92,https://github.com/grafana/grafana/issues/109104#issuecomment-3153332553,repo: grafana/grafana | issue: Alerting: Notification policies - Contact Points not available for selections after upgrade to 12.1 | keyword: workaround
"Same here 12.1.0, under a subpath A temp workaround for apache2 users: Create a rewrite rule inside your sites-available/XX.conf file ```bash RewriteEngine On RewriteRule ^/apis/(.*)$ /YOUR_SUBPATH/apis/$1 [R] ``` After that reload apache2 This will redirect all `/apis/` request that hit your base domain to your subpath",,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/LumePart,https://github.com/grafana/grafana/issues/109104#issuecomment-3154267703,repo: grafana/grafana | issue: Alerting: Notification policies - Contact Points not available for selections after upgrade to 12.1 | keyword: workaround
"GitSync provisioning in v12: ""get client for kind: the server could not find the requested resource"" ### What happened? Since i configured and use GitSync yesterday on my self-hosted grafana i get following error when i try to edit/save a provisioned dashboard: ``` logger=grafana-apiserver t=2025-05-08T08:58:13.705581035Z level=error msg=""Unhandled Error"" err=""apiserver received an error that is not an metav1.Status: &fmt.wrapError{msg:\""get client for kind: the server could not find the reques…",,,,,,Anecdotal,issue,,,,,,,,2025-05-08,github/ojsef39,https://github.com/grafana/grafana/issues/105092,repo: grafana/grafana | keyword: workaround | state: closed
"i'm also facing issue with new gitsync feature in grafana self hosted version 12 . ``logger=dashboard-service t=2025-05-10T04:18:58.817484719Z level=info msg=""No last resource version found, starting from scratch"" orgID=1` ` and in UI, it shows error as invalid url(https://github.com/test-user/observability-dev)",,,,,,Anecdotal,comment,,,,,,,,2025-05-10,github/kadhamecha-conga,https://github.com/grafana/grafana/issues/105092#issuecomment-2868335751,"repo: grafana/grafana | issue: GitSync provisioning in v12: ""get client for kind: the server could not find the requested resource"" | keyword: workaround"
"I'm seeing the same issue, wondering if it could be a dashboard version conflict that's being thrown [here](https://github.com/grafana/grafana/blob/main/pkg/registry/apis/provisioning/jobs/export/resources.go#L24)? EDIT: This issue was resolved by running Grafana the nightly build version from [here](https://grafana.com/grafana/download/12.1.0-88168?platform=docker).",,,,,,Anecdotal,comment,,,,,,,,2025-05-13,github/mosesbug,https://github.com/grafana/grafana/issues/105092#issuecomment-2877528746,"repo: grafana/grafana | issue: GitSync provisioning in v12: ""get client for kind: the server could not find the requested resource"" | keyword: workaround"
"I'm in the same situation with: * Grafana app version: `12.0.0-security-01` * ORG owned GitHub PAT (Personal Access Token) * OSS version deployed using helm chart: `grafana-9.2.1` * Using Grafana organizations - modified dashboard NOT in the default one --- My status error code seems to be different from the reported one: ```bash logger=grafana-apiserver t=2025-05-29T15:36:04.224966804Z level=error msg=""Unhandled Error"" err=""apiserver received an error that is not an metav1.Status: &fmt.wrapErr…",,,,,,Anecdotal,comment,,,,,,,,2025-05-29,github/DavideAG,https://github.com/grafana/grafana/issues/105092#issuecomment-2919813502,"repo: grafana/grafana | issue: GitSync provisioning in v12: ""get client for kind: the server could not find the requested resource"" | keyword: workaround"
"I'm in the same situation here: Grafana app version: v12.0.2 ORG owned GitHub PAT (Personal Access Token) OSS version deployed using helm chart: 9.2.1 ` logger=context userId=1 orgId=1 uname=admin t=2025-06-18T09:35:00.591520739Z level=info msg=""Request Completed"" method=GET path=/apis/folder.grafana.app/v0alpha1/namespaces/default/folders/negocio-bqoh9qarck1ck6z-flg3yc4otmzbwm1z status=404 remote_addr=127.0.0.1 time_ms=56 duration=56.638052ms size=206 referer=""https://grafana.test.com/dashboar…",,,,,,Anecdotal,comment,,,,,,,,2025-06-18,github/bernardolankheet,https://github.com/grafana/grafana/issues/105092#issuecomment-2983437926,"repo: grafana/grafana | issue: GitSync provisioning in v12: ""get client for kind: the server could not find the requested resource"" | keyword: workaround"
"Errors are different when saving a new dashboard or modifying a provisioned dashboard > get client for kind: the server could not find the requested resource vs when saving an imported (from google monitoring plugin) dashboard > repositories.provisioning.grafana.app ""stackdriver"" not found Running chart 9.2.7 with 12.0.2 grafana",,,,,,Anecdotal,comment,,,,,,,,2025-06-26,github/MarijnMB,https://github.com/grafana/grafana/issues/105092#issuecomment-3006584750,"repo: grafana/grafana | issue: GitSync provisioning in v12: ""get client for kind: the server could not find the requested resource"" | keyword: workaround"
"Having the same issue here, ``` Error saving dashboard get client for kind: the server could not find the requested resource ```` I'm having to manually copy the JSON modal -> paste it into the git tree -> commit and only then it works. Not sure how to fix this. I have **GitSync** setup with `grafana/dashboards` folder, but I'm assuming it's trying to get the file info from `{root}/<folder_name>/<dash_name>` instead of `{grafana_root}/<folder_name>/<dash_name>`. I'm on `kube-prometheus-stack` c…",,,,,,Anecdotal,comment,,,,,,,,2025-06-27,github/CaffeineDuck,https://github.com/grafana/grafana/issues/105092#issuecomment-3011640497,"repo: grafana/grafana | issue: GitSync provisioning in v12: ""get client for kind: the server could not find the requested resource"" | keyword: workaround"
"> I have **GitSync** setup with `grafana/dashboards` folder, but I'm assuming it's trying to get the file info from `{root}/<folder_name>/<dash_name>` instead of `{grafana_root}/<folder_name>/<dash_name>`. I have it directly in the root and also have this issue",,,,,,Anecdotal,comment,,,,,,,,2025-06-27,github/ojsef39,https://github.com/grafana/grafana/issues/105092#issuecomment-3011950021,"repo: grafana/grafana | issue: GitSync provisioning in v12: ""get client for kind: the server could not find the requested resource"" | keyword: workaround"
"> > I have **GitSync** setup with `grafana/dashboards` folder, but I'm assuming it's trying to get the file info from `{root}/<folder_name>/<dash_name>` instead of `{grafana_root}/<folder_name>/<dash_name>`. > > I have it directly in the root and also have this issue Did you find any solution/ workaround? How are you navigating around it?",,,,,,Anecdotal,comment,,,,,,,,2025-06-27,github/CaffeineDuck,https://github.com/grafana/grafana/issues/105092#issuecomment-3012016112,"repo: grafana/grafana | issue: GitSync provisioning in v12: ""get client for kind: the server could not find the requested resource"" | keyword: workaround"
"@CaffeineDuck i use the nightly build like it was mentioned previously, not perfect but most stuff seems to work there (altho a newer nightly build than it was mentioned but i dont remember which one exactly) > EDIT: This issue was resolved by running Grafana the nightly build version from [here](https://grafana.com/grafana/download/12.1.0-88168?platform=docker).",,,,,,Anecdotal,comment,,,,,,,,2025-06-27,github/ojsef39,https://github.com/grafana/grafana/issues/105092#issuecomment-3012022492,"repo: grafana/grafana | issue: GitSync provisioning in v12: ""get client for kind: the server could not find the requested resource"" | keyword: workaround"
"I’m seeing the same error (get client for kind: the server could not find the requested resource) when trying to save a dashboard after editing it in the grafana UI. Using Grafana 12.0.2 with Git Sync enabled, deployed via the official Helm chart. Both provisioning and kubernetesDashboards feature toggles are set. ""PR option while saving"" is enabled as well. I have the same expectation mentioned in this issue — that Git Sync should create a PR to the target branch with proposed changes from UI.",,,,,,Anecdotal,comment,,,,,,,,2025-07-10,github/mursheda,https://github.com/grafana/grafana/issues/105092#issuecomment-3057118373,"repo: grafana/grafana | issue: GitSync provisioning in v12: ""get client for kind: the server could not find the requested resource"" | keyword: workaround"
"Quick update: the nightly build 12.1.0-91168 fixes basically everything, BUT now its not posting a comment into the MR anymore ``` 2025-07-12 13:48:25.174 | t=2025-07-12T11:48:25.174637271Z level=info caller=logger.go:214 time=2025-07-12T11:48:25.174620913Z msg=""context done"" channel=watch/provisioning.grafana.app/v0alpha1/repositories/cenrxfj5wykg0f channel=watch/provisioning.grafana.app/v0alpha1/repositories/cenrxfj5wykg0f 2025-07-12 13:47:28.530 | t=2025-07-12T11:47:28.53043873Z level=info c…",,,,,,Anecdotal,comment,,,,,,,,2025-07-12,github/ojsef39,https://github.com/grafana/grafana/issues/105092#issuecomment-3065125333,"repo: grafana/grafana | issue: GitSync provisioning in v12: ""get client for kind: the server could not find the requested resource"" | keyword: workaround"
I have the same issue as @ojsef39 except with OnCreate. I imagine the huge PR #107889 might have something to do with it. It looks like the github client isn't authenticated when creating or modifying the webhooks. UPDATE: The latest nightly container build ([12.1.0-254718](https://hub.docker.com/layers/grafana/grafana-dev/12.1.0-254718/images/sha256-c6995e86f924314d767609ea95e79f4e7e99a4fcc620d2bd4c5fe7dcfd114923) fixed it. Everything works as expected now! The initial setup was quite rough th…,,,,,,Anecdotal,comment,,,,,,,,2025-07-17,github/andsens,https://github.com/grafana/grafana/issues/105092#issuecomment-3083421348,"repo: grafana/grafana | issue: GitSync provisioning in v12: ""get client for kind: the server could not find the requested resource"" | keyword: workaround"
@ojsef39 the webhook issue was also solved since you posted your message. We are in the process of bug hunting and while improving the experimental feature.,,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/MissingRoberto,https://github.com/grafana/grafana/issues/105092#issuecomment-3167561535,"repo: grafana/grafana | issue: GitSync provisioning in v12: ""get client for kind: the server could not find the requested resource"" | keyword: workaround"
"@andsens yeah, the migration from legacy dashboards is not very robust and if you select the ""history"" option when you connect the repository, you may create a ton of commits . I think if you try the migration now, you may find it smoother as it will only create 1 commit for all dashboards if you don't select the history option. closing this issue. Thank you again for reporting and trying things out!",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/MissingRoberto,https://github.com/grafana/grafana/issues/105092#issuecomment-3167567387,"repo: grafana/grafana | issue: GitSync provisioning in v12: ""get client for kind: the server could not find the requested resource"" | keyword: workaround"
"Config: date formats no longer configurable via environment variables in 12.1.0 ### What happened? The [date formats](https://grafana.com/docs/grafana/latest/setup-grafana/configure-grafana/#date_formats) should be configurable and should also be [overridable via environment variables](https://grafana.com/docs/grafana/latest/setup-grafana/configure-grafana/#override-configuration-with-environment-variables). This works in 12.0.3, but not in 12.1.0. I'm using docker and environment variables; I …",,,,,,Anecdotal,issue,,,,,,,,2025-07-28,github/chennin,https://github.com/grafana/grafana/issues/108808,repo: grafana/grafana | keyword: workaround | state: closed
"Hey @chennin - this is a regression we slipped in from https://github.com/grafana/grafana/pull/102254. Sorry about that! This PR incorrectly put a new config section in the middle of the `[date_formats]` section, which 'moved' the rest of the date format values. We look for GF_ env vars based on the structure for the config files. Ther should be two workarounds until we land a fix: - use `GF_TIME_PICKER_INTERVAL_DAY` environment variable instead until we fix this - add a `config.ini` that conta…",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/joshhunt,https://github.com/grafana/grafana/issues/108808#issuecomment-3160570136,repo: grafana/grafana | issue: Config: date formats no longer configurable via environment variables in 12.1.0 | keyword: workaround
"Feature to change time zone in Grafana Alerting Notification Template <!-- Please only use this template for submitting feature requests --> **Why is this needed**: I am currently facing an issue with the alerting notification template in Grafana where the timezone is always set to UTC, regardless of my settings to change the timezone on Grafana dashboard. **What would you like to be added**: The Grafana alert notification should have a feature to change the timezone and it should display the t…",,,,,,Anecdotal,issue,,,,,,,,2024-06-12,github/aditya-opsverse,https://github.com/grafana/grafana/issues/89086,repo: grafana/grafana | keyword: workaround | state: closed
This issue has been automatically marked as stale because it has not had activity in the last year. It will be closed in 30 days if no further activity occurs. Please feel free to leave a comment if you believe the issue is still relevant. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-07-08,github/github-actions[bot],https://github.com/grafana/grafana/issues/89086#issuecomment-3047129402,repo: grafana/grafana | issue: Feature to change time zone in Grafana Alerting Notification Template | keyword: workaround
This issue has been automatically closed because it has not had any further activity in the last 30 days. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-08-07,github/github-actions[bot],https://github.com/grafana/grafana/issues/89086#issuecomment-3162190135,repo: grafana/grafana | issue: Feature to change time zone in Grafana Alerting Notification Template | keyword: workaround
Update dependency eslint-plugin-jsdoc to v52 This PR contains the following updates: | Package | Change | Age | Confidence | |---|---|---|---| | [eslint-plugin-jsdoc](https://redirect.github.com/gajus/eslint-plugin-jsdoc) | [`50.6.3` -> `52.0.2`](https://renovatebot.com/diffs/npm/eslint-plugin-jsdoc/50.6.3/52.0.2) | [![age](https://developer.mend.io/api/mc/badges/age/npm/eslint-plugin-jsdoc/52.0.2?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.men…,,,,,,Anecdotal,issue,,,,,,,,2025-08-04,github/renovate[bot],https://github.com/grafana/grafana/pull/109129,repo: grafana/grafana | keyword: workaround | state: closed
"Prometheus: Fallback series endpoint when labels endpoint is not supported or implemented For some edge cases we have datasources which don't have support for `labels` endpoint support. Instead of that they have `series` endpoint support. See `labels` API - https://prometheus.io/docs/prometheus/latest/querying/api/#getting-label-names - https://prometheus.io/docs/prometheus/latest/querying/api/#querying-label-values So while requesting the resources like labels and label values, if the labels a…",,,,,,Anecdotal,issue,,,,,,,,2025-05-16,github/itsmylife,https://github.com/grafana/grafana/issues/105562,repo: grafana/grafana | keyword: workaround | state: closed
"**Project Update** **Highlight** New resource client is implemented. This fallback logic will be implemented soon. **Trending** <!-- Keep one, delete the rest--> <!-- data key=""trending"" start --> 🟢 on track <!-- data end --> **Next** <!-- This section is for what is coming up next--> Implement fallback logic in new resource client <!-- data key=""isReport"" value=""true"" -->",,,,,,Anecdotal,comment,,,,,,,,2025-06-21,github/itsmylife,https://github.com/grafana/grafana/issues/105562#issuecomment-2993610600,repo: grafana/grafana | issue: Prometheus: Fallback series endpoint when labels endpoint is not supported or implemented | keyword: workaround
"**Project Update** No update. **Trending** <!-- Keep one, delete the rest--> <!-- data key=""trending"" start --> 🟢 on track <!-- data end --> **Next** Validating product alignment. <!-- data key=""isReport"" value=""true"" -->",,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/zoltanbedi,https://github.com/grafana/grafana/issues/105562#issuecomment-3019099351,repo: grafana/grafana | issue: Prometheus: Fallback series endpoint when labels endpoint is not supported or implemented | keyword: workaround
"**Issue Summary** We have identified compatibility challenges with accounts that utilize a specialized fork of Thanos, which presents limitations when interfacing with standard Prometheus implementations. The current workaround requires implementing custom modifications specifically tailored to accommodate this particular Thanos variant. This approach introduces additional complexity to our Prometheus data source integration. Wa need to prioritize maintaining universal solutions that can serve …",,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/k-munoz,https://github.com/grafana/grafana/issues/105562#issuecomment-3150752081,repo: grafana/grafana | issue: Prometheus: Fallback series endpoint when labels endpoint is not supported or implemented | keyword: workaround
"Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) ### What happened? I have 32 GB of memory and an Intel Core i7-10700 CPU @ 2.90GHz. Running windows 11 latest build. After loading a fairly complex dashboard of 30 panels, and scrolling halfway to the bottom of the dashboard, the dashboard freezes and I get a browser out of memory error message for both Chrome and Firefox. Windows task manager reports 98% memory usage. After closing the grafana tab, memory …",,,,,,Anecdotal,issue,,,,,,,,2024-10-24,github/BuxtonCalvin,https://github.com/grafana/grafana/issues/95370,repo: grafana/grafana | keyword: workaround | state: closed
"Hello @BuxtonCalvin Could you please, if possible, share dashboard.json and `.HAR` file so we can investigate a little bit further? With a crashing browser it might be tricky but it would help us greatly.",,,,,,Anecdotal,comment,,,,,,,,2024-11-05,github/itsmylife,https://github.com/grafana/grafana/issues/95370#issuecomment-2457527565,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"Also, I can only send the .har file for grafana:9.5.20 as the out of memory bug freezes the browser for version 11.2.2.",,,,,,Anecdotal,comment,,,,,,,,2024-11-05,github/BuxtonCalvin,https://github.com/grafana/grafana/issues/95370#issuecomment-2458174423,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"@BuxtonCalvin, I'd suggest you use a har file sanitizer. When you search it you'll find many of them. i.e. https://blog.cloudflare.com/introducing-har-sanitizer-secure-har-sharing/ or you can use a script to sanitize it. an example script: ```sh #!/bin/bash # Usage: ./obfuscate_har.sh input.har output_obfuscated.har input_file=$1 output_file=$2 obfuscation_string=""****OBFUSCATED****"" # Check if jq is installed if ! command -v jq &> /dev/null then echo ""jq could not be found. Please install jq t…",,,,,,Anecdotal,comment,,,,,,,,2024-11-06,github/itsmylife,https://github.com/grafana/grafana/issues/95370#issuecomment-2459800486,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"@itsmylife OK thx. Attached are the files. The ""redacted_11.2.2.har.dmp"" file is from the initial browser load before scrolling down (which seems to be the action that kicks off the out of memory bug). The redacted_9.5.20.har.dmp file is from the working version of Grafana. [GrafanaDashboard.json](https://github.com/user-attachments/files/17651519/GrafanaDashboard.json) [redacted_11.2.2.har.zip](https://github.com/user-attachments/files/17651550/redacted_11.2.2.har.zip) [redacted_9.5.20.har.zip…",,,,,,Anecdotal,comment,,,,,,,,2024-11-06,github/BuxtonCalvin,https://github.com/grafana/grafana/issues/95370#issuecomment-2460507886,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"The attached dashboard contains ` ""transformations"": [ { ""id"": ""joinByField"", ""options"": { ""byField"": ""ifName"", ""mode"": ""inner"" } },` for sure a duplicate of #90659",,,,,,Anecdotal,comment,,,,,,,,2025-04-11,github/vcill,https://github.com/grafana/grafana/issues/95370#issuecomment-2796063663,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"> The attached dashboard contains ` ""transformations"": [ { ""id"": ""joinByField"", ""options"": { ""byField"": ""ifName"", ""mode"": ""inner"" } },` > > for sure a duplicate of [#90659](https://github.com/grafana/grafana/issues/90659) Good job! I removed the two panels that used this transform and I was able to load the dashboard without a problem using the latest Grafana docker release. This workaround doesn't solve the problem, but at least it's clear what the problem is.",,,,,,Anecdotal,comment,,,,,,,,2025-04-11,github/BuxtonCalvin,https://github.com/grafana/grafana/issues/95370#issuecomment-2797912411,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
@BuxtonCalvin Sorry for late reply but I am glad that you are able to load your dashboard. The problem seems like a transformation issue. I am gonna tag the right team that handles transformations.,,,,,,Anecdotal,comment,,,,,,,,2025-04-22,github/itsmylife,https://github.com/grafana/grafana/issues/95370#issuecomment-2822093977,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"We use Azure-managed Grafana, and recently we were forced to upgrade from v10 to v11. Unfortunately, we're now experiencing the same issue. Currently, we've implemented a workaround using a small PowerShell script that automatically closes and reopens Edge browser windows every hour, but this is quite annoying. Do you know when the ""Transformations"" team is expected to start working on a fix for this issue?",,,,,,Anecdotal,comment,,,,,,,,2025-07-11,github/Vrx555,https://github.com/grafana/grafana/issues/95370#issuecomment-3060838538,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"For everyone experiencing problems with the JOIN transform, I'd be curious what your experience is with Grafana 12.1 https://github.com/grafana/grafana/pull/105592 was merged last month and was a significant performance improvement on joins.",,,,,,Anecdotal,comment,,,,,,,,2025-07-16,github/gelicia,https://github.com/grafana/grafana/issues/95370#issuecomment-3079976347,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"> For everyone experiencing problems with the JOIN transform, I'd be curious what your experience is with Grafana 12.1 > > [#105592](https://github.com/grafana/grafana/pull/105592) was merged last month and was a significant performance improvement on joins. @gelicia As far as I can tell, the ""latest"" docker version available to the public is ""12.0.2"", which does not work with the JOIN transform. When will you push the 12.1 update to the docker repository?",,,,,,Anecdotal,comment,,,,,,,,2025-07-17,github/BuxtonCalvin,https://github.com/grafana/grafana/issues/95370#issuecomment-3082493751,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
@gelicia I tried with grafana-nightly_12.1.0-254011_254011 but the browser still crashes (out of memory error on join). For now we've downgraded to grafana 11.0.11 where this works fine. It seems the issue slipped in with the fix for #87409,,,,,,Anecdotal,comment,,,,,,,,2025-07-17,github/vcill,https://github.com/grafana/grafana/issues/95370#issuecomment-3082694318,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"> When will you push the 12.1 update to the docker repository? Right now, its planned that the 12.1 release is in a week or so. > I tried with grafana-nightly_12.1.0-254011_254011 but the browser still crashes How big are the data sets you are joining? Any details that could help us try to replicate your scenario?",,,,,,Anecdotal,comment,,,,,,,,2025-07-17,github/gelicia,https://github.com/grafana/grafana/issues/95370#issuecomment-3085070977,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"@gelicia Basically we are joining 5 tables, each of them has ~200 rows. I attached my dashboard to #90659 already, maybe it can help? Or what else can I provide to help you investigate the issue?",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/vcill,https://github.com/grafana/grafana/issues/95370#issuecomment-3095783994,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"@vcill I'm mostly curious on your data. I'm wondering if just the join is not the correct transformation for you. For example, I'm wondering if `uname`, the data you are joining by, is not very unique on your prometheus data. That will balloon out the data in a join. Take for example this https://play.grafana.org/d/feavrpzb4wu0wa/kristina-demos?orgId=1&from=now-30d&to=now&timezone=utc&var-fruit=apple&var-fuelType=Battery%20storage&var-stand=ice-king&var-employeeLastName=Swan&editPanel=30&tab=qu…",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/gelicia,https://github.com/grafana/grafana/issues/95370#issuecomment-3097696276,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"> I'm wondering if a group by first would help if this is what's happening - grouping by last value of the uname or something like that, to be sure you're not joining by duplicates. I pulled the 12.1 release and tried to open the panels that use innerjoin. The panels still will not open. Then I pulled a separate instance of Grafana 9.5.2.0 and tried to open the panels and they came right up. In 9.5, I then tried to insert a group by transform, saved the panel and imported the panel into 12.1 Th…",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/BuxtonCalvin,https://github.com/grafana/grafana/issues/95370#issuecomment-3124871337,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"> I attached my dashboard to https://github.com/grafana/grafana/issues/90659 already, maybe it can help? Or what else can I provide to help you investigate the issue? unfortunately, what is attached is not very helpful because it does not contain the data. if you can attach a panel debug dashboard, then we can figure out what's going on. https://grafana.com/docs/grafana/latest/troubleshooting/send-panel-to-grafana-support/",,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/leeoniya,https://github.com/grafana/grafana/issues/95370#issuecomment-3125061384,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
Here's one of my panels with diagnostic data: [POEPanel.json.zip](https://github.com/user-attachments/files/21460705/POEPanel.json.zip),,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/BuxtonCalvin,https://github.com/grafana/grafana/issues/95370#issuecomment-3125420728,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"Going to break up comments a bit. First, the replication and RCA. Just wanted to confirm I was able to replicate this (kind of). The data provided was too large, so I reduced it to 4 frames of 5 rows each. I confirmed in the latest version, this results in 625 rows (5^4), and in 9.5.2, it results in 1 row. I made a dashboard that is both small and uses CSV data for further development use with some easy data to see the discrepency. Note when using the docker image for 9.5.2, you will first need…",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/gelicia,https://github.com/grafana/grafana/issues/95370#issuecomment-3134299986,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"To those running into this, you are most likely joining on a duplicated value. If this is the case, you may want to first use the Group by transformation to ensure the data you are joining on is unique. Grouping off the value you are JOINing first, and then selecting the last value of everything else, should help this quite a bit. Obviously my data set is way smaller than yours, but let me know what you find. <img width=""1228"" height=""456"" alt=""Image"" src=""https://github.com/user-attachments/as…",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/gelicia,https://github.com/grafana/grafana/issues/95370#issuecomment-3134304704,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"As for what we plan to do about this, I will discuss it with @leeoniya who implemented the performance improvement on join, and we can figure out a path forward. Thank you for your patience! **Investigation** Tested this in a SQL sandbox, the current behavior is how a SQL join would act. <img width=""1137"" height=""764"" alt=""Image"" src=""https://github.com/user-attachments/assets/84837507-b013-4a1e-a958-c4efa3ae2130"" /> https://www.sandboxsql.com/674dfa20-528c-4da4-af36-baa02e9175c5 ```sql SELECT …",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/gelicia,https://github.com/grafana/grafana/issues/95370#issuecomment-3134308118,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"Hello, after some internal discussion we decided we would rather keep the transformation with the updated logic, and align it with how SQL joins work. We realize this is a breaking change for some users who relied on the previous behavior, but we would ask that you use the ""Group by"" transformation first to ensure what you are joining by is unique before joining. Using group by to get the last value for each group, and then joining by the group, will make it equivalent to the join logic that wa…",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/gelicia,https://github.com/grafana/grafana/issues/95370#issuecomment-3144769114,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"Aligning with standard SQL is a good idea. In standard SQL, a ""GROUP BY"" clause always acts on a data set derived from a SELECT statement: it's not a pre-filter for data used for a join, or perhaps I'm misinterpreting what you're saying. The ""SELECT DISTINCT"" clause is often used to filter data prior to a join. So is SELECT INTO, but I'm by no means a SQL expert so perhaps not all of the Grafana syntax can align with SQL. As stated above, I tried using the GROUP BY clause and it did not work. W…",,,,,,Anecdotal,comment,,,,,,,,2025-08-02,github/BuxtonCalvin,https://github.com/grafana/grafana/issues/95370#issuecomment-3146187769,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"@BuxtonCalvin Your example was really good at getting me started with the RCA, but at 57mb it's too large for me to import directly, I had to cut down the data set. Would it be difficult to reduce the time range to as small as possible that repros the issue and re-upload? I'd be happy to look into why the Group by isn't working, but a smaller data set will be much easier to work with. If not, I can try to finagle the data you already provided, but it's most likely easier to reduce the data and …",,,,,,Anecdotal,comment,,,,,,,,2025-08-02,github/gelicia,https://github.com/grafana/grafana/issues/95370#issuecomment-3146538654,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
This should work. it has about 100 data points [debug-PoE Current House-2025-08-02 21_35_51.json.zip](https://github.com/user-attachments/files/21564029/debug-PoE.Current.House-2025-08-02.21_35_51.json.zip),,,,,,Anecdotal,comment,,,,,,,,2025-08-03,github/BuxtonCalvin,https://github.com/grafana/grafana/issues/95370#issuecomment-3146979803,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"@BuxtonCalvin Perfect, thanks! So, here are the steps I took for this. 1. Took your dashboard, imported it into a 9.5.2 version of grafana and got the result. Via the query inspector, exported out the CSV of the data with the transformations applied so we could be sure the data matched. 2. Opened the dashboard JSON in a text editor and deleted the transforms so we wouldn't get OOM'd on import 3. Imported the edited JSON into Grafana 12.2 (off `main`). Created a panel with the testdata datasourc…",,,,,,Anecdotal,comment,,,,,,,,2025-08-03,github/gelicia,https://github.com/grafana/grafana/issues/95370#issuecomment-3148715822,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"That works! Thank you very much. Originally, I tried to do something similar with the group by transform, but did not use ""calculate last"" for the value transforms. Once I put that formula into place, the transforms worked in the way that I wanted them to. Thank you again and yes this PR can be considered closed on my part.",,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/BuxtonCalvin,https://github.com/grafana/grafana/issues/95370#issuecomment-3148920068,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"@BuxtonCalvin one last thing too I forgot to mention, with the calculate last non null, you might be able to get rid of some of the overrides that filter those out. Not super positive on that but it seems like it's doing the same thing! Thanks for sticking with us as we figured this out!",,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/gelicia,https://github.com/grafana/grafana/issues/95370#issuecomment-3150426641,repo: grafana/grafana | issue: Out of memory on Firefox and Chrome from version grafana:9.5.20 to latest today (11.2.2) | keyword: workaround
"Unable to chose provisioned evaluation group when creating a new alert ### What happened? I'm not able to create a new alert and point it to an existing evaluation group. It shows the group in the dropdown and has the ""provisioned"" label next to it but I'm not able to select. This makes for an annoying workflow when trying to create a new alert. I create the alert in the UI but then I have to create a new bogus evaluation group, then export the yaml for the alerts then go in manually and change…",,,,,,Anecdotal,issue,,,,,,,,2023-12-14,github/danfinn,https://github.com/grafana/grafana/issues/79528,repo: grafana/grafana | keyword: workaround | state: closed
"Would like to see this fixed aswell. We normally create a alert in a evaluation group, export the alert and terraform it. We have to do the same workaround as you just in terraform.",,,,,,Anecdotal,comment,,,,,,,,2024-01-05,github/SimenAsphaug,https://github.com/grafana/grafana/issues/79528#issuecomment-1878731059,repo: grafana/grafana | issue: Unable to chose provisioned evaluation group when creating a new alert | keyword: workaround
Thank you for creating this issue How did you provision the alert rule? If you do it via API you can keep the alerting resources editable in the UI by including the request header `X-Disable-Provenance` ![image](https://github.com/grafana/grafana/assets/45235678/2d4a1c20-e8a0-437b-9663-9a3021eb4452) ![image](https://github.com/grafana/grafana/assets/45235678/29bdf58c-b0b2-410d-92cc-568c34f940f8) https://grafana.com/docs/grafana/latest/developers/http_api/alerting_provisioning/#span-idroute-post…,,,,,,Anecdotal,comment,,,,,,,,2024-01-15,github/tonypowa,https://github.com/grafana/grafana/issues/79528#issuecomment-1892301724,repo: grafana/grafana | issue: Unable to chose provisioned evaluation group when creating a new alert | keyword: workaround
I’m provisioning from a yaml file. I think that even if it’s not editable it should still be an option to select an existing evaluation group without making any changes to it.,,,,,,Anecdotal,comment,,,,,,,,2024-01-15,github/danfinn,https://github.com/grafana/grafana/issues/79528#issuecomment-1892601698,repo: grafana/grafana | issue: Unable to chose provisioned evaluation group when creating a new alert | keyword: workaround
"We cannot allow the creation of a non-provisioned alert rule in a provisioned rule group which is why it is currently not selectable. HOWEVER We are thinking about letting you select a provisioned group but, when doing this, the ""Save button"" becomes an ""Export rule group"" and that would include the new alert rule. We think this would simplify the UX while keeping the provisioned rule group safe from manual edits.",,,,,,Anecdotal,comment,,,,,,,,2024-01-17,github/armandgrillet,https://github.com/grafana/grafana/issues/79528#issuecomment-1895512218,repo: grafana/grafana | issue: Unable to chose provisioned evaluation group when creating a new alert | keyword: workaround
"> We are thinking about letting you select a provisioned group but, when doing this, the ""Save button"" becomes an ""Export rule group"" and that would include the new alert rule. We think this would simplify the UX while keeping the provisioned rule group safe from manual edits. I would love this. I see the workflow as: 1. Select create new alert 2. Fill in everything 3. Export as HCL 4. Add to Terraform and apply for each stack The provisioning whether JSON, YAML, or HCL is pretty tedious to mak…",,,,,,Anecdotal,comment,,,,,,,,2024-05-13,github/joshludwig,https://github.com/grafana/grafana/issues/79528#issuecomment-2108540015,repo: grafana/grafana | issue: Unable to chose provisioned evaluation group when creating a new alert | keyword: workaround
"> We cannot allow the creation of a non-provisioned alert rule in a provisioned rule group which is why it is currently not selectable. > > HOWEVER > > We are thinking about letting you select a provisioned group but, when doing this, the ""Save button"" becomes an ""Export rule group"" and that would include the new alert rule. We think this would simplify the UX while keeping the provisioned rule group safe from manual edits. This sounds like it might not work so well if you are trying to add mul…",,,,,,Anecdotal,comment,,,,,,,,2024-05-13,github/danfinn,https://github.com/grafana/grafana/issues/79528#issuecomment-2108562723,repo: grafana/grafana | issue: Unable to chose provisioned evaluation group when creating a new alert | keyword: workaround
"> We are thinking about letting you select a provisioned group but, when doing this, the ""Save button"" becomes an ""Export rule group"" and that would include the new alert rule. We think this would simplify the UX while keeping the provisioned rule group safe from manual edits. This is a great idea",,,,,,Anecdotal,comment,,,,,,,,2024-07-04,github/lapwat,https://github.com/grafana/grafana/issues/79528#issuecomment-2209125955,repo: grafana/grafana | issue: Unable to chose provisioned evaluation group when creating a new alert | keyword: workaround
This issue has been automatically marked as stale because it has not had activity in the last year. It will be closed in 30 days if no further activity occurs. Please feel free to leave a comment if you believe the issue is still relevant. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-07-05,github/github-actions[bot],https://github.com/grafana/grafana/issues/79528#issuecomment-3037685355,repo: grafana/grafana | issue: Unable to chose provisioned evaluation group when creating a new alert | keyword: workaround
This issue has been automatically closed because it has not had any further activity in the last 30 days. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/github-actions[bot],https://github.com/grafana/grafana/issues/79528#issuecomment-3148928227,repo: grafana/grafana | issue: Unable to chose provisioned evaluation group when creating a new alert | keyword: workaround
"[New Data Source]: Google Spanner ### Background Google Spanner is an always on database with virtually unlimited scale that uses standard GoogleSQL, similar to BigQuery. ### Use case We use Spanner as a production database. Ideally, we'd get equivalent features to the existing BigQuery plugin. ### Contribution - [ ] Are you looking to create the plugin? - [ ] Are you affiliated with the project/product the data source integrates with? - [x] Does the plugin integrate with a commercial product?",,,,,,Anecdotal,issue,,,,,,,,2025-06-23,github/jasuchard,https://github.com/grafana/grafana/issues/107087,repo: grafana/grafana | keyword: gotcha | state: open
"Hi @sympatheticmoose, Yes, I've tried using both plugins, but neither actually supports querying arbitrary data in Spanner. To get Spanner data from the BQ data source, you must use federated queries, which (as far as I can tell) requires you to not use named schemas. This means it doesn't support arbitrary Spanner queries: https://cloud.google.com/bigquery/docs/spanner-external-datasets#limitations To get Spanner data from the Postgres data source, you must have your underlying Spanner instanc…",,,,,,Anecdotal,comment,,,,,,,,2025-07-07,github/jasuchard,https://github.com/grafana/grafana/issues/107087#issuecomment-3046069323,repo: grafana/grafana | issue: [New Data Source]: Google Spanner | keyword: gotcha
Alerting: Add support for inhibition rules This issue tracks the progress for adding support for inhibitions ([`inhibit_rule`](https://prometheus.io/docs/alerting/latest/configuration/#inhibit_rule)). The first version will likely be a very rough implementation allowing mutations to the list of inhibit rules without support for previews of which alerts would be inhibited.,,,,,,Anecdotal,issue,,,,,,,,2023-05-22,github/gillesdemey,https://github.com/grafana/grafana/issues/68822,repo: grafana/grafana | keyword: gotcha | state: open
This issue has been automatically marked as stale because it has not had activity in the last year. It will be closed in 30 days if no further activity occurs. Please feel free to leave a comment if you believe the issue is still relevant. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2024-05-26,github/github-actions[bot],https://github.com/grafana/grafana/issues/68822#issuecomment-2131934990,repo: grafana/grafana | issue: Alerting: Add support for inhibition rules | keyword: gotcha
This issue has been automatically closed because it has not had any further activity in the last 30 days. Thank you for your contributions!,,,,,,Anecdotal,comment,,,,,,,,2024-06-26,github/github-actions[bot],https://github.com/grafana/grafana/issues/68822#issuecomment-2190380293,repo: grafana/grafana | issue: Alerting: Add support for inhibition rules | keyword: gotcha
"It looks like GitHub Actions just automatically closed this as not planned. Does that mean that the feature request for inhibit rules has been declined, and that this just won't be considered? It seems like a pretty massive feature gap for Grafana, and in trying to find a solution for my own needs, I've come across many, many community posts that could probably be solved by either inhibit rules, or some new addition to alert rules themselves that provide a similar functionality.",,,,,,Anecdotal,comment,,,,,,,,2024-09-15,github/jantman,https://github.com/grafana/grafana/issues/68822#issuecomment-2351581997,repo: grafana/grafana | issue: Alerting: Add support for inhibition rules | keyword: gotcha
"The bot just assumes that anything that hasn't had a response in a while isn't likely to get implemented and marks them as stale; there's no consensus within the team to not implement this :) --- Having said that the motivation to implement is quite low, we don't have a lot of users asking us about this particular feature and inhibitions come with several gotchas (George wrote down some of them in [this blog](https://www.grobinson.net/best-practices-for-avoiding-race-conditions-in-inhibition-ru…",,,,,,Anecdotal,comment,,,,,,,,2024-09-18,github/gillesdemey,https://github.com/grafana/grafana/issues/68822#issuecomment-2358905872,repo: grafana/grafana | issue: Alerting: Add support for inhibition rules | keyword: gotcha
"I just stumbled upon some questions in community forums. I would like to re-open it and collect sentiments. If we plan to unifiy Grafana and Mimir Alertmanagers, I think this needs to be implemented sooner or later.",,,,,,Anecdotal,comment,,,,,,,,2024-09-20,github/yuri-tceretian,https://github.com/grafana/grafana/issues/68822#issuecomment-2364570544,repo: grafana/grafana | issue: Alerting: Add support for inhibition rules | keyword: gotcha
"Thanks so much for the attention and feedback on this, and thanks @gillesdemey for the blog post link - that was quite informative, and certainly has increased my appreciation of how nuanced something like inhibition rules can be as. And I can say that I'd definitely be concerned about volume of erroneous complaints, for both the positive and negative failure conditions (not being sent, or not being inhibited). Perhaps inhibition rules would need some new built-in diagnostic mechanism, to make …",,,,,,Anecdotal,comment,,,,,,,,2024-09-23,github/jantman,https://github.com/grafana/grafana/issues/68822#issuecomment-2369133715,repo: grafana/grafana | issue: Alerting: Add support for inhibition rules | keyword: gotcha
"Hi! In my opinion, inhibit rules could be implemented ""as is,"" similar to Alertmanager, without additional logic and, possibly, without a user interface, with minimum functionality ( only configuration files ). Besides, using inhibit rules still requires preliminary prepare, checks and testing, which can be done using amtool. This way, the functionality would only be accessible to those familiar with the subject. Such an approach could allow many users to abandon the use of a external Alertmana…",,,,,,Anecdotal,comment,,,,,,,,2025-01-05,github/heejew,https://github.com/grafana/grafana/issues/68822#issuecomment-2571613292,repo: grafana/grafana | issue: Alerting: Add support for inhibition rules | keyword: gotcha
"I'd like to see this. It's enough for me to push back on Grafana alertmanager altogether at my company and use an external alertmanager. Also, this is a footgun in itself because Grafana alertmanager advertises as an extending Prometheus alertmanager: https://grafana.com/docs/grafana/latest/alerting/set-up/configure-alertmanager/ > Grafana Alertmanager: Grafana includes a built-in Alertmanager that extends the [Prometheus Alertmanager](https://prometheus.io/docs/alerting/latest/alertmanager/). …",,,,,,Anecdotal,comment,,,,,,,,2025-01-23,github/brigitops,https://github.com/grafana/grafana/issues/68822#issuecomment-2610773428,repo: grafana/grafana | issue: Alerting: Add support for inhibition rules | keyword: gotcha
"Also bumping this because as we've been increasingly using Grafana Alerting, we're running into scenarios where we really miss this functionality.",,,,,,Anecdotal,comment,,,,,,,,2025-07-02,github/jdegendt,https://github.com/grafana/grafana/issues/68822#issuecomment-3027650688,repo: grafana/grafana | issue: Alerting: Add support for inhibition rules | keyword: gotcha
Folders: Migrate getFolder API to app platform Changes the getFolder and it's `useGetFolderQueryHook` to use APP latform APIs. This is done by creating a `useGetFolderQueryFacade` that based on the `foldersAppPlatformAPI` either uses the legacy client or the recreates the same FolderDTO from multiple app platform API calls. This should make this change mostly transparent to the front end so no changes to the rendering is needed.,,,,,,Anecdotal,issue,,,,,,,,2025-07-04,github/aocenas,https://github.com/grafana/grafana/pull/107617,repo: grafana/grafana | keyword: gotcha | state: closed
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/16193500216) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **aocenas/folders/migrate-api-2** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#…,,,,,,Anecdotal,comment,,,,,,,,2025-07-10,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/107617#issuecomment-3056977623,repo: grafana/grafana | issue: Folders: Migrate getFolder API to app platform | keyword: gotcha
**Error building instance**: Contact #proj-ephemeral-hg-instances if it is not a compile error. [Logs](https://github.com/grafana/grafana/actions/runs/16193500216),,,,,,Anecdotal,comment,,,,,,,,2025-07-10,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/107617#issuecomment-3057023530,repo: grafana/grafana | issue: Folders: Migrate getFolder API to app platform | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/16194885517) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **aocenas/folders/migrate-api-2** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#…,,,,,,Anecdotal,comment,,,,,,,,2025-07-10,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/107617#issuecomment-3057212270,repo: grafana/grafana | issue: Folders: Migrate getFolder API to app platform | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821107617aocena.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-07-10,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/107617#issuecomment-3057278792,repo: grafana/grafana | issue: Folders: Migrate getFolder API to app platform | keyword: gotcha
"I noticed I get 404 errors for the `general`, `access` and `parents` endpoints (`general` errors when not in the root folder), can be also seen in the ephemeral instance.",,,,,,Anecdotal,comment,,,,,,,,2025-07-10,github/Clarity-89,https://github.com/grafana/grafana/pull/107617#issuecomment-3057729307,repo: grafana/grafana | issue: Folders: Migrate getFolder API to app platform | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/16284533496) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **aocenas/folders/migrate-api-2** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#…,,,,,,Anecdotal,comment,,,,,,,,2025-07-15,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/107617#issuecomment-3071961643,repo: grafana/grafana | issue: Folders: Migrate getFolder API to app platform | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821107617aocena.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-07-15,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/107617#issuecomment-3072009280,repo: grafana/grafana | issue: Folders: Migrate getFolder API to app platform | keyword: gotcha
"Not sure if this one is out of scope for the current implementation, but I _think_ its a bug in the new hook? Or an issue with permissions on the new API. Minimal reproduction, using enterprise...: * Get a test user that has granular read permissions on just 2 folders (and no basic role, so they can't see any other folders) * Go to the Move Folder modal and try to move one folder to a new one * There will be no results shown in the dropdown list (just the `Dashboards` item in the nested picker)…",,,,,,Anecdotal,comment,,,,,,,,2025-07-18,github/tomratcliffe,https://github.com/grafana/grafana/pull/107617#issuecomment-3089929024,repo: grafana/grafana | issue: Folders: Migrate getFolder API to app platform | keyword: gotcha
"unified-storage: fix auth handling in distributor Currently the distributor is not working if you enable certain auth flags which are enabled by default in cloud. Not sure what's going on as I was not able to enable the cloud setup locally, so this PR proposes an alternative: skip auth check in the distributor. The backing resource server will still do the auth check so it's pointless to do it two times anyway.",,,,,,Anecdotal,issue,,,,,,,,2025-05-27,github/gassiss,https://github.com/grafana/grafana/pull/106062,repo: grafana/grafana | keyword: gotcha | state: closed
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/15274621740) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **unistore-distributor-debug-logs** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instance…,,,,,,Anecdotal,comment,,,,,,,,2025-05-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/106062#issuecomment-2912244944,repo: grafana/grafana | issue: unified-storage: fix auth handling in distributor | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821106062gassis.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-05-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/106062#issuecomment-2912302787,repo: grafana/grafana | issue: unified-storage: fix auth handling in distributor | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/15276341994) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **unistore-distributor-debug-logs** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instance…,,,,,,Anecdotal,comment,,,,,,,,2025-05-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/106062#issuecomment-2912484911,repo: grafana/grafana | issue: unified-storage: fix auth handling in distributor | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821106062gassis.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-05-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/106062#issuecomment-2912560858,repo: grafana/grafana | issue: unified-storage: fix auth handling in distributor | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/15277244718) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **unistore-distributor-debug-logs** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instance…,,,,,,Anecdotal,comment,,,,,,,,2025-05-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/106062#issuecomment-2912615259,repo: grafana/grafana | issue: unified-storage: fix auth handling in distributor | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821106062gassis.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-05-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/106062#issuecomment-2912680819,repo: grafana/grafana | issue: unified-storage: fix auth handling in distributor | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/15278708212) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **unistore-distributor-debug-logs** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instance…,,,,,,Anecdotal,comment,,,,,,,,2025-05-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/106062#issuecomment-2912848821,repo: grafana/grafana | issue: unified-storage: fix auth handling in distributor | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821106062gassis.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-05-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/106062#issuecomment-2912989848,repo: grafana/grafana | issue: unified-storage: fix auth handling in distributor | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/15281403490) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **unistore-distributor-debug-logs** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instance…,,,,,,Anecdotal,comment,,,,,,,,2025-05-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/106062#issuecomment-2913330167,repo: grafana/grafana | issue: unified-storage: fix auth handling in distributor | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821106062gassis.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-05-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/106062#issuecomment-2913383656,repo: grafana/grafana | issue: unified-storage: fix auth handling in distributor | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/15281946617) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **unistore-distributor-debug-logs** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instance…,,,,,,Anecdotal,comment,,,,,,,,2025-05-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/106062#issuecomment-2913401326,repo: grafana/grafana | issue: unified-storage: fix auth handling in distributor | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821106062gassis.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-05-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/106062#issuecomment-2913459685,repo: grafana/grafana | issue: unified-storage: fix auth handling in distributor | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/15282897341) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **unistore-distributor-debug-logs** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instance…,,,,,,Anecdotal,comment,,,,,,,,2025-05-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/106062#issuecomment-2913535408,repo: grafana/grafana | issue: unified-storage: fix auth handling in distributor | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821106062gassis.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-05-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/106062#issuecomment-2913589665,repo: grafana/grafana | issue: unified-storage: fix auth handling in distributor | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/15283587189) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **unistore-distributor-debug-logs** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instance…,,,,,,Anecdotal,comment,,,,,,,,2025-05-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/106062#issuecomment-2913646799,repo: grafana/grafana | issue: unified-storage: fix auth handling in distributor | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/15283760340) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **unistore-distributor-debug-logs** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instance…,,,,,,Anecdotal,comment,,,,,,,,2025-05-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/106062#issuecomment-2913700598,repo: grafana/grafana | issue: unified-storage: fix auth handling in distributor | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821106062gassis.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-05-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/106062#issuecomment-2913781340,repo: grafana/grafana | issue: unified-storage: fix auth handling in distributor | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/15285604290) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **unistore-distributor-debug-logs** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instance…,,,,,,Anecdotal,comment,,,,,,,,2025-05-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/106062#issuecomment-2914032461,repo: grafana/grafana | issue: unified-storage: fix auth handling in distributor | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821106062gassis.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-05-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/106062#issuecomment-2914088355,repo: grafana/grafana | issue: unified-storage: fix auth handling in distributor | keyword: gotcha
"refactor(alerting): save state on a ticker async What is this PR about? This PR adds the possibility to save the state of alert instances periodically using a ticker instead of saving them on each evaluation. This will help us to scale beyond what is possible today, as Grafana servers with a lot of alert instances run into write amplification problems over time. What does this PR change? - A new feature flag `alertSaveStateAsync` is introduced which controls if the state is saved on evaluation …",,,,,,Anecdotal,issue,,,,,,,,2023-09-18,github/JohnnyQQQQ,https://github.com/grafana/grafana/pull/74998,repo: grafana/grafana | keyword: gotcha | state: closed
"> I'm not sure if this is a bug, but a strange behaviour I observed in testing is that it continues to save all my states every 30 seconds even after I've deleted all my rules. Is that expected? > > ``` > INFO [09-21|22:08:01] Full state sync start logger=ngalert.state.manager > INFO [09-21|22:08:01] Full state sync done logger=ngalert.state.manager duration=0.181 instances=5000 > INFO [09-21|22:08:31] Full state sync start logger=ngalert.state.manager > INFO [09-21|22:08:31] Full state sync do…",,,,,,Anecdotal,comment,,,,,,,,2023-09-21,github/JohnnyQQQQ,https://github.com/grafana/grafana/pull/74998#issuecomment-1730314348,repo: grafana/grafana | issue: refactor(alerting): save state on a ticker async | keyword: gotcha
"This pull request has been automatically marked as stale because it has not had activity in the last 30 days. It will be closed in 2 weeks if no further activity occurs. Please feel free to give a status update now, ping for review, or re-open when it's ready. Thank you for your contributions!",,,,,,Anecdotal,comment,,,,,,,,2023-12-02,github/github-actions[bot],https://github.com/grafana/grafana/pull/74998#issuecomment-1836987987,repo: grafana/grafana | issue: refactor(alerting): save state on a ticker async | keyword: gotcha
"Can we please have some update on this one? Some of our customers reported that they have been getting several DatasourceError from our alerts with the following annotation: Error: failed to build query 'A': Error 1040: Too many connections They all seem to be on ""prod-us-central-0"" cluster.",,,,,,Anecdotal,comment,,,,,,,,2024-01-05,github/grvsoni,https://github.com/grafana/grafana/pull/74998#issuecomment-1878811986,repo: grafana/grafana | issue: refactor(alerting): save state on a ticker async | keyword: gotcha
"Azure Monitor : Updated error messages for query errors in Azure Resource Graph to be more user friendly <!-- Thank you for sending a pull request! Here are some tips: 1. If this is your first time, please read our contribution guide at https://github.com/grafana/grafana/blob/main/CONTRIBUTING.md 2. Ensure you include and run the appropriate tests as part of your Pull Request. 3. In a new feature or configuration option, an update to the documentation is necessary. Everything related to the doc…",,,,,,Anecdotal,issue,,,,,,,,2022-01-27,github/yaelleC,https://github.com/grafana/grafana/pull/44538,repo: grafana/grafana | keyword: gotcha | state: closed
"To avoid adding too specific parsing to the backend we decided to focus on a frontend fix for now, displaying the JSON in a JSON format: https://github.com/grafana/grafana/pull/44877 A bigger discussion around third party error messages formatting would be needed to provide the issue with a better resolution Closing this PR",,,,,,Anecdotal,comment,,,,,,,,2022-02-11,github/yaelleC,https://github.com/grafana/grafana/pull/44538#issuecomment-1035987568,repo: grafana/grafana | issue: Azure Monitor : Updated error messages for query errors in Azure Resource Graph to be more user friendly | keyword: gotcha
POC: Only run certain build steps for PR's if certain files have changed Proof of concepts for skipping certain build steps for PR's if certain files haven't been changed. Attempt of quick win to reduce build times. **Notes:** - Not sure comparing against origin/master will work in all cases (forks etc). - CircleCI provides parameters for revision (current commit) and base revision (commit before) so using that will only allow to compare changed files since the last commit - maybe that's what w…,,,,,,Anecdotal,issue,,,,,,,,2020-06-11,github/marefr,https://github.com/grafana/grafana/pull/25540,repo: grafana/grafana | keyword: gotcha | state: closed
@hugohaggmark end-to-end tests can only be run if both backend and frontend have been built. So either we have to always build backend and frontend if there are backend or frontend changes or only run end-to-end test if there are backend and frontend changes. Third alternative would be to always build backend if backend or frontend changes and always run end-to-end if frontend changes. Thought?,,,,,,Anecdotal,comment,,,,,,,,2020-06-11,github/marefr,https://github.com/grafana/grafana/pull/25540#issuecomment-642736724,repo: grafana/grafana | issue: POC: Only run certain build steps for PR's if certain files have changed | keyword: gotcha
"> @hugohaggmark end-to-end tests can only be run if both backend and frontend have been built. So either we have to always build backend and frontend if there are backend or frontend changes or only run end-to-end test if there are backend and frontend changes. > > Third alternative would be to always build backend if backend or frontend changes and always run end-to-end if frontend changes. > > Thought? Gotcha, as far as I can remember when e2e has failed it's because of changes in FrontEnd (t…",,,,,,Anecdotal,comment,,,,,,,,2020-06-15,github/hugohaggmark,https://github.com/grafana/grafana/pull/25540#issuecomment-643895597,repo: grafana/grafana | issue: POC: Only run certain build steps for PR's if certain files have changed | keyword: gotcha
@marefr outstanding work on this! 🌟 It's not my intention to hold up this improvement so go ahead with the changes you think make the most sense.,,,,,,Anecdotal,comment,,,,,,,,2020-06-17,github/hugohaggmark,https://github.com/grafana/grafana/pull/25540#issuecomment-645217556,repo: grafana/grafana | issue: POC: Only run certain build steps for PR's if certain files have changed | keyword: gotcha
"> Gotcha, as far as I can remember when e2e has failed it's because of changes in FrontEnd (that might change) so maybe we should run e2e every time there is a FrontEnd change? I guess that means we always need to build back end when front end changes, not sure what the time cost would be for that? Yes, it's very important that e2e tests run on PRs and for that, we need to build release package (tarball) as e2e tests use that",,,,,,Anecdotal,comment,,,,,,,,2020-06-17,github/torkelo,https://github.com/grafana/grafana/pull/25540#issuecomment-645314692,repo: grafana/grafana | issue: POC: Only run certain build steps for PR's if certain files have changed | keyword: gotcha
"This pull request has been automatically marked as stale because it has not had activity in the last 2 weeks. It will be closed in 30 days if no further activity occurs. Please feel free to give a status update now, ping for review, or re-open when it's ready. Thank you for your contributions!",,,,,,Anecdotal,comment,,,,,,,,2020-07-01,github/stale[bot],https://github.com/grafana/grafana/pull/25540#issuecomment-652376053,repo: grafana/grafana | issue: POC: Only run certain build steps for PR's if certain files have changed | keyword: gotcha
"This pull request has been automatically closed because it has not had activity in the last 30 days. Please feel free to give a status update now, ping for review, or re-open when it's ready. Thank you for your contributions!",,,,,,Anecdotal,comment,,,,,,,,2020-07-31,github/stale[bot],https://github.com/grafana/grafana/pull/25540#issuecomment-667218105,repo: grafana/grafana | issue: POC: Only run certain build steps for PR's if certain files have changed | keyword: gotcha
docs(alerting): Examples of dynamic labels and dynamic thresholds Added two new examples of advanced alerting setups that we often see the community asking for. Plus: - small enhancements to previous examples - all examples now links to their respective examples in Grafana Play ⭐ Preview: - [Example of dynamic thresholds per dimension](https://deploy-preview-grafana-105776-zb444pucvq-vp.a.run.app/docs/grafana/latest/alerting/best-practices/dynamic-thresholds/) - [Example of dynamic labels in al…,,,,,,Anecdotal,issue,,,,,,,,2025-05-21,github/ppcano,https://github.com/grafana/grafana/pull/105776,repo: grafana/grafana | keyword: gotcha | state: closed
> One thing I would add: make it explicitly what would happen if dimension in data source A does not have corresponding one in thresholds expression. Like this ![image](https://private-user-images.githubusercontent.com/25988953/447180832-a3810c5c-f86c-4974-b545-5a8fb3ddfac3.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDgyNDUwODEsIm5iZiI6MTc0ODI0NDc4MSwicGF0aCI6Ii8yNTk4ODk1My80NDcxODA4MzItYTM4MTB…,,,,,,Anecdotal,comment,,,,,,,,2025-05-26,github/ppcano,https://github.com/grafana/grafana/pull/105776#issuecomment-2908938996,repo: grafana/grafana | issue: docs(alerting): Examples of dynamic labels and dynamic thresholds | keyword: gotcha
"TableNG: Handle all cell overflow using css ## What does this PR do? 📓 Fixes #103948 We were doing some javascript to determine if the cell should overflow, but I don't think we really need it(?) We _should_ be able to handle it with a pure css approach. We can use pseduo classes (`:has`) to target the parent element. Please test this one thoroughly 🙏",,,,,,Anecdotal,issue,,,,,,,,2025-04-17,github/alexjonspencer1,https://github.com/grafana/grafana/pull/104178,repo: grafana/grafana | keyword: gotcha | state: closed
hovering partially-scrolled-out cell goes over header. z-index? ![image](https://github.com/user-attachments/assets/f1a0d3e5-4d04-4cd8-ab06-2af54b1d54cb) this should not be rerendering so much? we need to attach the css to the parent (the react-data-grid element) not to the div inside. [Kooha-2025-04-25-16-29-10.webm](https://github.com/user-attachments/assets/27a568aa-1401-4713-abcc-98d7573357bb),,,,,,Anecdotal,comment,,,,,,,,2025-04-25,github/leeoniya,https://github.com/grafana/grafana/pull/104178#issuecomment-2831468394,repo: grafana/grafana | issue: TableNG: Handle all cell overflow using css | keyword: gotcha
Here is a debug dashboard: [debug-Panel Title-2025-05-01 21_04_41.json.txt](https://github.com/user-attachments/files/20003573/debug-Panel.Title-2025-05-01.21_04_41.json.txt) cc @drew08t,,,,,,Anecdotal,comment,,,,,,,,2025-05-01,github/Develer,https://github.com/grafana/grafana/pull/104178#issuecomment-2845521059,repo: grafana/grafana | issue: TableNG: Handle all cell overflow using css | keyword: gotcha
I am also able to get it to flicker in multiple browsers. Try this @alexjonspencer1: hover outside of the purple region towards the bottom and you should experience it. ![image](https://github.com/user-attachments/assets/24d4bcd6-b01c-46d3-b61e-53c35d530a48),,,,,,Anecdotal,comment,,,,,,,,2025-05-02,github/drew08t,https://github.com/grafana/grafana/pull/104178#issuecomment-2847779589,repo: grafana/grafana | issue: TableNG: Handle all cell overflow using css | keyword: gotcha
> I am also able to get it to flicker in multiple browsers. Try this @alexjonspencer1: hover outside of the purple region towards the bottom and you should experience it. > > ![image](https://private-user-images.githubusercontent.com/60050885/440008350-24d4bcd6-b01c-46d3-b61e-53c35d530a48.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDYyMDg4MDgsIm5iZiI6MTc0NjIwODUwOCwicGF0aCI6Ii82MDA1MDg4NS80NDAw…,,,,,,Anecdotal,comment,,,,,,,,2025-05-02,github/alexjonspencer1,https://github.com/grafana/grafana/pull/104178#issuecomment-2847787297,repo: grafana/grafana | issue: TableNG: Handle all cell overflow using css | keyword: gotcha
"There's going to be a much more comprehensive way to handle row/cell styling. It will clean up the code in a lot of good ways. Instead of this logic (targeting the parent on hover), we should use the work implemented here in react-data-grid: https://github.com/adazzle/react-data-grid/pull/3775",,,,,,Anecdotal,comment,,,,,,,,2025-05-16,github/alexjonspencer1,https://github.com/grafana/grafana/pull/104178#issuecomment-2886964772,repo: grafana/grafana | issue: TableNG: Handle all cell overflow using css | keyword: gotcha
"Advisor: Allow to skip a step <!-- Thank you for sending a pull request! Here are some tips: 1. If this is your first time, please read our contribution guide at https://github.com/grafana/grafana/blob/main/CONTRIBUTING.md 2. Ensure you include and run the appropriate tests as part of your Pull Request. 3. In a new feature or configuration option, an update to the documentation is necessary. Everything related to the documentation is under the docs folder in the root of the repository. 4. If th…",,,,,,Anecdotal,issue,,,,,,,,2025-04-24,github/andresmgot,https://github.com/grafana/grafana/pull/104454,repo: grafana/grafana | keyword: gotcha | state: closed
"[release-12.0.1] docs(alerting): Correct gotcha when using dynamic labels Backport b53f68ea5e7c545fad84b1188b061f8ec633377b from #105347 --- Correct explanation when using dynamic labels. NoData state does not trigger, instead the alert is considered stale",,,,,,Anecdotal,issue,,,,,,,,2025-05-14,github/ppcano,https://github.com/grafana/grafana/pull/105366,repo: grafana/grafana | keyword: gotcha | state: closed
"docs(alerting): Correct gotcha when using dynamic labels Correct explanation when using dynamic labels. NoData state does not trigger, instead the alert is considered stale",,,,,,Anecdotal,issue,,,,,,,,2025-05-13,github/ppcano,https://github.com/grafana/grafana/pull/105347,repo: grafana/grafana | keyword: gotcha | state: closed
"The backport to `release-12.0.1` failed: ``` error pushing: error running command 'git push origin backport-105347-to-release-12.0.1' error: exit status 1 stdout: stderr: remote: fatal error in commit_refs To https://github.com/grafana/grafana.git ! [remote rejected] backport-105347-to-release-12.0.1 -> backport-105347-to-release-12.0.1 (failure) error: failed to push some refs to 'https://github.com/grafana/grafana.git' ``` To backport manually, run these commands in your terminal: ```bash git…",,,,,,Anecdotal,comment,,,,,,,,2025-05-14,github/github-actions[bot],https://github.com/grafana/grafana/pull/105347#issuecomment-2879184736,repo: grafana/grafana | issue: docs(alerting): Correct gotcha when using dynamic labels | keyword: gotcha
"unified-storage: setup ring to shard requests This PR introduces an experimental feature to shard the requests to the index server based on the tenant. It makes use of the `ring` (and friends) utilities from `dskit` 1. Updates the `instrumentation_server` service to use `mux` instead of the builtin router, and have it store the router in the module server: this is so we can register the `/ring` endpoint to check the status of the ring 2. Create a new `Ring` service that depends on the instrumen…",,,,,,Anecdotal,issue,,,,,,,,2025-04-10,github/gassiss,https://github.com/grafana/grafana/pull/103783,repo: grafana/grafana | keyword: gotcha | state: closed
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/14498256411) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **unistore-ring** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#deploying-a-graf…,,,,,,Anecdotal,comment,,,,,,,,2025-04-16,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/103783#issuecomment-2810190203,repo: grafana/grafana | issue: unified-storage: setup ring to shard requests | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821103783gassis.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-04-16,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/103783#issuecomment-2810227603,repo: grafana/grafana | issue: unified-storage: setup ring to shard requests | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/14515325041) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **unistore-ring** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#deploying-a-graf…,,,,,,Anecdotal,comment,,,,,,,,2025-04-17,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/103783#issuecomment-2812708070,repo: grafana/grafana | issue: unified-storage: setup ring to shard requests | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821103783gassis.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-04-17,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/103783#issuecomment-2812749048,repo: grafana/grafana | issue: unified-storage: setup ring to shard requests | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/14518720231) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **unistore-ring** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#deploying-a-graf…,,,,,,Anecdotal,comment,,,,,,,,2025-04-17,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/103783#issuecomment-2813244934,repo: grafana/grafana | issue: unified-storage: setup ring to shard requests | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821103783gassis.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-04-17,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/103783#issuecomment-2813303526,repo: grafana/grafana | issue: unified-storage: setup ring to shard requests | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/14521120907) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **unistore-ring** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#deploying-a-graf…,,,,,,Anecdotal,comment,,,,,,,,2025-04-17,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/103783#issuecomment-2813591249,repo: grafana/grafana | issue: unified-storage: setup ring to shard requests | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821103783gassis.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-04-17,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/103783#issuecomment-2813628260,repo: grafana/grafana | issue: unified-storage: setup ring to shard requests | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/14522246097) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **unistore-ring** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#deploying-a-graf…,,,,,,Anecdotal,comment,,,,,,,,2025-04-17,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/103783#issuecomment-2813723391,repo: grafana/grafana | issue: unified-storage: setup ring to shard requests | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821103783gassis.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-04-17,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/103783#issuecomment-2813757409,repo: grafana/grafana | issue: unified-storage: setup ring to shard requests | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/14523434784) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **unistore-ring** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#deploying-a-graf…,,,,,,Anecdotal,comment,,,,,,,,2025-04-17,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/103783#issuecomment-2813852742,repo: grafana/grafana | issue: unified-storage: setup ring to shard requests | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821103783gassis.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-04-17,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/103783#issuecomment-2813885333,repo: grafana/grafana | issue: unified-storage: setup ring to shard requests | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/14627831728) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **unistore-ring** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#deploying-a-graf…,,,,,,Anecdotal,comment,,,,,,,,2025-04-23,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/103783#issuecomment-2825473515,repo: grafana/grafana | issue: unified-storage: setup ring to shard requests | keyword: gotcha
"**Error building instance**: Contact #proj-ephemeral-hg-instances if it is not a compile error. [Logs](https://github.com/grafana/grafana/actions/runs/14627831728) <details><summary>Error message</summary> handling pull request comment event: calling gcom to upsert instance: updating instance: unexpected response status: status=409 responseBody={ ""code"": ""InvalidArgument"", ""message"": ""Unexpected parameter: version"", ""requestId"": ""edd38e03-192d-45a4-8df3-84f13ce02141"" }</details>",,,,,,Anecdotal,comment,,,,,,,,2025-04-23,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/103783#issuecomment-2825507792,repo: grafana/grafana | issue: unified-storage: setup ring to shard requests | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/14629354560) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **unistore-ring** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#deploying-a-graf…,,,,,,Anecdotal,comment,,,,,,,,2025-04-23,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/103783#issuecomment-2825633144,repo: grafana/grafana | issue: unified-storage: setup ring to shard requests | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821103783gassis.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-04-23,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/103783#issuecomment-2825664308,repo: grafana/grafana | issue: unified-storage: setup ring to shard requests | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/14644839851) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **unistore-ring** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#deploying-a-graf…,,,,,,Anecdotal,comment,,,,,,,,2025-04-24,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/103783#issuecomment-2828023248,repo: grafana/grafana | issue: unified-storage: setup ring to shard requests | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/14645050569) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **unistore-ring** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#deploying-a-graf…,,,,,,Anecdotal,comment,,,,,,,,2025-04-24,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/103783#issuecomment-2828046260,repo: grafana/grafana | issue: unified-storage: setup ring to shard requests | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821103783gassis.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-04-24,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/103783#issuecomment-2828079785,repo: grafana/grafana | issue: unified-storage: setup ring to shard requests | keyword: gotcha
"Plugin Extensions: Clean up the deprecated APIs Fixes https://github.com/grafana/grafana/issues/101460, https://github.com/grafana/grafana/issues/89473 ### What changed? **Earliest date for merging:** `26th of March` **Deadline for merging:** `11th of April` The PR is removing the following deprecated APIs, and is **also migrating one of the last usages of `getPluginLinkExtensions()` in core**. - [x] https://github.com/grafana/grafana/pull/103063 - [x] Fix the mocks in the core Grafana tests - …",,,,,,Anecdotal,issue,,,,,,,,2025-03-13,github/leventebalogh,https://github.com/grafana/grafana/pull/102102,repo: grafana/grafana | keyword: gotcha | state: closed
"⚠️ &nbsp;&nbsp;**Possible breaking changes (md version)**&nbsp;&nbsp; ⚠️ <h3>grafana-data</h3><h4>Removals</h4><b>AppPlugin.configureExtensionLink</b><br> <sub>/home/runner/work/grafana/grafana/base/grafana-data/dist/cjs/index.d.cts</sub><br> <pre lang=""typescript""> configureExtensionLink<Context extends object>(extension: Omit<PluginExtensionLinkConfig<Context>, 'type'>): this; </pre><br> <b>AppPlugin.configureExtensionComponent</b><br> <sub>/home/runner/work/grafana/grafana/base/grafana-data/…",,,,,,Anecdotal,comment,,,,,,,,2025-03-13,github/grafana-pr-automation[bot],https://github.com/grafana/grafana/pull/102102#issuecomment-2721250896,repo: grafana/grafana | issue: Plugin Extensions: Clean up the deprecated APIs | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/13924997337) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **plugin-extensions-panel-menu** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#d…,,,,,,Anecdotal,comment,,,,,,,,2025-03-18,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/102102#issuecomment-2733348156,repo: grafana/grafana | issue: Plugin Extensions: Clean up the deprecated APIs | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821102102levent.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-03-18,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/102102#issuecomment-2733418989,repo: grafana/grafana | issue: Plugin Extensions: Clean up the deprecated APIs | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/14031126732) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **plugin-extensions-panel-menu** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#d…,,,,,,Anecdotal,comment,,,,,,,,2025-03-24,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/102102#issuecomment-2747361097,repo: grafana/grafana | issue: Plugin Extensions: Clean up the deprecated APIs | keyword: gotcha
**Error building instance**: Contact #proj-ephemeral-hg-instances if it is not a compile error. [Logs](https://github.com/grafana/grafana/actions/runs/14031126732),,,,,,Anecdotal,comment,,,,,,,,2025-03-24,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/102102#issuecomment-2747383362,repo: grafana/grafana | issue: Plugin Extensions: Clean up the deprecated APIs | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/14054844652) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **plugin-extensions-panel-menu** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#d…,,,,,,Anecdotal,comment,,,,,,,,2025-03-25,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/102102#issuecomment-2750456705,repo: grafana/grafana | issue: Plugin Extensions: Clean up the deprecated APIs | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821102102levent.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-03-25,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/102102#issuecomment-2750507275,repo: grafana/grafana | issue: Plugin Extensions: Clean up the deprecated APIs | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/14189254960) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **plugin-extensions-panel-menu** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#d…,,,,,,Anecdotal,comment,,,,,,,,2025-04-01,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/102102#issuecomment-2768375589,repo: grafana/grafana | issue: Plugin Extensions: Clean up the deprecated APIs | keyword: gotcha
**Error building instance**: Contact #proj-ephemeral-hg-instances if it is not a compile error. [Logs](https://github.com/grafana/grafana/actions/runs/14189254960),,,,,,Anecdotal,comment,,,,,,,,2025-04-01,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/102102#issuecomment-2768402642,repo: grafana/grafana | issue: Plugin Extensions: Clean up the deprecated APIs | keyword: gotcha
"⚠️ &nbsp;&nbsp;**Possible breaking changes (md version)**&nbsp;&nbsp; ⚠️ <h3>grafana-data</h3><h4>Removals</h4><b>AppPlugin.configureExtensionLink</b><br> <sub>/home/runner/work/grafana/grafana/base/grafana-data/dist/cjs/index.d.cts</sub><br> <pre lang=""typescript""> configureExtensionLink<Context extends object>(extension: Omit<PluginExtensionLinkConfig<Context>, 'type'>): this; </pre><br> <b>AppPlugin.configureExtensionComponent</b><br> <sub>/home/runner/work/grafana/grafana/base/grafana-data/…",,,,,,Anecdotal,comment,,,,,,,,2025-04-02,github/grafana-pr-automation[bot],https://github.com/grafana/grafana/pull/102102#issuecomment-2772354161,repo: grafana/grafana | issue: Plugin Extensions: Clean up the deprecated APIs | keyword: gotcha
"⚠️ &nbsp;&nbsp;**Possible breaking changes (md version)**&nbsp;&nbsp; ⚠️ <h3>grafana-data</h3><h4>Removals</h4><b>AppPlugin.configureExtensionLink</b><br> <sub>/home/runner/work/grafana/grafana/base/grafana-data/dist/cjs/index.d.cts</sub><br> <pre lang=""typescript""> configureExtensionLink<Context extends object>(extension: Omit<PluginExtensionLinkConfig<Context>, 'type'>): this; </pre><br> <b>AppPlugin.configureExtensionComponent</b><br> <sub>/home/runner/work/grafana/grafana/base/grafana-data/…",,,,,,Anecdotal,comment,,,,,,,,2025-04-03,github/grafana-pr-automation[bot],https://github.com/grafana/grafana/pull/102102#issuecomment-2774743224,repo: grafana/grafana | issue: Plugin Extensions: Clean up the deprecated APIs | keyword: gotcha
"⚠️ &nbsp;&nbsp;**Possible breaking changes (md version)**&nbsp;&nbsp; ⚠️ <h3>grafana-data</h3><h4>Removals</h4><b>AppPlugin.configureExtensionLink</b><br> <sub>/home/runner/work/grafana/grafana/base/grafana-data/dist/cjs/index.d.cts</sub><br> <pre lang=""typescript""> configureExtensionLink<Context extends object>(extension: Omit<PluginExtensionLinkConfig<Context>, 'type'>): this; </pre><br> <b>AppPlugin.configureExtensionComponent</b><br> <sub>/home/runner/work/grafana/grafana/base/grafana-data/…",,,,,,Anecdotal,comment,,,,,,,,2025-04-07,github/grafana-pr-automation[bot],https://github.com/grafana/grafana/pull/102102#issuecomment-2782348497,repo: grafana/grafana | issue: Plugin Extensions: Clean up the deprecated APIs | keyword: gotcha
"⚠️ &nbsp;&nbsp;**Possible breaking changes (md version)**&nbsp;&nbsp; ⚠️ <h3>grafana-data</h3><h4>Removals</h4><b>AppPlugin.configureExtensionLink</b><br> <sub>/home/runner/work/grafana/grafana/base/grafana-data/dist/cjs/index.d.cts</sub><br> <pre lang=""typescript""> configureExtensionLink<Context extends object>(extension: Omit<PluginExtensionLinkConfig<Context>, 'type'>): this; </pre><br> <b>AppPlugin.configureExtensionComponent</b><br> <sub>/home/runner/work/grafana/grafana/base/grafana-data/…",,,,,,Anecdotal,comment,,,,,,,,2025-04-08,github/grafana-pr-automation[bot],https://github.com/grafana/grafana/pull/102102#issuecomment-2785421375,repo: grafana/grafana | issue: Plugin Extensions: Clean up the deprecated APIs | keyword: gotcha
"⚠️ &nbsp;&nbsp;**Possible breaking changes (md version)**&nbsp;&nbsp; ⚠️ <h3>grafana-data</h3><h4>Removals</h4><b>AppPlugin.configureExtensionLink</b><br> <sub>/home/runner/work/grafana/grafana/base/grafana-data/dist/cjs/index.d.cts</sub><br> <pre lang=""typescript""> configureExtensionLink<Context extends object>(extension: Omit<PluginExtensionLinkConfig<Context>, 'type'>): this; </pre><br> <b>AppPlugin.configureExtensionComponent</b><br> <sub>/home/runner/work/grafana/grafana/base/grafana-data/…",,,,,,Anecdotal,comment,,,,,,,,2025-04-09,github/grafana-pr-automation[bot],https://github.com/grafana/grafana/pull/102102#issuecomment-2788339220,repo: grafana/grafana | issue: Plugin Extensions: Clean up the deprecated APIs | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/14350546612) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **plugin-extensions-panel-menu** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#d…,,,,,,Anecdotal,comment,,,,,,,,2025-04-09,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/102102#issuecomment-2788438576,repo: grafana/grafana | issue: Plugin Extensions: Clean up the deprecated APIs | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821102102levent.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-04-09,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/102102#issuecomment-2788507618,repo: grafana/grafana | issue: Plugin Extensions: Clean up the deprecated APIs | keyword: gotcha
"i18n: exposes languages in grafana/data <!-- Thank you for sending a pull request! Here are some tips: 1. If this is your first time, please read our contribution guide at https://github.com/grafana/grafana/blob/main/CONTRIBUTING.md 2. Ensure you include and run the appropriate tests as part of your Pull Request. 3. In a new feature or configuration option, an update to the documentation is necessary. Everything related to the documentation is under the docs folder in the root of the repository…",,,,,,Anecdotal,issue,,,,,,,,2025-03-27,github/hugohaggmark,https://github.com/grafana/grafana/pull/102958,repo: grafana/grafana | keyword: gotcha | state: closed
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/14102207110) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **hugoh/i18n-available-languages** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances…,,,,,,Anecdotal,comment,,,,,,,,2025-03-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/102958#issuecomment-2757190861,repo: grafana/grafana | issue: i18n: exposes languages in grafana/data | keyword: gotcha
"**Error building instance**: Contact #proj-ephemeral-hg-instances if it is not a compile error. [Logs](https://github.com/grafana/grafana/actions/runs/14102207110) <details><summary>Error message</summary> handling pull request comment event: calling gcom to update instance config: unexpected response status: status=409 responseBody={ ""code"": ""Conflict"", ""message"": ""Hosted Grafana error: operation in progress, please try again later"", ""requestId"": ""1019c112-61e8-4b21-9f7f-3591b8c7de9f"" }</detai…",,,,,,Anecdotal,comment,,,,,,,,2025-03-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/102958#issuecomment-2757232986,repo: grafana/grafana | issue: i18n: exposes languages in grafana/data | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/14102581814) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **hugoh/i18n-available-languages** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances…,,,,,,Anecdotal,comment,,,,,,,,2025-03-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/102958#issuecomment-2757239741,repo: grafana/grafana | issue: i18n: exposes languages in grafana/data | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821102958hugoha.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-03-27,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/102958#issuecomment-2757287014,repo: grafana/grafana | issue: i18n: exposes languages in grafana/data | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/14305268146) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **hugoh/i18n-available-languages** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances…,,,,,,Anecdotal,comment,,,,,,,,2025-04-07,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/102958#issuecomment-2782640016,repo: grafana/grafana | issue: i18n: exposes languages in grafana/data | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821102958hugoha.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-04-07,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/102958#issuecomment-2782715466,repo: grafana/grafana | issue: i18n: exposes languages in grafana/data | keyword: gotcha
[Search] Fix CodeQL warnings Fixes https://github.com/grafana/grafana/security/code-scanning/817 Fixes https://github.com/grafana/grafana/security/code-scanning/774 Part of https://github.com/grafana/search-and-storage-team/issues/198 **Special notes for your reviewer:** How to run CodeQL scans currently Go to the [CodeQL action](https://github.com/grafana/grafana/actions/workflows/codeql-analysis.yml) and click on run workflow dropdown. Select your branch and click on Run Workflow green button…,,,,,,Anecdotal,issue,,,,,,,,2025-02-26,github/leonorfmartins,https://github.com/grafana/grafana/pull/101364,repo: grafana/grafana | keyword: gotcha | state: closed
## [Codecov](https://codecov.grafana-dev.net/gh/grafana/grafana/pull/101364?dropdown=coverage&src=pr&el=h1) Report Attention: Patch coverage is `63.63636%` with `12 lines` in your changes missing coverage. Please review. | [Files with missing lines](https://codecov.grafana-dev.net/gh/grafana/grafana/pull/101364?dropdown=coverage&src=pr&el=tree) | Patch % | Lines | |---|---|---| | [pkg/storage/unified/search/bleve.go](https://codecov.grafana-dev.net/gh/grafana/grafana/pull/101364?src=pr&el=tree&…,,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/ghost,https://github.com/grafana/grafana/pull/101364#issuecomment-2710855022,repo: grafana/grafana | issue: [Search] Fix CodeQL warnings | keyword: gotcha
Plugins: Fix better UX for disabled Angular plugins **What is this feature?** This feature makes the user experience slightly better for disabled Angular plugins in the Plugins Catalog when the `angular_support_enabled = false` is set. **BEFORE** ![image](https://github.com/user-attachments/assets/2db0a9c4-f537-43b3-8050-95a29bd81b39) **AFTER** ![image](https://github.com/user-attachments/assets/33aa5807-d22c-4046-a67a-aa624ec13d7c) - This PR also adds tests for `public/app/features/plugins/adm…,,,,,,Anecdotal,issue,,,,,,,,2025-02-26,github/hugohaggmark,https://github.com/grafana/grafana/pull/101333,repo: grafana/grafana | keyword: gotcha | state: closed
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/13537750270) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **hugoh/better-user-experience-for-angular** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral…,,,,,,Anecdotal,comment,,,,,,,,2025-02-26,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/101333#issuecomment-2684016537,repo: grafana/grafana | issue: Plugins: Fix better UX for disabled Angular plugins | keyword: gotcha
**Error building instance**: Contact #proj-ephemeral-hg-instances if it is not a compile error. [Logs](https://github.com/grafana/grafana/actions/runs/13537750270),,,,,,Anecdotal,comment,,,,,,,,2025-02-26,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/101333#issuecomment-2684031187,repo: grafana/grafana | issue: Plugins: Fix better UX for disabled Angular plugins | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/13539400931) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **hugoh/better-user-experience-for-angular** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral…,,,,,,Anecdotal,comment,,,,,,,,2025-02-26,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/101333#issuecomment-2684245738,repo: grafana/grafana | issue: Plugins: Fix better UX for disabled Angular plugins | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821101333hugoha.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-02-26,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/101333#issuecomment-2684284009,repo: grafana/grafana | issue: Plugins: Fix better UX for disabled Angular plugins | keyword: gotcha
"@hugohaggmark - awesome, great to see the work on this and I really appreciate the picture in the summary 🙌 Some quick questions, you mention > when the angular_support_enabled = false is set. My (possibly flawed) understanding is that with G12 we remove that flag as part of removing Angular. I assume there is no actual dependence on this setting that would then break? Why is uninstall not possible when the plugin is disabled? (for my own understanding). Given this, are we able to provide appro…",,,,,,Anecdotal,comment,,,,,,,,2025-02-28,github/sympatheticmoose,https://github.com/grafana/grafana/pull/101333#issuecomment-2690299083,repo: grafana/grafana | issue: Plugins: Fix better UX for disabled Angular plugins | keyword: gotcha
"> @hugohaggmark - awesome, great to see the work on this and I really appreciate the picture in the summary 🙌 👍 > Some quick questions, you mention > > > when the angular_support_enabled = false is set. > > My (possibly flawed) understanding is that with G12 we remove that flag as part of removing Angular. I assume there is no actual dependence on this setting that would then break? Not really but it sort of has this dependency but indirectly. In the frontend we check for disabled Angular plugi…",,,,,,Anecdotal,comment,,,,,,,,2025-03-03,github/hugohaggmark,https://github.com/grafana/grafana/pull/101333#issuecomment-2693289716,repo: grafana/grafana | issue: Plugins: Fix better UX for disabled Angular plugins | keyword: gotcha
"Gotcha - thanks! > I could add a tooltip explaining that you can only uninstall it from the cli? Grafana.com would probably be the easiest for Cloud users. If we're able to change the message based on how Grafana is running, that's ideal, if not we should mention both methods",,,,,,Anecdotal,comment,,,,,,,,2025-03-03,github/sympatheticmoose,https://github.com/grafana/grafana/pull/101333#issuecomment-2694490007,repo: grafana/grafana | issue: Plugins: Fix better UX for disabled Angular plugins | keyword: gotcha
## [Codecov](https://codecov.grafana-dev.net/gh/grafana/grafana/pull/101333?dropdown=coverage&src=pr&el=h1) Report All modified and coverable lines are covered by tests :white_check_mark: | [Flag](https://codecov.grafana-dev.net/gh/grafana/grafana/pull/101333/flags?src=pr&el=flags) | Coverage Δ | | |---|---|---| | [be-unit](https://codecov.grafana-dev.net/gh/grafana/grafana/pull/101333/flags?src=pr&el=flag) | `41.68% <ø> (+<0.01%)` | :arrow_up: | Flags with carried forward coverage won't be sho…,,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/ghost,https://github.com/grafana/grafana/pull/101333#issuecomment-2717218107,repo: grafana/grafana | issue: Plugins: Fix better UX for disabled Angular plugins | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/13808846144) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **hugoh/better-user-experience-for-angular** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral…,,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/101333#issuecomment-2717353942,repo: grafana/grafana | issue: Plugins: Fix better UX for disabled Angular plugins | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821101333hugoha.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/101333#issuecomment-2717410849,repo: grafana/grafana | issue: Plugins: Fix better UX for disabled Angular plugins | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/13811936427) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **hugoh/better-user-experience-for-angular** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral…,,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/101333#issuecomment-2717808718,repo: grafana/grafana | issue: Plugins: Fix better UX for disabled Angular plugins | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821101333hugoha.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/101333#issuecomment-2717860163,repo: grafana/grafana | issue: Plugins: Fix better UX for disabled Angular plugins | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/13845030517) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **hugoh/better-user-experience-for-angular** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral…,,,,,,Anecdotal,comment,,,,,,,,2025-03-13,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/101333#issuecomment-2722735609,repo: grafana/grafana | issue: Plugins: Fix better UX for disabled Angular plugins | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821101333hugoha.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-03-13,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/101333#issuecomment-2722766492,repo: grafana/grafana | issue: Plugins: Fix better UX for disabled Angular plugins | keyword: gotcha
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/13846693069) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **hugoh/better-user-experience-for-angular** oss branch and **main** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral…,,,,,,Anecdotal,comment,,,,,,,,2025-03-13,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/101333#issuecomment-2722934710,repo: grafana/grafana | issue: Plugins: Fix better UX for disabled Angular plugins | keyword: gotcha
- Your instance can be accessed at: https://ephemeral15111821101333hugoha.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-03-13,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/101333#issuecomment-2722954740,repo: grafana/grafana | issue: Plugins: Fix better UX for disabled Angular plugins | keyword: gotcha
[release-11.5.3] docs(alerting): minor enhancements to intro concepts and example Backport 64b65ffc72c65bd595a12f5d1db17d84c04386e3 from #102023\n\n---\n\nMinor changes to the intro: - replaced an example to avoid a potential gotcha which made the previous example unsuitable for an intro - reinforced the usage and benefits of annotations,,,,,,Anecdotal,issue,,,,,,,,2025-03-12,github/ppcano,https://github.com/grafana/grafana/pull/102068,repo: grafana/grafana | keyword: gotcha | state: closed
docs(alerting): minor enhancements to intro concepts and example Minor changes to the intro: - replaced an example to avoid a potential gotcha which made the previous example unsuitable for an intro - reinforced the usage and benefits of annotations,,,,,,Anecdotal,issue,,,,,,,,2025-03-12,github/ppcano,https://github.com/grafana/grafana/pull/102023,repo: grafana/grafana | keyword: gotcha | state: closed
"The backport to `release-11.5.3` failed: ``` error pushing: error running command 'git push origin backport-102023-to-release-11.5.3': exit status 1 ``` To backport manually, run these commands in your terminal: ```bash git fetch git switch --create backport-102023-to-release-11.5.3 origin/release-11.5.3 git cherry-pick -x 64b65ffc72c65bd595a12f5d1db17d84c04386e3 ``` Resolve the conflicts, then add the changes and run `git cherry-pick --continue`: ```bash git add . && git cherry-pick --continue…",,,,,,Anecdotal,comment,,,,,,,,2025-03-12,github/grafana-delivery-bot[bot],https://github.com/grafana/grafana/pull/102023#issuecomment-2719132050,repo: grafana/grafana | issue: docs(alerting): minor enhancements to intro concepts and example | keyword: gotcha
"GrafanaUI: Add external border radius mixin This PR adds two helper mixins to calculate external border radii, based on the underlying theme token. The effect can be seen in the `RadioButtonGroup` `radioLabel` component. ## Why? Adds better visual treatment to components that have nested border radii: - `getExternalRadius` calculates the **parent's** border radius, based on the **child's** styling. Useful for menus, containers, etc - `getInternalRadius` calculates the **child's** border radius,…",,,,,,Anecdotal,issue,,,,,,,,2025-02-07,github/EdPoole,https://github.com/grafana/grafana/pull/100266,repo: grafana/grafana | keyword: gotcha | state: closed
[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/grafana/grafana?pullRequest=100266) <br/>All committers have signed the CLA.,,,,,,Anecdotal,comment,,,,,,,,2025-02-07,github/CLAassistant,https://github.com/grafana/grafana/pull/100266#issuecomment-2642713187,repo: grafana/grafana | issue: GrafanaUI: Add external border radius mixin | keyword: gotcha
"Thanks, yeah some good points. > * i don't know of anywhere using anything other than 1px border width in grafana, so let's just assume it for now. we can always add `borderWidth` as an optional prop later if we need it The thing is, we need to calculate the value of the radius with, and without, a border (regardless of thickness). For items such as the `RadioButtonGroup`, we have the border present on its container, so need to include its thickness in the calculation. For things like dropdown …",,,,,,Anecdotal,comment,,,,,,,,2025-02-11,github/EdPoole,https://github.com/grafana/grafana/pull/100266#issuecomment-2650589689,repo: grafana/grafana | issue: GrafanaUI: Add external border radius mixin | keyword: gotcha
"ok apologies for the delay, lots of PRs to review 😂 so... > The thing is, we need to calculate the value of the radius with, and without, a border (regardless of thickness). For items such as the RadioButtonGroup, we have the border present on its container, so need to include its thickness in the calculation. For things like dropdown sheets/menus, we don't have a border on the container, so we can't include that in the calculation. I can throw a demo together if we like. yeah gotcha 👍 i've mad…",,,,,,Anecdotal,comment,,,,,,,,2025-02-12,github/ashharrison90,https://github.com/grafana/grafana/pull/100266#issuecomment-2653781190,repo: grafana/grafana | issue: GrafanaUI: Add external border radius mixin | keyword: gotcha
"ok i've pushed a small update to tidy up the interface. for the vast majority of cases, we'll just be calling `getExternalRadius({ offset: <number> })` or `getInternalRadius({ offset: <number> })` have also now called it correctly in `RadioButton` and ensured we're using the same constant for the padding and offset. finally added the internal story to play around with when running locally: ![image](https://github.com/user-attachments/assets/84b49d09-1b50-433b-ab32-4941f7c70b34) i'm gonna approv…",,,,,,Anecdotal,comment,,,,,,,,2025-02-28,github/ashharrison90,https://github.com/grafana/grafana/pull/100266#issuecomment-2690844428,repo: grafana/grafana | issue: GrafanaUI: Add external border radius mixin | keyword: gotcha
"i18n: imports use @grafana/i18n <!-- Thank you for sending a pull request! Here are some tips: 1. If this is your first time, please read our contribution guide at https://github.com/grafana/grafana/blob/main/CONTRIBUTING.md 2. Ensure you include and run the appropriate tests as part of your Pull Request. 3. In a new feature or configuration option, an update to the documentation is necessary. Everything related to the documentation is under the docs folder in the root of the repository. 4. If …",,,,,,Anecdotal,issue,,,,,,,,2025-05-09,github/hugohaggmark,https://github.com/grafana/grafana/pull/105177,repo: grafana/grafana | keyword: pro tip | state: closed
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/14995087783) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **hugoh/i18n-leftovers** oss branch and **hugoh/i18n-leftovers** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-ins…,,,,,,Anecdotal,comment,,,,,,,,2025-05-13,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/105177#issuecomment-2876059395,repo: grafana/grafana | issue: i18n: imports use @grafana/i18n | keyword: pro tip
"For your convenience to avoid loading all the files, replace with your username: `https://github.com/grafana/grafana/pull/105177/files?owned-by%5B%5D=MY_USERNAME_HERE`",,,,,,Anecdotal,comment,,,,,,,,2025-05-13,github/tomratcliffe,https://github.com/grafana/grafana/pull/105177#issuecomment-2876096024,repo: grafana/grafana | issue: i18n: imports use @grafana/i18n | keyword: pro tip
- Your instance can be accessed at: https://ephemeral15111821105177hugoha.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-05-13,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/105177#issuecomment-2876128682,repo: grafana/grafana | issue: i18n: imports use @grafana/i18n | keyword: pro tip
"> i don't see this locally if i remove the comment btw... wonder if it's a caching issue? does the linting fail in CI without this comment? 🤔 @ashharrison90 yes, it worked locally but failed several times during CI",,,,,,Anecdotal,comment,,,,,,,,2025-05-14,github/hugohaggmark,https://github.com/grafana/grafana/pull/105177#issuecomment-2878591052,repo: grafana/grafana | issue: i18n: imports use @grafana/i18n | keyword: pro tip
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/15013564583) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **hugoh/i18n-leftovers** oss branch and **hugoh/i18n-leftovers** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-ins…,,,,,,Anecdotal,comment,,,,,,,,2025-05-14,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/105177#issuecomment-2878799484,repo: grafana/grafana | issue: i18n: imports use @grafana/i18n | keyword: pro tip
- Your instance can be accessed at: https://ephemeral15111821105177hugoha.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-05-14,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/105177#issuecomment-2878847168,repo: grafana/grafana | issue: i18n: imports use @grafana/i18n | keyword: pro tip
- Preparing your instance. A comment containing your instance's url will be added to this PR when the instance is ready. - Your instance will be ready in ~10 minutes. [Follow the workflow progress](https://github.com/grafana/grafana/actions/runs/15038488946) - Slack channel: #proj-ephemeral-hg-instances - Building instance with **hugoh/i18n-leftovers** oss branch and **hugoh/i18n-leftovers** enterprise branch. [How to choose a branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-ins…,,,,,,Anecdotal,comment,,,,,,,,2025-05-15,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/105177#issuecomment-2882753005,repo: grafana/grafana | issue: i18n: imports use @grafana/i18n | keyword: pro tip
- Your instance can be accessed at: https://ephemeral15111821105177hugoha.grafana-dev.net - The instance is not using the CDN assets. - [How to access / How to update instance config / How to build a specific branch](https://github.com/grafana/hosted-grafana/wiki/Ephemeral-instances#choosing-which-oss-and-enterprise-branches-are-used-to-build-the-instance),,,,,,Anecdotal,comment,,,,,,,,2025-05-15,github/ephemeral-instances-bot[bot],https://github.com/grafana/grafana/pull/105177#issuecomment-2882795920,repo: grafana/grafana | issue: i18n: imports use @grafana/i18n | keyword: pro tip
"AnnotationList: Fix link for annotation with no panel or dashboard <!-- Thank you for sending a pull request! Here are some tips: 1. If this is your first time, please read our contribution guide at https://github.com/grafana/grafana/blob/main/CONTRIBUTING.md 2. Ensure you include and run the appropriate tests as part of your Pull Request. 3. In a new feature or configuration option, an update to the documentation is necessary. Everything related to the documentation is under the docs folder in…",,,,,,Anecdotal,issue,,,,,,,,2024-04-29,github/tskarhed,https://github.com/grafana/grafana/pull/87048,repo: grafana/grafana | keyword: pro tip | state: closed
"grafana alert rule only sends email once ### What happened? grafana alert rule only sends email once ![image](https://github.com/grafana/grafana/assets/20555274/e8901be3-a8e8-4455-8674-e290e036f600) ### What did you expect to happen? grafana alert rule sends email after each evaluation ### Did this work before? no ### How do we reproduce it? 1. create an alert rule with an alert condition 2. set evaluation period is 10 seconds 3 save the rule I can see the state for the rule is always firing, b…",,,,,,Anecdotal,issue,,,,,,,,2024-03-01,github/bodycombathuang,https://github.com/grafana/grafana/issues/83749,repo: grafana/grafana | keyword: pro tip | state: closed
hi @bodycombathuang double check the `repeat interval` option in your notification policy > Timing options pro tip: you can see what notification policy is routing your alert by clicking `Preview routing` within the alert rule editor.,,,,,,Anecdotal,comment,,,,,,,,2024-04-08,github/tonypowa,https://github.com/grafana/grafana/issues/83749#issuecomment-2042462001,repo: grafana/grafana | issue: grafana alert rule only sends email once | keyword: pro tip
"We've closed this issue since it needs more information and hasn't had any activity recently. We can re-open it after you you add more information. To avoid having your issue closed in the future, please read our [CONTRIBUTING](https://github.com/grafana/grafana/blob/main/CONTRIBUTING.md) guidelines. Happy graphing!",,,,,,Anecdotal,comment,,,,,,,,2024-05-20,github/grafanabot,https://github.com/grafana/grafana/issues/83749#issuecomment-2119924947,repo: grafana/grafana | issue: grafana alert rule only sends email once | keyword: pro tip
"Can't import GPG key with gpg/rpm --import without an error `gpg --import https://rpm.grafana.com/gpg.key` reports the following on a Rocky 8 machine: ``` [root@test_machine djuarezg]# gpg --import gpg.key gpg: key 8C8C34C524098CB6: no public key - can't apply revocation certificate gpg: key 9E439B102CF3C0C6: public key ""Grafana Labs <engineering@grafana.com>"" imported gpg: Total number processed: 2 gpg: imported: 1 ``` PS: ``` [root@test_machine djuarezg]# rpm --import ./gpg.key -vv ufdio: 1 r…",,,,,,Anecdotal,issue,,,,,,,,2023-01-13,github/djuarezg,https://github.com/grafana/grafana/issues/61491,repo: grafana/grafana | keyword: pro tip | state: closed
"I have some issue, when try to check updating showing error of repository and gpg key, I fix it changing repository and retrive new gpg key With root permission Get new Grafana key `wget -q -O /usr/share/keyrings/grafana.key https://apt.grafana.com/gpg.key` Add new repo: `echo ""deb [signed-by=/usr/share/keyrings/grafana.key] https://apt.grafana.com stable main"" > /etc/apt/sources.list.d/grafana.list`",,,,,,Anecdotal,comment,,,,,,,,2023-01-13,github/Kalarumeth,https://github.com/grafana/grafana/issues/61491#issuecomment-1381940673,repo: grafana/grafana | issue: Can't import GPG key with gpg/rpm --import without an error | keyword: pro tip
"I see the same thing in CentOS 7: ``` [root@host ~]# gpg --import gpg.key gpg: key 24098CB6: no public key - can't apply revocation certificate gpg: /root/.gnupg/trustdb.gpg: trustdb created gpg: key 2CF3C0C6: public key ""Grafana Labs <engineering@grafana.com>"" imported gpg: Total number processed: 2 gpg: imported: 1 (RSA: 1) ``` I was using the repo (the old one at packages.grafana.com, I hadn't noticed the updated url) yesterday without issues, so presumably it's a recent change.",,,,,,Anecdotal,comment,,,,,,,,2023-01-13,github/jeremyspencer39171,https://github.com/grafana/grafana/issues/61491#issuecomment-1381943095,repo: grafana/grafana | issue: Can't import GPG key with gpg/rpm --import without an error | keyword: pro tip
"Hi, thanks for the feedback. When we tested this we didn't run into any issues however some reports have been coming in about the revocation certificate on rpm distributions. If this is blocking you, here's a link to the public key without the revocation certificate. https://rpm.grafana.com/gpg-2023-01-12.key In the meantime, we'll continue to investigate our options. For more information on why we rotated this GPG key, visit our blog post: https://grafana.com/blog/2023/01/12/grafana-labs-updat…",,,,,,Anecdotal,comment,,,,,,,,2023-01-13,github/kminehart,https://github.com/grafana/grafana/issues/61491#issuecomment-1382018027,repo: grafana/grafana | issue: Can't import GPG key with gpg/rpm --import without an error | keyword: pro tip
"The revocation certificate in the pubic key file is also not recognised by the Ansible `apt_key` module with Ubuntu 22.04 LTS. ``` fatal: [172.30.4.219]: FAILED! => { ""ansible_facts"": { ""discovered_interpreter_python"": ""/usr/bin/python3"" }, ""changed"": false, ""invocation"": { ""module_args"": { ""data"": null, ""file"": null, ""id"": null, ""key"": null, ""keyring"": ""/usr/share/keyrings/grafana.gpg"", ""keyserver"": null, ""state"": ""present"", ""url"": ""https://packages.grafana.com/gpg.key"", ""validate_certs"": true…",,,,,,Anecdotal,comment,,,,,,,,2023-01-13,github/ashleykleynhans,https://github.com/grafana/grafana/issues/61491#issuecomment-1382039339,repo: grafana/grafana | issue: Can't import GPG key with gpg/rpm --import without an error | keyword: pro tip
"Confirming that this is change broke builds that use Ansible's [apt_key](https://docs.ansible.com/ansible/latest/collections/ansible/builtin/apt_key_module.html) module. Using the workaround key (https://rpm.grafana.com/gpg-2023-01-12.key) for now, thanks to Grafana team for providing it.",,,,,,Anecdotal,comment,,,,,,,,2023-01-13,github/mdebord-dlr,https://github.com/grafana/grafana/issues/61491#issuecomment-1382349660,repo: grafana/grafana | issue: Can't import GPG key with gpg/rpm --import without an error | keyword: pro tip
"Raspberry Pi 4 here: `$ sudo apt-get update Hit:1 http://raspbian.raspberrypi.org/raspbian buster InRelease Hit:2 http://phoscon.de/apt/deconz buster InRelease Hit:3 http://archive.raspberrypi.org/debian buster InRelease Get:4 https://download.docker.com/linux/raspbian buster InRelease [33.6 kB] Get:5 https://packages.grafana.com/enterprise/deb stable InRelease [5,984 B] Err:5 https://packages.grafana.com/enterprise/deb stable InRelease The following signatures couldn't be verified because the …",,,,,,Anecdotal,comment,,,,,,,,2023-01-14,github/solmoller,https://github.com/grafana/grafana/issues/61491#issuecomment-1382917823,repo: grafana/grafana | issue: Can't import GPG key with gpg/rpm --import without an error | keyword: pro tip
"@solmoller its related but your error is actually expected when the public key is revoked, and you just need to install the new one to replace the old one. We had a different issue where the revocation certificate that was included within the same file for the new public key was not being recognised, but the revocation certificate now seems to have been removed from the file containing the new public key and the issue is resolved.",,,,,,Anecdotal,comment,,,,,,,,2023-01-15,github/ashleykleynhans,https://github.com/grafana/grafana/issues/61491#issuecomment-1383104483,repo: grafana/grafana | issue: Can't import GPG key with gpg/rpm --import without an error | keyword: pro tip
"Hi, So you can do something like this (depending on your paths...) : Have a look to your file path, in my example : ``` less /etc/apt/sources.list.d/packages_grafana_com_oss_deb.list ``` Give me ``` deb [arch=amd64 signed-by=/etc/apt/trusted.gpg.d/grafana.gpg] https://packages.grafana.com/oss/deb stable main ``` So you can see the path to the gpg file and set it in the following curl command ``` curl https://apt.grafana.com/gpg.key | gpg --dearmor > /etc/apt/trusted.gpg.d/grafana.gpg ``` And th…",,,,,,Anecdotal,comment,,,,,,,,2023-01-15,github/pulse-mind,https://github.com/grafana/grafana/issues/61491#issuecomment-1383106725,repo: grafana/grafana | issue: Can't import GPG key with gpg/rpm --import without an error | keyword: pro tip
"That´s fine, but please change this in Tutorial to install Grafana on Raspberry. https://grafana.com/tutorials/install-grafana-on-raspberry-pi Now i have this line working fine on my Raspberry: `curl https://apt.grafana.com/gpg.key | gpg --dearmor > /etc/apt/trusted.gpg.d/grafana.gpg` `echo ""deb [signed-by=/usr/share/keyrings/grafana.gpg] https://apt.grafana.com stable main"" | sudo tee -a /etc/apt/sources.list.d/grafana.list`",,,,,,Anecdotal,comment,,,,,,,,2023-01-16,github/Starfoxfs,https://github.com/grafana/grafana/issues/61491#issuecomment-1383668487,repo: grafana/grafana | issue: Can't import GPG key with gpg/rpm --import without an error | keyword: pro tip
"> Hi, thanks for the feedback. > > When we tested this we didn't run into any issues however some reports have been coming in about the revocation certificate on rpm distributions. > > If this is blocking you, here's a link to the public key without the revocation certificate. https://rpm.grafana.com/gpg-2023-01-12.key > > In the meantime, we'll continue to investigate our options. For more information on why we rotated this GPG key, visit our blog post: https://grafana.com/blog/2023/01/12/graf…",,,,,,Anecdotal,comment,,,,,,,,2023-01-16,github/tonypowa,https://github.com/grafana/grafana/issues/61491#issuecomment-1383755890,repo: grafana/grafana | issue: Can't import GPG key with gpg/rpm --import without an error | keyword: pro tip
Hi @tonypowa There are still issues with the writeup about Debian/Ubuntu where I'm getting the errors described in #61524,,,,,,Anecdotal,comment,,,,,,,,2023-01-16,github/tomjensendk,https://github.com/grafana/grafana/issues/61491#issuecomment-1383804979,repo: grafana/grafana | issue: Can't import GPG key with gpg/rpm --import without an error | keyword: pro tip
> Hi @tonypowa > > There are still issues with the writeup about Debian/Ubuntu where I'm getting the errors described in #61524 closing this issue since it has been solved for the reporting user we will address the writeup in your linked issue @tomjensendk 👍,,,,,,Anecdotal,comment,,,,,,,,2023-01-16,github/tonypowa,https://github.com/grafana/grafana/issues/61491#issuecomment-1383919183,repo: grafana/grafana | issue: Can't import GPG key with gpg/rpm --import without an error | keyword: pro tip
"> That´s fine, but please change this in Tutorial to install Grafana on Raspberry. > > https://grafana.com/tutorials/install-grafana-on-raspberry-pi > > Now i have this line working fine on my Raspberry: > > `curl https://apt.grafana.com/gpg.key | gpg --dearmor > /etc/apt/trusted.gpg.d/grafana.gpg` > > `echo ""deb [signed-by=/usr/share/keyrings/grafana.gpg] https://apt.grafana.com stable main"" | sudo tee -a /etc/apt/sources.list.d/grafana.list` hi @Starfoxfs the debian instructions should work (…",,,,,,Anecdotal,comment,,,,,,,,2023-01-16,github/tonypowa,https://github.com/grafana/grafana/issues/61491#issuecomment-1384091875,repo: grafana/grafana | issue: Can't import GPG key with gpg/rpm --import without an error | keyword: pro tip
"Good morning, thanks for the Info @tonypowa , the .KEY File is secure enough ? all other Files in `/usr/share/keyrings` are in GPG Format and not readable with normal Editor like Nano. The Grafana.key is readable.",,,,,,Anecdotal,comment,,,,,,,,2023-01-17,github/Starfoxfs,https://github.com/grafana/grafana/issues/61491#issuecomment-1385099773,repo: grafana/grafana | issue: Can't import GPG key with gpg/rpm --import without an error | keyword: pro tip
"Hello everyone, The only solution after struggling all this week was replacing the stable repo with beta repo. After that, it worked. Cheers",,,,,,Anecdotal,comment,,,,,,,,2023-02-01,github/gabrielwpulido,https://github.com/grafana/grafana/issues/61491#issuecomment-1411278455,repo: grafana/grafana | issue: Can't import GPG key with gpg/rpm --import without an error | keyword: pro tip
Maybe unrelated but -alternative- for Debian12 is new DEB822 format ``` - name: Download and import grafana GPG key ansible.builtin.get_url: url: https://apt.grafana.com/gpg.key dest: /etc/apt/trusted.gpg.d/grafana.key checksum: sha256:58052c148058ace26dbd01ce057afa5709fb9d39c8a8ab16800be3f42fc02ab2 - name: Add repo using key from URL deb822_repository: name: grafana types: deb uris: https://apt.grafana.com/ suites: stable components: main architectures: amd64 signed_by: /etc/apt/trusted.gpg.d/…,,,,,,Anecdotal,comment,,,,,,,,2024-03-28,github/smarakdas314,https://github.com/grafana/grafana/issues/61491#issuecomment-2025026002,repo: grafana/grafana | issue: Can't import GPG key with gpg/rpm --import without an error | keyword: pro tip
"Semi-relative time range breaks since Grafana 8 when 'to' is before 'from' **Context**: In response to the blog post [Pro tip: How to use semi-relative time ranges in Grafana](https://grafana.com/blog/2022/02/03/pro-tip-how-to-use-semi-relative-time-ranges-in-grafana/), a user reported an issue, which I was able to reproduce. There is a dashboard which displays metrics for ""the day so far"", starting at 9:30am local time. This is achieved with the following time range controls: * **from**: `now/…",,,,,,Anecdotal,issue,,,,,,,,2022-05-24,github/adeverteuil,https://github.com/grafana/grafana/issues/49495,repo: grafana/grafana | keyword: pro tip | state: closed
"Can confirm I can reproduce this, it is due to this piece of code when initialising the `TimeSrv`: ``` if (range.to.isBefore(range.from)) { this.setTime( { from: range.raw.to, to: range.raw.from, }, false ); } ``` I generally understand the purpose of this, to prevent empty graphs, but also find it strange that we are taking this action that the user didn't really ask for. I will bring this to the team to ask for opinions today and then move it to our backlog accordingly.",,,,,,Anecdotal,comment,,,,,,,,2023-02-28,github/JoaoSilvaGrafana,https://github.com/grafana/grafana/issues/49495#issuecomment-1448050092,repo: grafana/grafana | issue: Semi-relative time range breaks since Grafana 8 when 'to' is before 'from' | keyword: pro tip
"We've decided we want to undo this change - if the 'To' time is before 'From', we will no longer swap them. Instead we'll just show a small discrete warning that it may not return any data.",,,,,,Anecdotal,comment,,,,,,,,2023-11-24,github/joshhunt,https://github.com/grafana/grafana/issues/49495#issuecomment-1825475751,repo: grafana/grafana | issue: Semi-relative time range breaks since Grafana 8 when 'to' is before 'from' | keyword: pro tip
"Internationalization: Markup for Playlist Page **What is this feature?** Provides markup for PlaylistPage **Which issue(s) does this PR fix?**: This PR fixes https://github.com/grafana/grafana/issues/74365 <!-- - Automatically closes linked issue when the Pull Request is merged. Usage: ""Fixes #<issue number>"", or ""Fixes (paste link of issue)"" -->",,,,,,Anecdotal,issue,,,,,,,,2023-09-07,github/khushijain21,https://github.com/grafana/grafana/pull/74509,repo: grafana/grafana | keyword: pro tip | state: closed
"I18N: Script for estimating word count We want a _rough_ estimate for a word count on how many phrases we have to translate in Grafana Automating this is inherently prone to producing both false positives and false negatives, so the resulting count must be taken with a grain of salt, but it should be good for ""back of the napkin"" math. Considerations: - All JSX text nodes (e.g. the `Hello world` in `<p>Hello world</p>`) should be translated - Some JSX prop string values should be translated, li…",,,,,,Anecdotal,issue,,,,,,,,2022-03-11,github/joshhunt,https://github.com/grafana/grafana/issues/46451,repo: grafana/grafana | keyword: pro tip | state: closed
<details> <summary>📂 core - 451 words</summary> <blockquote> <details> <summary>📂 components - 433 words</summary> <blockquote> <details> <summary>📂 AccessControl - 24 words</summary> <blockquote> <details> <summary>📃 AddPermission.tsx - 20 words</summary> <blockquote> |Word count|String|Type|Identifier| |-|-|-|-| |2|Missing permission|PropValue|title| |17|You are missing the permission to list users (org.users:read). Please contact your administrator to get this resolved.|JSXText|Alert| |1|Sav…,,,,,,Anecdotal,comment,,,,,,,,2022-03-11,github/joshhunt,https://github.com/grafana/grafana/issues/46451#issuecomment-1065104201,repo: grafana/grafana | issue: I18N: Script for estimating word count | keyword: pro tip
"Gauge/BarGauge: Added support for value mapping of ""no data""-state to text/value **What this PR does / why we need it**: This enables our users to map a ""no data""-state and display an value/text instead of the ""no data"" text. I tried to prevent breaking and old behaviour by checking for mappings of null. The only case where this might happen is if null already is mapped to another value then ""no data"". **Which issue(s) this PR fixes**: Fixes #20706 **Special notes for your reviewer**: I wanted …",,,,,,Anecdotal,issue,,,,,,,,2019-12-03,github/mckn,https://github.com/grafana/grafana/pull/20842,repo: grafana/grafana | keyword: pro tip | state: closed
"> I found some strange behaviour when testing the branch. When I change the `mode` of the BarGauge somehow the value is changed. > ![Kapture 2019-12-04 at 6 53 24](https://user-images.githubusercontent.com/562238/70116374-d4bd4000-1662-11ea-8f0d-7295d0ccad33.gif) Interesting, will have a look and see if my change is the causing the behaviour or if it is as expected(?). Great findings and feedback! @hugohaggmark 🙏",,,,,,Anecdotal,comment,,,,,,,,2019-12-04,github/mckn,https://github.com/grafana/grafana/pull/20842#issuecomment-561490597,"repo: grafana/grafana | issue: Gauge/BarGauge: Added support for value mapping of ""no data""-state to text/value | keyword: pro tip"
"> Code looks good, great stuff with adding some tests! Looking forward to know what's the problem with the issue @hugohaggmark found :) So the reason why we got this behaviour was due to `display.numeric: null` and consumers where expecting `display.numeric: 0` when there are no data. So I added a special test case for this scenario to make it super clear in the future and changed the code a bit. Suuper good that @hugohaggmark found it!",,,,,,Anecdotal,comment,,,,,,,,2019-12-04,github/mckn,https://github.com/grafana/grafana/pull/20842#issuecomment-561529743,"repo: grafana/grafana | issue: Gauge/BarGauge: Added support for value mapping of ""no data""-state to text/value | keyword: pro tip"
@oddlittlebird @marcusolsson I would be super happy if you guys could have a look at the updated documentation I have added in this PR.,,,,,,Anecdotal,comment,,,,,,,,2019-12-04,github/mckn,https://github.com/grafana/grafana/pull/20842#issuecomment-561569988,"repo: grafana/grafana | issue: Gauge/BarGauge: Added support for value mapping of ""no data""-state to text/value | keyword: pro tip"
"Hi Guys, it looks like that the new feature for mapping ""no data"" has no effect when the panel has more than one query, as you can see from the below image: ![image](https://user-images.githubusercontent.com/8842677/70801568-84a35380-1daf-11ea-957a-217212e2e786.png) Here the mappings: ![image](https://user-images.githubusercontent.com/8842677/70801642-b61c1f00-1daf-11ea-9897-be01ff9dd7e1.png)",,,,,,Anecdotal,comment,,,,,,,,2019-12-13,github/gfinocchiaro,https://github.com/grafana/grafana/pull/20842#issuecomment-565431175,"repo: grafana/grafana | issue: Gauge/BarGauge: Added support for value mapping of ""no data""-state to text/value | keyword: pro tip"
"What are your queries returning? Is it returning a null value? 2 queries, where 1 query returns 1 series the other does not, results in 1 series, the panel (visualization) has no knowledge one query returned no data. So you have to modify your query so that it returns a null value when there is no data",,,,,,Anecdotal,comment,,,,,,,,2019-12-13,github/torkelo,https://github.com/grafana/grafana/pull/20842#issuecomment-565439627,"repo: grafana/grafana | issue: Gauge/BarGauge: Added support for value mapping of ""no data""-state to text/value | keyword: pro tip"
"added empty cta to playlist page + hid playlist table when empty fixes #12490 <img width=""1108"" alt=""skarmavbild 2018-08-13 kl 16 22 07"" src=""https://user-images.githubusercontent.com/23470681/44037665-276287c8-9f15-11e8-875c-756e7b0af52e.png"">",,,,,,Anecdotal,issue,,,,,,,,2018-08-08,github/Ijin08,https://github.com/grafana/grafana/pull/12841,repo: grafana/grafana | keyword: pro tip | state: closed
"[wip]added empty list cta to team list if statement toggles view for when the list is empty or not <img width=""1120"" alt=""skarmavbild 2018-08-10 kl 08 54 09"" src=""https://user-images.githubusercontent.com/23470681/43943216-ff90c184-9c7a-11e8-94bf-fc4149dfd895.png""> fixes #12853",,,,,,Anecdotal,issue,,,,,,,,2018-08-09,github/Ijin08,https://github.com/grafana/grafana/pull/12854,repo: grafana/grafana | keyword: pro tip | state: closed
"Visually this looks good but there's a typo in the messaging. Also - what are we going to suggest as a pro-tip? I like the idea, if we have a plan for the content. I'm not sure about using the word ""defined"" here. Maybe created, or added instead. Sounds a little more conversational. ""You haven't created any teams yet.""",,,,,,Anecdotal,comment,,,,,,,,2018-08-09,github/bulletfactory,https://github.com/grafana/grafana/pull/12854#issuecomment-411788764,repo: grafana/grafana | issue: [wip]added empty list cta to team list | keyword: pro tip
Thanks for feedback @bulletfactory Changed the messaging. Not sure about the pro-tip. All I can think of is something about teams and API or permissions. But I don't think we have information on teams and permissions.,,,,,,Anecdotal,comment,,,,,,,,2018-08-10,github/Ijin08,https://github.com/grafana/grafana/pull/12854#issuecomment-411992888,repo: grafana/grafana | issue: [wip]added empty list cta to team list | keyword: pro tip
Seems like we're lacking a specific documentation page for teams why adding a link to protip may be hard or we need to create such a page? The only reference right now is http://docs.grafana.org/guides/whats-new-in-v5/#teams Suggestion: ProTip: Assign folder and dashboard permissions to teams instead of users to ease administration.,,,,,,Anecdotal,comment,,,,,,,,2018-08-13,github/marefr,https://github.com/grafana/grafana/pull/12854#issuecomment-412518587,repo: grafana/grafana | issue: [wip]added empty list cta to team list | keyword: pro tip
"Instantiating Multiple Providers with a loop ### Current Terraform Version <!--- Run `terraform -v` to show the version, and paste the result between the ``` marks below. This will record which version was current at the time of your feature request, to help manage the request backlog. If you're not using the latest version, please check to see if something related to your request has already been implemented in a later version. --> ``` Terraform v0.11.11 ``` ### Use-cases In my current situati…",,,,,,Anecdotal,issue,,,,,,,,2019-01-07,github/JakeNeyer,https://github.com/hashicorp/terraform/issues/19932,repo: hashicorp/terraform | keyword: best practice | state: open
"With a more generic formulation, it would be great to have a ```count``` attribute in **providers**, not only to instantiate multiple times the same provider but also to choose between several **providers**: some having a zero count, some having a non-zero count.",,,,,,Anecdotal,comment,,,,,,,,2019-01-11,github/debovema,https://github.com/hashicorp/terraform/issues/19932#issuecomment-453526838,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
Has anyone ever found any kind of workaround for this kind of scenario? We have multiple AWS accounts and would like to be able to deploy modules across all accounts without having to have repeatable code for each account.,,,,,,Anecdotal,comment,,,,,,,,2019-05-23,github/hhh0505,https://github.com/hashicorp/terraform/issues/19932#issuecomment-495365845,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
@jaken551 Have you tried this with Terraform 0.12 ? Did you make any progress? I ask because I'm looking for the same functionality.,,,,,,Anecdotal,comment,,,,,,,,2019-07-06,github/sc250024,https://github.com/hashicorp/terraform/issues/19932#issuecomment-508954002,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"We are using terraform `0.12` and have the same issue when using the `terraform-provider-aws` and a few other providers. Basically this is a problem in any provider that wasnt designed to work across multiple organizations/accounts/workspaces/etc. By comparison, `terraform-provider-google` and `terraform-provider-tfe` dont really require this feature. To summarize, the ask here is to allow `provider` blocks to instantiated dynamically and be dependent on computed values from other resources, su…",,,,,,Anecdotal,comment,,,,,,,,2019-09-21,github/kuwas,https://github.com/hashicorp/terraform/issues/19932#issuecomment-533806910,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"@jaken551 @kuwas - I'd also like to see provider blocks support for_each. My use case being creating aws_ses_domain_identity resources in multiple regions. These regions being defined by a list variable i.e. ``` resource ""aws_ses_domain_identity"" ""example"" { for_each = var.ses_regions provider = each.value domain = var.ses_domain }",,,,,,Anecdotal,comment,,,,,,,,2020-01-08,github/autodeck,https://github.com/hashicorp/terraform/issues/19932#issuecomment-572078410,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"One workaround is, you need to maintain separate directory for each account and call the same terraform module by passing correct values to the modules / keep some default values. to run plan / apply in all directory use wrapper such as terragrunt. https://blog.gruntwork.io/terragrunt-how-to-keep-your-terraform-code-dry-and-maintainable-f61ae06959d8 https://davidbegin.github.io/terragrunt/use_cases/execute-terraform-commands-on-multiple-modules-at-once.html",,,,,,Anecdotal,comment,,,,,,,,2020-01-14,github/shankarsundaram,https://github.com/hashicorp/terraform/issues/19932#issuecomment-573982925,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"I'm an enterprise user and doing a for_each over the vault provider would reduce a lot of code if you could just do: ``` variable ""namespaces"" { type = set(string) default = [] description = ""Names to be created"" } resource ""vault_namespace"" ""namespace"" { for_each = var.namespaces path = each.key } provider ""vault"" { for_each = var.namespaces alias = each.key namespace = each.key } ``` instead of copy pasting the same piece for each namespace.",,,,,,Anecdotal,comment,,,,,,,,2020-09-16,github/rgevaert,https://github.com/hashicorp/terraform/issues/19932#issuecomment-693464489,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"Terraform deployment environment protection using separated Cloud Providers accounts is a great method to reduce configuration/deployment error Blast Radius. E.g. different AWS accounts can be used to make deployment environments of any kind (devel, stage, prod, sandbox), with potentially a lot of account with same ""**common configuration**"". Without a solution like for_each in provider block lot of identical code must be replicated, raising a lot the risk of typing errors on addition of each n…",,,,,,Anecdotal,comment,,,,,,,,2020-09-27,github/Roxyrob,https://github.com/hashicorp/terraform/issues/19932#issuecomment-699638233,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"+1 on this. We would like to use Terraform to deploy/manage IAM Roles in a list of AWS Accounts -- assuming a known role in each account for access. Below is an example implementation that would work for us with Terraform >v0.13 and the existing `module.for_each` -- just also need an equivalent `provider.for_each`: ```terraform ## Just some data... a list(map()) locals { aws_accounts = [ { ""aws_account_id"": ""123456789012"", ""foo_value"": ""foo"", ""bar_value"": ""bar"" }, { ""aws_account_id"": ""987654321…",,,,,,Anecdotal,comment,,,,,,,,2020-10-20,github/bryankaraffa,https://github.com/hashicorp/terraform/issues/19932#issuecomment-713080710,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"Allowing alias _as a variable_ in the provider config would also make for cleaner code: ``` variable awsEnvironments { type = list default = [ ""paas-luse1"", ""paas-lusw2"", ""gi-luse1"", ""gi-lusw2"" ] } variable awsRegion { type = map default = { paas-luse1 = ""us-east-1"" paas-lusw2 = ""us-west-2"" gi-luse1 = ""us-east-1"" gi-lusw2 = ""us-west-2"" } } variable project { type = map default = { paas-lusw2 = { localEnvironment = ""paas-luse1"", localVPCSuffix = ""paas"", peerEnvironment = ""paas-lusw2"", peerVPCSuf…",,,,,,Anecdotal,comment,,,,,,,,2020-12-07,github/DonBower,https://github.com/hashicorp/terraform/issues/19932#issuecomment-740144403,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"> @DonBower AFAIK, alias _is_ allowed in the providers config for modules It is. but not as a variable.... ``` Error: Variables not allowed on provider.tf line 27, in provider ""aws"": 27: alias = var.awsEnvironments[count.index] Variables may not be used here. ``` original comment updated....",,,,,,Anecdotal,comment,,,,,,,,2020-12-07,github/DonBower,https://github.com/hashicorp/terraform/issues/19932#issuecomment-740171607,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"@ajbouh @DonBower Yes, you can pass aliased providers into modules, but the providers and their aliases are still statically defined. Furthermore, when using `for_each` with modules, you must pass in the **same** provider instance to every module instance within the loop. I recall reading a comment from @apparentlymart on the reasoning around this current limitation. Example 1: this does not work ``` provider ""azurerm"" { for_each = toset([""one"", ""two""]) alias = each.value } module ""test"" { for_…",,,,,,Anecdotal,comment,,,,,,,,2020-12-07,github/kuwas,https://github.com/hashicorp/terraform/issues/19932#issuecomment-740190413,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"I've literally tried to this exact same thing and have been unable. ``` module ""peering"" { for_each = var.vnet_peering_config source = ""./config/network_peering"" providers = { azurerm.custom = ""azurerm.${each.value.provider_alias}"" } resource_group_name = module.resource_group.rg.name ne_virtual_network = azurerm_virtual_network.default we_virtual_network = azurerm_virtual_network.default remote_virtual_network_name = each.virtual_network_name remote_resource_group_name = each.resource_group_na…",,,,,,Anecdotal,comment,,,,,,,,2020-12-08,github/oliverlucas85,https://github.com/hashicorp/terraform/issues/19932#issuecomment-740652779,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"Any updates? Also depends from this ... For AWS provider the only way to use multi-regions for me is dublicate code many times ```tf provider ""aws"" { alias = ""euwestprovider"" region = ""eu-west-1"" ... } provider ""aws"" { alias = ""eucentralprovider"" region = ""eu-central-1"" ... } resource ""aws_instance"" ""ec2-eu-west""{ provider = ""aws.euwestprovider"" ami = ""ami-030dbca661d402413"" instance_type = ""t2.nano"" } resource ""aws_instance"" ""ec2-eu-central"" { provider = ""aws.eucentralprovider"" ami = ""ami-0ebe…",,,,,,Anecdotal,comment,,,,,,,,2021-04-09,github/augustgerro,https://github.com/hashicorp/terraform/issues/19932#issuecomment-816687083,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"+1 on the topic. Can you give us some feedback? We are using multiple accounts in my company, and it will be great to be able to loop through organization accounts to create the same resources. for_each in providers block is really needed. Thanks.",,,,,,Anecdotal,comment,,,,,,,,2021-04-12,github/ced3eals,https://github.com/hashicorp/terraform/issues/19932#issuecomment-817869607,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"Hey all 👋 - I found a ~~solution~~ workaround that works well enough to create multiple sets of S3 buckets (or any resource) across multiple regions using multiple [providers](https://www.terraform.io/docs/language/meta-arguments/module-providers.html) and [modules](https://www.terraform.io/docs/language/modules/index.html). It's not as clean as provider interpolation would be, but it **does** work. An example using two S3 buckets in _different_ regions: ``` provider ""aws"" { alias = ""use1"" regi…",,,,,,Anecdotal,comment,,,,,,,,2021-04-21,github/egeexyz,https://github.com/hashicorp/terraform/issues/19932#issuecomment-823748793,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
Big +1 to get this fixed please. Just dealing with the available regions is hard; I can't imagine having to deal with this for multiple AWS accounts.,,,,,,Anecdotal,comment,,,,,,,,2021-05-10,github/gswallow,https://github.com/hashicorp/terraform/issues/19932#issuecomment-836104254,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"With the `azurerm` module, it is not possible to create a resource group in a specific subscription unless you make the provider explicit. This limitation makes it impossible to iterate over a map or list of configuration data, when the underlying subscription is also parameterized. ```terraform provider ""azurerm"" { features {} alias = ""specific_subscription"" subscription_id = ""00000000-0000-0000-0000-000000000000"" } resource ""azurerm_resource_group"" ""foo"" { name = ""foo-rg"" location = ""centralu…",,,,,,Anecdotal,comment,,,,,,,,2021-05-13,github/NeverOddOrEven,https://github.com/hashicorp/terraform/issues/19932#issuecomment-840655296,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"I know this sounds a bit crazy, but I solved this using M4 macros. You can use `esyscmd` to do whatever you want, and then the output of that will be handled as macros. So, for instance, here's a simplified `main.tf.m4` that does something for every AWS region: ```m4 /** * ******** DO NOT EDIT THIS FILE ******** * * This file is auto-generated. If a new region has launched, please run make. */ dnl dnl You can edit this file, though. The end of the file will shell out to the AWS CLI and dnl magi…",,,,,,Anecdotal,comment,,,,,,,,2021-06-07,github/ziggythehamster,https://github.com/hashicorp/terraform/issues/19932#issuecomment-856191323,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"I dealt with it by using a shell script that calls terraform to output a map variable that contains the account structure, it then iterates over each element (account) to build a terraform environment for that environment (copy in the base code, copy in the variable structure, build a backend file so everyone has their own S3 state, etc.) and then launch a new terraform instance for every single one, capture the output and exit codes, and then dump the output from each one back to the caller an…",,,,,,Anecdotal,comment,,,,,,,,2021-06-07,github/PCjrBasic,https://github.com/hashicorp/terraform/issues/19932#issuecomment-856312490,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"I'd like to add another use cases to justify this demand. 1. When you setup an AWS Organization and follow best practices, you often want to create a centralized network account and share the VPCs via AWS RAM. You also might want to place a private route53 zone in each account you share the VPC to (e.g. apps.integration.private, database.integration.private) and associate the VPCs with the zones. Currently you need to configure a provider for each target account and pass it to a module. Neither…",,,,,,Anecdotal,comment,,,,,,,,2021-06-21,github/yves-vogl,https://github.com/hashicorp/terraform/issues/19932#issuecomment-864970638,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"@apparentlymart @jbardin Can we have an update on this? Other issues have been closed in favor of using this one for tracking, but I cannot find a progress update in the past year on this issue. Last I heard this was [planned for Terraform 1.0](https://github.com/hashicorp/terraform/issues/24476#issuecomment-700368878) (or maybe delayed until is more accurate), which of course you know is now out. We need it to generically work with an arbitrary set of accounts.",,,,,,Anecdotal,comment,,,,,,,,2021-07-07,github/Nuru,https://github.com/hashicorp/terraform/issues/19932#issuecomment-875931295,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"Hi @Nuru, Sorry, I don't have any update at the moment. While it seems like a simple request on the surface, the underlying architecture of terraform is not suited for using providers in this manner. What this essentially means is that it's going to be a very large project which must complete in resources with many other large projects. While this is definitely a desired feature, and fulfilling this use case has a high priority, I cannot say which release may be targeted for implementation.",,,,,,Anecdotal,comment,,,,,,,,2021-07-09,github/jbardin,https://github.com/hashicorp/terraform/issues/19932#issuecomment-877355744,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"`enabled` parameter to avoid logical statements in `count` Terraform v0.12.2 ### Use-cases We often use `count` to disable a resource (above all in modules) from a boolean var, with statements like `count = var.enabled ? 1 : 0` or `count = var.enabled ? 1 : length([some list of resources or datasources])` Here are among others some code snippets from `terraform-aws-vpc` module : ``` resource ""aws_subnet"" ""private"" { count = var.create_vpc && length(var.private_subnets) > 0 ? length(var.private_…",,,,,,Anecdotal,issue,,,,,,,,2019-07-02,github/romachalm,https://github.com/hashicorp/terraform/issues/21953,repo: hashicorp/terraform | keyword: best practice | state: open
"This is in my opinion such a huge problem with what appears a ""simple"" solution. Relying on count to disable a module has a lot of bad effects from how you retrieve the output to having warnings about unsupported ""count"" from submodules that define providers themselves. What is the technical/functional obstacle of having a simple boolean switch on modules and/or resources? One can neatly separate module calls by using terraform workspace name and much more, avoiding tons of nasty hacks that are…",,,,,,Anecdotal,comment,,,,,,,,2020-09-25,github/PCatinean,https://github.com/hashicorp/terraform/issues/21953#issuecomment-699144350,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"@PCatinean The argument was that there are no clear use-cases. I fully support yours and @romachalm and here's mine: I deploy Landing Zone for 200 Products from my `landing-zone` module, which actually consists of different modules such as `network`, `key-vault`, `rbac`, `des` and so on. In some cases, deployment of a feature requires other inputs. e.g. to deploy disk encryption set `des`, I need first the Security team to deploy the HSM key into a vault that I deploy via `key-vault` of the `la…",,,,,,Anecdotal,comment,,,,,,,,2021-02-26,github/Tbohunek,https://github.com/hashicorp/terraform/issues/21953#issuecomment-786729696,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"Adding additional context @schollii provided in #27936 ---- [As recommended by @apparentlymart in https://github.com/hashicorp/hcl/issues/450, I'm moving this improvement request here.] The notion of ""nullness"" or ""existence"" must be first class in a language, and the language should minimize how much ""how"" the user needs to express. Currently in order to make resources and modules optional, we have to resort to using count = 0 or 1. Examples: ``` resource ""provider_type"" ""name"" { count = var.n…",,,,,,Anecdotal,comment,,,,,,,,2021-03-01,github/jbardin,https://github.com/hashicorp/terraform/issues/21953#issuecomment-787987116,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"Thank you @jbardin for taking the time to write that up. The sheer length of the post forced me to also take some time to read it and understand it, and you summarized it very well. How does it look now? Did the chances of this going through increase? It would help many Terraform users build better, nicer, cleaner and more flexible code, and adopt `enabling` ""as they go"" with no code modification effort (like adding `[0]` indices). On the point of chained resources, I would say it doesn't requi…",,,,,,Anecdotal,comment,,,,,,,,2021-03-22,github/Tbohunek,https://github.com/hashicorp/terraform/issues/21953#issuecomment-804357343,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"Hi @Tbohunek, don't thank me, thank @schollii, who originally did that nice writeup as feature request in the hcl repository ;) I agree it is a compelling feature, but there are many competing compelling features to consider for terraform, and we cannot prioritize them all. I cannot say when this might be considered, but it is not a minor feature and will take considerable planning to handle in a future version of terraform.",,,,,,Anecdotal,comment,,,,,,,,2021-03-22,github/jbardin,https://github.com/hashicorp/terraform/issues/21953#issuecomment-804380597,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"Thanks @schollii ! :) @jbardin if `count` could co-exist with this `enabled`, the change should be transparent to current users. If `count` would change then obviously the sooner the better. More users stuck with `count` makes such change harder up to a point where it won't matter anymore.",,,,,,Anecdotal,comment,,,,,,,,2021-03-27,github/Tbohunek,https://github.com/hashicorp/terraform/issues/21953#issuecomment-808694863,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"@jbardin This looks like a minor and backward-compatible effort though. While waiting for the big features to be prioritized, we keep struggling daily and produce hard-to-maintain unreadable code!",,,,,,Anecdotal,comment,,,,,,,,2021-04-10,github/nikolay,https://github.com/hashicorp/terraform/issues/21953#issuecomment-817212216,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"Consider Meta-Argument `condition` or `include_if` so people stop using `count= 0 or 1 ` or `for_each empty or single item` And `content` which allows passing a data structure instead of DSL When choosing a Meta-Argument consider classes with existing resources. ``` locals { something = { prevent_destroy = true } } dynamic ""lifecycle"" { condition = can(var.ENVIRONMENT) # include_if = can(var.ENVIRONMENT) content = something # as long as the map is what the resource expects in this block. i.e. i…",,,,,,Anecdotal,comment,,,,,,,,2021-08-07,github/damon-atkins,https://github.com/hashicorp/terraform/issues/21953#issuecomment-894608054,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"@damon-atkins both condition and include_if are fine by me, but I'm not sure I agree with content. Can you provide an example? I think count and enabled / condition / include_if would have to be mutually exclusive. Count means you want an array, possibly of 0 size, and the language should make it simple to handle the case where count is 0, a bit like `one()` in does. Whereas enabled / condition is never an array, the resource is either there or null, and the DSL should make it easy to deal with…",,,,,,Anecdotal,comment,,,,,,,,2021-08-07,github/schollii,https://github.com/hashicorp/terraform/issues/21953#issuecomment-894658311,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"I just ran into this when trying to create OpenStack resources, optionally using a keypair or creating one depending on a value. Unfortunately it is extremely ugly to make everything consider the keypair to be a list.",,,,,,Anecdotal,comment,,,,,,,,2021-12-10,github/tculp,https://github.com/hashicorp/terraform/issues/21953#issuecomment-991192054,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"content as show allow you to pass a data structure representing the HCL ``` locals { something = { prevent_destroy = true etc.... } } dynamic ""lifecycle"" { include_if= can(var.ENVIRONMENT) # condition = can(var.ENVIRONMENT) content = something # as long as the map is what the resource expects in this block. i.e. its passing prevent_destroy } ``` vs ``` dynamic ""lifecycle"" { include_if = can(var.ENVIRONMENT) # condition = can(var.ENVIRONMENT) prevent_destroy = true etc..... } ```",,,,,,Anecdotal,comment,,,,,,,,2021-12-13,github/damon-atkins,https://github.com/hashicorp/terraform/issues/21953#issuecomment-992159210,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"Hi all! I think there are at least four remaining design questions to be solved before something like this could be implemented: * The usual reason given to motivate this addition is the desire to be able to disable something without having to change all of the references to it. Currently if I add `count` to `aws_instance.foo` then every other reference to it must become `aws_instance.foo[count.index]` where `count.index` is `0`, or must be a carefully-guarded hard-coded reference to `aws_insta…",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/apparentlymart,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1216017283,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"@apparentlymart I think `enabled = <condition>` could work just like `count = <condition> ? 1 : 0` and people would have to use `one(aws_instance.foo[*])`. Basically, `enbaled` would be syntactic sugar and nothing more and thus fully backward-compatible. I've also seen people writing `for_each = <condition> ? [1] : []`, which could also be replaced with `enabled = <condition>`. Lastly, HCL could be extended with the comprehension-like `resource ... {} if <condition>` instead of making this an a…",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/nikolay,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1216071967,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"The best sugar of it all would be to not have to add unpredictable `.[0]` into every reference of the `counted` resource, and conditions to `outputs` (i.e. `output` would default to `null` or `[]` respectively if the resources get deployed or not).",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/Tbohunek,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1216200424,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"@Tbohunek I wish `one(<reference>[*])` could be sugared somehow. Talking about outputs - I wish they had `for_each`, `count`, and `enabled`, too!",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/nikolay,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1216261257,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"count has order issues, for_each does not, hence people have change code to use for_each. `include_if` or `enable` could be the same as commenting out the section of code. e.g. this does not exist do not include it, or I do not want a setting to be set in this account/resource. Thats why I suggest name `include_if` Sometime people create two tf files which are the same with parts of one deleted, because a flag like this does not exist. `count=0` and `for_each=[]` still create an empty item.",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/damon-atkins,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1216268318,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
To actually react on @apparentlymart's post > * enabled = aws_instance.foo != null How about `enabled = aws_instance.foo.enabled`? For `output` so far I defaulted all of them into `null` because it doesn't really matter.,,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/Tbohunek,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1216501719,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"The following could be the same, i.e. if `include_if` is false treat the following as if it is commented out. ``` dynamic ""lifecycle"" { include_if= can(var.DESTROY) # condition = can(var.DESTROY) Evaluated to TRUE, included lines prevent_destroy = var.DESTROY } ``` ``` # dynamic ""lifecycle"" { # include_if= can(var.DESTROY) # condition = can(var.DESTROY) Evaluated to FALSE, exclude lines # prevent_destroy = var.DESTROY # } ```",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/damon-atkins,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1216952936,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"We cannot add any new synthetic attributes to an expression like `aws_instance.foo`, because that's already defined to be an object of a type defined by the provider's schema. If we wanted to expose metadata about a `resource` block in addition to the actual data for that object then I expect we'd need to do it under a different namespace such as `meta.aws_instance.foo`, although I've not deeply considered the implications of doing that.",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/apparentlymart,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1217257356,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
Wouldn't the chosen attribute like `.enabled` become part of the schema for all objects? The caveat here is rather to make `.enabled` accessible on an object that doesn't actually exist.,,,,,,Anecdotal,comment,,,,,,,,2022-08-17,github/Tbohunek,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1217483758,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"This comes back to my comment about defining what exactly `aws_instance.foo` evaluates to when we have a resource with `enabled = true` or `enabled = false` set. It's true that Terraform _could_ add a synthetic additional attribute to the resource type's schema so that `.enabled` could be `true` in the case where it's enabled, but as you noted `<null object>.enabled` would fail with an error, so I don't think this design is quite right yet. (Note that this is also in the vicinity of the other p…",,,,,,Anecdotal,comment,,,,,,,,2022-08-17,github/apparentlymart,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1218239509,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"In some languages (eg Python), null is an object ie it has a type and attributes just like any other (it is called None in Python but same thing). Could something like that work? So there would be a built-in resource that represents null and has enabled = false. Every other object would have enabled = true. Also, let's not forget that the real goal is to not have to litter the code with `[0]` and code that checks for empty list. Groovy's approach to handling null should be considered : somethin…",,,,,,Anecdotal,comment,,,,,,,,2022-08-17,github/schollii,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1218526585,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
@apparentlymart or anyone why do we need to read anything back? Why just have the HCL parser ignoring a set of lines not enough to do the job? On a side note we already have `can()` which can test something exists or not.,,,,,,Anecdotal,comment,,,,,,,,2022-08-18,github/damon-atkins,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1218821517,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"Of course it depends on the task at hand but at minimum it's typical for a module to output something about the object it declared so that the caller can use it: ```hcl resource ""aws_instance"" ""example"" { count = var.enable_web_servers ? 5 : 0 # ... } output ""web_server_ip_addresses"" { value = toset(aws_instance.example[*].private_ip) } ``` My comment above was saying that a full proposal for adding any new mechanism for changing how many instances there are of a resource (even if it's just lim…",,,,,,Anecdotal,comment,,,,,,,,2022-08-18,github/apparentlymart,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1219680940,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"Any reason why output can not also have `include_if` or something. ``` resource ""aws_instance"" ""webserver"" { include_if= can(env.MAX_WEB_SERVERS) count = env.MAX_WEB_SERVERS # ... } output ""web_server_ip_addresses"" { include_if= can(aws_instance.webserver[*].private_ip) value = toset(aws_instance.webserver[*].private_ip) } ``` If `include_if` is false do not include the current block `{` `}` and children blocks",,,,,,Anecdotal,comment,,,,,,,,2022-08-19,github/damon-atkins,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1220988768,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"@damon-atkins you can already do what you describe with `one()`, no? ``` output ""web_server_ip_addresses"" { value = one(aws_instance.webserver[*].private_ip) } ``` will be null if the expression does not evaluate and will be the specified attribute of `item[0]` if there is a zeroth item. I've never sat down and tried to see if `one()` might be a suitable workaround for the problem that this current github issue attempts to address. It is certainly not a solution, but it might be a workaround.",,,,,,Anecdotal,comment,,,,,,,,2022-08-19,github/schollii,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1221169985,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
@schollii what I am suggesting can be placed anywhere as part of a block ``` layerone { include_if = ... layertwo { include_if = ... layerN { include_if = ... } } } ```,,,,,,Anecdotal,comment,,,,,,,,2022-08-20,github/damon-atkins,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1221203008,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"I think there is a much simpler answer, both for everyone here and for your team to implement @apparentlymart. > We need to decide what type of value aws_instance.foo would be both when it's enabled and when it's disabled, and that decision will in turn decide what references to that value would look like. I don't believe that is necessary for the situation where something should or should not exist. I'm going to suggest a different answer below that will satisfy everything I personally want ou…",,,,,,Anecdotal,comment,,,,,,,,2022-08-20,github/jorhett,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1221389742,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"I'm not sure why everybody's so inclined for these long and not very meaningful `include_if` and `only_if`! Do we have `include_if` or `only_if` in the `for` expressions? No! So, why can't this just be `if` if you don't like `enabled`? I've seen tons of modules, already using `enabled` in parameter along with the logic `count = var.enabled ? 1 : 0`, so, using `enabled` would match what people already use and simply replace the above with `enabled = var.enabled`. But changing HCL to have an `if`…",,,,,,Anecdotal,comment,,,,,,,,2022-08-20,github/nikolay,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1221403327,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"prevent_destroy should let you succeed Call me crazy, but I'm willing to call the current implementation of `prevent_destroy` a bug. Here is why: The current implementation of this flag prevents you from using it for 1/2 the use case. The net result is more frustration when trying to get Terraform to succeed instead of destroying your resources.. `prevent_destroy` adds to the frustration more than alleviating it. `prevent_destroy` is for these two primary use cases, right? 1) you don't want thi…",,,,,,Anecdotal,issue,,,,,,,,2015-11-12,github/ketzacoatl,https://github.com/hashicorp/terraform/issues/3874,repo: hashicorp/terraform | keyword: best practice | state: open
"Hi @ketzacoatl - thanks for opening this! Based on your description I'm certainly sympathetic to the idea that Terraform should not terminate with an error code if the user intent is to prevent resources being deleted, but I'm inclined to say that the output should indicate that resources where `prevent_destroy` was a factor in the execution should indicate this. @phinze, do you have any thoughts on this?",,,,,,Anecdotal,comment,,,,,,,,2015-11-12,github/jen20,https://github.com/hashicorp/terraform/issues/3874#issuecomment-156144124,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"Definitely sympathetic about this use-case too. I think a concern is that if Terraform fails to include one part of the update then that may have downstream impact in the dependency graph, which can be fine if you're intentionally doing it but would be confusing if Terraform just did it ""by default"". Do you think having the ability to exclude resources from plan, as proposed in #3366, would address your use-case? I'm imagining the following workflow: - Run `terraform plan` and see the error you…",,,,,,Anecdotal,comment,,,,,,,,2015-11-12,github/apparentlymart,https://github.com/hashicorp/terraform/issues/3874#issuecomment-156160003,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"I'd agree @jen20, I am primarily looking for the ability to tell TF that it does not need to quit/error out hard. Same on @apparentlymart's comment on default behavior - I agree, this is a specific use case and not meant as a default. > Do you think having the ability to exclude resources from plan, as proposed in #3366, would address your use-case? I had to re-read that a few times to make enough sense out of how that works (the doc addition helps: _Prefixing the resource with ! will exclude t…",,,,,,Anecdotal,comment,,,,,,,,2015-11-13,github/ketzacoatl,https://github.com/hashicorp/terraform/issues/3874#issuecomment-156288038,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"Would it be possible to get an additional flag when calling: terraform plan -destroy [ -keep-prevent-destroy ] I have the same problem, I have a few EIP associated with some instances. I want to be able to destroy every but keep the EIP for obvious reasons like whitelisting but I get the same kind of problem. I understand what destroy is all about, but in some cases it would be nice getting a warning saying this and that didn't get destroyed because of lifecycle.prevent_destroy = true. @ketzaco…",,,,,,Anecdotal,comment,,,,,,,,2015-11-16,github/mrfoobar1,https://github.com/hashicorp/terraform/issues/3874#issuecomment-157086868,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"+1, I need something along these lines as well. Would #3366 allow you to skip destroying a resource, but modify it instead? My specific use case is that I have a staging RDS instance I want to persist (never be destroyed), but I want the rest of my staging infrastructure to disappear. As a side effect of the staging environment disappearing, I need to modify the security groups on the RDS instance, since it is being deleted. So, if I had - two AWS instances - one rds security group - one rds in…",,,,,,Anecdotal,comment,,,,,,,,2015-11-19,github/erichmond,https://github.com/hashicorp/terraform/issues/3874#issuecomment-157945975,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"Hey folks, Good discussion here. It does sound like there's enough real world use cases to warrant a feature here. What about maintaining the current semantics of `prevent_destroy` and adding a new key called something like `skip_destroy` indicating: _any plan that would destroy this resource should be automatically modified to **not** destroy it_. Would something like this address all the needs expressed in this thread? If so, we can spec out the feature more formally and get it added.",,,,,,Anecdotal,comment,,,,,,,,2015-12-03,github/phinze,https://github.com/hashicorp/terraform/issues/3874#issuecomment-161715361,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"@phinze, that sounds good, yes.. I'd hope that in most cases, TF would be able to let the apply proceed, and let the user flag some resources as being left alone/not destroyed, and your proposal seems to provide the level of control needed, while retaining sensible semantics.",,,,,,Anecdotal,comment,,,,,,,,2015-12-03,github/ketzacoatl,https://github.com/hashicorp/terraform/issues/3874#issuecomment-161737325,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"I keep running in to this, I would like the ability for TF to only create if it does not exist and do not delete it. I would like to keep some ebs or rds data around and keep the rest of my stack as ephemeral (letting TF apply/destroy at will). Currently been doing this with different projects/directories. But it would be nice to keep the entire stack together as one piece. I too thought the prevent_destroy would not create an error and have been hacking my way around it quite a bit :(",,,,,,Anecdotal,comment,,,,,,,,2016-01-09,github/chadgrant,https://github.com/hashicorp/terraform/issues/3874#issuecomment-170273569,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
":+1: to what @phinze said. During apply, I want it to be created but ignored during destroy. Currently, I have to explicitly define the rest of the targets just to ignore 1 s3 resource.",,,,,,Anecdotal,comment,,,,,,,,2016-03-14,github/tsailiming,https://github.com/hashicorp/terraform/issues/3874#issuecomment-196392632,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"+1 - just ran into this. Another example are key pairs. I want to create (import) them if they don't exist, but if I destroy, I don't want to delete the keypair as other instances may be using the shared keypair. Is there a way around this for now?",,,,,,Anecdotal,comment,,,,,,,,2016-03-27,github/gservat,https://github.com/hashicorp/terraform/issues/3874#issuecomment-202054221,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"Yes, split your terraform project into multiple parts. Example: - base - core (like persistent data) - application stack Le dimanche 27 mars 2016, gservat notifications@github.com a écrit : > +1 - just ran into this. Another example are key pairs. I want to create > them if they don't exist, but if I destroy, I don't want to delete the > keypair as other instances may be using the shared keypair. > > Is there a way around this for now? > > — > You are receiving this because you commented. > Rep…",,,,,,Anecdotal,comment,,,,,,,,2016-03-28,github/mrfoobar1,https://github.com/hashicorp/terraform/issues/3874#issuecomment-202569646,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
Is this being looked at? I can't imagine there are many use cases that would NOT benefit from it. One example is 'Anyone using key pairs ever'.,,,,,,Anecdotal,comment,,,,,,,,2016-12-14,github/jbrown-rp,https://github.com/hashicorp/terraform/issues/3874#issuecomment-267093193,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"This is absolutely one of the banes of my life too. I've got *dozens* of resources I want to preserve from accidental overwrites - such as DynamoDB tables. A pair of flags for: - Keeping items that you prevent_destroy on (i.e. Don't delete the users from DynamoDB, ever - just skip it during a routine destroy) - Destroy, force. The flags could be something explicit like: - terraform destroy --skip-protected - terraform destroy --force-destroy-protected This would allow us to have the desired beh…",,,,,,Anecdotal,comment,,,,,,,,2017-01-04,github/steve-gray,https://github.com/hashicorp/terraform/issues/3874#issuecomment-270296400,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"Here's the use case we'd like this for: we have a module that we can use either for production (where some resources like Elastic IPs should not be accidentally deleted) or for running integration tests (where all resources should be destroyed afterwards). Because of #10730/#3116, we can't set these resources to be conditionally prevent_destroy, which would be the ideal solution. As a workaround, we'd be happy to have our integration test scripts run `terraform destroy --ignore-prevent-destroy`…",,,,,,Anecdotal,comment,,,,,,,,2017-01-19,github/glasser,https://github.com/hashicorp/terraform/issues/3874#issuecomment-273662886,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"This would defintely be a useful feature. I've been using terraform for less than a month and ran into this required feature in order to protect DNS Managed zone ... everything else in my infrastucture is transient but dealing with a new DNS zone comes with it computed ( potentially new ) Name Servers on what is a delegated zone, and this would introduce an unnecessary manual step to update the parent DNS managed zone - not to mention the DNS change time delay permeating making any auto testing…",,,,,,Anecdotal,comment,,,,,,,,2017-01-24,github/andyjcboyle,https://github.com/hashicorp/terraform/issues/3874#issuecomment-274786378,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"I'm hitting a slightly different use case with Vault. I'm not 100% sure whether this belongs here. Might be best handled in the Vault resource itself. Example: ```hcl resource ""vault_generic_secret"" ""github_auth_enable"" { path = ""sys/auth/github"" data_json = ""...some json..."" } resource ""vault_generic_secret"" ""github_auth_config"" { path = ""auth/github/config"" data_json = ""...some json..."" depends_on = [""vault_generic_secret.github_auth_enable""] } ``` The problem is that the 'auth/github/config'…",,,,,,Anecdotal,comment,,,,,,,,2017-01-25,github/kaii-zen,https://github.com/hashicorp/terraform/issues/3874#issuecomment-274996945,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"So my instance and issue would be things like rapid development and say docker_image / docker_container usage. I set `prevent_destroy = true` on the docker_image resources because I don't want terraform deleting the image from my disk so that I can rapidly destroy/create and run through development. When I set that, now I have to use a fancy scripting method to execute my targeted destroy list to destroy everything BUT the docker_image resources: ```bash TARGETS=$(for I in $(terraform state lis…",,,,,,Anecdotal,comment,,,,,,,,2017-02-24,github/mengesb,https://github.com/hashicorp/terraform/issues/3874#issuecomment-282431612,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"Any update on this. This has been open for 1.5 years and it is not fun to try to organize terraform around this shortcoming. The workaround for this is pretty ridiculous, I have a separate modules for ""persistent"", ""ephemeral"" in every project, but still need to use target or some way of skipping of and not running destroying modules that are persistent (or they spew errors).",,,,,,Anecdotal,comment,,,,,,,,2017-05-22,github/bradenwright,https://github.com/hashicorp/terraform/issues/3874#issuecomment-303233117,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"Is there any work being done on this? It feels like this feature ""prevent_destroy"" is designed as ""annoy you because you put this flag in if you want to destroy resources"" rather than... destroy what I want to destroy except for the things I don't want to destroy, notated by the ""prevent_destroy"" flag. Use case 1 in the original post seems like a silly use case because it's only designed to alert you and error out. In reality, adding prevent_destroy on a resource actually seems to mean prevent …",,,,,,Anecdotal,comment,,,,,,,,2017-06-06,github/HighwayofLife,https://github.com/hashicorp/terraform/issues/3874#issuecomment-306638305,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"To echo what @andyjcboyle said; when creating an aws_route53_zone you get a delegation set of 4 random name servers. I use the zone to define a subdomain, my domain is however not managed by terraform and I must insert the ns records manually. If I want to teardown my environment and then redeploy it (which I do often) I must manually reinsert the new name servers. It would be much nicer if I could have lifecycle flag like ignore_destroy/skip_destroy that allowed everything else in the plan to …",,,,,,Anecdotal,comment,,,,,,,,2017-07-17,github/Harrison-Miller,https://github.com/hashicorp/terraform/issues/3874#issuecomment-315911357,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"Hashicorp does not want this. It's the only explanation why it has not been implemented yet. On Mon, 17 Jul 2017 at 20:12 Verrazano <notifications@github.com> wrote: > To echo what @andyjcboyle <https://github.com/andyjcboyle> said; when > creating an aws_route53_zone you get a delegation set of 4 random name > servers. I use the zone to define a subdomain, my domain is however not > managed by terraform and I must insert the ns records manually. If I want > to tear down my environment and then…",,,,,,Anecdotal,comment,,,,,,,,2017-07-22,github/cescoferraro,https://github.com/hashicorp/terraform/issues/3874#issuecomment-317201553,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"This feature was added originally more as a ""prevent _replace_"", to avoid accidentally changing a ""forces new resource"" attribute on a critical object. Its current interaction with `terraform destroy` is not a critical part of that, so I think it would be reasonable to strike a compromise here: * If a diff contains a replacement of a `prevent_destroy` instance, that is a fatal error since there is no way to make the changes indicated without violating the constraint. * If the diff would contain…",,,,,,Anecdotal,comment,,,,,,,,2017-07-22,github/apparentlymart,https://github.com/hashicorp/terraform/issues/3874#issuecomment-317206197,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
That doesn't satisfy the use case in my comment above (which is honestly really an argument for the ability to set prevent_destroy conditionally rather than for this precise feature).,,,,,,Anecdotal,comment,,,,,,,,2017-07-23,github/glasser,https://github.com/hashicorp/terraform/issues/3874#issuecomment-317225876,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"Allow version to be variable in (sub)modules I would like to have default sub module version and a variable input for submodules. A solution could look like this: ``` module ""solution"" { source = ""app.terraform.io/customer/solution"" version = ""1.1.5"" global_settings = var.global_settings account_id = var.global_settings.account_id networking_module_version = var.networking_module_version application_module_version = var.application_module_version } variable ""networking_module_version"" { default…",,,,,,Anecdotal,issue,,,,,,,,2021-06-09,github/wjvanroosmalen,https://github.com/hashicorp/terraform/issues/28912,repo: hashicorp/terraform | keyword: best practice | state: open
"Upvote! Currently tf throws ""Variables not allowed: Variables may not be used here."", when you try to feed version from a var or local. It would be nice to allow this, so we can bind a certain version as variable default, but leave the freedom for consumers to use older or preview/beta versions.",,,,,,Anecdotal,comment,,,,,,,,2021-09-27,github/Ledermayer,https://github.com/hashicorp/terraform/issues/28912#issuecomment-927970761,repo: hashicorp/terraform | issue: Allow version to be variable in (sub)modules | keyword: best practice
"Thanks for your interest in this issue! This is just a reminder to please avoid ""+1"" comments, and to use the upvote mechanism (click or add the 👍 emoji to the original post) to indicate your support for this issue. Thanks again for the feedback!",,,,,,Anecdotal,comment,,,,,,,,2022-05-12,github/crw,https://github.com/hashicorp/terraform/issues/28912#issuecomment-1125401647,repo: hashicorp/terraform | issue: Allow version to be variable in (sub)modules | keyword: best practice
"I am trying to solve a slightly different problem: promoting module versions across environments without requiring a module to wrap the entire stack definition. There are a variety of pre-processors that can be used as workaround: [Terrafile](https://github.com/coretech/terrafile), [Terraspace custom helpers](https://terraspace.cloud/docs/patterns/different-versions/), and [Terramate generation](https://github.com/mineiros-io/terramate/blob/main/docs/codegen/generate-hcl.md#tm_dynamic-block).",,,,,,Anecdotal,comment,,,,,,,,2022-08-07,github/cgetzen,https://github.com/hashicorp/terraform/issues/28912#issuecomment-1207492404,repo: hashicorp/terraform | issue: Allow version to be variable in (sub)modules | keyword: best practice
">without requiring a module to wrap the entire stack definition. Hey @cgetzen , interesting pointers to aforementioned solutions, but what do you mean by this?",,,,,,Anecdotal,comment,,,,,,,,2022-08-07,github/dimisjim,https://github.com/hashicorp/terraform/issues/28912#issuecomment-1207495712,repo: hashicorp/terraform | issue: Allow version to be variable in (sub)modules | keyword: best practice
"I would already be pleased with a static local. E.g. ```terraform module ""ref1"" { source = ""somemodulereference"" version = local.moduleversion ... } terraform module ""ref2"" { source = ""somemodulereference"" version = local.moduleversion ... } terraform module ""ref3"" { source = ""somemodulereference"" version = local.moduleversion ... } locals { moduleversion = ""5.6.3"" } ``` I believe static locals would resolve fine during the init phase. In Terraform cli's current version 1.3.3 this throws errors…",,,,,,Anecdotal,comment,,,,,,,,2022-11-02,github/cveld,https://github.com/hashicorp/terraform/issues/28912#issuecomment-1300449957,repo: hashicorp/terraform | issue: Allow version to be variable in (sub)modules | keyword: best practice
"Would love to see this feature. Now I understand why people are creating two different code bases for dev and prod and why Google has it as their Best Practice, Do we have any updates on this?",,,,,,Anecdotal,comment,,,,,,,,2023-11-09,github/Schillman,https://github.com/hashicorp/terraform/issues/28912#issuecomment-1803507354,repo: hashicorp/terraform | issue: Allow version to be variable in (sub)modules | keyword: best practice
"Is there a way to do this already, that's just not as clear? It's getting to the point where I'm about to build my terraform files from jinja2 templates instead writing them directly.",,,,,,Anecdotal,comment,,,,,,,,2024-01-26,github/draeath,https://github.com/hashicorp/terraform/issues/28912#issuecomment-1912695661,repo: hashicorp/terraform | issue: Allow version to be variable in (sub)modules | keyword: best practice
"We have the same issue. This is a major blocker for us. I need to allow passing variables to module calls for version. Same issue as here: https://github.com/hashicorp/terraform-provider-tfe/issues/849 ```tf module ""example"" { source = ""app.terraform.io/XXXX/eks-cluster/aws"" version = var.module_version # <-- support this please } ```",,,,,,Anecdotal,comment,,,,,,,,2024-03-21,github/Satak,https://github.com/hashicorp/terraform/issues/28912#issuecomment-2012216256,repo: hashicorp/terraform | issue: Allow version to be variable in (sub)modules | keyword: best practice
This would help a lot! I for example have multiple times to use the same module with version xxx and I have to maintain all these places. Please provide this feature.,,,,,,Anecdotal,comment,,,,,,,,2024-03-29,github/ahoehma,https://github.com/hashicorp/terraform/issues/28912#issuecomment-2026878127,repo: hashicorp/terraform | issue: Allow version to be variable in (sub)modules | keyword: best practice
"> Would love to see this feature. Now I understand why people are creating two different code bases for dev and prod and why Google has it as their Best Practice, Do we have any updates on this? Frankly, Google's branches per environment is a terrible practice.",,,,,,Anecdotal,comment,,,,,,,,2024-05-06,github/dcherniv,https://github.com/hashicorp/terraform/issues/28912#issuecomment-2096615173,repo: hashicorp/terraform | issue: Allow version to be variable in (sub)modules | keyword: best practice
"> > Would love to see this feature. Now I understand why people are creating two different code bases for dev and prod and why Google has it as their Best Practice, Do we have any updates on this? > > Frankly, Google's branches per environment is a terrible practice. Wasn't referring to that, was referring to their not so DRY friendly way of working. Where they have several codebases for each environment. Never heard of Googles way of working when it comes to dedicated branches per environment.…",,,,,,Anecdotal,comment,,,,,,,,2024-05-06,github/Schillman,https://github.com/hashicorp/terraform/issues/28912#issuecomment-2096841137,repo: hashicorp/terraform | issue: Allow version to be variable in (sub)modules | keyword: best practice
"Is this on roadmap? Any plans to develop this feature? Super important for Hashicorp Terraform users. **222 up-votes**, **4 years(!)** and counting. Hashicorp, this is the year for this, yes?",,,,,,Anecdotal,comment,,,,,,,,2025-02-18,github/Satak,https://github.com/hashicorp/terraform/issues/28912#issuecomment-2665483533,repo: hashicorp/terraform | issue: Allow version to be variable in (sub)modules | keyword: best practice
"We have to work around this in multiple places across our IAC estate, and those workarounds noticeably slow us down in many cases. If this was resolved it would be a significant win.",,,,,,Anecdotal,comment,,,,,,,,2025-02-18,github/andrewjmurphy,https://github.com/hashicorp/terraform/issues/28912#issuecomment-2665845439,repo: hashicorp/terraform | issue: Allow version to be variable in (sub)modules | keyword: best practice
"I ended up leveraging the [terraform override functionality](https://developer.hashicorp.com/terraform/language/files/override) by wrapping my tf with `terragrunt` and dynamically generate `*_override.tf` where I can define the module versions stored in yaml format (This only works on the first sub-module level) ```hcl locals { modules = yamldecode(file(""modules.yaml"")) } # Generate an override.tf to dynamically set module source references generate ""module_source_override"" { path = ""terragrunt…",,,,,,Anecdotal,comment,,,,,,,,2025-06-23,github/s1mark,https://github.com/hashicorp/terraform/issues/28912#issuecomment-2996735967,repo: hashicorp/terraform | issue: Allow version to be variable in (sub)modules | keyword: best practice
"Ability to pass providers to modules in for_each ### Use-cases <!--- In order to properly evaluate a feature request, it is necessary to understand the use-cases for it. Please describe below the _end goal_ you are trying to achieve that has led you to request this feature. Please keep this section focused on the problem and not on the suggested solution. We'll get to that in a moment, below! --> I'd like to be able to provision the same set of resources in multiple regions a `for_each` on a mo…",,,,,,Anecdotal,issue,,,,,,,,2020-03-26,github/mightyguava,https://github.com/hashicorp/terraform/issues/24476,repo: hashicorp/terraform | keyword: best practice | state: open
"Hello! :robot: This issue seems to be covering the same problem or request as #9448, so we're going to close it just to consolidate the discussion over there. Thanks!",,,,,,Anecdotal,comment,,,,,,,,2020-03-27,github/hashibot,https://github.com/hashicorp/terraform/issues/24476#issuecomment-604994867,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"Hey there @mightyguava & @jspiro, I'm going to re-open the issue as I agree that the concerns are not the same. I did rename it for clarity; to distinguish this request from instantiating providers with `for_each`.",,,,,,Anecdotal,comment,,,,,,,,2020-03-30,github/pkolyvas,https://github.com/hashicorp/terraform/issues/24476#issuecomment-606047618,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"@mightyguava I ran into the same abstraction issue with the `azurerm` provider. My goal was to automate multiple azure subscriptions and keep the code DRY as possible. Since I have to use Service Principals for auth with the `azurerm` provider, each subscription requires a separate provider declaration. I have ended up using `terragrunt`'s `generate` function (https://terragrunt.gruntwork.io/docs/reference/config-blocks-and-attributes/#generate) ```bash . ├── dev │ └── terragrunt.hcl ├── module…",,,,,,Anecdotal,comment,,,,,,,,2020-04-25,github/s1mark,https://github.com/hashicorp/terraform/issues/24476#issuecomment-619450972,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"First of all, thanks for the great work adding iteration and depends_on for modules - both are going to be really useful and I wished for them so many times back during 0.11 days when we were building the majority of our config. In addition to each.key, I'd expect to be able to freely use maps with for_each and have each.&lt;property&gt; be a provider. This would require the ability to assign a provider ""instance"" to a local or list/map members. For example: ``` locals { modules_vars = { instan…",,,,,,Anecdotal,comment,,,,,,,,2020-08-25,github/vivanov-dp,https://github.com/hashicorp/terraform/issues/24476#issuecomment-679859179,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"While not strictly the same as #9448 I think they might be solved together. First, like @vivanov-dp said, thanks for adding the `for_each` support for modules. I had been expecting it for a long time. However I had not realized that provider configuration in modules was deprecated. Here is my use case: * I use terraform to manage the list of AWS accounts I have in my organization * When I create a new account (from a variable list), I then want to provision it with a few common standard resourc…",,,,,,Anecdotal,comment,,,,,,,,2020-09-04,github/gbataille,https://github.com/hashicorp/terraform/issues/24476#issuecomment-686968681,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"I personally think that inline provider declaration, which honors the module `for_each` or `count` is the cleanest solution: ``` module ""some_module"" { source = ""./some-module"" for_each = local.modules_elements provider ""provider1"" { ... } provider ""provider2"" { ... } var1 = each.var1 var2 = each.var2 } ``` Ideally, this would support `dynamic` for providers as well. Another option is to add `for_each` for `provider` as well along these lines: ``` provider ""aws"" { for_each = var.regions region …",,,,,,Anecdotal,comment,,,,,,,,2020-09-25,github/nikolay,https://github.com/hashicorp/terraform/issues/24476#issuecomment-698677004,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"@nikolay That can do the job, but why creating new providers for each module invocation ? Even if it is ""for free"" in terms of performance, which I don't really know, there are a bunch of properties to configure the provider and this approach would require to put them all into `local.modules_elements` and list them all in each provider declaration in each module invocation. You can't really declare an AWS provider just by setting the region. It requires a `profile`, or `access_key`&`secret_key`…",,,,,,Anecdotal,comment,,,,,,,,2020-09-25,github/vivanov-dp,https://github.com/hashicorp/terraform/issues/24476#issuecomment-698803143,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"@vivanov-dp This was pseudocode just to illustrate my point, which was that the logic of how the provider should be initialized could be encapsulated in the module. I can't think of a situation where the instantiation of a provider would be an expensive operation. Also, providers with `assume_role` have session information, which may not make sense to be reused across different modules, but will happen due to natural laziness if we have to create too many aliases.",,,,,,Anecdotal,comment,,,,,,,,2020-09-25,github/nikolay,https://github.com/hashicorp/terraform/issues/24476#issuecomment-698806084,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"@nikolay What I understand is that you propose to have this: ``` locals { modules_vars = { instance_1 = { var1 = ... var2 = ... region = ... profile = ... role_arn = ... } instance_2 = { var1 = ... var2 = ... region = ... profile = ... role_arn = ... } } } module ""some_module"" { source = ""./some-module"" for_each = local.modules_vars provider ""aws"" { region = each.region profile = each.profile assume_role { role_arn = each.role_arn } } var1 = each.var1 var2 = each.var2 } ``` instead of: ``` prov…",,,,,,Anecdotal,comment,,,,,,,,2020-09-25,github/vivanov-dp,https://github.com/hashicorp/terraform/issues/24476#issuecomment-698825661,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"A use case for me would be to configure a dynamic provider based on output from a module using `for_each` such as creating multiple kubernetes clusters (`foo`) and optionally applying resources (`bar`) ```hcl module ""foo"" { source = ""./foo"" for_each = var.foo_things var1 = each.key var2 = each.values.something } module ""bar"" { source = ""./bar"" for_each = { for k, v in var.bar_things : k => v if v.add_bar_to_foo == true } provider ""some_provider"" { config1 = module.foo[each.values.foo_thing].out…",,,,,,Anecdotal,comment,,,,,,,,2020-09-25,github/jon-walton,https://github.com/hashicorp/terraform/issues/24476#issuecomment-698831672,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"@vivanov-dp The ideal approach is to have identical code and only data, which varies between environment and clusters within the environment. Right now, almost everything has `for_each`/`count` except providers.",,,,,,Anecdotal,comment,,,,,,,,2020-09-26,github/nikolay,https://github.com/hashicorp/terraform/issues/24476#issuecomment-699375147,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"> @jon-walton Your example is identical to mine, but I illustrated if the module needs more than a single provider. My example illustrates the provider config being supplied to a module is set by the output of another module which also uses `for_each`",,,,,,Anecdotal,comment,,,,,,,,2020-09-26,github/jon-walton,https://github.com/hashicorp/terraform/issues/24476#issuecomment-699443102,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"@nikolay Sure, having `for_each` for providers sounds logical and natural and I fully support it, I believe it deserves its own feature request",,,,,,Anecdotal,comment,,,,,,,,2020-09-26,github/vivanov-dp,https://github.com/hashicorp/terraform/issues/24476#issuecomment-699447307,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"@jon-walton Fair enough, we need dynamic providers - one way or another. Right now providers and outputs are the only two static resources in Terraform.",,,,,,Anecdotal,comment,,,,,,,,2020-09-28,github/nikolay,https://github.com/hashicorp/terraform/issues/24476#issuecomment-700131550,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"Hi all! Thanks for the interesting discussion here. It feels to me that both this issue and #9448 are covering the same underlying use-case, which I would describe as: the ability to dynamically declare and use zero or more provider configurations based on data determined at runtime. These various proposals all have in common a single underlying design constraint: unlike most other concepts in Terraform, provider configurations must be available for operations on resources that belong to them, …",,,,,,Anecdotal,comment,,,,,,,,2020-09-29,github/apparentlymart,https://github.com/hashicorp/terraform/issues/24476#issuecomment-700368878,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"@apparentlymart Having `providerconfig(aws)` is a bit limiting as you can't pass the dynamic index from a TFC variable or `terraform.tfvars.json` file. The easiest and probably quickest to implement it just to allow something like `provider.aws[var.provider_alias]` - you still have static providers, just dynamic references to them.",,,,,,Anecdotal,comment,,,,,,,,2020-09-29,github/nikolay,https://github.com/hashicorp/terraform/issues/24476#issuecomment-700413697,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"I refer to the [blog announcement](https://www.hashicorp.com/blog/terraform-0-13-brings-powerful-meta-arguments-to-modular-workflows) for TF 0.13 with this block of code: ``` variable ""project_id"" { type = string } variable ""regions"" { type = map(object({ region = string network = string subnetwork = string ip_range_pods = string ip_range_services = string })) } module ""kubernetes_cluster"" { source = ""terraform-google-modules/kubernetes-engine/google"" for_each = var.regions project_id = var.pro…",,,,,,Anecdotal,comment,,,,,,,,2020-10-12,github/cregkly,https://github.com/hashicorp/terraform/issues/24476#issuecomment-706802751,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"@cregkly This example is with Google cloud - the provider instance is not constrained within the region with Google, so you don't need multiple provider instances to use different regions - resources have 'region' properties themselves",,,,,,Anecdotal,comment,,,,,,,,2020-10-12,github/vivanov-dp,https://github.com/hashicorp/terraform/issues/24476#issuecomment-706961509,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"> @cregkly This example is with Google cloud - the provider instance is not constrained within the region with Google, so you don't need multiple provider instances to use different regions - resources have 'region' properties themselves And I quote the original post: > I'd like to be able to provision the same set of resources in multiple regions a for_each on a module. However, looping over providers (which are tied to regions) is currently not supported. And then they gave a google cloud exa…",,,,,,Anecdotal,comment,,,,,,,,2020-10-12,github/cregkly,https://github.com/hashicorp/terraform/issues/24476#issuecomment-707344139,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"> @cregkly Yes, but we're talking about providers here, not modules. Ability to pass **providers** to **modules** in for_each",,,,,,Anecdotal,comment,,,,,,,,2020-10-12,github/cregkly,https://github.com/hashicorp/terraform/issues/24476#issuecomment-707344834,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"@apparentlymart Can you guys put a better example up on the blog post about TF 13 then? It uses the example of for_each over regions with google cloud. Naturally it is the first thing I wanted to try out with in AWS, then it turns out it can't be done. At the very least link to the something that explains why this works with Google Cloud and not others like AWS. I appreciate you insights and transparency on the development to version 1.",,,,,,Anecdotal,comment,,,,,,,,2020-10-12,github/cregkly,https://github.com/hashicorp/terraform/issues/24476#issuecomment-707361678,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"I think the person who wrote that blog post was motivated to find an existing registry module with a relatively simple interface so that the module's own complexity wouldn't overwhelm the article with module-specific complexity. The point of it is just to be a generic (but working) example of what the syntax looks like for marketing purposes, not to be documentation. In general I'd suggest thinking of HashiCorp blog posts as being more ""notification that the thing exists"" than ""guide/example on…",,,,,,Anecdotal,comment,,,,,,,,2020-10-12,github/apparentlymart,https://github.com/hashicorp/terraform/issues/24476#issuecomment-707379710,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"I updated the blog post a while ago, but I am waiting for another team to push the changes live. It looks like our blogging platform was updated between the release of 0.13 and today. The replaced example is designed to signal the `for_each` feature without misleading users to believing they can copy paste code and use it as is. I apologize for the delay in getting this remediated. Update: I went back to check and the blog post has been updated.",,,,,,Anecdotal,comment,,,,,,,,2020-10-15,github/pkolyvas,https://github.com/hashicorp/terraform/issues/24476#issuecomment-708816449,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
Our use-case is the multi account setup where we deploy stuff like IAM roles for monitoring permission to all accounts and do have a centrally Grafana that does collect these data. Looks like currently there is no way to handle this without an addon like terragrunt? The following would be an example on how this could be handled if you require the provider to stay on root level. But this also requires to have the `for_each` available on `providers`. ```hcl # A list of AWS accounts that also migh…,,,,,,Anecdotal,comment,,,,,,,,2020-10-15,github/timmjd,https://github.com/hashicorp/terraform/issues/24476#issuecomment-709070083,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"We have the same use case as https://github.com/hashicorp/terraform/issues/24476#issuecomment-709070083 for AWS account bootstrap (has to iterate by each provider) ```terraform module ""account"" { for_each = local.accounts something = each.value.something providers ""aws"" { aws = aws[each.key] } } ```",,,,,,Anecdotal,comment,,,,,,,,2020-10-15,github/rjudin,https://github.com/hashicorp/terraform/issues/24476#issuecomment-709167395,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"Support Provider Differences in Modules ### Terraform Version ```shell Terraform v1.5.1 ``` ### Use Cases We have a library of common modules that are used by hundreds of deployments. We are also in a regulated environment, so change control is key, and therefore we pin all of our deployments to a specific version of Terraform and Providers. We run into a problem often where we need to expose a new property, or a new resource type, but in order to do so we need to force every deployment to upgr…",,,,,,Anecdotal,issue,,,,,,,,2023-08-10,github/NYRangers30,https://github.com/hashicorp/terraform/issues/33660,repo: hashicorp/terraform | keyword: best practice | state: open
"Hi @NYRangers30, Thanks for filing the request. I don't think this is something which is technically possible given the architecture of Terraform. Terraform requires the provider schema to decode the configuration, which means that the configuration must match the schema for each resource and having an errant `encryption` block in the body is going to fail to decode. We would also have to contend with other schema changes besides the addition of a block, like the addition and removal of block a…",,,,,,Anecdotal,comment,,,,,,,,2023-08-11,github/jbardin,https://github.com/hashicorp/terraform/issues/33660#issuecomment-1674738890,repo: hashicorp/terraform | issue: Support Provider Differences in Modules | keyword: best practice
"@jbardin - I think this could be accomplished if we could have multiple instances of a provider defined in the `required_providers` block with different local names: ## root/main.tf ```hcl terraform { required_providers { azurerm = { source = ""hashicorp/azurerm"" version = ""~> 2.0"" } azurerm-3 = { source = ""hashicorp/azurerm"" version = ""~> 3.0"" } } } provider ""azurerm"" { features {} } provider ""azurerm-3"" { features {} } ``` Then @NYRangers30 can accomplish this, partially, by following semantic…",,,,,,Anecdotal,comment,,,,,,,,2024-05-23,github/dustindortch,https://github.com/hashicorp/terraform/issues/33660#issuecomment-2127033794,repo: hashicorp/terraform | issue: Support Provider Differences in Modules | keyword: best practice
"+1 for this. I understand this would probably need to be a significant change to the way terraform config works but I'm currently looking at having to simultaneously upgrade the provider and every module across multiple projects because there's no way for modules to support different provider versions. Could there be a way to conditionally exclude blocks of code before the resources are fully parsed? So you could do something like this: ``` resource ""old_resource_name"" ""my_resource"" { count = v…",,,,,,Anecdotal,comment,,,,,,,,2025-03-06,github/donuk,https://github.com/hashicorp/terraform/issues/33660#issuecomment-2704314008,repo: hashicorp/terraform | issue: Support Provider Differences in Modules | keyword: best practice
"Accept recursive calls to templatefile() ### Terraform Version ```shell Terraform v1.6.0-dev on windows_amd64 ``` ### Use Cases We're going to use templatefile() for creating Azure Devops templates. For easier maintenance we want to be able to import templates from templates. ### Attempted Solutions main.tf ``` resource ""local_file"" ""pipeline-templater"" { content = templatefile(""pipeline.tftpl"", { vars = local.variables }) filename = ""pipeline.yaml"" } ``` pipeline.tftpl ``` ${ templatefile(""tem…",,,,,,Anecdotal,issue,,,,,,,,2023-05-27,github/Exchizz,https://github.com/hashicorp/terraform/issues/33272,repo: hashicorp/terraform | keyword: best practice | state: open
"@Exchizz, thanks for the great research in the bug description. Usually when there is a well-documented restriction in the Terraform language, it is due to considered trade-offs in the design. I will bring this up in triage but my guess is that a PR that changes this behavior would not be accepted given the intentional design decision to not allow recursion in `templatefile`.",,,,,,Anecdotal,comment,,,,,,,,2023-06-06,github/crw,https://github.com/hashicorp/terraform/issues/33272#issuecomment-1579211424,repo: hashicorp/terraform | issue: Accept recursive calls to templatefile() | keyword: best practice
"@crw Hm, alright - thanks :) What if it was an environment variable or an option to terraform that controls whether templatefile() can be called from the template ? Example: ``` TF_ALLOW_TEMPLATEFILE_FROM_TEMPLATE=true ``` Having this option to include files before running the templater would really improve the templating mechanism in terraform.",,,,,,Anecdotal,comment,,,,,,,,2023-06-06,github/Exchizz,https://github.com/hashicorp/terraform/issues/33272#issuecomment-1579399152,repo: hashicorp/terraform | issue: Accept recursive calls to templatefile() | keyword: best practice
"As someone also invested in this: would it be possible to clarify the reason for the design decision, if this is rejected? From the comment in the source code it reads to me to as _""there is a potential for edge cases entering infinite loops, therefore we disable the entire feature""_, which seems questionable. If that is the reason for the design choice, would a PR that implements recursion limits (or an explicit opt-in, as in @Exchizz's comment) be acceptable?",,,,,,Anecdotal,comment,,,,,,,,2023-06-07,github/birjj,https://github.com/hashicorp/terraform/issues/33272#issuecomment-1580235920,repo: hashicorp/terraform | issue: Accept recursive calls to templatefile() | keyword: best practice
"Yes, I brought this up in triage and the bottom-line feedback is that this issue would need more interest to be considered, particularly as it would mean revisiting a purposeful design decision. The code comment is a bit of a simplification, this was specifically engineered to be restricted to prevent more complicated design patterns (""programming with templates"", as you might see with Jinja2 or Django). Even with the opt-in, the team would then need to support the more complicated logic of tem…",,,,,,Anecdotal,comment,,,,,,,,2023-06-26,github/crw,https://github.com/hashicorp/terraform/issues/33272#issuecomment-1608051710,repo: hashicorp/terraform | issue: Accept recursive calls to templatefile() | keyword: best practice
"I would love this feature for my use case, though I understand the reasoning to protect the current design. In our case, we have a rather large startup script for one of our Google Cloud Compute instances: ``` # main.yml resource ""google_compute_instance"" ""some_compute_instance"" { name = var.app_name # ... metadata = { startup-script = templatefile(""${path.module}/startup-script.tftpl"", { var: var }) } } ``` It would be useful to extract pieces of the startup-script into their own templates for…",,,,,,Anecdotal,comment,,,,,,,,2023-07-07,github/jasonwc,https://github.com/hashicorp/terraform/issues/33272#issuecomment-1624457824,repo: hashicorp/terraform | issue: Accept recursive calls to templatefile() | keyword: best practice
"Same situation here. I'm using `cloud-init` to setup all my droplets without using any other tool. In my case I have a `boostrap.tftpl` with the basic setup that must run if there's no specific setup for the given droplet. ```yaml # droplets user_data = templatefile( fileexists(""templates/${each.value.droplet_name}.tftpl"") ? ""templates/${each.value.droplet_name}.tftpl"" : ""templates/base/boostrap.tftpl"", { username = each.value.droplet_user keys_path = ""${var.config.keys_path}/${each.value.env_s…",,,,,,Anecdotal,comment,,,,,,,,2023-09-14,github/maycon,https://github.com/hashicorp/terraform/issues/33272#issuecomment-1718665719,repo: hashicorp/terraform | issue: Accept recursive calls to templatefile() | keyword: best practice
"Using variables in terraform backend config block ### Terraform Version v0.9.0 ### Affected Resource(s) terraform backend config ### Terraform Configuration Files ```hcl variable ""azure_subscription_id"" { type = ""string"" default = ""74732435-e81f-4a43-bf68-ced435436edf"" } variable ""azure_tenant_id"" { type = ""string"" default = ""74732435-e81f-4a43-bf68-ced435436edf"" } terraform { required_version = "">= 0.9.0"" backend ""azure"" { resource_group_name = ""stuff"" storage_account_name = ""morestuff"" contai…",,,,,,Anecdotal,issue,,,,,,,,2017-03-23,github/glenjamin,https://github.com/hashicorp/terraform/issues/13022,repo: hashicorp/terraform | keyword: best practice | state: open
"I am trying to do something like this; getting the same ""configuration cannot contain interpolations"" error. While it seems like this is being worked on, I wanted to also ask if this is the right way for me to use access and secret keys? Does it have to be placed here so that I don't have to check the access and secret keys to github > terraform { backend ""s3"" { bucket = ""ops"" key = ""terraform/state/ops-com"" region = ""us-east-1"" encrypt = ""true"" access_key = ""${var.aws_access_key}"" secret_key =…",,,,,,Anecdotal,comment,,,,,,,,2017-04-07,github/darrensimio,https://github.com/hashicorp/terraform/issues/13022#issuecomment-292646014,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"I have the same problem i.e. would love to see interpolations in the backend config. Now that we have ""environments"" in terraform, I was hoping to have a single config.tf with the backend configuration and use environments for my states. The problem is that I want to assume an AWS role based on the environment I'm deploying to. I can do this in ""provider"" blocks as the provider block allows interpolations so I can assume the relevant role for the environment I'm deploying to, however if I also …",,,,,,Anecdotal,comment,,,,,,,,2017-04-10,github/antonosmond,https://github.com/hashicorp/terraform/issues/13022#issuecomment-292978779,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"I managed to get it working by using AWS profiles instead of the access keys directly. What I did though was not optimal; but in my build steps, I ran a bash script that called AWS configure that ultimately set the default access key and secret.",,,,,,Anecdotal,comment,,,,,,,,2017-04-11,github/darrensimio,https://github.com/hashicorp/terraform/issues/13022#issuecomment-293172268,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"We want to archive something similar than @antonosmond. At the moment we use multiple environments prod/stage and want to upload tfstate files to S3. ``` ## State Backend terraform { backend ""s3"" { bucket = ""mybucket"" key = ""aws/${var.project}/${var.environment}"" region = ""eu-central-1"" profile = ""default"" encrypt = ""true"" lock_table = ""terraform"" } } ``` In this case with above backend definition leads us to this Error: * terraform.backend: configuration cannot contain interpolations Now if we…",,,,,,Anecdotal,comment,,,,,,,,2017-04-11,github/wasfree,https://github.com/hashicorp/terraform/issues/13022#issuecomment-293272099,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"Hi, I'm trying to the the same as @NickMetz, I'm running terraform 0.9.3 ``` $terraform version Terraform v0.9.3 This is my code terraform { backend ""s3"" { bucket = ""tstbckt27"" key = ""/${var.env}/t1/terraform.tfstate"" region = ""us-east-1"" } } ``` This is the message when I try to run terraform init ``` $ terraform init Initializing the backend... Error loading backend config: 1 error(s) occurred: * terraform.backend: configuration cannot contain interpolations The backend configuration is loade…",,,,,,Anecdotal,comment,,,,,,,,2017-04-14,github/gsirvas,https://github.com/hashicorp/terraform/issues/13022#issuecomment-294249184,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"In case it's helpful to anyone, the way I get around this is as follows: ``` terraform { backend ""s3"" {} } data ""terraform_remote_state"" ""state"" { backend = ""s3"" config { bucket = ""${var.tf_state_bucket}"" lock_table = ""${var.tf_state_table}"" region = ""${var.region}"" key = ""${var.application}/${var.environment}"" } } ``` All of the relevant variables are exported at the deployment pipeline level for me, so it's easy to init with the correct information for each environment. ``` terraform init \ -…",,,,,,Anecdotal,comment,,,,,,,,2017-04-15,github/umeat,https://github.com/hashicorp/terraform/issues/13022#issuecomment-294262392,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"@gsirvas @umeat To archive multiple environment with the same backend configuration it is not necessary to use variables/interpolation .It is expected that is not possible to use variables/interpolation in backend configuration see comment from @christofferh. Just write it like this: ``` terraform { backend ""s3"" { bucket = ""tstbckt27"" key = ""project/terraform/terraform.tfstate"" region = ""us-east-1"" } } ``` Terraform will split and store environment state files in a path like this: `env:/${var.e…",,,,,,Anecdotal,comment,,,,,,,,2017-04-15,github/wasfree,https://github.com/hashicorp/terraform/issues/13022#issuecomment-294276156,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"@NickMetz it's trying to do multiple environments with multiple backend buckets, not a single backend. You can't specify a different backend bucket in terraform environments. In my example you could still use terraform environments to prefix the state file object name, but you get to specify different buckets for the backend. Perhaps it's better to just give accross account access to the user / role which is being used to deploy your terraform. Deploying your terraform to a different account, b…",,,,,,Anecdotal,comment,,,,,,,,2017-04-15,github/umeat,https://github.com/hashicorp/terraform/issues/13022#issuecomment-294277604,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"@umeat in that case you are right, it is not possible at the moment to use different backends for each environment. It would be more comfortable to have a backend mapping for all environments what is not implemented yet.",,,,,,Anecdotal,comment,,,,,,,,2017-04-15,github/wasfree,https://github.com/hashicorp/terraform/issues/13022#issuecomment-294279339,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
Perhaps a middle ground would be to not error out on interpolation when the variable was declared in the environment as `TF_VAR_foo`? Though this might require making such variables immutable? (Which is fine for my use case; not sure about others.),,,,,,Anecdotal,comment,,,,,,,,2017-04-26,github/joestump,https://github.com/hashicorp/terraform/issues/13022#issuecomment-297482726,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"I also would like to be able to use interpolation in my backend config, using v 0.9.4, confirming this frustrating point still exists. In my use case i need to reuse the same piece of code (without writing a new repo each time i'd want to consume it as a module) to maintain multiple separate statefiles.",,,,,,Anecdotal,comment,,,,,,,,2017-04-27,github/knope,https://github.com/hashicorp/terraform/issues/13022#issuecomment-297770627,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"Same thing for me. I am using Terraform v0.9.4. ``` provider ""aws"" { region = ""${var.region}"" } terraform { backend ""${var.tf_state_backend}"" { bucket = ""${var.tf_state_backend_bucket}"" key = ""${var.tf_state_backend_bucket}/terraform.tfstate"" region = ""${var.s3_location_region}"" } } ``` Here is the error Output of `terraform validate`: ``` Error validating: 1 error(s) occurred: * terraform.backend: configuration cannot contain interpolations The backend configuration is loaded by Terraform extr…",,,,,,Anecdotal,comment,,,,,,,,2017-05-10,github/nkhanal0,https://github.com/hashicorp/terraform/issues/13022#issuecomment-300569794,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"I needs dis! For many features being developed, we want our devs to spin up their own infrastructure that will persist only for the length of time their feature branch exists... to me, the best way to do that would be to use the name of the branch to create the key for the path used to store the tfstate (we're using amazon infrastructure, so in our case, the s3 bucket like the examples above). I've knocked up a bash script which will update TF_VAR_git_branch every time a new command is run from…",,,,,,Anecdotal,comment,,,,,,,,2017-06-05,github/kilna-magento,https://github.com/hashicorp/terraform/issues/13022#issuecomment-306224881,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
@NickMetz said... > Terraform will split and store environment state files in a path like this: > ```env:/${var.env}/project/terraform/terraform.tfstate``` Your top-level structure looks nice and tidy for traditional dev/staging/prod ... sure: ``` env:/prod/project1/terraform/terraform.tfstate env:/prod/project2/terraform/terraform.tfstate env:/staging/project1/terraform/terraform.tfstate env:/staging/project2/terraform/terraform.tfstate env:/dev/project1/terraform/terraform.tfstate env:/dev/pr…,,,,,,Anecdotal,comment,,,,,,,,2017-06-05,github/kilna-magento,https://github.com/hashicorp/terraform/issues/13022#issuecomment-306236904,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"In Terraform 0.10 there will be a new setting `workspace_key_prefix` on the AWS provider to customize the prefix used for separate environments (now called ""workspaces""), overriding this `env:` convention.",,,,,,Anecdotal,comment,,,,,,,,2017-07-10,github/apparentlymart,https://github.com/hashicorp/terraform/issues/13022#issuecomment-314282559,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
I was hoping to do the same thing as described in #13603 but the lack of interpolation in the terraform block prevents this.,,,,,,Anecdotal,comment,,,,,,,,2017-10-24,github/mhowell-ims,https://github.com/hashicorp/terraform/issues/13022#issuecomment-339068437,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"I think this would also be useful for https://github.com/hashicorp/terraform/issues/18632 Specifically, following the structure: ``` environments/ |-- dev/ # dev configuration |-- dev.tf |-- secret.auto.tfvars |-- prod/ # prod configuration |-- prod.tf |-- secret.auto.tfvars resources/ # shared module for elements common to all environments |-- main.tf ``` If i have a `secret.auto.tfvars` file in both dev and prod with different credentials they don't actually get used for the init and my `~/.a…",,,,,,Anecdotal,comment,,,,,,,,2018-08-26,github/davidgoate,https://github.com/hashicorp/terraform/issues/13022#issuecomment-416046973,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"Same issue with etcd: ``` Error loading backend config: 1 error(s) occurred: * terraform.backend: configuration cannot contain interpolations The backend configuration is loaded by Terraform extremely early, before the core of Terraform can be initialized. This is necessary because the backend dictates the behavior of that core. The core is what handles interpolation Initializing the backend... processing. Because of this, interpolations cannot be used in backend configuration. If you'd like to…",,,,,,Anecdotal,comment,,,,,,,,2018-10-12,github/iahmad-khan,https://github.com/hashicorp/terraform/issues/13022#issuecomment-429271016,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
Facing the same issue even for version 0.11.10 **terraform.backend: configuration cannot contain interpolations** It doesn't seem a good option to specify creds twice once in variables and again in config. Can we get any update on this as this is open from almost a year.,,,,,,Anecdotal,comment,,,,,,,,2018-11-21,github/ElizabethAnthony94,https://github.com/hashicorp/terraform/issues/13022#issuecomment-440622398,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
I used workspaces to create my dev and prod environments. Now I need to store state for them in different aws accounts. What kind of workaround do you recommend? I just need to pass one variable to my backend config... somehow...,,,,,,Anecdotal,comment,,,,,,,,2018-11-28,github/laur1s,https://github.com/hashicorp/terraform/issues/13022#issuecomment-442489869,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"The best workaround for this I've ended up with takes advantage of the fact that whatever you pass into `init` gets rememebered by terraform. So instead of `terraform init`, I use a small wrapper script which grabs these variables from somewhere (like a .tf.json file used elsewhere, or an environment variable perhaps) and then does the call to `init` along with the correct `-backend-config` flags.",,,,,,Anecdotal,comment,,,,,,,,2018-11-28,github/glenjamin,https://github.com/hashicorp/terraform/issues/13022#issuecomment-442499201,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"As @glenjamin said, while interpolation from **terraform variables** isn't possible, this is supported ""outside"" of the terraform configuration proper by using [partial configuration of a backend](https://www.terraform.io/docs/backends/config.html#partial-configuration) and arguments to ``terraform init``. As an example, you can have a configuration containing ``` terraform { backend ""s3"" {} } ``` and providing your configuration as command-line flags to init: ``` terraform init \ -backend-conf…",,,,,,Anecdotal,comment,,,,,,,,2018-12-11,github/jantman,https://github.com/hashicorp/terraform/issues/13022#issuecomment-446309503,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"@jantman I understand your comment and this is what we do on our side, we wrap execution of terraform with another layer of scripts. *But* it would be nicer if this could work OOTB so that we provide only environment vars and then execute`init`, `plan` etc. without thinking about the partial backend setup parameters from CLI. Thus, I still think that there is place for a nicer framework solution, although a workaround exists.",,,,,,Anecdotal,comment,,,,,,,,2018-12-11,github/milanaleksic,https://github.com/hashicorp/terraform/issues/13022#issuecomment-446316890,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"Give better feedback when two ""moved"" blocks target the same resource instance address ### Terraform Version ```shell Terraform v1.5.5 on windows_amd64 ``` ### Terraform Configuration Files [code to reproduce issue](https://github.com/KenSpur/terraform-ambiguous-move-statements) ### Debug Output Should be easy to recreate using the provided code . │ Error: Ambiguous move statements │ │ on module\main.tf line 23: │ 23: moved { │ │ A statement at main.tf:19,1 declared that azurerm_resource_group.…",,,,,,Anecdotal,issue,,,,,,,,2023-08-12,github/KenSpur,https://github.com/hashicorp/terraform/issues/33671,repo: hashicorp/terraform | keyword: best practice | state: open
"Hi @KenSpur! Thanks for reporting this. Based on your code example, I think Terraform is working as intended here. Terraform requires an unambiguous graph of moves so that the validity of the move statements can be decided entirely by the configuration, and conversely so that it isn't possible to get yourself into a trap where you can't proceed because the state contains two resource instances that have apparently moved to the same address. You can get a similar effect to what you described her…",,,,,,Anecdotal,comment,,,,,,,,2023-08-14,github/apparentlymart,https://github.com/hashicorp/terraform/issues/33671#issuecomment-1677626443,"repo: hashicorp/terraform | issue: Give better feedback when two ""moved"" blocks target the same resource instance address | keyword: best practice"
"Hey @apparentlymart, Thank you for the detailed response. I appreciate the insight into Terraform's design considerations regarding the moved statement. From the scenarios you outlined, I gather that chaining is an integral part of managing the state transitions effectively. With this in mind, would it be accurate to infer that the best practice, especially when refactoring or working with modules, is to always bind to the initial from address in a move chain?",,,,,,Anecdotal,comment,,,,,,,,2023-08-14,github/KenSpur,https://github.com/hashicorp/terraform/issues/33671#issuecomment-1677683830,"repo: hashicorp/terraform | issue: Give better feedback when two ""moved"" blocks target the same resource instance address | keyword: best practice"
"In your particular situation there is only one possible valid chain, because the root module can declare that an object has moved into its child but the child cannot declare that the object has moved from its parent. If this were all happening in the _same_ module then you as the author would need to make a decision about which object should ""win"" in the edge case where there are multiple objects bound to addresses in the chain. For example: ```hcl moved { from = null_resource.a to = null_resou…",,,,,,Anecdotal,comment,,,,,,,,2023-08-14,github/apparentlymart,https://github.com/hashicorp/terraform/issues/33671#issuecomment-1677710997,"repo: hashicorp/terraform | issue: Give better feedback when two ""moved"" blocks target the same resource instance address | keyword: best practice"
"@apparentlymart Thank you for the detailed explanation. Based on your guidance, I've updated my code using the null_resource to create a working example. I appreciate your assistance in clarifying this for me!",,,,,,Anecdotal,comment,,,,,,,,2023-08-14,github/KenSpur,https://github.com/hashicorp/terraform/issues/33671#issuecomment-1677793871,"repo: hashicorp/terraform | issue: Give better feedback when two ""moved"" blocks target the same resource instance address | keyword: best practice"
"Don't make redundant copies of the same module package ### Current Terraform Version ``` 1.0.5 ``` ### Use-cases We have a Github repository of modules with each module in its own subfolder. Module usage in a project looks something like this ``` module ""alb"" { source = ""git@github.com:org/repo.git//modules/alb_module?ref=1.52.0"" name = var.name subdomain = local.subdomain idle_timeout = local.timeout environment = module.environment } ``` A project can reference quite a few of such modules fro…",,,,,,Anecdotal,issue,,,,,,,,2021-09-01,github/YuriGal,https://github.com/hashicorp/terraform/issues/29503,repo: hashicorp/terraform | keyword: best practice | state: open
"Hi @YuriGal! Thanks for sharing this use-case. The important design consideration here is that existing modules unfortunately expect to be able to create new files inside `path.module`, and thus each module must have its own separate directory so that multiple modules with the same source address won't conflict with one another. We don't yet have a design that can address that need in a backward-compatible way, and thus we have so far retained the original design of having each `module` block g…",,,,,,Anecdotal,comment,,,,,,,,2021-09-01,github/apparentlymart,https://github.com/hashicorp/terraform/issues/29503#issuecomment-910850869,repo: hashicorp/terraform | issue: Don't make redundant copies of the same module package | keyword: best practice
"@apparentlymart What about creating the folder and instead of copying the files, on those OS+FS that allow it we just produce symlinks? Would that work ?? TBH, I'm not sure what kind of size we are talking about .. but AFAIK, if u follow best practices for both Terraform Modules & Git repositories..u shouldn't be on scenarios where the storage can become a problem?",,,,,,Anecdotal,comment,,,,,,,,2021-09-05,github/fblgit,https://github.com/hashicorp/terraform/issues/29503#issuecomment-913185996,repo: hashicorp/terraform | issue: Don't make redundant copies of the same module package | keyword: best practice
"@apparentlymart regarding what I put in https://github.com/hashicorp/terraform/issues/31422, can you please elaborate on the concerns about compatibility? I'm unclear on how having a `.terraform/sources` directory that the `.terraform/modules/modules.json` points to would run into compatibility concerns when it seems to be basically the same runtime behavior as how Terraform handles local sources where you don't have a separate copy of the module source in the `.terraform/modules/<name>` direct…",,,,,,Anecdotal,comment,,,,,,,,2022-07-12,github/joe-a-t,https://github.com/hashicorp/terraform/issues/29503#issuecomment-1182544588,repo: hashicorp/terraform | issue: Don't make redundant copies of the same module package | keyword: best practice
"To be concrete about it, it is currently possible to write something like the following in a Terraform module and have it work, and so we are bound to keep this working even though I would assert that it's not a good idea to use the local filesystem for transient storage during a Terraform operation: ```hcl variable ""content"" { type = string } resource ""local_file"" ""example"" { content = var.content filename = ""${path.module}/temporary.txt"" } data ""local_file"" ""example"" { filename = local_file.e…",,,,,,Anecdotal,comment,,,,,,,,2022-07-13,github/apparentlymart,https://github.com/hashicorp/terraform/issues/29503#issuecomment-1183656548,repo: hashicorp/terraform | issue: Don't make redundant copies of the same module package | keyword: best practice
"Ok, so sticking with the backwards compatibility on that, could we either: 1. Leave the `.terraform/modules/...` as a place where any files that Terraform writes can get written but the source is not download/copied into. Instead Terraform looks to see if a `.terraform/modules/...` exists for that module, and if so, read in both the contents of that `.terraform/modules/...` directory and the contents from the relevant `.terraform/sources/...` directory (as specified in a new part of the `manife…",,,,,,Anecdotal,comment,,,,,,,,2022-07-13,github/joe-a-t,https://github.com/hashicorp/terraform/issues/29503#issuecomment-1183692706,repo: hashicorp/terraform | issue: Don't make redundant copies of the same module package | keyword: best practice
"This (more reasonable, more common) pattern must also keep working: ```hcl resource ""aws_s3_bucket_object"" ""example"" { bucket = ""example"" key = ""anything"" source = ""${path.module}/bucket-content/anything"" } ``` In this case, `bucket-content/anything` is a file distributed as part of the module package (e.g. it's included in the git repository along with this `.tf` file). The key design challenge here is that `path.module` today represents both ""the directory where my source files are"" _and_ ""a …",,,,,,Anecdotal,comment,,,,,,,,2022-07-13,github/apparentlymart,https://github.com/hashicorp/terraform/issues/29503#issuecomment-1183716662,repo: hashicorp/terraform | issue: Don't make redundant copies of the same module package | keyword: best practice
"So what I was thinking of for reads is that when Terraform sees `path.module`, it looks in `.terraform/modules/...`. If it does not find the desired directory/file, it then goes to the `.terraform/sources/...` directory and tries looking for it from there. So basically changing the current read from a ""Try A, if not, then fail"" to a ""Try A, if not, Try B, if not, then fail"".",,,,,,Anecdotal,comment,,,,,,,,2022-07-13,github/joe-a-t,https://github.com/hashicorp/terraform/issues/29503#issuecomment-1183724399,repo: hashicorp/terraform | issue: Don't make redundant copies of the same module package | keyword: best practice
"`path.module` is just a static string that gets interpolated into the path like any other, so Terraform must decide what `path.module` evaluates to once for the entire module, not separately for each use. For example, consider that in my second case which contains an argument `source = ""${path.module}/bucket-content/anything""` it is the AWS provider rather than Terraform Core that is actually reading the file, and indeed only the AWS provider _knows_ that it's treating that string as a filename…",,,,,,Anecdotal,comment,,,,,,,,2022-07-13,github/apparentlymart,https://github.com/hashicorp/terraform/issues/29503#issuecomment-1183728915,repo: hashicorp/terraform | issue: Don't make redundant copies of the same module package | keyword: best practice
"Ahhhh, I did not realize exactly where the boundary was there between Terraform Core and the providers. I did not realize that providers directly interact with the file system without going through Terraform Core. I thought maybe Terraform Core could handle the retry logic and implement it once for all providers, but it sounds like you're saying that all providers would need to implement the retry logic themselves (unless if maybe Terraform Core was intercepting those filesystem calls and able …",,,,,,Anecdotal,comment,,,,,,,,2022-07-13,github/joe-a-t,https://github.com/hashicorp/terraform/issues/29503#issuecomment-1183733983,repo: hashicorp/terraform | issue: Don't make redundant copies of the same module package | keyword: best practice
"Indeed... the only situation where Terraform Core is directly interacting with the filesystem is in function calls like `file(""${path.module}/foo.txt"")` and even in that case the interpolation of `path.module` as a fixed string happens before calling the function and so the function only sees the resulting path string, can't cannot tell that it was constructed from `path.module`.",,,,,,Anecdotal,comment,,,,,,,,2022-07-13,github/apparentlymart,https://github.com/hashicorp/terraform/issues/29503#issuecomment-1183776144,repo: hashicorp/terraform | issue: Don't make redundant copies of the same module package | keyword: best practice
"Just to clarify a bit further, the racing herd problem when writing files to `path.module` is an **existing** issue when/if people are using local paths for module sources. So anyone that is using `path.module` as part of write operations will likely see different behavior if they are doing so with local or remote source and using the module multiple times. I also want to note that even if people only have a single module block, but they have a `for_each` inside that block, they will also have …",,,,,,Anecdotal,comment,,,,,,,,2022-07-14,github/joe-a-t,https://github.com/hashicorp/terraform/issues/29503#issuecomment-1184643745,repo: hashicorp/terraform | issue: Don't make redundant copies of the same module package | keyword: best practice
Also I opened https://github.com/hashicorp/terraform/issues/31441 as a documentation issue to add the risks with `path.module` that you've called out in this conversation to the documentation for `path.module`.,,,,,,Anecdotal,comment,,,,,,,,2022-07-14,github/joe-a-t,https://github.com/hashicorp/terraform/issues/29503#issuecomment-1184663554,repo: hashicorp/terraform | issue: Don't make redundant copies of the same module package | keyword: best practice
"Unfortunately we know that the existing uses are prevalent enough that we cannot use the lack of documentation as justification for the breaking change. The following language in our compatibility promises summarizes this compromise: > If the actual behavior of a feature differs from what we explicitly documented as the feature's behavior, we will usually treat that as a bug and change the feature to match the documentation, although we will avoid making such changes if they seem likely to caus…",,,,,,Anecdotal,comment,,,,,,,,2022-07-14,github/apparentlymart,https://github.com/hashicorp/terraform/issues/29503#issuecomment-1185033156,repo: hashicorp/terraform | issue: Don't make redundant copies of the same module package | keyword: best practice
"Understood. Hopefully this refactor can make the list for `v2.0.0` and I'm not sure what the timeline is for that, but hopefully it can come relatively soon (with relatively meaning before 2024). In the meantime we will probably have to do our own workarounds like I described as possibilities in https://github.com/hashicorp/terraform/issues/31422",,,,,,Anecdotal,comment,,,,,,,,2022-07-15,github/joe-a-t,https://github.com/hashicorp/terraform/issues/29503#issuecomment-1185762615,repo: hashicorp/terraform | issue: Don't make redundant copies of the same module package | keyword: best practice
"@apparentlymart Is it possible to allow the user calling the module to specify the behaviour (symlink or copy) in the module block. e.g. ``` module ""foo"" { source = ""bar"" download_method = ""copy"" # or linked } ``` This way its in the users hands whether to incur a copy at extra disk space, or linked with the disadvantages with files above. In my case I am invoking the same module for every AWS account for every AWS region - thats about 800 invocations. It is adding up to quite a lot of disk spa…",,,,,,Anecdotal,comment,,,,,,,,2022-11-07,github/gtmtech,https://github.com/hashicorp/terraform/issues/29503#issuecomment-1306062900,repo: hashicorp/terraform | issue: Don't make redundant copies of the same module package | keyword: best practice
"It seems to me that whether it's safe/correct to reuse the same directory for multiple instances of the same module is a property of the module itself rather than a property of the call to that module, and so if we were going to make this an opt-in sort of thing I expect it would be better for that opt-in to be inside the module being called rather than in the `module` block that calls the module. It would essentially be a promise from the module developer that planning and applying the module …",,,,,,Anecdotal,comment,,,,,,,,2022-11-07,github/apparentlymart,https://github.com/hashicorp/terraform/issues/29503#issuecomment-1306084549,repo: hashicorp/terraform | issue: Don't make redundant copies of the same module package | keyword: best practice
"I'm using my own modules and I know what's inside. I'm pretty sure there's no local file write and it's safe to re-use same module directory. But, Terraform makes 100 copies of same module. If this happens, `terraform plan` takes intolerable amount of time in case of terraform cloud backend. I really hope there's an option like https://github.com/hashicorp/terraform/issues/29503#issuecomment-1306062900",,,,,,Anecdotal,comment,,,,,,,,2024-10-19,github/sato-s,https://github.com/hashicorp/terraform/issues/29503#issuecomment-2423857402,repo: hashicorp/terraform | issue: Don't make redundant copies of the same module package | keyword: best practice
"Terraform performance with large number of resources Hi Team, I have the following configuration: * one VPC * 6 different networks inside the VPC * 2 nat gw * 36 opsworks stacks * 300 or so security groups * 1200 or opswork layers In total about 1600 resources in my configs. Creation of this whole config in aws took about 30 min - most of the time could probably be attributed to waiting and connection handling Plan with a simple change (e.g. adding 6 new layers) takes around 8 min. Probably mos…",,,,,,Anecdotal,issue,,,,,,,,2017-10-17,github/zeridon,https://github.com/hashicorp/terraform/issues/16375,repo: hashicorp/terraform | keyword: best practice | state: open
"Hi @zeridon, Thanks for the use case here. Could you provide some more details about your configuration, like how many ""provider"" blocks you have in your config, are you using modules, and what is the layout, etc? These are known issues, and optimization with large configurations is something we do want to tackle. 2Gb of memory usage is not unexpected, especially with the AWS provider in which the sdk has fairly hefty client instances for each service. The time after retrieving the state where …",,,,,,Anecdotal,comment,,,,,,,,2017-10-19,github/jbardin,https://github.com/hashicorp/terraform/issues/16375#issuecomment-337778322,repo: hashicorp/terraform | issue: Terraform performance with large number of resources | keyword: best practice
"Hi @jbardin , Sanitized project can be found at: https://gist.github.com/zeridon/9cd3fa1c85cbc2d72c4914b58c7b03cf Short answers to your main questions: * How many provider blocks - 1 just in main.tf to set aws region * Modules - yes * layout - https://gist.github.com/zeridon/9cd3fa1c85cbc2d72c4914b58c7b03cf#file-dir-tree Dependency tree is: ``` main.tf |- aws-vpc |- sg-infra |- eip-shortcuts |- eip |- stacks |- stack customer-xxxx.tf |- customer-layer (sg, 1 or more opsworks layers in different…",,,,,,Anecdotal,comment,,,,,,,,2017-10-19,github/zeridon,https://github.com/hashicorp/terraform/issues/16375#issuecomment-337831241,repo: hashicorp/terraform | issue: Terraform performance with large number of resources | keyword: best practice
@jbardin do you have any suggestions on how we can improve the runtimes of our Terraform plans? We also have a large number of resources and it seems to takes several minutes just to refresh state. A TF plan takes ~15 minutes to run on Terraform Enterprise.,,,,,,Anecdotal,comment,,,,,,,,2018-10-17,github/ktham,https://github.com/hashicorp/terraform/issues/16375#issuecomment-430806603,repo: hashicorp/terraform | issue: Terraform performance with large number of resources | keyword: best practice
"@ktham, Several minutes to refresh a large number of resources is not unexpected, the api calls to check the state of each resource can only happen so quickly. One option you have to to try carefully increasing the `-parallelism` value to run more api calls at a time.",,,,,,Anecdotal,comment,,,,,,,,2018-10-26,github/jbardin,https://github.com/hashicorp/terraform/issues/16375#issuecomment-433527119,repo: hashicorp/terraform | issue: Terraform performance with large number of resources | keyword: best practice
"That's understandable, though I would expect TF should be able to issue and handle quite a lot of API calls at once even on a single core machine. Ok, I'll look into the `-parallelism` value, though I'm not sure if I can set that because we use Terraform Enterprise",,,,,,Anecdotal,comment,,,,,,,,2018-10-26,github/ktham,https://github.com/hashicorp/terraform/issues/16375#issuecomment-433558460,repo: hashicorp/terraform | issue: Terraform performance with large number of resources | keyword: best practice
"I ran some benchmarks: # Config * 61 resources * Using VPN connection that is decent * 2017 MacBook Pro 13"" * Using --auto-approve # TF Destroy * Command: `terraform destroy --auto-approve --parallelism 10 terraform/` * Default: 45s * Parallelism 10: 53s # TF Apply * Command: `terraform destroy --auto-approve --parallelism 10 terraform/` * Default: 1m43s * Parallelism 10: 1m46s I guess my project doesn't benefit from parallelism? Is there any way to show a nice dependency graph? I found `terraf…",,,,,,Anecdotal,comment,,,,,,,,2018-11-07,github/Clete2,https://github.com/hashicorp/terraform/issues/16375#issuecomment-436608553,repo: hashicorp/terraform | issue: Terraform performance with large number of resources | keyword: best practice
"Hi @Clete2, 10 is the default parallelism, so you were only seeing variance within the api calls themselves. The `graph` command is really the only way to show the actual dependency graph, but I also have yet to find a good method for viewing large graphs in general.",,,,,,Anecdotal,comment,,,,,,,,2018-11-07,github/jbardin,https://github.com/hashicorp/terraform/issues/16375#issuecomment-436695989,repo: hashicorp/terraform | issue: Terraform performance with large number of resources | keyword: best practice
"Judging by another thread, the parallelism really depends on the number of cores in a system, which is kind of ridiculous for a program that spends most of the time waiting for API calls: https://github.com/hashicorp/terraform/issues/11766",,,,,,Anecdotal,comment,,,,,,,,2019-01-04,github/nielsole,https://github.com/hashicorp/terraform/issues/16375#issuecomment-451399105,repo: hashicorp/terraform | issue: Terraform performance with large number of resources | keyword: best practice
"@nielsole, No, the number of concurrent calls is not effected by the number of cores in the system. It's only artificially limited by the `-parallelism` value because there is no universally ""good"" value for all providers and configurations. Note that this is all unrelated to the original issue here, which is the efficiency of large graph graph transformations, which has nothing to do with concurrent provider operations.",,,,,,Anecdotal,comment,,,,,,,,2019-01-04,github/jbardin,https://github.com/hashicorp/terraform/issues/16375#issuecomment-451498618,repo: hashicorp/terraform | issue: Terraform performance with large number of resources | keyword: best practice
Terraform seems to spend a lot of its time computing these dependency graphs - I am curious why a dependency graph of just a few hundred nodes should take several minutes to compute. Is there are whitepaper or discussion I can read somewhere?,,,,,,Anecdotal,comment,,,,,,,,2019-04-18,github/ToonSpinISAAC,https://github.com/hashicorp/terraform/issues/16375#issuecomment-484415271,repo: hashicorp/terraform | issue: Terraform performance with large number of resources | keyword: best practice
"Hi @ToonSpinISAAC, The issues in this thread are mainly concerned with the parallelization of the api calls to resources, which is capped at 10 by default. Very large numbers of resources can take a long time to process when there are a large number of slow API calls. A graph of a few hundred nodes should not take minutes to compute. The primary graph related performance issue is in highly connected graphs, and can be referenced in issue #18981.",,,,,,Anecdotal,comment,,,,,,,,2019-04-22,github/jbardin,https://github.com/hashicorp/terraform/issues/16375#issuecomment-485408311,repo: hashicorp/terraform | issue: Terraform performance with large number of resources | keyword: best practice
"I suspect a lot more people have ran into this issue than just the 10 people who have +1'd it. When Terraform plans got too slow I broke our terraform codebase up into multiple separate states with the help of Terragrunt. I suspect other people have gone down the same path, either with Terragrunt or separate git repositories. This has helped, but even with multiple states we still run into performance issues when one state grows large.",,,,,,Anecdotal,comment,,,,,,,,2024-07-26,github/calebAtIspot,https://github.com/hashicorp/terraform/issues/16375#issuecomment-2251669660,repo: hashicorp/terraform | issue: Terraform performance with large number of resources | keyword: best practice
We realized our code base was too big. For some projects we broke it up. For others we used more tf variables and used multiple state files to deploy different parts. Generally I try to keep under 200 resources per deployment.,,,,,,Anecdotal,comment,,,,,,,,2024-07-26,github/Clete2,https://github.com/hashicorp/terraform/issues/16375#issuecomment-2251855914,repo: hashicorp/terraform | issue: Terraform performance with large number of resources | keyword: best practice
"> We realized our code base was too big. This is shorthand for ""too big for all currently available releases of terraform to handle"". Name another language that has superlinear slow down when there are ""too many"" objects. This should be fixed, regardless of whether its workarounds have become best practice in the meantime.",,,,,,Anecdotal,comment,,,,,,,,2024-07-31,github/bukzor,https://github.com/hashicorp/terraform/issues/16375#issuecomment-2260338115,repo: hashicorp/terraform | issue: Terraform performance with large number of resources | keyword: best practice
"Support provisioning using `docker exec` Instead of requiring an ssh connection to run a provisioner on a docker container, it would be nice to just do a `docker exec` so that we don't need to set up an ssh daemon on the container. (I know I know, I'm not supposed to run a provisioner on a docker image, configuration should be done at build time. But I'm trying to mirror my prod installation which isn't docker)",,,,,,Anecdotal,issue,,,,,,,,2016-01-15,github/clofresh,https://github.com/hashicorp/terraform/issues/4686,repo: hashicorp/terraform | keyword: best practice | state: open
"Would a hypothetical new `docker-exec` provisioner suit your use-case? ``` js resource ""docker_container"" ""foo"" { // ... provisioner ""docker-exec"" { inline = [ ""echo hello world"" ] } } ```",,,,,,Anecdotal,comment,,,,,,,,2016-01-17,github/apparentlymart,https://github.com/hashicorp/terraform/issues/4686#issuecomment-172281921,repo: hashicorp/terraform | issue: Support provisioning using `docker exec` | keyword: best practice
I think it would be a terrific feature! But wouldn't an hypothetical new `docker connection type` a better solution as it would be used in both `remote-exec` and `file` provisioners allowing to upload files to the container as well. I had a look to the docker provider code and it uses https://github.com/fsouza/go-dockerclient as docker client. This library seems to support both [file uploads](https://github.com/fsouza/go-dockerclient/blob/ab2ce4ff546ce97e9597be3b996535bf1683da3c/container.go#L9…,,,,,,Anecdotal,comment,,,,,,,,2016-03-16,github/loicalbertin,https://github.com/hashicorp/terraform/issues/4686#issuecomment-197404457,repo: hashicorp/terraform | issue: Support provisioning using `docker exec` | keyword: best practice
"HashiCorp guys working on terraform (@phinze, @mitchellh, @catsby, @jen20, ...) what do you think about this idea of a new docker connection type ? Thanks in advance for your feedback. Loïc",,,,,,Anecdotal,comment,,,,,,,,2016-03-24,github/loicalbertin,https://github.com/hashicorp/terraform/issues/4686#issuecomment-200797439,repo: hashicorp/terraform | issue: Support provisioning using `docker exec` | keyword: best practice
@jen20 I'm interested in contributing such feature. But I'd like to discuss it a little bit and specially check if the docker connection is actually the good way to implement this.,,,,,,Anecdotal,comment,,,,,,,,2016-03-31,github/loicalbertin,https://github.com/hashicorp/terraform/issues/4686#issuecomment-204122568,repo: hashicorp/terraform | issue: Support provisioning using `docker exec` | keyword: best practice
What happened to this? It's just easier to use docker machine and docker swarm? No need for HashiCorp. Hmmm? Get your fingers out.,,,,,,Anecdotal,comment,,,,,,,,2019-04-13,github/richard-senior,https://github.com/hashicorp/terraform/issues/4686#issuecomment-482819928,repo: hashicorp/terraform | issue: Support provisioning using `docker exec` | keyword: best practice
"> Would a hypothetical new `docker-exec` provisioner suit your use-case? > > ```js > resource ""docker_container"" ""foo"" { > // ... > > provisioner ""docker-exec"" { > inline = [ > ""echo hello world"" > ] > } > } > ``` Is this going to be implemented? It's a great idea.",,,,,,Anecdotal,comment,,,,,,,,2020-09-23,github/mesea-mms,https://github.com/hashicorp/terraform/issues/4686#issuecomment-697247092,repo: hashicorp/terraform | issue: Support provisioning using `docker exec` | keyword: best practice
:+1: for a docker connection type. Either on existing containers or being able to create new containers based off of on image would be appreciated. Same with kubernetes pods.,,,,,,Anecdotal,comment,,,,,,,,2024-06-21,github/mjsir911,https://github.com/hashicorp/terraform/issues/4686#issuecomment-2183078096,repo: hashicorp/terraform | issue: Support provisioning using `docker exec` | keyword: best practice
"The concept of provisioners has since emerged as largely a mistake: they don't really do anything that a managed resource type can't do and Terraform can't track them well because they are not stateful, so Terraform ends up having to make worst-case assumptions like that the failure of any provisioner means that the entire resource object is damaged (""tainted"") and therefore needs replacing. The current provisioners remain largely for backward compatibility and because they have only minimal de…",,,,,,Anecdotal,comment,,,,,,,,2024-06-24,github/apparentlymart,https://github.com/hashicorp/terraform/issues/4686#issuecomment-2186918321,repo: hashicorp/terraform | issue: Support provisioning using `docker exec` | keyword: best practice
Docker/Kubernetes/Linux Containers and many hypervisors support APIs to manipulate files and execute commands inside container/VM. I think the best way is to extend provisioner `connection` block with addition connection types. I think provider can export supported connection type for provisioner and provide connectivity to the container. For example: - Docker provider could export connection type `docker` - Kubernetes provider could export connection type `kube` - LXD provider could export con…,,,,,,Anecdotal,comment,,,,,,,,2024-06-25,github/tregubovav-dev,https://github.com/hashicorp/terraform/issues/4686#issuecomment-2190126569,repo: hashicorp/terraform | issue: Support provisioning using `docker exec` | keyword: best practice
"The ""communicator"" abstraction (which is what `connection` blocks are configuring) is poorly-specified and already very strained from a design standpoint. It was originally designed only for SSH and had WinRM retrofitted in a clumsy way where the `connection` content gets decoded by different code depending on the type but is nonetheless expected to follow the same schema in both cases. I don't think that abstraction has any future and is preserved primarily for backward compatibility. For a sy…",,,,,,Anecdotal,comment,,,,,,,,2024-06-25,github/apparentlymart,https://github.com/hashicorp/terraform/issues/4686#issuecomment-2190199278,repo: hashicorp/terraform | issue: Support provisioning using `docker exec` | keyword: best practice
"Hello Martin, Nice to see Hashicorp's visions for provisioners' functionality. Please declare that provisioner functionality is obsolete and should be used only for compatibility purposes in the Terraform language documentation. Based on your comment, it appears that we, as customers, need to take the initiative and request providers' maintainers/developers to implement the corresponding functionality, correct?",,,,,,Anecdotal,comment,,,,,,,,2024-06-26,github/tregubovav-dev,https://github.com/hashicorp/terraform/issues/4686#issuecomment-2190295706,repo: hashicorp/terraform | issue: Support provisioning using `docker exec` | keyword: best practice
"The current recommendation is that [provisioners are a last resort](https://developer.hashicorp.com/terraform/language/resources/provisioners/syntax#provisioners-are-a-last-resort), but they cannot be removed during the 1.x series because they are protected by compatibility promises. The Terraform team intends to preserve the current behaviors but to not change them, as described in [the _Provisioners_ section of the Terraform v1.x compatibility promises](https://developer.hashicorp.com/terrafo…",,,,,,Anecdotal,comment,,,,,,,,2024-06-26,github/apparentlymart,https://github.com/hashicorp/terraform/issues/4686#issuecomment-2190377080,repo: hashicorp/terraform | issue: Support provisioning using `docker exec` | keyword: best practice
"Ability to add auto-apply protection ### Terraform Version ```shell 1.5.0 ``` ### Use Cases We sometimes runs auto-apply option, however there has been cases where the -auto-apply flag was enabled on production workspaces/projects applying unexpected changes, we would like the ability to add some kind of auto-apply protection which will not allow the auto-apply flag if protection is enabled for the workspace or project we are running, ideally for teams that runs terraform locally ### Attempted …",,,,,,Anecdotal,issue,,,,,,,,2024-06-13,github/bryan-rhm,https://github.com/hashicorp/terraform/issues/35339,repo: hashicorp/terraform | keyword: best practice | state: open
"Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the 👍 reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!",,,,,,Anecdotal,comment,,,,,,,,2024-06-13,github/crw,https://github.com/hashicorp/terraform/issues/35339#issuecomment-2166204499,repo: hashicorp/terraform | issue: Ability to add auto-apply protection | keyword: best practice
"Hi @bryan-rhm! Thanks for sharing this use-case. From your framing of the problem I understood this as something you'd likely want to set only for production and not for staging/development environments. Is that right? If so, doing this in the configuration doesn't feel quite right to me: Terraform modules are usually shared between multiple environments, and so anything which needs to vary per environment either needs to be expressed in the configuration as a rule rather than as a fixed settin…",,,,,,Anecdotal,comment,,,,,,,,2024-06-13,github/apparentlymart,https://github.com/hashicorp/terraform/issues/35339#issuecomment-2166467121,repo: hashicorp/terraform | issue: Ability to add auto-apply protection | keyword: best practice
"@apparentlymart I was not talking about adding it to a module, I was talking to add this as a flag we could define and change through a variable or something between environments, var.enable_protection = true ```sh terraform { auto-apply-protection = var.enable_protection } ``` I know running this locally is not the best practice but will mitigate human errors, I know lot of people and companies runs terraform locally and manage state trhough s3 and dynamodb locking",,,,,,Anecdotal,comment,,,,,,,,2024-06-13,github/bryan-rhm,https://github.com/hashicorp/terraform/issues/35339#issuecomment-2166673903,repo: hashicorp/terraform | issue: Ability to add auto-apply protection | keyword: best practice
"Hi @bryan-rhm, You've illustrated a `terraform` block, which I understood as being in a `.tf` file and therefore part of Terraform's modules language. That was what I meant by ""in a module"". Did you have something else in mind, or do you think we're talking about the same thing? Today most things in the `terraform` block belong to `terraform init` rather than to the plan/apply phase and so there's currently nothing in the `terraform` block that supports expression evaluation like references to …",,,,,,Anecdotal,comment,,,,,,,,2024-06-13,github/apparentlymart,https://github.com/hashicorp/terraform/issues/35339#issuecomment-2166917964,repo: hashicorp/terraform | issue: Ability to add auto-apply protection | keyword: best practice
"@apparentlymart that was just an idea, where do you think that this could be added? is not possible at the state level?",,,,,,Anecdotal,comment,,,,,,,,2024-06-14,github/bryan-rhm,https://github.com/hashicorp/terraform/issues/35339#issuecomment-2168350440,repo: hashicorp/terraform | issue: Ability to add auto-apply protection | keyword: best practice
"Allow expect_failures to target child module input variables in Terraform test run blocks ### Terraform Version ```shell 1.7.4 ``` ### Use Cases I'm looking to add automated test cases to ensure that our variable validation blocks are working as expected, since some of the involved conditional logic is non-trivial. (So, for avoidance of doubt, I'm looking at input variables only here, rather than resources or other checkable objects - see #34700 for that.) Whilst most of our validations are at …",,,,,,Anecdotal,issue,,,,,,,,2024-04-05,github/trajan3,https://github.com/hashicorp/terraform/issues/34951,repo: hashicorp/terraform | keyword: best practice | state: open
"Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the 👍 reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!",,,,,,Anecdotal,comment,,,,,,,,2024-04-05,github/crw,https://github.com/hashicorp/terraform/issues/34951#issuecomment-2040340125,repo: hashicorp/terraform | issue: Allow expect_failures to target child module input variables in Terraform test run blocks | keyword: best practice
"Hi, just wanted to check if there were any thoughts on this yet? Specifically, I'd like to understand if there are any suggestions on how best to workaround this until a good mechanism is available... I understand that testing at the child module level is possible, but this would require crafting more complex variable blocks (including derived variables) and wouldn't necessarily be a good check of the workflow end-to-end. Also, whilst allowing cross-variable referencing will allow me to shift o…",,,,,,Anecdotal,comment,,,,,,,,2024-04-25,github/trajan3,https://github.com/hashicorp/terraform/issues/34951#issuecomment-2076869538,repo: hashicorp/terraform | issue: Allow expect_failures to target child module input variables in Terraform test run blocks | keyword: best practice
"Hi @trajan3, I would suggest asking this in [the community forum](https://discuss.hashicorp.com/c/terraform-core/27) where there are more people ready to help. The GitHub issues here are monitored only by a few core maintainers. Thanks!",,,,,,Anecdotal,comment,,,,,,,,2024-04-25,github/crw,https://github.com/hashicorp/terraform/issues/34951#issuecomment-2077684941,repo: hashicorp/terraform | issue: Allow expect_failures to target child module input variables in Terraform test run blocks | keyword: best practice
terraform init seems to not support basic terraform best practice Terraform all versions. It's terraform best practice to store environment config in tfvars files . e.g: ``` environments/ |-- dev.tfvars # store dev configuration |-- prod.tfvars # store stage configuration resources/ |-- main.tf # resources etc. ``` (Nice clean separation of environment config from other config from terraform resources - good!) And to use e.g. `terraform plan -var-file environments/dev.tfvars` But (in terms of A…,,,,,,Anecdotal,issue,,,,,,,,2018-08-08,github/gtmtech,https://github.com/hashicorp/terraform/issues/18632,repo: hashicorp/terraform | keyword: best practice | state: open
"Hi @gtmtech! Thanks for writing this up. In terms of customizing the location of the `.terraform` directory to start: this isn't _directly_ possible, but you can get the same effect by using multiple working directories: ``` (assuming the current working directory is the one containing the root module config files) $ mkdir prod $ cd prod $ terraform init .. -backend-config=bucket=terraform-dev ... $ terraform apply .. ... ``` The above, while admittedly not intuitive, will allow you a separate …",,,,,,Anecdotal,comment,,,,,,,,2018-08-11,github/apparentlymart,https://github.com/hashicorp/terraform/issues/18632#issuecomment-412247266,repo: hashicorp/terraform | issue: terraform init seems to not support basic terraform best practice | keyword: best practice
"@apparentlymart Thanks for taking the time for a detailed explanation! I have seen the directory per environment approach used above, and appreciate the comments you make about it. In my experience this approach leads to teams having 2 places to put resources - in the (common) resources folder, or in the (environment specific) folder. Over time teams can put more and more in the environment specific folders (as things may initially just be for one environment) and they end up with a difficult-t…",,,,,,Anecdotal,comment,,,,,,,,2018-08-14,github/gtmtech,https://github.com/hashicorp/terraform/issues/18632#issuecomment-412818218,repo: hashicorp/terraform | issue: terraform init seems to not support basic terraform best practice | keyword: best practice
"Thanks for that extra context, @gtmtech! It is true that the module-per-environment approach does rely on humans to police themselves and each other (complying with a policy on which resources -- if any -- belong in the environment-specific module) whereas using variables implicitly forces that through the limitations of a `.tfvars` file. Our usual attitude is that Terraform should encourage a best-practice but give you room to step out of it when it's not appropriate, and so we generally lean …",,,,,,Anecdotal,comment,,,,,,,,2018-08-14,github/apparentlymart,https://github.com/hashicorp/terraform/issues/18632#issuecomment-412963877,repo: hashicorp/terraform | issue: terraform init seems to not support basic terraform best practice | keyword: best practice
"@apparentlymart A couple of questions about the recommended structure above: ``` environments/ |-- dev/ # dev configuration |-- dev.tf |-- prod/ # prod configuration |-- prod.tf resources/ # shared module for elements common to all environments |-- main.tf ``` using example `dev.tf` ```yaml module ""main"" { source = ""../resources"" # per-environment settings here, as you had in the .tfvars files in your example } ``` 1. Is the first source attribute meant to be `../../resources` instead? 1. How d…",,,,,,Anecdotal,comment,,,,,,,,2018-08-26,github/davidgoate,https://github.com/hashicorp/terraform/issues/18632#issuecomment-416030041,repo: hashicorp/terraform | issue: terraform init seems to not support basic terraform best practice | keyword: best practice
"@apparentlymart I think I figured out a way to get the outputs in the structure, from within the `dev` directory I now run `terraform output --module=main`",,,,,,Anecdotal,comment,,,,,,,,2018-08-26,github/davidgoate,https://github.com/hashicorp/terraform/issues/18632#issuecomment-416043019,repo: hashicorp/terraform | issue: terraform init seems to not support basic terraform best practice | keyword: best practice
"It also can help in automation when you want to run terraform init from operational directory, instead of going to each directory of the project.",,,,,,Anecdotal,comment,,,,,,,,2019-01-16,github/vasilij-icabbi,https://github.com/hashicorp/terraform/issues/18632#issuecomment-454983247,repo: hashicorp/terraform | issue: terraform init seems to not support basic terraform best practice | keyword: best practice
"Does there exist a repo on github (or elsewhere) that demonstrates this layout by @apparentlymart (either an example repo or a real-thing)? I'm diving into a new AWS project as a newbie, and while trawling for info on how to organize my terraform code while handling multiple environments I ran across this issue. The examples given are a great start, but if I could see a non-trivial example of this in action, it would help immensely.",,,,,,Anecdotal,comment,,,,,,,,2019-01-30,github/abeluck,https://github.com/hashicorp/terraform/issues/18632#issuecomment-458976559,repo: hashicorp/terraform | issue: terraform init seems to not support basic terraform best practice | keyword: best practice
"I think a general challenge with showing ""full examples"" for something like this is that they necessarily involve making some decisions that are not really relevant to the pattern and can distract from the point being made. Defining environments in Terraform involves deciding what an environment is ""made of"" in your organization, which is a subjective architectural decision separate from how the Terraform configurations for representing those environments are organized. With that said, in a pri…",,,,,,Anecdotal,comment,,,,,,,,2019-01-30,github/apparentlymart,https://github.com/hashicorp/terraform/issues/18632#issuecomment-459042268,repo: hashicorp/terraform | issue: terraform init seems to not support basic terraform best practice | keyword: best practice
"I've been sent this issue and after reviewing, I'm don't agree with the below statement. > I'd actually recommend a different layout as a ""**best practice**"": > > ``` > environments/ > |-- dev/ # dev configuration > |-- dev.tf > |-- prod/ # prod configuration > |-- prod.tf > resources/ # shared module for elements common to all environments > |-- main.tf > ``` What this is suggesting is managing multiple tf files per environment. This being stated as best practice is a concern as it really seem…",,,,,,Anecdotal,comment,,,,,,,,2019-03-26,github/olileach,https://github.com/hashicorp/terraform/issues/18632#issuecomment-476801530,repo: hashicorp/terraform | issue: terraform init seems to not support basic terraform best practice | keyword: best practice
"You can mitigate that concern by making sure that each environment module contains only a `backend` configuration and a single `module` block with the same source in each environment. Then the environment modules contain only the information that is actually different between the environments, and the shared module contains the common elements. How similar the environments are is ultimately up to you.",,,,,,Anecdotal,comment,,,,,,,,2019-03-27,github/apparentlymart,https://github.com/hashicorp/terraform/issues/18632#issuecomment-476961041,repo: hashicorp/terraform | issue: terraform init seems to not support basic terraform best practice | keyword: best practice
"@apparentlymart - this is indeed what I do now, however its a bit more complicated than that - You need to have at a minimum, `backend.tf`, single `module` block, `variable` declarations and `provider` declarations. Hmm, well I guess you can define `providers within the module actually, but the variables are needed. This can be condensed into a single file, but the previous tfvars collections of file look and feel so much better, I've gone back to using them this way, and wrapping terraform aga…",,,,,,Anecdotal,comment,,,,,,,,2019-06-05,github/gtmtech,https://github.com/hashicorp/terraform/issues/18632#issuecomment-499131144,repo: hashicorp/terraform | issue: terraform init seems to not support basic terraform best practice | keyword: best practice
"Is possible to achieve something like this, similar code but able to run it independently for each region/env and dynamic backend setup using the variable `TF_DATA_DIR` to use `.terraform-REGION-ENV` instead of `.terraform` and additional backend setup running terraform init like ```sh TF_DATA_DIR=.terraform-$(MY_REGION) terraform init \ -backend-config=""region=$(MY_REGION)""` \ -backend-config=""bucket=$(BUCKET_NAME)"" \ . . . ```",,,,,,Anecdotal,comment,,,,,,,,2024-04-04,github/julian-alarcon,https://github.com/hashicorp/terraform/issues/18632#issuecomment-2036465697,repo: hashicorp/terraform | issue: terraform init seems to not support basic terraform best practice | keyword: best practice
"Make Terraform workspaces explicit ## The problem I am facing I use and love Tf workspaces, but the problem I see is that they are **implicit**: the behavior of terraform depends on the last `terraform workspace select`. Consider the following. In the same directory, the effects of the `terraform apply` depends on the current, implicit workspace: ```console terraform workspace select foo ... hack hack hack ... terraform apply ``` ```console terraform workspace select bar ... hack hack hack ... …",,,,,,Anecdotal,issue,,,,,,,,2018-01-24,github/marco-m,https://github.com/hashicorp/terraform/issues/17185,repo: hashicorp/terraform | keyword: best practice | state: open
"Hi @marco-m! Thanks for this feature request. The current usage model for workspaces was modeled after git branches, with `terraform workspace select <name>` being an approximate analog to `git checkout <name>`. However, I do see your point that it can make it hard to understand the context in which a command was run when e.g. preparing for an outage retrospective, and indeed Terraform does _not_ have a corresponding feature to `git reflog` to see a history of changes to the working directory's…",,,,,,Anecdotal,comment,,,,,,,,2018-01-27,github/apparentlymart,https://github.com/hashicorp/terraform/issues/17185#issuecomment-361011986,repo: hashicorp/terraform | issue: Make Terraform workspaces explicit | keyword: best practice
"Hello @apparentlymart, thanks for the detailed answer, I appreciate the explanation and the suggestions. I agree that at the end terraform should be invoked from a CI system, and this is my final goal. On the other hand, I am using terraform to bootstrap also the CI system, so I have a chicken and egg problem until I arrive there :-)",,,,,,Anecdotal,comment,,,,,,,,2018-01-27,github/marco-m,https://github.com/hashicorp/terraform/issues/17185#issuecomment-361013714,repo: hashicorp/terraform | issue: Make Terraform workspaces explicit | keyword: best practice
"Using Terraform to deploy your automation for Terraform is definitely an interesting challenge! It's true that this means there's always at least one Terraform configuration that _isn't_ managed in your CI. In my own experience doing this (in a previous role, before I joined HashiCorp) we just accepted that this one configuration was managed outside of our usual process, and took steps to make sure that mistakes here couldn't possibly affect our ""real"" production systems, such as deploying it o…",,,,,,Anecdotal,comment,,,,,,,,2018-01-28,github/apparentlymart,https://github.com/hashicorp/terraform/issues/17185#issuecomment-361105575,repo: hashicorp/terraform | issue: Make Terraform workspaces explicit | keyword: best practice
"Here's my workaround based on https://github.com/hashicorp/terraform/issues/15469#issuecomment-343499242 : http://blog.ampli.fi/explicit-workspaces-terraform/ TL;DR: main.tf: ``` provider ""aws"" { region = ""eu-west-1"" } variable ""my_env"" {} variable ""my_server_name"" {} resource ""null_resource"" ""is_environment_name_correct"" { count = ""${var.my_env == terraform.workspace ? 0 : 1}"" ""ERROR: Workspace does not match given environment name!"" = true } resource ""aws_instance"" ""my_test"" { # Amazon Linux …",,,,,,Anecdotal,comment,,,,,,,,2018-05-08,github/xird,https://github.com/hashicorp/terraform/issues/17185#issuecomment-387425536,repo: hashicorp/terraform | issue: Make Terraform workspaces explicit | keyword: best practice
"My big problem was visual feedback of which workspace was the active one. I come up with this small bash helper function for my PS1: ```bash __tf_ps1 () { local workspace="""" local print_format=${1:-(%s)} if [[ -z ${TF_WORKSPACE+x} ]]; then if [[ -f .terraform/environment ]]; then workspace=`cat .terraform/environment` else workspace="""" fi else workspace=${TF_WORKSPACE} fi if [[ $workspace != """" ]]; then printf ""$print_format"" ""$workspace"" fi unset workspace print_format return 0 } ``` Here is a…",,,,,,Anecdotal,comment,,,,,,,,2018-06-28,github/nbetm,https://github.com/hashicorp/terraform/issues/17185#issuecomment-401042812,repo: hashicorp/terraform | issue: Make Terraform workspaces explicit | keyword: best practice
"Thanks for sharing that snippet, @nbetm! On the subject of making the currently-selected workspace more visible, we also recently merged a change (#18253) that includes the workspace name in the `terraform apply` and `terraform destroy` confirmation prompts so that an operator has that additional context available when deciding whether to apply the generated plan.",,,,,,Anecdotal,comment,,,,,,,,2018-06-28,github/apparentlymart,https://github.com/hashicorp/terraform/issues/17185#issuecomment-401190922,repo: hashicorp/terraform | issue: Make Terraform workspaces explicit | keyword: best practice
"Terraform, in the spirit of its legacy in describing infra as code, should also support ""Workspace as code"". Hear me out. Example: ```tf # main.tf terraform { workspace { dev { // Define workspace-specific variables. ec2_instance_type = ""t2.micro"" } prod { ec2_instance_type = ""t2.large"" } } } ``` Running `terraform apply` now fails with an error message saying that workspace is not selected. This makes it explicit and prevents any accidental deploys from unexpected workspace. To deploy a worksp…",,,,,,Anecdotal,comment,,,,,,,,2024-02-28,github/pavitra-infocusp,https://github.com/hashicorp/terraform/issues/17185#issuecomment-1968691890,repo: hashicorp/terraform | issue: Make Terraform workspaces explicit | keyword: best practice
"Allow destroy-time provisioners to access variables <!-- Hi there, Thank you for opening an issue. Please note that we try to keep the Terraform issue tracker reserved for bug reports and feature requests. For general usage questions, please see: https://www.terraform.io/community.html. If your issue relates to a specific Terraform provider, please open it in the provider's own repository. The index of providers is at https://github.com/terraform-providers . --> ### Current Terraform Version <!…",,,,,,Anecdotal,issue,,,,,,,,2019-12-14,github/shaunc,https://github.com/hashicorp/terraform/issues/23679,repo: hashicorp/terraform | keyword: best practice | state: open
"The second use case seems especially important, to be able to define variables in the state of the `null_resource`. In our use case, the `null_resource` provisionner receives the IP of the VM throught its `connection` block. But now, since the connection block is built using variables, even if the `destroy` provisionner does not use any variable, it emits a deprecation warning.",,,,,,Anecdotal,comment,,,,,,,,2019-12-17,github/gvcgael,https://github.com/hashicorp/terraform/issues/23679#issuecomment-566432131,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"Hi @shaunc, You can already use `triggers` in `null_resource` as a place to retain data you need at destroy time: ```hcl resource ""null_resource"" ""foo"" { triggers { interpreter = var.local_exec_interpreter } provisioner { when = destroy interpreter = self.triggers.interpreter ... } } ``` We don't intend to make an exception for referring to variables because within descendant modules a variable is just as likely to cause a dependency as anything else. While it is true that _root module_ variabl…",,,,,,Anecdotal,comment,,,,,,,,2019-12-21,github/teamterraform,https://github.com/hashicorp/terraform/issues/23679#issuecomment-568137941,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"Thanks for the explanation. The interpreter is customized just because I was contributing to [terraform-aws-eks-cluster](https://github.com/cloudposse/terraform-aws-eks-cluster) -- you'd have to ask them about the design; I do imagine that there are many more uses for variables in when=destroy provisioners, though. However, I think that using them in triggers may be exactly what I meant when I talked about ""two passes"". I'm curious in your implementation why referring to variables via triggers …",,,,,,Anecdotal,comment,,,,,,,,2019-12-21,github/shaunc,https://github.com/hashicorp/terraform/issues/23679#issuecomment-568140790,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"I think I'm running into a similar situation for the same reasons, though perhaps more mundane. @shaunc's proposals would seem to have merit. Like him, I also understand the concerns around possibly unknown state at destroy-time. While the documentation currently indicates this should be possible, I'm getting the deprecation warning for the `connection` block when setting the ssh private key from a variable: ``` connection { host = coalesce(self.public_ip, self.private_ip) type = ""ssh"" user = ""…",,,,,,Anecdotal,comment,,,,,,,,2019-12-30,github/rjhornsby,https://github.com/hashicorp/terraform/issues/23679#issuecomment-569577135,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"My use case is granting permissions (injecting the connection key) using external variables when running terraform. On destroy, i have some pre-destroy cleanup to do on the machine and some external services. How would one accomplish that now ? ``` connection { type = ""ssh"" user = var.vm_admin_user private_key = var.ops_private_key host = var.vm_ip_address } ```",,,,,,Anecdotal,comment,,,,,,,,2020-01-07,github/bigdot,https://github.com/hashicorp/terraform/issues/23679#issuecomment-571639584,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"It would be nice if the error message could be improved. When you read: ```Error: local-exec provisioner command must be a non-empty string``` and your config looks like: ``` provisioner ""local-exec"" { command = ""echo ${some_resource.some_attribute}"" } ``` it's not at all obvious that the root cause is the variable cannot be interpolated.",,,,,,Anecdotal,comment,,,,,,,,2020-01-09,github/marcelloromani,https://github.com/hashicorp/terraform/issues/23679#issuecomment-572563063,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"> You can already use `triggers` in `null_resource` as a place to retain data you need at destroy time Unfortunately, those triggers also cause a new resource to be created if they change. If you have more complex idempotence requirements then this won't work. In my case I compare data from a `data` source with my local config and derive a trigger value from that. E.g. make the trigger `foo` if they differ and `bar` if they're the same. If the trigger value changes, _then_ I need access to the …",,,,,,Anecdotal,comment,,,,,,,,2020-01-17,github/sdickhoven,https://github.com/hashicorp/terraform/issues/23679#issuecomment-575814965,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"there's also the use case of: ``` provisioner ""local-exec"" { when = destroy command = format(""%s/scripts/tag.sh"", path.module) ... ``` if i want to use a `local-exec` provisioner for a `null_resource` in a module i'll need access to `path.module`. it's not really cool if all my resources get recreated if the path of the module changes (which is what would happen if i stored this value in the `trigger` config of the `null_resource`).",,,,,,Anecdotal,comment,,,,,,,,2020-01-17,github/sdickhoven,https://github.com/hashicorp/terraform/issues/23679#issuecomment-575821487,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
@sdickhoven the `path.module` use case is addressed in the recently-closed https://github.com/hashicorp/terraform/issues/23675,,,,,,Anecdotal,comment,,,,,,,,2020-01-17,github/dmrzzz,https://github.com/hashicorp/terraform/issues/23679#issuecomment-575826388,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"I like the idea of a separate block of variables that don't trigger create on change, but are usable in destroy. `path.module` is just one of many possible things that might be needed; even if `path.module` is what you need, you often want it in combination with something else, and not referencing the computed value means your code isn't DRY.",,,,,,Anecdotal,comment,,,,,,,,2020-01-17,github/shaunc,https://github.com/hashicorp/terraform/issues/23679#issuecomment-575827118,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"I'm having a similar issue with ``` locals { cmd1 = ""aws cmd --profile ${var.account_profile} ..."" cmd2 = ""aws cmd --profile ${var.other_account_profile} ..."" } resource ""null_resource"" ""aws_create_vpc_asocciation_auth"" { triggers = { vpc_id = var.app_vpc_id } provisioner ""local-exec"" { command = ""${local.cmd1} && sleep 30"" } provisioner ""local-exec"" { when = destroy command = ""${local.cmd2} && sleep 30"" } } ``` which also results in ```Destroy-time provisioners and their connection configurati…",,,,,,Anecdotal,comment,,,,,,,,2020-01-27,github/jdeluyck,https://github.com/hashicorp/terraform/issues/23679#issuecomment-578645114,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"I'm using a destroy provisioner to uninstall an application on a remote device, and the only way to provide connection details currently is to hard-code them. The `triggers` workaround is not a proper workaround as it produces significantly different behavior. One solution would be to introduce a new meta-argument that has no effect for storing arbitrary data.",,,,,,Anecdotal,comment,,,,,,,,2020-01-28,github/xanderflood,https://github.com/hashicorp/terraform/issues/23679#issuecomment-579030392,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"Also, would someone from the TF team mind providing an example of how this could produce a circular reference? It's not really clear to me why preventing direct use but allowing indirect use through `triggers` would change the range of possible dependency graphs - if there are any troublesome patterns, it seems like you could produce them just as easily going through `triggers`, no?",,,,,,Anecdotal,comment,,,,,,,,2020-02-03,github/xanderflood,https://github.com/hashicorp/terraform/issues/23679#issuecomment-581461056,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"I have yet another use case where I get this warning and I can't tell how to avoid it. I have `aws_instance`, `aws_ebs_volume` and `aws_volume_attachment`. The instances are rebuilt frequently but the EBS volumes are kept. When the `aws_volume_attachment` is destroyed, which happens before the instance is destroyed, I need to run a command on the instance to stop cleanly the services that rely on the storage, or they'll crash badly and leave corrupted data. I was able to accomplish this with a …",,,,,,Anecdotal,comment,,,,,,,,2020-02-04,github/adona9,https://github.com/hashicorp/terraform/issues/23679#issuecomment-582167057,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"Our use case is bastion hosts/users for remote execs. They work fine for the forward provisioners, but not the destroy time provisioner. https://github.com/brightbox/kubernetes-cluster/blob/21169f9c575316eda10340c95857904fcca89855/master/main.tf#L108 The bastion user is calculated from the creation of the bastion host with Terraform - which in turn depends upon which cloud operating system image the end user selects. Again I can't see how to get around this without splitting the runs into two.",,,,,,Anecdotal,comment,,,,,,,,2020-02-18,github/NeilW,https://github.com/hashicorp/terraform/issues/23679#issuecomment-587559374,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"We are also running into something similar. Following @apparentlymart's suggestion in [this thread](https://discuss.hashicorp.com/t/how-to-rewrite-null-resource-with-local-exec-provisioner-when-destroy-to-prepare-for-deprecation-after-0-12-8/4580/2) I tried to rewrite the following problematic template: ```hcl resource ""null_resource"" ""kops_delete_flag"" { triggers = { cluster_delete = module.kops.should_recreate } provisioner ""local-exec"" { when = destroy command = <<CMD if kops get cluster ""${…",,,,,,Anecdotal,comment,,,,,,,,2020-02-25,github/Bowbaq,https://github.com/hashicorp/terraform/issues/23679#issuecomment-590918459,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
Here's a use-case for variable access: I need to work around https://github.com/hashicorp/terraform/issues/516 / https://support.hashicorp.com/hc/en-us/requests/19325 to avoid Terraform leaking the database master password into the state file. Note for Hashicorp staff: All of this would be unnecessary if Terraform had the equivalent of CloudFormation's resolve feature or something similar to make `aws_ssm_parameter` safe to use. We've requested this repeatedly with our account reps. Is there an…,,,,,,Anecdotal,comment,,,,,,,,2020-02-27,github/acdha,https://github.com/hashicorp/terraform/issues/23679#issuecomment-592048741,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"I had the same problem as @Bowbaq. Presumably it's because the saved tfstate doesn't have an old value saved for the new trigger. I was able to work around it by commenting out the delete provisioner, ""deleting"" the resource with `terraform destroy --target=module.my_module.null_resource.my_name`, actually deleting it by manually executing the `local-exec` command (in my case, `kubectl delete`), and then re-applying. But this is both a pain, and doesn't solve the ""Adding triggers causes the nul…",,,,,,Anecdotal,comment,,,,,,,,2020-03-03,github/armbraggins,https://github.com/hashicorp/terraform/issues/23679#issuecomment-593918403,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"Hello, Here is my use case: I'm using a python script in order to empty an S3 bucket containing thousands of objects (""force_destroy = true"" option of the aws_s3_bucket Terraform resource is too slow: more than 2h vs ~1min with python script) ``` locals { # Sanitize a resource name prefix: resource_name_prefix = replace(replace(""${var.product_name}-${terraform.workspace}"", ""_"", """"), "" "", """") # data bucket name construction: data_bucket_name = ""${local.resource_name_prefix}-${var.region}-data"" t…",,,,,,Anecdotal,comment,,,,,,,,2020-03-04,github/sdesousa86,https://github.com/hashicorp/terraform/issues/23679#issuecomment-594353584,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"Is this issue really getting ignored? in Changelog of 0.13.0 is writtern, that the here described ""Warnings"" will result in Errors. > config: Inside provisioner blocks that have when = destroy set, and inside any connection blocks that are used by such provisioner blocks, it is now an error to refer to any objects other than self, count, or each [GH-24083] https://github.com/hashicorp/terraform/blob/master/CHANGELOG.md @hashicorp-support / @apparentlymart : Is this issue here really getting ign…",,,,,,Anecdotal,comment,,,,,,,,2020-03-09,github/schmichri,https://github.com/hashicorp/terraform/issues/23679#issuecomment-596428283,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"I apologize for leaving this enhancement request hanging for so long. I think the topic on this issue has shifted from the specific suggestion it started with, to a general discussion of how the https://github.com/hashicorp/terraform/pull/23559 deprecation and [planned removal](https://github.com/hashicorp/terraform/pull/24083) of destroy provisioner references negatively impacts people’s workflows. I genuinely appreciate that people in this discussion have tried to help each other with workaro…",,,,,,Anecdotal,comment,,,,,,,,2020-03-11,github/danieldreier,https://github.com/hashicorp/terraform/issues/23679#issuecomment-597388015,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"> > You can already use `triggers` in `null_resource` as a place to retain data you need at destroy time > > Unfortunately, those triggers also cause a new resource to be created if they change. >... > I like the fact that terraform will isolate the destruction provisioner. But that _does_ necessitate an additional block in the `null_resource` for storing values that should _not_ trigger recreation of the `null_resource`. It appears that for _some_ use cases I can do this: ```hcl resource ""null…",,,,,,Anecdotal,comment,,,,,,,,2020-03-17,github/dmrzzz,https://github.com/hashicorp/terraform/issues/23679#issuecomment-600255618,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"Hi, Using triggers is not applicable for me for security reasons. On ressources creation, I need of a null ressource + local exec to build an object on remote server. On ressources destroy, I need to remove this object on the remote server. To connect to the remote server, I need of a token. Using triggers would store the token in the tfstate which is not a good practise here. So the workaround I imagine when I need to destroy the resources - terraform apply with a condition using a variable (d…",,,,,,Anecdotal,comment,,,,,,,,2020-03-25,github/mldmld68,https://github.com/hashicorp/terraform/issues/23679#issuecomment-603713100,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"My use-case: I'm using `docker-machine` to handle the installation and configuration of docker on machines provisioned using terraform using a `local-exec`. Very simple and quick one-liner. When I `destroy`, I also want `docker-machine rm` executed to remove the machine from being handled also by docker-machine as it won't exist any more.",,,,,,Anecdotal,comment,,,,,,,,2020-03-27,github/J7mbo,https://github.com/hashicorp/terraform/issues/23679#issuecomment-605075379,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"We also depend on variables in the destroy provisioner command to revoke the Puppet certificate of a instance. ```terraform provisioner ""local-exec"" { when = destroy command = ""${path.module}/scripts/puppet_cert_clean.sh ${local.instance_prefix}.${var.instance_name}${format(""%02d"", count.index + 1)}"" } ``` Please allow more allow `$path`/`$local`/`$var`! :)",,,,,,Anecdotal,comment,,,,,,,,2020-04-17,github/baurmatt,https://github.com/hashicorp/terraform/issues/23679#issuecomment-615172997,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"My use case involves first provisioning a server and then subsequently using the remote-exec provisioner inside of a null_resource to register the instance as a Gitlab runner. Upon destroy, I need to run a command against the server to unregister the instance as a Gitlab runner. I can think of ways to ""hard code"" all properties within the connection object aside from the host, which must be driven from the previously provisioned server. See the snippet included below for more information. ``` c…",,,,,,Anecdotal,comment,,,,,,,,2020-04-21,github/davewoodward,https://github.com/hashicorp/terraform/issues/23679#issuecomment-616934110,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"Hi all, I finally found a workaround for my use case (s3 bucket resource, where ""triggers"" block is not allowed), hope it will help some of you: ``` locals { # Sanitize a resource name prefix: resource_name_prefix = replace(replace(""${var.product_name}-${terraform.workspace}"", ""_"", """"), "" "", """") tags = { ""Environment"" = terraform.workspace ""Product"" = lower(var.product_name) ""TechOwner"" = var.product_tech_owner_mail ""Owner"" = var.product_owner_mail } } resource ""aws_s3_bucket"" ""data"" { bucket =…",,,,,,Anecdotal,comment,,,,,,,,2020-04-21,github/sdesousa86,https://github.com/hashicorp/terraform/issues/23679#issuecomment-616990081,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"Similar to @davewoodward I'm using `remote-exec` with `when = destroy`to de-register a GitLab runner when terminating the instance (`terraform destroy`): ``` resource aws_instance ""instance"" { ...snip... provisioner ""remote-exec"" { when = destroy inline = [ ""sudo gitlab-runner unregister --all-runners"" ] connection { type = ""ssh"" host = self.public_ip user = var.host_ssh_user # deprecated private_key = var.host_ssh_pubkey # deprecated } ```",,,,,,Anecdotal,comment,,,,,,,,2020-04-27,github/111a5ab1,https://github.com/hashicorp/terraform/issues/23679#issuecomment-619847641,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
We have sensitive variables in our destroy time provisionsers. Moving them to the triggers block writes them to the state file AND to the console which we've been trying extremely hard to avoid. Is there an alternative option available?,,,,,,Anecdotal,comment,,,,,,,,2020-05-06,github/andrew-sumner,https://github.com/hashicorp/terraform/issues/23679#issuecomment-624448381,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"Run Terraform on workstation with gcs backend fails with ""Error: Failed to get existing workspaces (403)"" when using GCP service account impersonation Hi there, I want to use service accounts without physically downloading the keys (this is even mentioned by Google as best practice). It's working well without `backend ""gcs""`. However, when I set Terraform backend to gcs (on workstation), it always fails with ""Error: Failed to get existing workspaces (403)"". BTW, it's also working well with down…",,,,,,Anecdotal,issue,,,,,,,,2022-06-30,github/aibazhang,https://github.com/hashicorp/terraform/issues/31344,repo: hashicorp/terraform | keyword: best practice | state: open
"I have a similar error when i try to impersonate a service account : ``` │ Error: error loading state: Failed to open state file at gs://terraform-state-projectname.../...path.../default.tfstate: Get ""https://storage.googleapis.com/terraform-state-projectname.../...path.../default.tfstate"": impersonate: status code 403: { │ ""error"": { │ ""code"": 403, │ ""message"": ""The caller does not have permission"", │ ""status"": ""PERMISSION_DENIED"" │ } │ } ``` No error when i use the SA's key with GOOGLE_APPLIC…",,,,,,Anecdotal,comment,,,,,,,,2022-09-08,github/tofke,https://github.com/hashicorp/terraform/issues/31344#issuecomment-1240436854,"repo: hashicorp/terraform | issue: Run Terraform on workstation with gcs backend fails with ""Error: Failed to get existing workspaces (403)"" when using GCP service account impersonation | keyword: best practice"
"any Update on it? I am also facing the same issue using workload identity provider bound to the GitHub OIDC. ``` fadiorg Initializing the backend... (DEDACTED) fadiorg ╷ (DEDACTED) │ Error: Failed to get existing workspaces: querying Cloud Storage failed: Get ""https://storage.googleapis.com/storage/v1/b/{BUCKET_NAME}t/o?alt=json&delimiter=%2F&pageToken=&prefix=kcd-state%2F&prettyPrint=false&projection=full&versions=false"": oauth2/google: status code 403 ```",,,,,,Anecdotal,comment,,,,,,,,2022-10-18,github/leewoobin789,https://github.com/hashicorp/terraform/issues/31344#issuecomment-1282300633,"repo: hashicorp/terraform | issue: Run Terraform on workstation with gcs backend fails with ""Error: Failed to get existing workspaces (403)"" when using GCP service account impersonation | keyword: best practice"
"Has anyone figured this out yet? I'm seeing the same thing with GitHub OIDC. I get one of two errors back, the one you guys describe, as well as: ```none Error: Failed to get existing workspaces: querying Cloud Storage failed: Get ""https://storage.googleapis.com/storage/v1/b/<clip>terraform%2F&prettyPrint=false&projection=full&versions=false"": oauth2/google: status code 403: { │ ""error"": { │ ""code"": 403, │ ""message"": ""Permission 'iam.serviceAccounts.getAccessToken' denied on resource (or it may…",,,,,,Anecdotal,comment,,,,,,,,2022-12-10,github/brettcurtis,https://github.com/hashicorp/terraform/issues/31344#issuecomment-1345389825,"repo: hashicorp/terraform | issue: Run Terraform on workstation with gcs backend fails with ""Error: Failed to get existing workspaces (403)"" when using GCP service account impersonation | keyword: best practice"
@brettcurtis No. It's been half a year and we don't have any information from the TF team :/ seems like this isn't actually an option,,,,,,Anecdotal,comment,,,,,,,,2022-12-20,github/dinigo,https://github.com/hashicorp/terraform/issues/31344#issuecomment-1359562604,"repo: hashicorp/terraform | issue: Run Terraform on workstation with gcs backend fails with ""Error: Failed to get existing workspaces (403)"" when using GCP service account impersonation | keyword: best practice"
"@dinigo - In my case it was a code issue, we have since then fixed it and are successfully using OIDC from GitHub to our Terraform Backend GCS. Sounds like the context of our problems may be different.",,,,,,Anecdotal,comment,,,,,,,,2022-12-20,github/brettcurtis,https://github.com/hashicorp/terraform/issues/31344#issuecomment-1359645391,"repo: hashicorp/terraform | issue: Run Terraform on workstation with gcs backend fails with ""Error: Failed to get existing workspaces (403)"" when using GCP service account impersonation | keyword: best practice"
"We've had this exact same issue. Colleague found out what it was. The underlying issue was that the gcloud cli had a project set, that did not exist anymore. This happened because we use the same gcloud cli for manual interaction - as well use it for TF. When we deleted a project via TF, the issue presented - because the deleted project was still configured in the gcloud cli. You can verify the set project in; `~/.config/gcloud/application_default_credentials.json ` You set a project manually w…",,,,,,Anecdotal,comment,,,,,,,,2023-01-06,github/fancybear-dev,https://github.com/hashicorp/terraform/issues/31344#issuecomment-1373392697,"repo: hashicorp/terraform | issue: Run Terraform on workstation with gcs backend fails with ""Error: Failed to get existing workspaces (403)"" when using GCP service account impersonation | keyword: best practice"
I ran into exact same problem. I am using Google GKE Workload Identity. GKE/K8s SA is properly mapped to IAM SA. IAM policy binding exist for workloadIdentityUser. pod is annotated with correct SA name. Not sure whats wrong ? Anybody found fix to this problem? Thanks. $ terraform init Initializing modules... - bucket in hmcp-gcp-tf-modules/custom/gcs - vm in hmcp-gcp-tf-modules/custom/compute-vm Initializing the backend... ╷ │ Error: Failed to get existing workspaces: querying Cloud Storage fai…,,,,,,Anecdotal,comment,,,,,,,,2023-01-16,github/OmkarG1986,https://github.com/hashicorp/terraform/issues/31344#issuecomment-1383841780,"repo: hashicorp/terraform | issue: Run Terraform on workstation with gcs backend fails with ""Error: Failed to get existing workspaces (403)"" when using GCP service account impersonation | keyword: best practice"
"@OmkarG1986 - can you run this [test ](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity#verify_the_setup )from your POD and get the correct SA back? Your use case adds a different layer of complexity to the use case compared to mine since you're running on k8s. I'm confused tho if you're trying to run this from a compute node or a POD, the POD is the thing that will be able to use workload identity, not the compute?",,,,,,Anecdotal,comment,,,,,,,,2023-01-16,github/brettcurtis,https://github.com/hashicorp/terraform/issues/31344#issuecomment-1383992358,"repo: hashicorp/terraform | issue: Run Terraform on workstation with gcs backend fails with ""Error: Failed to get existing workspaces (403)"" when using GCP service account impersonation | keyword: best practice"
> I ran into exact same problem. I am using Google GKE Workload Identity. GKE/K8s SA is properly mapped to IAM SA. IAM policy binding exist for workloadIdentityUser. pod is annotated with correct SA name. Not sure whats wrong ? Anybody found fix to this problem? Thanks. I am having the same issue! > can you run this [test ](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity#verify_the_setup)from your POD and get the correct SA back? I get the correct SA back.,,,,,,Anecdotal,comment,,,,,,,,2023-01-26,github/dermatologist,https://github.com/hashicorp/terraform/issues/31344#issuecomment-1405414149,"repo: hashicorp/terraform | issue: Run Terraform on workstation with gcs backend fails with ""Error: Failed to get existing workspaces (403)"" when using GCP service account impersonation | keyword: best practice"
@dermatologist and just to confirm you're getting the IAM service account back that has the role of `iam.workloadIdentityUser` that has the member of the k8s service account?,,,,,,Anecdotal,comment,,,,,,,,2023-01-27,github/brettcurtis,https://github.com/hashicorp/terraform/issues/31344#issuecomment-1406570257,"repo: hashicorp/terraform | issue: Run Terraform on workstation with gcs backend fails with ""Error: Failed to get existing workspaces (403)"" when using GCP service account impersonation | keyword: best practice"
"This code works for us: ```hcl # Google Service Account Data Source # https://registry.terraform.io/providers/hashicorp/google/latest/docs/data-sources/service_account data ""google_service_account"" ""this"" { account_id = var.service_account_id } # Google Service Account IAM Binding Resource # https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/google_service_account_iam#google_service_account_iam_member resource ""google_service_account_iam_member"" ""workload_identity"" {…",,,,,,Anecdotal,comment,,,,,,,,2023-01-27,github/brettcurtis,https://github.com/hashicorp/terraform/issues/31344#issuecomment-1406575403,"repo: hashicorp/terraform | issue: Run Terraform on workstation with gcs backend fails with ""Error: Failed to get existing workspaces (403)"" when using GCP service account impersonation | keyword: best practice"
"I also had this issue, and found it happened on a very newly-created backend bucket. I worked around it by adding a retry with a two-minute delay to make it succeed on the second try.",,,,,,Anecdotal,comment,,,,,,,,2023-04-12,github/jameswjr,https://github.com/hashicorp/terraform/issues/31344#issuecomment-1506093050,"repo: hashicorp/terraform | issue: Run Terraform on workstation with gcs backend fails with ""Error: Failed to get existing workspaces (403)"" when using GCP service account impersonation | keyword: best practice"
"I am having the exact same problem in GKE with workload identity binding and GCP service account impersonation. During my troubleshooting, I discovered that terraform is getting the service account access token with the scope `devstorage.read_write`, so I reproduced the same within the pod running terraform: ```bash curl -sSL -H 'Metadata-Flavor: Google' 'http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token?scopes=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fd…",,,,,,Anecdotal,comment,,,,,,,,2023-09-28,github/fallard84,https://github.com/hashicorp/terraform/issues/31344#issuecomment-1738393490,"repo: hashicorp/terraform | issue: Run Terraform on workstation with gcs backend fails with ""Error: Failed to get existing workspaces (403)"" when using GCP service account impersonation | keyword: best practice"
"I was able to fix my problem. In my case the problem was a combination of being on GKE 1.25 and terraform 0.14. Upgrading terraform to 0.15 fixed my issue, mainly because of this [rework](https://github.com/hashicorp/terraform/pull/28296) on the impersonation implementation. I can't reproduce the issue with terraform 0.14 on GKE 1.24. My hypothesis is that the new version of the gke-metadata-server on GKE 1.25 behaves differently when being passed the `scopes` parameter. I sniffed the traffic w…",,,,,,Anecdotal,comment,,,,,,,,2023-09-29,github/fallard84,https://github.com/hashicorp/terraform/issues/31344#issuecomment-1741102790,"repo: hashicorp/terraform | issue: Run Terraform on workstation with gcs backend fails with ""Error: Failed to get existing workspaces (403)"" when using GCP service account impersonation | keyword: best practice"
"I fixed following issue with following changes; ---ERROR--- Run terraform init Initializing the backend... ╷ │ Error: Failed to get existing workspaces: querying Cloud Storage failed: Get ""https://storage.googleapis.com/storage/v1/b/a_bucket/o?alt=json&delimiter=%2F&endOffset=&includeTrailingDelimiter=false&pageToken=&prefix=terraform%2Fstate%2F&prettyPrint=false&projection=full&startOffset=&versions=false"": oauth2/google: status code 403: { │ ""error"": { │ ""code"": 403, │ ""message"": ""Permission …",,,,,,Anecdotal,comment,,,,,,,,2023-12-22,github/selimacerbas,https://github.com/hashicorp/terraform/issues/31344#issuecomment-1867070311,"repo: hashicorp/terraform | issue: Run Terraform on workstation with gcs backend fails with ""Error: Failed to get existing workspaces (403)"" when using GCP service account impersonation | keyword: best practice"
GitHub Workflows security hardening This PR adds explicit [permissions section](https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#permissions) to workflows. This is a security best practice because by default workflows run with [extended set of permissions](https://docs.github.com/en/actions/security-guides/automatic-token-authentication#permissions-for-the-github_token) (except from `on: pull_request` [from external forks](https://securitylab.github.com/rese…,,,,,,Anecdotal,issue,,,,,,,,2022-09-25,github/sashashura,https://github.com/hashicorp/terraform/pull/31863,repo: hashicorp/terraform | keyword: best practice | state: open
[![CLA assistant check](https://cla.hashicorp.com/pull/badge/signed)](https://cla.hashicorp.com/hashicorp/terraform?pullRequest=31863) <br/>All committers have signed the CLA.,,,,,,Anecdotal,comment,,,,,,,,2022-09-25,github/hashicorp-cla,https://github.com/hashicorp/terraform/pull/31863#issuecomment-1257214101,repo: hashicorp/terraform | issue: GitHub Workflows security hardening | keyword: best practice
"Thanks for this submission! The safety and security of our products is deeply important to us. Our build process is designed and vetted by our internal release engineering and security teams. Although we do not typically accept pull requests for the build and release process, I will run this PR past those teams for review. Thanks for your consideration on this issue.",,,,,,Anecdotal,comment,,,,,,,,2022-09-27,github/crw,https://github.com/hashicorp/terraform/pull/31863#issuecomment-1260172368,repo: hashicorp/terraform | issue: GitHub Workflows security hardening | keyword: best practice
An example of a recent workflow run with unrestricted permissions: ![image](https://user-images.githubusercontent.com/93376818/204942677-90d879ed-0b36-4192-9f9c-2e303750b309.png),,,,,,Anecdotal,comment,,,,,,,,2022-12-01,github/sashashura,https://github.com/hashicorp/terraform/pull/31863#issuecomment-1333008554,repo: hashicorp/terraform | issue: GitHub Workflows security hardening | keyword: best practice
The internal security team approved the general approach (per @sarahethompson). The core maintainers would still need to review and accept this specific PR. I will raise it with the team. Apologies for the delay.,,,,,,Anecdotal,comment,,,,,,,,2022-12-02,github/crw,https://github.com/hashicorp/terraform/pull/31863#issuecomment-1335949976,repo: hashicorp/terraform | issue: GitHub Workflows security hardening | keyword: best practice
"[Feature] Module Variable and Output Inheritance (or simular) ### Current Terraform Version ``` Terraform v1.1.1 on darwin_arm64 ``` ### Use-cases It would be nice if you could reuse inputs, outputs from child modules without the need to duplicate code. Take a module like `base-template` it would be easier to maintain if when wrapping this module I didn't need to refine every input and output I wanted to pass to base-template. ```tf module ""base_template"" { name = var.name } variable ""name"" { }…",,,,,,Anecdotal,issue,,,,,,,,2022-07-20,github/kderck,https://github.com/hashicorp/terraform/issues/31485,repo: hashicorp/terraform | keyword: best practice | state: open
"Hi @kderck! Thanks for sharing this use-case. I understand that you are seeking some way to reuse some definitions between different modules, but I'd like to understand more about how you imagine that working. We don't typically consider what you shared to be ""duplicate code"" because each of those blocks is declaring something different: the `variable ""name""` in your first block declares that the root module has a variable called `name`, and the `variable ""name""` in your second block declares t…",,,,,,Anecdotal,comment,,,,,,,,2022-07-21,github/apparentlymart,https://github.com/hashicorp/terraform/issues/31485#issuecomment-1190900270,repo: hashicorp/terraform | issue: [Feature] Module Variable and Output Inheritance (or simular) | keyword: best practice
"> We don't typically consider what you shared to be ""duplicate code"" because each of those blocks is declaring something different: the variable ""name"" in your first block declares that the root module has a variable called name, and the variable ""name"" in your second block declares that the child module has a variable called name I disagree, `var.name` would duplicated because it's just being passed to the base module. For example If I take the [AWS EKS Module](https://github.com/terraform-aws…",,,,,,Anecdotal,comment,,,,,,,,2022-07-21,github/kderck,https://github.com/hashicorp/terraform/issues/31485#issuecomment-1191965679,repo: hashicorp/terraform | issue: [Feature] Module Variable and Output Inheritance (or simular) | keyword: best practice
"I've seen this feature being requested in one form or another a handful of times now -- I know because I periodically search for it hoping it will one day materialize -- and it's always shot down. I'm not sure if the use case isn't properly understood or if there's just a philosophical disagreement between users and Terraform maintainers in this case. We create a number of internal modules to our codify deployment best practices. Almost always, all I want to do is take the upstream module, over…",,,,,,Anecdotal,comment,,,,,,,,2022-07-28,github/jtackaberry,https://github.com/hashicorp/terraform/issues/31485#issuecomment-1198300479,repo: hashicorp/terraform | issue: [Feature] Module Variable and Output Inheritance (or simular) | keyword: best practice
"The question from my earlier comment still stands: can folks share examples from _other_ languages which allow using the signature of one function to define another, or something else that you find comparable to the idea of one module wrapping another? In order to make progress here we either need to understand what patterns from other languages we're intending to emulate, or to justify why Terraform is different enough from other languages to justify doing something novel. Terraform's current …",,,,,,Anecdotal,comment,,,,,,,,2022-08-15,github/apparentlymart,https://github.com/hashicorp/terraform/issues/31485#issuecomment-1215994796,repo: hashicorp/terraform | issue: [Feature] Module Variable and Output Inheritance (or simular) | keyword: best practice
"The best analogy for me is inheritance in OOP: if there's a class that implements 99% of the functionality I want but I just want to augment a portion of its behavior or add some new capability, then I'll create and use a subclass. This is rather contrived, but hopefully illustrative: ```python @dataclass class TerraformAWSEKS: cluster_name: str cluster_version: str cluster_addons: list cluster_endpoint_public_access_cidrs: list eks_managed_node_groups: dict ... def __init__(self): ... def plan…",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/jtackaberry,https://github.com/hashicorp/terraform/issues/31485#issuecomment-1216004731,repo: hashicorp/terraform | issue: [Feature] Module Variable and Output Inheritance (or simular) | keyword: best practice
"Thanks for sharing that, @jtackaberry! It sounds like you're suggesting that we consider an analogy between a class in a class-based OOP language (Python here, as an example) and a module in Terraform. Given that, here's my sense of how the different aspects of a Python class might map to a Terraform module, based on what you've shown here. | Class-based OOP concept | Terraform module concept | |--|--| | Public data members | Input variables | | Private data members | Local values | | ??? | Out…",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/apparentlymart,https://github.com/hashicorp/terraform/issues/31485#issuecomment-1216037812,repo: hashicorp/terraform | issue: [Feature] Module Variable and Output Inheritance (or simular) | keyword: best practice
"@apparentlymart Can't you generate templates with go? Can't see how this wouldn't be possible with Go aside the Language Paradigm. Like Lombok does with Java, or Data Classes.",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/kderck,https://github.com/hashicorp/terraform/issues/31485#issuecomment-1216948667,repo: hashicorp/terraform | issue: [Feature] Module Variable and Output Inheritance (or simular) | keyword: best practice
"@apparentlymart Hi - We want to be able to use the [EKS Module](https://github.com/terraform-aws-modules/terraform-aws-eks) and wrap it with our own logic for example we add [Flux Provider](https://registry.terraform.io/providers/fluxcd/flux/latest). This allows easily create a Kubernetes Cluster with Flux Provided with less boilerplate. However we still need to define the cluster name, cluster vpc.... pass them to the eks module and add them as outputs so that other modules can read them.",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/kderck,https://github.com/hashicorp/terraform/issues/31485#issuecomment-1216954120,repo: hashicorp/terraform | issue: [Feature] Module Variable and Output Inheritance (or simular) | keyword: best practice
"Hi @kderck, Under our existing design recommendations in [Module Composition](https://www.terraform.io/language/modules/develop/composition), my first instinct for what you described would be to do something like this: ```hcl terraform { required_providers { aws = { source = ""hashicorp/aws"" } flux = { source = ""fluxcd/flux"" } kubernetes = { source = ""hashicorp/kubernetes"" } } } provider ""aws"" { # ... } module ""eks"" { source = ""terraform-aws-modules/eks/aws"" cluster_name = ""my-cluster"" cluster_v…",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/apparentlymart,https://github.com/hashicorp/terraform/issues/31485#issuecomment-1217227315,repo: hashicorp/terraform | issue: [Feature] Module Variable and Output Inheritance (or simular) | keyword: best practice
"Yeah and if I want to change `cluster_name` for `eks` I now have to add an input, pass it to eks, and add an output. Even through it's already defined in the eks module. I'l have to to do that for every input that I'd like to change in the eks module.",,,,,,Anecdotal,comment,,,,,,,,2022-08-17,github/kderck,https://github.com/hashicorp/terraform/issues/31485#issuecomment-1217809524,repo: hashicorp/terraform | issue: [Feature] Module Variable and Output Inheritance (or simular) | keyword: best practice
"Hi @kderck, Assuming you're talking about the example I shared in my most recent comment, notice that in my example `module.eks` and `module.flux-setup` are _sibling_ modules, rather than `module.flux-setup` being a child of `module.eks`. Inside `./modules/flux-setup` you can define `eks_cluster` as accepting only exactly the subset of attributes your module needs: ```hcl variable ""eks_cluster"" { type = object({ cluster_endpoint = string }) } ``` There is no need to redeclare any of the EKS mod…",,,,,,Anecdotal,comment,,,,,,,,2022-08-17,github/apparentlymart,https://github.com/hashicorp/terraform/issues/31485#issuecomment-1218250027,repo: hashicorp/terraform | issue: [Feature] Module Variable and Output Inheritance (or simular) | keyword: best practice
"Please see: https://github.com/kderck/gh-31485. You can see if I compose two modules the vpc and eks module and I want to publish that as a module to be able to pass inputs, and receive outputs to the root module that created it I have to redefine all of the inputs and outputs that I want to use in the root module or sibling modules.",,,,,,Anecdotal,comment,,,,,,,,2022-08-19,github/kderck,https://github.com/hashicorp/terraform/issues/31485#issuecomment-1220840870,repo: hashicorp/terraform | issue: [Feature] Module Variable and Output Inheritance (or simular) | keyword: best practice
"Hi @kderck, What I was asking, I suppose, is whether it's truly necessary to publish that wrapper module as a separate module, rather than just putting the `module ""vpc""` and `module ""eks""` blocks directly in the root module. The module composition guide recommends keeping things ""flat"" specifically _because_ it avoids all of this extra boilerplate of declaring the union of all variables of the child modules you're wrapping. I'd typically expect a useful module to _encapsulate_ additional infor…",,,,,,Anecdotal,comment,,,,,,,,2022-08-19,github/apparentlymart,https://github.com/hashicorp/terraform/issues/31485#issuecomment-1221143323,repo: hashicorp/terraform | issue: [Feature] Module Variable and Output Inheritance (or simular) | keyword: best practice
"Yes - I don't want to be able to have to update a `vpc` and `eks` module across every environment I have, I want to be able to make a change and have that reflected by running terraform apply and having the new module version downloaded (or even versioned controlled by tag). I think the `module ""vpc"" and module ""eks"" blocks directly in the root module.` just encourages copy and paste infrastructure. It's worth saying we follow: https://terragrunt.gruntwork.io/docs/getting-started/quick-start/",,,,,,Anecdotal,comment,,,,,,,,2022-08-20,github/kderck,https://github.com/hashicorp/terraform/issues/31485#issuecomment-1221418885,repo: hashicorp/terraform | issue: [Feature] Module Variable and Output Inheritance (or simular) | keyword: best practice
"We also want to be able to provide these modules to Developers. Who may not know how to connect these up at the root module level, or even adequately secure it.",,,,,,Anecdotal,comment,,,,,,,,2022-08-20,github/kderck,https://github.com/hashicorp/terraform/issues/31485#issuecomment-1221419550,repo: hashicorp/terraform | issue: [Feature] Module Variable and Output Inheritance (or simular) | keyword: best practice
"i just found this thread. cool discussion. just wanted to add the usecase that brought me to it. planning cluster setup for example i define modules for the components i.e aks+blob container with this file structure: ``` infra ├── deployed │ ├── dev │ │ ├── backend.tf │ │ ├── cluster.tf └── modules ├── base │ ├── aks │ │ ├── aks.tf │ │ ├── ... │ │ ├── outputs.tf │ │ ├── variables.tf └── preset ├── dev │ ├── aks │ │ ├── aks.tf ``` with modules/base/preset/dev/aks ``` module ""aks"" { source = ""../…",,,,,,Anecdotal,comment,,,,,,,,2022-11-17,github/NunoMaga,https://github.com/hashicorp/terraform/issues/31485#issuecomment-1317907151,repo: hashicorp/terraform | issue: [Feature] Module Variable and Output Inheritance (or simular) | keyword: best practice
"Why do submodules not inherit required_providers? ### Terraform Version ```shell Terraform v1.12.2 on linux_amd64 ``` ### Terraform Configuration Files [```terraform ...terraform config... ``` ](https://github.com/aclstack/terraformRepo) ### Debug Output ``` ...debug output, or link to a gist... ``` ### Expected Behavior When I execute terraform plan in the /env/dev directory, the execution plan is returned normally. Although the submodule does not have a provider configured, I think it can inh…",,,,,,Anecdotal,issue,,,,,,,,2025-08-16,github/aclstack,https://github.com/hashicorp/terraform/issues/37455,repo: hashicorp/terraform | keyword: best practice | state: closed
"Question Is this the intended design, or am I missing a configuration? Do all submodules always need to repeat the required_providers block? Is there a recommended best practice to avoid this duplication across many submodules? Thanks in advance for clarifying! 🙏",,,,,,Anecdotal,comment,,,,,,,,2025-08-16,github/aclstack,https://github.com/hashicorp/terraform/issues/37455#issuecomment-3193567474,repo: hashicorp/terraform | issue: Why do submodules not inherit required_providers? | keyword: best practice
"Hi @aclstack, Fixed the issue and below is the PR. Added README.MD for better understading. PR: https://github.com/aclstack/terraformRepo/pulls Kindly, Merge the PR and make me assigner. If you have any doubts I am happy to help",,,,,,Anecdotal,comment,,,,,,,,2025-08-16,github/SAURAVPADHY,https://github.com/hashicorp/terraform/issues/37455#issuecomment-3193730470,repo: hashicorp/terraform | issue: Why do submodules not inherit required_providers? | keyword: best practice
"I think that's the case. When a submodule does not set a provider, it will automatically inherit the provider of the parent module. Making such a change can not only ensure that the existing functions are not affected, but also solve the problem that the submodule cannot inherit. I submitted a PR before, but it was rejected https://github.com/hashicorp/terraform/pull/37454",,,,,,Anecdotal,comment,,,,,,,,2025-08-16,github/aclstack,https://github.com/hashicorp/terraform/issues/37455#issuecomment-3193737003,repo: hashicorp/terraform | issue: Why do submodules not inherit required_providers? | keyword: best practice
"Hi @aclstack, A module must declare its provider dependencies so that Terraform can determine the version constraints before fetching the correct provider. It must also declare the source of the provider, because there may be multiple namespaces in which that provider name exists. The legacy providers under the hashicorp namespace are given a special default status where Terraform will assume the hashicorp namespace, but that often causes confusion, and that behavior may still be removed at som…",,,,,,Anecdotal,comment,,,,,,,,2025-08-18,github/jbardin,https://github.com/hashicorp/terraform/issues/37455#issuecomment-3196640002,repo: hashicorp/terraform | issue: Why do submodules not inherit required_providers? | keyword: best practice
"PSS: Make the state storage provider's config stateful (backend state, planfiles) This PR makes the nested provider block used for PSS stateful, as in that configuration is recorded into plan files and backend state files. The only values saved in these places are what the user has hardcoded in their configuration files; making this data stateful doesn't expose users to new risks, and they are expected to follow best practices such as providing secrets via environment variables (which are not i…",,,,,,Anecdotal,issue,,,,,,,,2025-07-02,github/SarahFrench,https://github.com/hashicorp/terraform/pull/37286,repo: hashicorp/terraform | keyword: best practice | state: closed
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-08-08,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/37286#issuecomment-3166376303,"repo: hashicorp/terraform | issue: PSS: Make the state storage provider's config stateful (backend state, planfiles) | keyword: best practice"
"Add support for module locks [Fixes #31301] This PR implements module dependency locking in Terraform, adding cryptographic hash verification for modules to prevent supply chain attacks. Module locks are now stored alongside provider locks in `.terraform.lock.hcl`. As described in #31301, Terraform lacks integrity verification for modules. If a module registry or git repository is compromised, malicious code could be served for a previously-trusted module version, potentially compromising infra…",,,,,,Anecdotal,issue,,,,,,,,2025-08-02,github/alex-kattathra-johnson,https://github.com/hashicorp/terraform/pull/37394,repo: hashicorp/terraform | keyword: best practice | state: closed
[![CLA assistant check](https://cla.hashicorp.com/pull/badge/signed)](https://cla.hashicorp.com/hashicorp/terraform?pullRequest=37394) <br/>All committers have signed the CLA.,,,,,,Anecdotal,comment,,,,,,,,2025-08-02,github/hashicorp-cla-app[bot],https://github.com/hashicorp/terraform/pull/37394#issuecomment-3146722087,repo: hashicorp/terraform | issue: Add support for module locks [Fixes #31301] | keyword: best practice
"[![CLA assistant check](https://cla.hashicorp.com/pull/badge/not_signed)](https://cla.hashicorp.com/hashicorp/terraform?pullRequest=37394) Thank you for your submission! We require that all contributors sign our Contributor License Agreement (""CLA"") before we can accept the contribution. [Read and sign the agreement](https://cla.hashicorp.com/hashicorp/terraform?pullRequest=37394) [Learn more about why HashiCorp requires a CLA and what the CLA includes](https://www.hashicorp.com/cla) <sub>Have …",,,,,,Anecdotal,comment,,,,,,,,2025-08-02,github/hashicorp-cla-app[bot],https://github.com/hashicorp/terraform/pull/37394#issuecomment-3146722088,repo: hashicorp/terraform | issue: Add support for module locks [Fixes #31301] | keyword: best practice
"Hi @alex-kattathra-johnson, thanks for your submission! To set expectations appropriately, this submission is unlikely to be reviewed for inclusion at this time. From a process perspective, please see the following section of the Contribution guide: https://github.com/hashicorp/terraform/blob/main/.github/CONTRIBUTING.md#proposing-a-change. Discussing the change with the maintainers beforehand, particularly for a change of this complexity, helps make sure the resulting solution accounts for any…",,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/crw,https://github.com/hashicorp/terraform/pull/37394#issuecomment-3156669088,repo: hashicorp/terraform | issue: Add support for module locks [Fixes #31301] | keyword: best practice
"@crw for sure! that is definitely a fair concern. and the fact that modules can contain timestamps. i'd updated the MR to hash based on `.tf` and any script files, but that concern makes sense. modules could update themselves.",,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/alex-kattathra-johnson,https://github.com/hashicorp/terraform/pull/37394#issuecomment-3156690698,repo: hashicorp/terraform | issue: Add support for module locks [Fixes #31301] | keyword: best practice
"Route53 Resolver Endpoint Hash Function Prevents Multiple IPs per Subnet When IP Address is Auto-Assigned ### Terraform Version ```shell Terraform v1.9.8 on darwin_arm64 + provider registry.terraform.io/hashicorp/aws v6.7.0 Tested across multiple versions: - AWS Provider v5.0.0 through v6.7.0 (all affected) - Terraform v1.1.9 through v1.9.8 (all affected) ``` ### Terraform Configuration Files ```terraform terraform { required_providers { aws = { source = ""hashicorp/aws"" version = ""~> 5.0"" } } }…",,,,,,Anecdotal,issue,,,,,,,,2025-08-05,github/ffreitas-te,https://github.com/hashicorp/terraform/issues/37404,repo: hashicorp/terraform | keyword: best practice | state: closed
"Hello @ffreitas-te, this appears to be an issue or question with the AWS provider, not with Terraform itself. You can see existing issues and file a new one in their repository here: https://github.com/terraform-providers/terraform-provider-aws/issues. If you have questions about Terraform or the AWS provider, it's better to use the [community forum](https://discuss.hashicorp.com/c/terraform-providers/tf-aws/33) where there are more people ready to help. The GitHub issues here are monitored onl…",,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/crw,https://github.com/hashicorp/terraform/issues/37404#issuecomment-3156592738,repo: hashicorp/terraform | issue: Route53 Resolver Endpoint Hash Function Prevents Multiple IPs per Subnet When IP Address is Auto-Assigned | keyword: best practice
"Storing sensitive values in state files #309 was the first change in Terraform that I could find that moved to store sensitive values in state files, in this case the `password` value for Amazon RDS. This was a bit of a surprise for me, as previously I've been sharing our state files publicly. I can't do that now, and feel pretty nervous about the idea of storing state files in version control at all (and definitely can't put them on github or anything). If Terraform is going to store secrets, …",,,,,,Anecdotal,issue,,,,,,,,2014-10-28,github/seanherron,https://github.com/hashicorp/terraform/issues/516,repo: hashicorp/terraform | keyword: best practice | state: closed
"See #874. I changed the RDS provider to store an SHA1 hash of the password. That said, I'm not sure I'd agree that it's Terraform's responsibility to protect data in the state file. Things other than passwords can be sensitive: for example if I had a security group restricting SSH access to a particular set of hosts, I wouldn't want the world to know which IP they need to spoof to gain access. The state file can be protected orthogonally: you can not put it on github, you can put it in a privat…",,,,,,Anecdotal,comment,,,,,,,,2015-01-28,github/bitglue,https://github.com/hashicorp/terraform/issues/516#issuecomment-71772765,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"Just want to give my opinion on this topic. I do think Terraform should address this issue. I think it will increase the usefulness and ease of use of Terraform. Some examples from other projects: Ansible has [vaults](http://docs.ansible.com/playbooks_vault.html), and on Travis CI you can [encrypt informaton in the `.travis.yml` file](http://docs.travis-ci.com/user/encryption-keys/).",,,,,,Anecdotal,comment,,,,,,,,2015-03-17,github/dentarg,https://github.com/hashicorp/terraform/issues/516#issuecomment-82393512,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
Ansible vaults is a feature I often want in other devops tools. Protecting these details is not as easy as protecting the state file.. what about using consul or Atlas as a remote/backend store? +1 on this,,,,,,Anecdotal,comment,,,,,,,,2015-03-28,github/ketzacoatl,https://github.com/hashicorp/terraform/issues/516#issuecomment-87169626,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"I just want to point out that, according to official documentation, storing the state file in version control is a best practice: https://www.terraform.io/intro/getting-started/build.html > Terraform also put some state into the terraform.tfstate file by default. This state file is extremely > important; it maps various resource metadata to actual resource IDs so that Terraform knows what > it is managing. This file must be saved and distributed to anyone who might run Terraform. **We > recomme…",,,,,,Anecdotal,comment,,,,,,,,2015-05-28,github/dayer4b,https://github.com/hashicorp/terraform/issues/516#issuecomment-106611574,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
:+1: on this idea -- it would be enough for our case to allow configuration of server-side encryption for S3 buckets. Any thoughts on implementing that?,,,,,,Anecdotal,comment,,,,,,,,2015-06-18,github/hobbeswalsh,https://github.com/hashicorp/terraform/issues/516#issuecomment-113320180,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"At the risk of adding scope to this discussion, I think another way to think of this is that Terraform's current architecture is based on a faulty assumption: Terraform assumes that all provider configuration is sensitive and that all resource configuration _isn't_ sensitive. That is wrong in both directions: - Several resources now take passwords as inputs or produce secret values as outputs. In this issue we see the RDS `password` as one example. The potential Vault provider discussed in #222…",,,,,,Anecdotal,comment,,,,,,,,2015-09-11,github/apparentlymart,https://github.com/hashicorp/terraform/issues/516#issuecomment-139688086,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"Hi, any progress on that? Terraform 0.6.3 still stores raw passwords in the state file. Also, as a related issue, if you do not want to keep passwords in configuration, you can create variable without default value. But, this will force you to pass this variable every time you run `plan`/`apply`, even if you're not going to change resource that has this password. I think, it would be nice to separate sensitive stuff from other attributes, so it will: - be stored as sha1 or smth in state file - …",,,,,,Anecdotal,comment,,,,,,,,2015-09-18,github/little-arhat,https://github.com/hashicorp/terraform/issues/516#issuecomment-141424057,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
A good solution for this would be useful for us as well - we're manually configuring certain things to keep them out of the `tfstate` file in the meantime.,,,,,,Anecdotal,comment,,,,,,,,2015-10-19,github/mwarkentin,https://github.com/hashicorp/terraform/issues/516#issuecomment-149285299,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"So as I slowly cobble together another clean-sheet infra with Terraform I see this problem still exists, and this issue is almost exactly 1 year old. What is the thinking in regards to solving this? the ability to mark specific attributes within a resource as sensitive and storing SHA1 or SHA2 hashes of their values in the state for comparison? I see [this comment](https://github.com/hashicorp/terraform/issues/2221#issuecomment-151827879) on a related ticket, does that mean that using Vault wil…",,,,,,Anecdotal,comment,,,,,,,,2015-10-30,github/ascendantlogic,https://github.com/hashicorp/terraform/issues/516#issuecomment-152594825,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"Moving secrets to vault, and using consul-template or integration with other custom solutions you have for CM certainly helps for a lot of cases, but completely avoiding secrets in TF or ending up in TF state is not always reasonable.",,,,,,Anecdotal,comment,,,,,,,,2015-10-30,github/ketzacoatl,https://github.com/hashicorp/terraform/issues/516#issuecomment-152605787,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"Sure, in this particular case I don't want to manually manage RDS but I don't want the PW in the state in cleartext, regardless of where I keep it. I'm sure this is a somewhat common issue. Maybe an overarching ability to mark arbitrary attributes as sensitive is shooting for the moon but a good start would be anything that is obviously sensitive, such as passwords.",,,,,,Anecdotal,comment,,,,,,,,2015-10-30,github/ascendantlogic,https://github.com/hashicorp/terraform/issues/516#issuecomment-152606522,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"Would it be feasible to open up state handling to plugins? The standard could be to store it in files, like it is currently done. Other options could be Vault, S3, Atlas, etc. That way this issue can be dealt with appropriately based on the use-case.",,,,,,Anecdotal,comment,,,,,,,,2015-11-26,github/jfuechsl,https://github.com/hashicorp/terraform/issues/516#issuecomment-159904387,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"I just got tripped up by this as well, as the docs explicitly tell you to store `.tfstate` files in version control, which is problematic if passwords and other secrets end up in the `.tfstate` files. At the bare minimum, the docs should be updated with a massive warning about this. Beyond that, there seem to be a few options: 1. Offer some way to mark variables as secret and either ensure they never get stored in `.tfstate` files or store them in a hashed form. 2. Encrypt the entire `.tfstate`…",,,,,,Anecdotal,comment,,,,,,,,2015-12-12,github/brikis98,https://github.com/hashicorp/terraform/issues/516#issuecomment-164104937,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"One thing to consider around this is output. When you create a resource with secrets (key pair, access keys, db password, etc.), you likely want to show the secret in question at least once (possibly in the stdout of the first run, as `output` do) Currently output are also stored in plain text in the `.tfstate`, and can be retrieved later with `terraform output`. One possible solution would be a mechanism to only show the secrets once, then not store them at all and not show them again (like AW…",,,,,,Anecdotal,comment,,,,,,,,2016-01-05,github/ejoubaud,https://github.com/hashicorp/terraform/issues/516#issuecomment-168928829,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
To get around this for now in my production RDS I just created the instance with a password of `changeme1234` and then went to the console and manually changed the PW.,,,,,,Anecdotal,comment,,,,,,,,2016-01-19,github/ascendantlogic,https://github.com/hashicorp/terraform/issues/516#issuecomment-172901643,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"+1 I notice also that Redshift is imminently going to be supported on terraform, and the same mistakes are being made all over again: ``` master_password - (Required) Password for the master DB user. Note that this may show up in logs, and it will be stored in the state file ``` ""Applications should not transmit or store passwords in unencrypted form"" Page 77 - ISO27001 : https://books.google.co.uk/books?id=Ur1lviHCd-4C&pg=PA77&dq=no+password+unencrypted+disk+iso27001&hl=en&sa=X&ved=0ahUKEwibuM…",,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/gtmtech,https://github.com/hashicorp/terraform/issues/516#issuecomment-173921491,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"@gtmtech a hash isn't cryptographically safe either because it can be reversed. The right solution here is something that can store the value securely, doing anything else IMO would be a waste of energy.",,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/johnrengelman,https://github.com/hashicorp/terraform/issues/516#issuecomment-173926752,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"@gtmtech, if this is a blocker, can you put in a goof password on first run, and then manually update it, as @ascendantlogic notes above? While not ""clean"", and while it ""gives you something to do"", that seems like a reasonable middle ground, no?",,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/ketzacoatl,https://github.com/hashicorp/terraform/issues/516#issuecomment-173932236,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"@johnrengelman forgive me if I am misunderstanding, but I thought hashes were, by definition, one way. Or at least any reasonable use of one to add some level of protection to secrets necessitates the use of a one-way?",,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/ascendantlogic,https://github.com/hashicorp/terraform/issues/516#issuecomment-173933339,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"So problems with the comments above are: - A goof password will mean every time we terraform plan it will advise us it wants to rewire the password - not a great experience when we're always after clean terraform states, but you're right its probably the only workaround we have right now so thanks for the suggestion... - A hash is one-way, and yes if you use SHA1, or even SHA256 you deserve everything you get, but there are other hash functions that are pretty secure - they are one way, and in …",,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/gtmtech,https://github.com/hashicorp/terraform/issues/516#issuecomment-173975311,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"@ascendantlogic ah yeah, sorry, don't know where my brain was this morning. I'm blaming that fact that I hadn't had coffee yet.",,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/johnrengelman,https://github.com/hashicorp/terraform/issues/516#issuecomment-173976634,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"I would recommend bcrypt or even scrypt for the password hashing. In the meantime, a goof password with `ignore_changes=[""master_password""]` will suffice so long as I can get terraform to NOT store it in the state file. A meta_parameter to accomplish not storing a particular atribute would be an alternative to having to do the hashing work - either one could accomplish the end goal",,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/gtmtech,https://github.com/hashicorp/terraform/issues/516#issuecomment-173977805,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
@gtmtech A goof password should not cause terraform states to complain every time. As long as the password in the tf files and the password in the state file are the same terraform should be happy. I created a RDS database using a password in the TF files and after creation went into the TF file and changed it to XXXXX. I also went into the state file and change it to XXXXX. terraform plan and terraform apply are happy.,,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/sstarcher,https://github.com/hashicorp/terraform/issues/516#issuecomment-173977978,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"@sstarcher interesting, what happens when you terraform refresh, does it not override your XXXXXing out of the password?",,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/gtmtech,https://github.com/hashicorp/terraform/issues/516#issuecomment-173978538,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
@gtmtech nothing happens it continues to work happily. The state file still contains XXXXX as terraform has no way of knowing what the actual database password is. I just tested to confirm the following results in no change and my password in the tf state is still XXXXX which is not my actual password - terraform refresh - terraform plan,,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/sstarcher,https://github.com/hashicorp/terraform/issues/516#issuecomment-173979431,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"@gtmtech that is not what happens. I modified the password after the fact in the console and TF is perfectly happy. I think it only wants to change the PW on the RDS resource if the value in the `tf` file doesn't match the `tfstate` file, the AWS API does not return the RDS password for TF to compare against the `tfstate`, that would be madness.",,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/ascendantlogic,https://github.com/hashicorp/terraform/issues/516#issuecomment-173980227,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"lang/funcs: add template function The `templatefile` function is great way ton render template in a consistent way. But so far, we don't have the ability to render dynamic templates in the same way and are forced to use the template_file data source. Here is our use case https://github.com/terraform-aws-modules/terraform-aws-eks/issues/882#issuecomment-629281750. This `template` (we can recall it `templatestring` if needed) function is more generic and take a template as a string and try to ren…",,,,,,Anecdotal,issue,,,,,,,,2020-05-17,github/barryib,https://github.com/hashicorp/terraform/pull/24978,repo: hashicorp/terraform | keyword: best practice | state: closed
[![CLA assistant check](https://cla.hashicorp.com/pull/badge/signed)](https://cla.hashicorp.com/hashicorp/terraform?pullRequest=24978) <br/>All committers have signed the CLA.,,,,,,Anecdotal,comment,,,,,,,,2020-05-17,github/hashicorp-cla,https://github.com/hashicorp/terraform/pull/24978#issuecomment-629779562,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: best practice
# [Codecov](https://codecov.io/gh/hashicorp/terraform/pull/24978?src=pr&el=h1) Report > Merging [#24978](https://codecov.io/gh/hashicorp/terraform/pull/24978?src=pr&el=desc) (d86c215) into [master](https://codecov.io/gh/hashicorp/terraform/commit/be263151e776e84688f1b0648084a1a6f280d78a?el=desc) (be26315) will **increase** coverage by `0.03%`. > The diff coverage is `88.67%`. | [Impacted Files](https://codecov.io/gh/hashicorp/terraform/pull/24978?src=pr&el=tree) | Coverage Δ | | |---|---|---| |…,,,,,,Anecdotal,comment,,,,,,,,2020-05-17,github/codecov[bot],https://github.com/hashicorp/terraform/pull/24978#issuecomment-629780072,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: best practice
Hi @barryib! Thanks for working on this. Not having _dynamic_ templates was an intentional designed decision for `templatefile` after seeing how much confusion and frustration the design of [the `template_file` data source](https://www.terraform.io/docs/providers/template/d/file.html) had caused: folks would frequently get the double-escaping wrong and then be unsure how to interpret the error messages that would result due to them being returned at the Terraform layer instead of at the templat…,,,,,,Anecdotal,comment,,,,,,,,2020-05-18,github/apparentlymart,https://github.com/hashicorp/terraform/pull/24978#issuecomment-630337484,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: best practice
Hi @apparentlymart Thanks for your answer. > Not having dynamic templates was an intentional designed decision for templatefile after seeing how much confusion and frustration the design of the template_file data source had caused: folks would frequently get the double-escaping wrong and then be unsure how to interpret the error messages that would result due to them being returned at the Terraform layer instead of at the template rendering layer. I'm know getting some explanation about why we …,,,,,,Anecdotal,comment,,,,,,,,2020-05-19,github/barryib,https://github.com/hashicorp/terraform/pull/24978#issuecomment-631062552,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: best practice
"Thanks for sharing those additional details, @barryib. If I'm understanding correctly the issue you had with `template_file`, I think it may be addressed by the change in #24904 which is planned to be in the forthcoming Terraform 0.13. That change will allow data sources to be read during the plan phase as long as there's enough information during plan to fully evaluate their configuration. That requirement (that the arguments be known at plan time) is the same as the constraint on the evaluati…",,,,,,Anecdotal,comment,,,,,,,,2020-05-19,github/apparentlymart,https://github.com/hashicorp/terraform/pull/24978#issuecomment-631080890,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: best practice
"@apparentlymart Indeed https://github.com/hashicorp/terraform/pull/24904 would improve the template_file datasource consistency in Terraform 0.13. Thanks for sharing this information. > With that said, I'd like to wait and see how the situation changes with #24904 merged. If that resolves the problem of Terraform pessimistically marking the template result as unknown during planning then I expect we'd choose to resolve this by changing the recommendation in the template_file docs to make an exc…",,,,,,Anecdotal,comment,,,,,,,,2020-05-20,github/barryib,https://github.com/hashicorp/terraform/pull/24978#issuecomment-631733766,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: best practice
"I have not yet tested #24904 with your module and likely will not have time to in the very immediate term, because that PR belongs to a project that other members of the team are working on and so my development focus is currently elsewhere. However, if you'd like to test yourself in the meantime you could potentially build Terraform yourself from the `master` branch, or if you wait a few weeks then Terraform 0.13.0-beta1 will be released and you could test against that. --- That cycle error su…",,,,,,Anecdotal,comment,,,,,,,,2020-05-20,github/apparentlymart,https://github.com/hashicorp/terraform/pull/24978#issuecomment-631747822,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: best practice
"I'll wait Terraform 0.13.0-beta1 for tests. But it'll be for my personal knowledge, because users are still far from using it in production. > The requirement here is that the `value` expression for `output ""user_data_vars""` should not refer to anything that is derived from `var.worker_groups_launch_template`. Modules themselves are not nodes in the dependency graph -- the individual outputs and variables are -- so a `module` block with an argument referring back to itself is valid as long as t…",,,,,,Anecdotal,comment,,,,,,,,2020-05-20,github/barryib,https://github.com/hashicorp/terraform/pull/24978#issuecomment-631798894,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: best practice
"Hello @apparentlymart, it sounds like the template_file data source is now archived https://github.com/hashicorp/terraform-provider-template and superseded by the templatefile function. With that we loose the ability dynamically generate templates (because the template file must be present before terraform runs) Shouldn't we reconsider this PR ?",,,,,,Anecdotal,comment,,,,,,,,2020-09-22,github/barryib,https://github.com/hashicorp/terraform/pull/24978#issuecomment-696545034,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: best practice
"@barryib Since this likely pinged people on-thread, I'll share some information about what's going on with my assignment here for some clarity :) The Terraform team recently took some time to evaluate some popular PRs, and this was one of them. What I'll be doing is evaluating whether or not to recommend this function addition move forward, before (or if at all) recommending updating this pull request to not-conflict with the latest Terraform codebase. There's no timeline attached to this, but …",,,,,,Anecdotal,comment,,,,,,,,2020-11-04,github/pselle,https://github.com/hashicorp/terraform/pull/24978#issuecomment-721867351,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: best practice
Thank you so much @pselle for sharing this information. Really appreciated. I'll continue to update this PR and resolve conflict until a decision is made. Thanks again for your answer. PS: All my pings were about to get more information on the current decisions for this PR. My principal concern was to know if I should continue to work on this and keep updating this PR. And you answered that.,,,,,,Anecdotal,comment,,,,,,,,2020-11-04,github/barryib,https://github.com/hashicorp/terraform/pull/24978#issuecomment-721886085,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: best practice
Just as an additional input for the decision making: Looks like template provider has issues with the brand new terraform 0.14. I've got this issue while passing sensitive variable to template_file: https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/issues/761 Provider is archived so I am not even sure if it is going to be resolved (or even should be resolved).,,,,,,Anecdotal,comment,,,,,,,,2020-12-08,github/nick4fake,https://github.com/hashicorp/terraform/pull/24978#issuecomment-741151774,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: best practice
"Hi, will this PR go somewhere ? Since the template datasource depreciate, it'll be nice to continue to support dynamic templating with this template function.",,,,,,Anecdotal,comment,,,,,,,,2021-05-04,github/barryib,https://github.com/hashicorp/terraform/pull/24978#issuecomment-832197096,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: best practice
"@barryib thank you for contributing this! @pselle, I appreciate you taking to push this PR. Has there been an update from the terraform core team?",,,,,,Anecdotal,comment,,,,,,,,2021-05-12,github/nitrocode,https://github.com/hashicorp/terraform/pull/24978#issuecomment-839880646,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: best practice
"## Changelog Warning Currently this PR would target a v1.13 release. Please add a changelog entry for in the .changes/v1.13 folder, or discuss which release you'd like to target with your reviewer. If you believe this change does not need a changelog entry, please add the 'no-changelog-needed' label.",,,,,,Anecdotal,comment,,,,,,,,2025-05-19,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/24978#issuecomment-2891219668,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: best practice
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-06-27,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/24978#issuecomment-3011151329,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: best practice
"Add the output reference page and align with the other block titles ## CHANGELOG entry <!-- If your change is user-facing, add a short description in a changelog entry. You can use `npx changie new` to create a new changelog entry or manually create a new file in the .changes/unreleasd directory (or .changes/backported if it's a bug fix that should be backported). --> - [ ] This change is user-facing and I added a changelog entry. - [x] This change is not user-facing.",,,,,,Anecdotal,issue,,,,,,,,2025-04-28,github/rkoron007,https://github.com/hashicorp/terraform/pull/36934,repo: hashicorp/terraform | keyword: best practice | state: closed
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-05-30,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/36934#issuecomment-2921054672,repo: hashicorp/terraform | issue: Add the output reference page and align with the other block titles  | keyword: best practice
"Replace custom conditions article with Validate your configuration This is part of our IA work -> making this a usage article around the J2BD of ""validating your configuration"".",,,,,,Anecdotal,issue,,,,,,,,2025-03-27,github/rkoron007,https://github.com/hashicorp/terraform/pull/36793,repo: hashicorp/terraform | keyword: best practice | state: closed
"### 📄 Content Checks Updated: Thu, 27 Mar 2025 17:18:24 GMT <details><summary>Found 7 error(s)</summary> #### `docs/language/mitigate-risk/validate-config.mdx` | Position | Description | Rule | |---|---|---| | `98:361-98:401` | Unexpected folder-relative link found: TODO. Ensure this link is an absolute Developer path. | `ensure-valid-link-format` | | `115:127-115:165` | Unexpected folder-relative link found: TODO. Ensure this link is an absolute Developer path. | `ensure-valid-link-format` | |…",,,,,,Anecdotal,comment,,,,,,,,2025-03-27,github/digital-content-events[bot],https://github.com/hashicorp/terraform/pull/36793#issuecomment-2758855990,repo: hashicorp/terraform | issue: Replace custom conditions article with Validate your configuration | keyword: best practice
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-05-02,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/36793#issuecomment-2846155024,repo: hashicorp/terraform | issue: Replace custom conditions article with Validate your configuration | keyword: best practice
"AzureRM backend support for users to plan without permission to apply <!-- Hi there, Thank you for opening an issue. Please note that we try to keep the Terraform issue tracker reserved for bug reports and feature requests. For general usage questions, please see: https://www.terraform.io/community.html. If your issue relates to a specific Terraform provider, please open it in the provider's own repository. The index of providers is at https://github.com/terraform-providers . --> ### Current Te…",,,,,,Anecdotal,issue,,,,,,,,2021-04-16,github/aidapsibr,https://github.com/hashicorp/terraform/issues/28423,repo: hashicorp/terraform | keyword: best practice | state: closed
This work would also enable `Storage Blob Data Contributor` to be sufficient for running apply and locked plans as an intended side-effect.,,,,,,Anecdotal,comment,,,,,,,,2021-04-16,github/aidapsibr,https://github.com/hashicorp/terraform/issues/28423#issuecomment-821573282,repo: hashicorp/terraform | issue: AzureRM backend support for users to plan without permission to apply | keyword: best practice
"Curious why this has been backlogged so long. The code just has to be adjusted to use RBAC for the operations rather than an account key, and the way it currently exists means each state file must have its own storage account for security since you basically use the ""root password"" for all operations, making RBAC like OIDC meaningless and misleading in the documentation. Users who do not know this esoteric detail are going to think they are safe by putting their state files in separate containe…",,,,,,Anecdotal,comment,,,,,,,,2022-06-01,github/JustinGrote,https://github.com/hashicorp/terraform/issues/28423#issuecomment-1144168971,repo: hashicorp/terraform | issue: AzureRM backend support for users to plan without permission to apply | keyword: best practice
"I'm just the one who opened the issue, our solution was to do the same as you described with separate storage accounts which is really unfortunate. Wish this would be prioritized.",,,,,,Anecdotal,comment,,,,,,,,2022-06-05,github/aidapsibr,https://github.com/hashicorp/terraform/issues/28423#issuecomment-1146898150,repo: hashicorp/terraform | issue: AzureRM backend support for users to plan without permission to apply | keyword: best practice
"This is still a major security issue. If you disable access keys on a storage account, remote state no longer functions. The ability to remove the use of root keys is critical in moving everything to managed identities.",,,,,,Anecdotal,comment,,,,,,,,2022-11-13,github/KoenR3,https://github.com/hashicorp/terraform/issues/28423#issuecomment-1312842382,repo: hashicorp/terraform | issue: AzureRM backend support for users to plan without permission to apply | keyword: best practice
"The latest versions seem to do this correctly, I've been able to not provide an access key and have everything in TFState work with RBAC, so my tfstate for different environments can be two containers in the same storage account and work fine now.",,,,,,Anecdotal,comment,,,,,,,,2022-11-14,github/JustinGrote,https://github.com/hashicorp/terraform/issues/28423#issuecomment-1314076947,repo: hashicorp/terraform | issue: AzureRM backend support for users to plan without permission to apply | keyword: best practice
Yes you do not have to pro ide an access key but when turning off access keys on the storage account the remote state fails...,,,,,,Anecdotal,comment,,,,,,,,2022-11-14,github/KoenR3,https://github.com/hashicorp/terraform/issues/28423#issuecomment-1314336573,repo: hashicorp/terraform | issue: AzureRM backend support for users to plan without permission to apply | keyword: best practice
"This is also the case with managed identities as well, the also need permission to list the storage keys, even if the managed identity has the correct permissions to the blob storage container itself",,,,,,Anecdotal,comment,,,,,,,,2023-06-30,github/nrjohnstone,https://github.com/hashicorp/terraform/issues/28423#issuecomment-1614645839,repo: hashicorp/terraform | issue: AzureRM backend support for users to plan without permission to apply | keyword: best practice
"I would be very happy if TF worked with Storage Blob Data Contributor permissions on the container level. I assumed that's what `use_azuread_auth = true` would do but apparently in reality, it just uses RBAC permissions to get the access key from the account-level and then TF works just like it works without `use_azuread_auth`. Access keys make RBAC useless.",,,,,,Anecdotal,comment,,,,,,,,2023-12-04,github/kimjamia,https://github.com/hashicorp/terraform/issues/28423#issuecomment-1838186913,repo: hashicorp/terraform | issue: AzureRM backend support for users to plan without permission to apply | keyword: best practice
"Does anyone know if OpenTofu has fixed this problem? If not someone should mirror this issue. I'm no longer using Azure, so not the best representative for it. I imagine this will have more traction there. I've given up on hashicorp. I was a paying customer of terraform cloud for the years this was open too...",,,,,,Anecdotal,comment,,,,,,,,2024-03-07,github/aidapsibr,https://github.com/hashicorp/terraform/issues/28423#issuecomment-1982938472,repo: hashicorp/terraform | issue: AzureRM backend support for users to plan without permission to apply | keyword: best practice
"I've been able to successfully use azurerm backend provider with use_azuread_auth to access (read + write) a state file from a container, without permission to listKeys on the storage account. And I've been doing this for a while now, it's actually working with Terraform as old as 1.4.6. I just used a custom role and `use_azuread_auth = true` in backend config. This custom role is assigned to the identity at the storage container scope, so the identity has access only to the container it was as…",,,,,,Anecdotal,comment,,,,,,,,2024-09-19,github/adelinn,https://github.com/hashicorp/terraform/issues/28423#issuecomment-2360655409,repo: hashicorp/terraform | issue: AzureRM backend support for users to plan without permission to apply | keyword: best practice
"I've just completed testing of this issue again. It is perfectly possible to 1. Create a `Storage Blob Data Reader` assignment at the container level you are targeting in the backend (and only this permission) 2. run terraform plan -lock=false and this is sufficient. (A pre-existing tfstate file is required) 3. The storage account in question **has access keys disabled** In my case the azurerm storage account backend is shared and located in a dedicated subscription, Earliest version I tested t…",,,,,,Anecdotal,comment,,,,,,,,2025-03-31,github/audunsolemdal,https://github.com/hashicorp/terraform/issues/28423#issuecomment-2765334013,repo: hashicorp/terraform | issue: AzureRM backend support for users to plan without permission to apply | keyword: best practice
"Thanks for taking the time to submit this issue. It looks like this has been resolved in more recent versions of Terraform (1.2.0 and 1.4.6 like mentioned above). As such, I am going to mark this issue as closed. If that is not the case, please provide additional information including the recent version in which you are not seeing this enhancement, thanks!",,,,,,Anecdotal,comment,,,,,,,,2025-03-31,github/rcskosir,https://github.com/hashicorp/terraform/issues/28423#issuecomment-2767089943,repo: hashicorp/terraform | issue: AzureRM backend support for users to plan without permission to apply | keyword: best practice
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-05-01,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/28423#issuecomment-2843941681,repo: hashicorp/terraform | issue: AzureRM backend support for users to plan without permission to apply | keyword: best practice
"docs: Add best practices for team collaboration in core workflow <!-- Describe in detail the changes you are proposing, and the rationale. See the contributing guide: https://github.com/hashicorp/terraform/blob/main/.github/CONTRIBUTING.md --> <!-- Link all GitHub issues fixed by this PR, and add references to prior related PRs. --> Fixes # ## Target Release <!-- In normal circumstances we only target changes at the upcoming minor release, or as a patch to the current minor version. If you need…",,,,,,Anecdotal,issue,,,,,,,,2025-03-24,github/PanchamKumarr,https://github.com/hashicorp/terraform/pull/36753,repo: hashicorp/terraform | keyword: best practice | state: closed
"[![CLA assistant check](https://cla.hashicorp.com/pull/badge/not_signed)](https://cla.hashicorp.com/hashicorp/terraform?pullRequest=36753) Thank you for your submission! We require that all contributors sign our Contributor License Agreement (""CLA"") before we can accept the contribution. [Read and sign the agreement](https://cla.hashicorp.com/hashicorp/terraform?pullRequest=36753) [Learn more about why HashiCorp requires a CLA and what the CLA includes](https://www.hashicorp.com/cla) <sub>Have …",,,,,,Anecdotal,comment,,,,,,,,2025-03-24,github/hashicorp-cla-app[bot],https://github.com/hashicorp/terraform/pull/36753#issuecomment-2748980479,repo: hashicorp/terraform | issue: docs: Add best practices for team collaboration in core workflow | keyword: best practice
"No description, unclear what the rationale is for adding this content to this page in this location. The content has a tinge of being LLM-generated. @PanchamKumarr, I would consider re-opening this for review if an argument could be made for why this content, in this location.",,,,,,Anecdotal,comment,,,,,,,,2025-03-24,github/crw,https://github.com/hashicorp/terraform/pull/36753#issuecomment-2749095168,repo: hashicorp/terraform | issue: docs: Add best practices for team collaboration in core workflow | keyword: best practice
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-04-24,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/36753#issuecomment-2826022553,repo: hashicorp/terraform | issue: docs: Add best practices for team collaboration in core workflow | keyword: best practice
Write-only argument GA Adding a page for the write-only argument GA. [Jira ticket](https://hashicorp.atlassian.net/browse/IPE-966?atlOrigin=eyJpIjoiN2E1MTY4M2I1ODI3NDk2MGJjYzdkMmM1ZmYxZGQzZGYiLCJwIjoiaiJ9) ## Target Release 1.11 ## Page changes 1. New [write-only argument page](https://terraform-178og2jfp-hashicorp.vercel.app/terraform/language/resources/ephemeral/write-only) 2. Updated [ephemerality in resources overview](https://terraform-178og2jfp-hashicorp.vercel.app/terraform/language/reso…,,,,,,Anecdotal,issue,,,,,,,,2025-02-24,github/rkoron007,https://github.com/hashicorp/terraform/pull/36572,repo: hashicorp/terraform | keyword: best practice | state: closed
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-03-30,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/36572#issuecomment-2764345991,repo: hashicorp/terraform | issue: Write-only argument GA | keyword: best practice
"Ability to preview changes for Refresh Command <!-- Hi there, Thank you for opening an issue. Please note that we try to keep the Terraform issue tracker reserved for bug reports and feature requests. For general usage questions, please see: https://www.terraform.io/community.html. If your issue relates to a specific Terraform provider, please open it in the provider's own repository. The index of providers is at https://github.com/terraform-providers . --> Terraform refresh command modifies th…",,,,,,Anecdotal,issue,,,,,,,,2020-09-02,github/jbigtani,https://github.com/hashicorp/terraform/issues/26093,repo: hashicorp/terraform | keyword: best practice | state: closed
"Hi @jbigtani! Thanks for this feature request. In an earlier issue hashicorp/terraform#15419 I discussed the idea of making it so that changes detected during refresh would be explicit in the UI, for similar reasons as you've mentioned here. I included in a later comment a UI mockup of what such a thing might look like. Some details of it are now outdated because I made that mockup during the 0.12 development phase some time ago, but I think it still gives a general impression of our team's cur…",,,,,,Anecdotal,comment,,,,,,,,2020-09-03,github/apparentlymart,https://github.com/hashicorp/terraform/issues/26093#issuecomment-686763063,repo: hashicorp/terraform | issue: Ability to preview changes for Refresh Command | keyword: best practice
"Hi @apparentlymart! I could not find earlier issue so I had created one. Sorry about it. I looked at your solution and I see it is not implemented yet. Happy to see though that some solution has been thought. I have some feedback though on proposed solution: 1. terraform plan is great command doing its job well. It is already providing many important information like addition, changes and deletion. Why overload this command with more functionality? This would make it difficult to read plan outp…",,,,,,Anecdotal,comment,,,,,,,,2020-09-04,github/jbigtani,https://github.com/hashicorp/terraform/issues/26093#issuecomment-686969545,repo: hashicorp/terraform | issue: Ability to preview changes for Refresh Command | keyword: best practice
"Hey there, I don't actually work on Terraform anymore, but as I continue to clear out a random pile of old things I had previously participated in but were not closed out I _think_ today's Terraform has a feature sufficient to close this issue. Instead of running `terraform refresh`, you can run `terraform apply -refresh-only`. That activates one of Terraform's other [planning modes](https://developer.hashicorp.com/terraform/cli/commands/plan#planning-modes) -- ""Refresh-only"" mode -- which effe…",,,,,,Anecdotal,comment,,,,,,,,2025-01-28,github/apparentlymart,https://github.com/hashicorp/terraform/issues/26093#issuecomment-2617489040,repo: hashicorp/terraform | issue: Ability to preview changes for Refresh Command | keyword: best practice
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-03-07,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/26093#issuecomment-2705361872,repo: hashicorp/terraform | issue: Ability to preview changes for Refresh Command | keyword: best practice
"1.10.0 breaks assume role for AWS provider ### Terraform Version ```shell Terraform v1.10.0 on darwin_arm64 + provider registry.terraform.io/hashicorp/aws v4.45.0 + provider registry.terraform.io/hashicorp/vault v3.11.0 ``` ### Terraform Configuration Files ```terraform provider ""aws"" { region = var.region assume_role { role_arn = ""arn:aws:iam::xxxxxxxxxxxx:role/OrganizationAccountAccessRole"" } } ``` ### Debug Output https://gist.github.com/cmmarslender/98a7f7dac6b6b80a4192ee1f5f03e72b ### Expe…",,,,,,Anecdotal,issue,,,,,,,,2024-11-27,github/cmmarslender,https://github.com/hashicorp/terraform/issues/36124,repo: hashicorp/terraform | keyword: best practice | state: closed
"Hi @cmmarslender, Thanks for filing the issue. I don't see any reason that Terraform would interpret the provider configuration differently in v1.10, and I can validate the given provider config with all recent versions. Can you show the exact, complete config file and error that you receive that file? Perhaps there's some preceding syntax which is causing an error.",,,,,,Anecdotal,comment,,,,,,,,2024-11-27,github/jbardin,https://github.com/hashicorp/terraform/issues/36124#issuecomment-2504769681,repo: hashicorp/terraform | issue: 1.10.0 breaks assume role for AWS provider | keyword: best practice
"Looking further at the config, I realized we have another instance of `role_arn`, and I was possibly looking at the wrong instance, assuming we only had to assume a role once. The problem is probably with the `role_arn` in the s3 backend configuration: ```terraform { backend ""s3"" { bucket = ""bucket-name"" key = ""prefix/terraform.state"" region = ""us-west-2"" role_arn = ""arn:aws:iam::xxxxxxxxxxxx:role/OrganizationAccountAccessRole"" } }``` Looking at docs for that, seems like at some point that was …",,,,,,Anecdotal,comment,,,,,,,,2024-11-27,github/cmmarslender,https://github.com/hashicorp/terraform/issues/36124#issuecomment-2504789530,repo: hashicorp/terraform | issue: 1.10.0 breaks assume role for AWS provider | keyword: best practice
"Ok yeah - wrapping the role_arn in the s3 backend in the `assume_role = { ... }` block solved the problem. I think the docs still show the `role_arn` as supported but deprecated, so may need to remove it completely from docs.",,,,,,Anecdotal,comment,,,,,,,,2024-11-27,github/cmmarslender,https://github.com/hashicorp/terraform/issues/36124#issuecomment-2504811970,repo: hashicorp/terraform | issue: 1.10.0 breaks assume role for AWS provider | keyword: best practice
"Our company ran into this same problem, stalling pipelines in tons of services due to a shared module. I think the larger issue here is that ""planned to be deprecated"" values were removed in a minor update, thus a Breaking Change. Since 1.x I would expect this to be more stable and for minor releases not to break our code if we are using a `< 2.0` version range. https://developer.hashicorp.com/terraform/language/v1-compatibility-promises To me this seems like a Breaking Change that does not fol…",,,,,,Anecdotal,comment,,,,,,,,2024-11-28,github/sean6bucks,https://github.com/hashicorp/terraform/issues/36124#issuecomment-2505611596,repo: hashicorp/terraform | issue: 1.10.0 breaks assume role for AWS provider | keyword: best practice
"Aside from the breaking change, did the deprecation itself even follow the compatibility promises? Running plan and apply with 1.9.x terraform yielded zero deprecation warnings on our hundreds of terraform roots with s3 backends declared in the `terraform` block. It's interesting that the `terraform` block was ignored when `data ""terraform_remote_state""` with s3 backend caused plenty of deprecation warnings in the same terraform versions.",,,,,,Anecdotal,comment,,,,,,,,2024-12-04,github/Indigenuity,https://github.com/hashicorp/terraform/issues/36124#issuecomment-2518518885,repo: hashicorp/terraform | issue: 1.10.0 breaks assume role for AWS provider | keyword: best practice
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-01-04,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/36124#issuecomment-2570006793,repo: hashicorp/terraform | issue: 1.10.0 breaks assume role for AWS provider | keyword: best practice
"Backport of Remove future-facing statement from import page into v1.2 ## Backport This PR is auto-generated from #31686 to be assessed for backporting due to the inclusion of the label 1.2-backport. The below text is copied from the body of the original PR. --- A user recently pointed out that we make a vague statement about improving a workflow ""in the future"" on this docs page. Best practices and our style guide dictate that we do not include statements like this in documentation. This PR rem…",,,,,,Anecdotal,issue,,,,,,,,2022-08-25,github/teamterraform,https://github.com/hashicorp/terraform/pull/31689,repo: hashicorp/terraform | keyword: best practice | state: closed
"Reminder for the merging maintainer: if this is a user-visible change, please update the changelog on the appropriate release branch.",,,,,,Anecdotal,comment,,,,,,,,2022-08-25,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/31689#issuecomment-1227431373,repo: hashicorp/terraform | issue: Backport of Remove future-facing statement from import page into v1.2 | keyword: best practice
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2022-09-25,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/31689#issuecomment-1257106529,repo: hashicorp/terraform | issue: Backport of Remove future-facing statement from import page into v1.2 | keyword: best practice
"Fast-changing terraform modules - tracking module git commit? Consider this terraform workflow: A team of people are updating a centralised terraform module repo - making frequent improvements. Another team of people are consumers of these modules, deploying infrastructure sets using these modules. These environments are long-lived. The consumers have no write access to the module codebase. 1. Consumer team member 1 deploys an environment, from HEAD/master of the modules, commits the `tfstate` …",,,,,,Anecdotal,issue,,,,,,,,2015-12-09,github/james-masson,https://github.com/hashicorp/terraform/issues/4234,repo: hashicorp/terraform | keyword: best practice | state: closed
"Coincidentally I had just the same thought a few hours ago and was pleasantly surprised to find it already written up in detail when I got here. Thanks! My specific issue (which is a subset of what you described, I think) is that if you want to destroy an infrastructure it's necessary to use a very similar config than was used to create it. We have a bunch of similar infrastructures built from the same config (using variables and multiple remote states) and today I was cleaning up a number of t…",,,,,,Anecdotal,comment,,,,,,,,2015-12-10,github/apparentlymart,https://github.com/hashicorp/terraform/issues/4234#issuecomment-163472669,repo: hashicorp/terraform | issue: Fast-changing terraform modules - tracking module git commit? | keyword: best practice
"We are facing a very similar problem with our use case. Fortunately in our case, the team writing to Git repo are aware of the downstream consumers problem and have been creating tags. We have another tool that generates the module stubs, and populates the module source ref field with the latest tag.",,,,,,Anecdotal,comment,,,,,,,,2016-01-12,github/sl1pm4t,https://github.com/hashicorp/terraform/issues/4234#issuecomment-170769781,repo: hashicorp/terraform | issue: Fast-changing terraform modules - tracking module git commit? | keyword: best practice
"So any updates for this issue, can we have a lock file , similar as ansible galaxy's `requirements.yml`, that easily lock to commit/version/tag when source the module in terraform?",,,,,,Anecdotal,comment,,,,,,,,2017-02-24,github/ozbillwang,https://github.com/hashicorp/terraform/issues/4234#issuecomment-282212732,repo: hashicorp/terraform | issue: Fast-changing terraform modules - tracking module git commit? | keyword: best practice
"👍 +1 Dependency resolution is a giant pain, and what works today should work 6-12 months from now, short of a provider deprecating APIs. The module that successfully deployed the infrastructure should be the same version, same code, as when you have to update or apply it 6 months from now. We already commit our vendor dependencies with our Golang projects because of the inability to lock dependencies at a version (you can but it is a pain and if the upstream dev deletes the code, you're hosed u…",,,,,,Anecdotal,comment,,,,,,,,2018-05-22,github/ooglek,https://github.com/hashicorp/terraform/issues/4234#issuecomment-390854571,repo: hashicorp/terraform | issue: Fast-changing terraform modules - tracking module git commit? | keyword: best practice
"This is still desired in TF 0.12+ I want to be able to know what git commits terraform checked out. I have terraform monorepos with modules in them getting used in CI. The commits are regularly changing between each run. I don't know what commit terraform is going to use until it runs, and even then, I don't know what commit it uses. I can target specific commit sha's for e.g. production environments, but it makes promotion much more manual (and since we still can't dynamically set the commit s…",,,,,,Anecdotal,comment,,,,,,,,2019-09-14,github/eedwards-sk,https://github.com/hashicorp/terraform/issues/4234#issuecomment-531435595,repo: hashicorp/terraform | issue: Fast-changing terraform modules - tracking module git commit? | keyword: best practice
"Any updates on this front? I'm trying to figure out the best way to expose the version of the module source used to configure some infrastructure, and this seems like the most straightforward.",,,,,,Anecdotal,comment,,,,,,,,2020-05-29,github/sfc-gh-tbacon,https://github.com/hashicorp/terraform/issues/4234#issuecomment-636100862,repo: hashicorp/terraform | issue: Fast-changing terraform modules - tracking module git commit? | keyword: best practice
"@sfc-gh-tbacon no updates - our current priority is wrapping up 0.13 work and finding/fixing bugs in the new functionality we're introducing, and no work related to this enhancement request is going into 0.13.0.",,,,,,Anecdotal,comment,,,,,,,,2020-05-29,github/danieldreier,https://github.com/hashicorp/terraform/issues/4234#issuecomment-636102859,repo: hashicorp/terraform | issue: Fast-changing terraform modules - tracking module git commit? | keyword: best practice
"Closing as duplicate of https://github.com/hashicorp/terraform/issues/31301 Despite this issue being older, the more current discussion and thinking on this topic is in the above-referenced issue. Module pinning is not yet implemented, and that seems to be the extant feature request being specified in this issue. Thanks!",,,,,,Anecdotal,comment,,,,,,,,2024-08-09,github/crw,https://github.com/hashicorp/terraform/issues/4234#issuecomment-2278534876,repo: hashicorp/terraform | issue: Fast-changing terraform modules - tracking module git commit? | keyword: best practice
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2024-09-10,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/4234#issuecomment-2339472332,repo: hashicorp/terraform | issue: Fast-changing terraform modules - tracking module git commit? | keyword: best practice
"Research Prototype: Finer-grain Concurrency Control for Resource Instance nodes This PR is not intended to be merged, and is instead just a snapshot of some experimentation I've been working on. For a long time now we've been interested in the idea of making batch requests to providers in various cases, so that we can reduce the number of roundtrips required when an underlying API allows describing multiple operations in a single request. (For older discussion on that, refer to https://github.c…",,,,,,Anecdotal,issue,,,,,,,,2024-06-27,github/apparentlymart,https://github.com/hashicorp/terraform/pull/35393,repo: hashicorp/terraform | keyword: best practice | state: closed
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2024-08-01,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/35393#issuecomment-2261813985,repo: hashicorp/terraform | issue: Research Prototype: Finer-grain Concurrency Control for Resource Instance nodes | keyword: best practice
"Add Learn link to Debugging Contains a Learn link to the Troubleshooting tutorial. Links to the ""best practices for bug reporting"" section.",,,,,,Anecdotal,issue,,,,,,,,2021-04-19,github/tr0njavolta,https://github.com/hashicorp/terraform/pull/28443,repo: hashicorp/terraform | keyword: best practice | state: closed
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2021-05-23,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/28443#issuecomment-846490273,repo: hashicorp/terraform | issue: Add Learn link to Debugging | keyword: best practice
"[Docs] Migrate ""Phases of Terraform Adoption"" from tutorials to docs <!-- Describe in detail the changes you are proposing, and the rationale. See the contributing guide: https://github.com/hashicorp/terraform/blob/main/.github/CONTRIBUTING.md --> Currently the ""[Phases of Terraform Adoption](https://developer.hashicorp.com/well-architected-framework/operational-excellence/operational-excellence-terraform-maturity)"" content lives in the tutorials as part of the Well-Architected Framework. We ar…",,,,,,Anecdotal,issue,,,,,,,,2024-05-28,github/BrianMMcClain,https://github.com/hashicorp/terraform/pull/35251,repo: hashicorp/terraform | keyword: best practice | state: closed
"Reminder for the merging maintainer: if this is a user-visible change, please update the changelog on the appropriate release branch.",,,,,,Anecdotal,comment,,,,,,,,2024-05-29,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/35251#issuecomment-2137488253,"repo: hashicorp/terraform | issue: [Docs] Migrate ""Phases of Terraform Adoption"" from tutorials to docs | keyword: best practice"
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2024-06-29,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/35251#issuecomment-2197854339,"repo: hashicorp/terraform | issue: [Docs] Migrate ""Phases of Terraform Adoption"" from tutorials to docs | keyword: best practice"
"Proposal: State Encryption Currently we have several resources that retrieve or generate secrets, and for any where these secrets are used to populate other resources or configure other providers these secrets must necessarily be stored in the state. Such resources include: - `aws_db_cluster` (password attribute) - `azurerm_virtual_machine` (machine login passwords) - `tls_private_key` - `vault_generic_secret` (both managed resource and data source) (#9158) - ...and many other resources that us…",,,,,,Anecdotal,issue,,,,,,,,2016-10-24,github/apparentlymart,https://github.com/hashicorp/terraform/issues/9556,repo: hashicorp/terraform | keyword: best practice | state: closed
"what is the status here ? From my point of view, there should be no link between config parameters (passwords) and state files. I still see a need to have the state file encrypted. I also think there should be NO password in the state file, but at least a pointer to where to find the password. I also don't understant people ""sharing"" the state file... If you have a need to share something, maybe that's something to be added to Terraform. The state file is an ""internal"" view of the currently run…",,,,,,Anecdotal,comment,,,,,,,,2017-02-06,github/prune998,https://github.com/hashicorp/terraform/issues/9556#issuecomment-277709229,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"The forthcoming version 0.9 contains some reworking of Terraform's handling of states that will, amongst other things, make this easier to implement in a future release. I can't say exactly when that will be (I don't have visibility into the official roadmap) but the technical blockers on this will be much diminished once 0.9 is released. --- I suppose it's worth noting that the usage examples in my original proposal here are no longer valid with the changes in 0.9. Instead of configuring encry…",,,,,,Anecdotal,comment,,,,,,,,2017-02-06,github/apparentlymart,https://github.com/hashicorp/terraform/issues/9556#issuecomment-277847071,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"@mkuzmin in principle that is possible, but I've seen the Vault team recommend against storing non-trivially-sized things in Vault's generic backend, and instead to use the transit backend to encrypt for storage elsewhere. That recommendation is what this design was based on.",,,,,,Anecdotal,comment,,,,,,,,2017-02-07,github/apparentlymart,https://github.com/hashicorp/terraform/issues/9556#issuecomment-278045191,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
@apparentlymart https://github.com/hashicorp/terraform/issues/9556#issuecomment-277847071 is this supported in 0.9 release,,,,,,Anecdotal,comment,,,,,,,,2017-03-21,github/frezbo,https://github.com/hashicorp/terraform/issues/9556#issuecomment-288162157,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"as discussed at Hashidays NY with @phinze I'm also interested in this feature of encrypting state before saving it to remote location. There are already open source projects that allow to encrypt JSON files with AWS KMS keys: https://github.com/agilebits/sm https://github.com/mozilla/sops Manual workflow could be: - save state locally, encrypt - upload to remote state on second machine: - fetch encrypted file - decrypt and import as local state.",,,,,,Anecdotal,comment,,,,,,,,2017-05-21,github/stumyp,https://github.com/hashicorp/terraform/issues/9556#issuecomment-302913160,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
i was considering writing a consul http proxy that you could use as a consul backend for tf. encrypted/decrypted all the values through vault transit. not sure if it's worth it now. depends on the timing. (can i get it ballparked at all without calling in a support contract?) i like the new direction for encryption provider. it's the same adapter pattern but internalized and simplified. the consul sharing works (should work - good idea!) for my team. but consul could be (or seem like) a barrier…,,,,,,Anecdotal,comment,,,,,,,,2017-05-26,github/automaticgiant,https://github.com/hashicorp/terraform/issues/9556#issuecomment-304156905,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"I'd like to see this feature implemented in core, along with other encryption efforts. In the mean time, I came across this tool: [terrahelp](https://github.com/opencredo/terrahelp)",,,,,,Anecdotal,comment,,,,,,,,2017-11-09,github/mcameron,https://github.com/hashicorp/terraform/issues/9556#issuecomment-343123567,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
@apparentlymart What's the status on this? Would this get accepted into terraform if I would implement it? Or are there any technical blockers on this?,,,,,,Anecdotal,comment,,,,,,,,2018-09-14,github/simonre,https://github.com/hashicorp/terraform/issues/9556#issuecomment-421252774,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"Hi @simonre! The architecture of ""backends"" in Terraform changed significantly since I originally proposed this (which was before I was a HashiCorp employee), so I expect we'll need to do another round of design work before deciding what is the right thing to do here. There has also been some disagreement in subsequent discussions about whether whole-state encryption is actually what's needed here, or whether encryption of specific sensitive values is actually the requirement: whole-state encry…",,,,,,Anecdotal,comment,,,,,,,,2018-09-14,github/apparentlymart,https://github.com/hashicorp/terraform/issues/9556#issuecomment-421421504,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"Hey, has there been any progress on this issue? We've been using the Consul backend and would like to start integrating our Terraform workflows with Vault, but leaking secrets to the unencrypted Consul backend more or less makes this a no-go.",,,,,,Anecdotal,comment,,,,,,,,2019-07-12,github/eripa,https://github.com/hashicorp/terraform/issues/9556#issuecomment-510950273,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"Thanks for the detailed proposal @apparentlymart, I know this has been open a while. I have a few things to add that may be of some use. Restricting access to cloud storage of choice is probably the best way to get at rest encryption and RBAC controls. There are still situations where bring-your-own-key still makes sense (e.g. non-cloud usage or app.terraform.io). Also, some companies want KEK support on all storage or on highly sensitive storage (which TF state is). A solution could leverage t…",,,,,,Anecdotal,comment,,,,,,,,2019-10-16,github/mbrancato,https://github.com/hashicorp/terraform/issues/9556#issuecomment-542496389,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"The problem with at-rest encryption provided by some cloud provider storage services like Azure Storage Accounts is that it is transparent encryption. When the encryption/decryption is transparently handled by the cloud provider it really just becomes a proxy for RBAC. While transparent at-rest encryption provides protection against some threats/risks (eg hard drive is stolen or lost from the cloud provider's facility), it provides no protection against much more common threats (eg trusted oper…",,,,,,Anecdotal,comment,,,,,,,,2019-10-16,github/jamesrcounts,https://github.com/hashicorp/terraform/issues/9556#issuecomment-542694978,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"Given that a lot of my focus recently has been on compliance, building in the ability to encrypt and decrypt the tfstate with a given kms key say, doesn't seem like it would be a lot of work and massively reduce the need for wrapper scripts. However, I do see a need for a user to be able to decrypt this state file manually in order to perform state surgery. I'd love to see this feature in terraform.",,,,,,Anecdotal,comment,,,,,,,,2020-02-07,github/sasq31,https://github.com/hashicorp/terraform/issues/9556#issuecomment-583381613,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"I agree with @jamesrcounts. I would essentially want a good way for the entire tfstate file to be an encrypted blob, because storing inside an S3 bucket isn't enough. The S3 bucket could be compromised by a phishing attack or a malicious insider. Essentially having to work around the unencrypted blob through permission controls. I know https://github.com/opencredo/terrahelp exists, that kind of, makes the encryption of the tfstate a 2 step thing, but I don't see why it couldn't be combined with…",,,,,,Anecdotal,comment,,,,,,,,2020-02-28,github/michaelasper,https://github.com/hashicorp/terraform/issues/9556#issuecomment-592663237,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"I have implemented a possible solution that should be able to transparently encrypt state for all remote storage backend providers, see draft PR https://github.com/hashicorp/terraform/pull/28278. Successfully tested it with the azurerm backend today.",,,,,,Anecdotal,comment,,,,,,,,2021-04-06,github/StephanHCB,https://github.com/hashicorp/terraform/issues/9556#issuecomment-814252558,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"> I have implemented a possible solution that should be able to transparently encrypt state for all remote storage backend providers, see draft PR #28278. Successfully tested it with the azurerm backend today. @StephanHCB nice work!",,,,,,Anecdotal,comment,,,,,,,,2021-04-06,github/binlab,https://github.com/hashicorp/terraform/issues/9556#issuecomment-814388705,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
Following up on the discussion on https://github.com/hashicorp/terraform/pull/21558 and a suggestion by @allantargino: **proposal how to implement client side remote state encryption** (with prototype code) The central element of the proposal is the introduction of a **state crypto provider** that transparently encrypts the statefile contents before they are sent to the remote backend. ``` // StateCryptoProvider is the interface that must be implemented for a transparent remote state // encrypt…,,,,,,Anecdotal,comment,,,,,,,,2021-04-25,github/StephanHCB,https://github.com/hashicorp/terraform/issues/9556#issuecomment-826341937,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"I am working on a project for a large financial institution and we do use Terraform. However, everytime when it comes to security meetings, I have a hard time explaining to them that Terraform does not come with Terraform statefile encryption. Even though we can use Azure Storage encryption to bypass this problem, it only migrates the risk to Azure RBAC as @jamesrcounts put it. It comes to me as a surprise that statefile encryption has not been prioritised before, even though enterprise clients…",,,,,,Anecdotal,comment,,,,,,,,2021-08-04,github/byteknacker,https://github.com/hashicorp/terraform/issues/9556#issuecomment-892400844,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
Any news on my pull request that implements remote state encryption? https://github.com/hashicorp/terraform/pull/28603 @jbardin you joined the PR. Is there something I am supposed to be doing/can do? It's now been over two months of no reaction from maintainers.,,,,,,Anecdotal,comment,,,,,,,,2021-08-06,github/StephanHCB,https://github.com/hashicorp/terraform/issues/9556#issuecomment-894193929,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"@StephanHCB I love your PR, it's far better than what I could have come up with. I've Approved it, but I'm not sure if that's at all meaningful, since I'm not a maintainer.",,,,,,Anecdotal,comment,,,,,,,,2021-08-16,github/maludwig,https://github.com/hashicorp/terraform/issues/9556#issuecomment-899730022,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"As @stumyp suggested a nice JSON encryption popular project exist [Mozilla SOPS](https://github.com/mozilla/sops) allowing encryption of defined values leaving tfstate json syntactically untached for backends. SOPS is a go language open source project supporting encryption using AWS KMS, GCP KMS, Azure Key Vault, age, and PGP (and at the same time more than one to make decryption robust). SOPS is already integrated with Terraform as resource provider ([sops Provider](https://registry.terraform.…",,,,,,Anecdotal,comment,,,,,,,,2022-01-13,github/Roxyrob,https://github.com/hashicorp/terraform/issues/9556#issuecomment-1011607151,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"@Roxyrob I think you make a good point. But I don't think it matters much because if I were to venture a guess I would say that in Hashicorp's philosophy they ""trust"" cloud providers. To a degree I can understand that point of view; If you are already running everything in their cloud (VM's, etc.) encrypting your statefile before uploading it isn't going to give you that much extra security. So from their point of view it might help against such one of a kind Amazon incident but other then that…",,,,,,Anecdotal,comment,,,,,,,,2022-01-13,github/siepkes,https://github.com/hashicorp/terraform/issues/9556#issuecomment-1011927157,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"@siepkes I undertand what you suppose but I think data security is primary concern for every tools and Terraform is a so great one that cannot neglet also if there isn't a simple implementation solution. Probably I'm wrong but IMHO any piece of data especially if sensible by nature as many data in tfstate are or potentially can be, should be managed following `security by design` and `security by default` principles. Data Security cannot be leaved to external factors. Without this basic assumpt…",,,,,,Anecdotal,comment,,,,,,,,2022-01-13,github/Roxyrob,https://github.com/hashicorp/terraform/issues/9556#issuecomment-1012167517,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"The issue is that ALL of these are making a ""copy"" of credentials. That is just wrong right out of the gate. Especially when it's using something like vault - it should be storing a reference to the credentials to be loaded when applied. If Terraform were operating as a client-server type implementation, where the state had to be independently accessed by a service on some other box in order to apply the state - then yeah, I could understand the current behavior -- but it's being 100% driven by…",,,,,,Anecdotal,comment,,,,,,,,2022-01-13,github/nneul,https://github.com/hashicorp/terraform/issues/9556#issuecomment-1012193394,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"@FernandoMiguel consul or every other solution does not change the context. Tfstate is always in cleartext somewhere, and someone can access the file and so secrets inside (at least if you do not take all on a server in a private room detached from networks and always watched). Sops like logic instead allow you to save JSON file (and so potentially tfstate json too) only with values (all or some) encrypted using e.g. AWS KMS CMK. Such an approach increse security (and probably sufficient risk m…",,,,,,Anecdotal,comment,,,,,,,,2022-01-13,github/Roxyrob,https://github.com/hashicorp/terraform/issues/9556#issuecomment-1012271024,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"@nneul a tfstate cleartext problem mitigation can be reached if we do not undermining probably basic principles of Terraform behavior and also: having encrypted value in tfatste file (or in other better solution for that purpose - like consul, and so on) can be potencially useful (to share between different terraform configs or different DevOps tools in the pipeline). I think that in cloud era we cannot avoid that secrets can be ""a little out of control"" (e.g. we will never have total control o…",,,,,,Anecdotal,comment,,,,,,,,2022-01-13,github/Roxyrob,https://github.com/hashicorp/terraform/issues/9556#issuecomment-1012287825,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"Frankly, I'm quite shocked and surprised that Hashicorp haven't placed a higher priority on providing a mechanism for encryption of the tfstate at rest. In many organisations, these state files contain the 'keys to the kingdom' and a comprehensive map of their infrastructure and should be considered highly sensitive. Knowing that most large companies use Terraform, and that there's a good chance that many/most will use the Amazon S3 backend, if I were intent on compromising one of them I would …",,,,,,Anecdotal,comment,,,,,,,,2022-01-15,github/rossigee,https://github.com/hashicorp/terraform/issues/9556#issuecomment-1013569833,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
build: Set up Dependabot to manage HashiCorp-owned GitHub Actions versioning This sets up Dependabot to manage version updates for HashiCorp-owned GitHub Actions; all other (third-party) version updates will be handled by HashiCorp's internal TSCCR tooling. TSCCR does not/cannot manage HashiCorp-owned Actions versioning which is why we need to bring in Dependabot to handle this component; see [this memo](https://hermes.hashicorp.services/document/151w0ldYS3Z3arAiSTRuFImV73VabC0rhZJPCv6fzl4Q) (i…,,,,,,Anecdotal,issue,,,,,,,,2024-05-07,github/xiehan,https://github.com/hashicorp/terraform/pull/35124,repo: hashicorp/terraform | keyword: best practice | state: closed
"Reminder for the merging maintainer: if this is a user-visible change, please update the changelog on the appropriate release branch.",,,,,,Anecdotal,comment,,,,,,,,2024-05-08,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/35124#issuecomment-2100232670,repo: hashicorp/terraform | issue: build: Set up Dependabot to manage HashiCorp-owned GitHub Actions versioning | keyword: best practice
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2024-06-08,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/35124#issuecomment-2155757487,repo: hashicorp/terraform | issue: build: Set up Dependabot to manage HashiCorp-owned GitHub Actions versioning | keyword: best practice
"Feature: Allow circular dependencies in resources ### Current Terraform Version ``` v0.14.0-rc1 ``` ### Use-cases * I have a aws_cognito_user_pool resource * On this resource there is a block where you can configure the ARN of Lambda triggers * I want to configure a Lambda trigger to have the Cognito user pool ID as an environment variable When I configure this in Terraform, it obviously doesn't work. I get: ``` $ terraform apply Error: Cycle: module.auth.aws_lambda_function.presignup, module.a…",,,,,,Anecdotal,issue,,,,,,,,2020-12-08,github/dansimau,https://github.com/hashicorp/terraform/issues/27188,repo: hashicorp/terraform | keyword: lesson learned | state: open
"Hi @dansimau! Thanks for sharing this use-case. As you've noted, the typical way to deal with this today is for the provider to explain to Terraform that ""update cognito user pool to add lambda trigger"" is a separate operation by representing it as a separate resource. That creates a relatively easy to explain execution model: there is only one action for each resource per plan (with the special exception of ""replace"", which is internally a combined destroy/create), and the ordering of those ac…",,,,,,Anecdotal,comment,,,,,,,,2020-12-09,github/apparentlymart,https://github.com/hashicorp/terraform/issues/27188#issuecomment-741438193,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"Thanks for the considered reply @apparentlymart. > we'll need several more examples of similar problems in order to start to analyze what they all have in common and thus how the problem might generalize. Indeed, I'd be interested to know how often this comes up. Judging by the fact that nobody filed an issue before, maybe not as much as I originally assumed when I hit this use case.",,,,,,Anecdotal,comment,,,,,,,,2020-12-09,github/dansimau,https://github.com/hashicorp/terraform/issues/27188#issuecomment-741782123,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"I don't have a proposed solution, but I can provide another example. This circular dependency scenario happens in the AzureRM provider with app services and Azure-managed SSL certificates. The azurerm_app_service resource can be given a custom hostname and SSL certificate via the azurerm_app_service_custom_hostname_binding resource. You normally specify the SSL certificate to use via the 'fingerprint' attribute, which is the SSL fingerprint of the desired certificate. If you wish to use a free …",,,,,,Anecdotal,comment,,,,,,,,2020-12-10,github/okaros,https://github.com/hashicorp/terraform/issues/27188#issuecomment-742778215,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"There's also the more general use-case of wanting to build resources that communicate with each other in Terraform. Consider: You want to create two Azure App Services that need to communicate with each other via connection strings configured in their environment variables. This is a circular dependency in Terraform, regardless of provider/platform, since you cannot have their resource objects reference each other unless they already exist, and they don't. You can cheat this in any number of wa…",,,,,,Anecdotal,comment,,,,,,,,2020-12-10,github/okaros,https://github.com/hashicorp/terraform/issues/27188#issuecomment-742832953,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"That's an interesting idea, @okaros, and reminds me a bit of functional reactive programming where programs react to events by merging the event data in with a previous value. It does seem like an idea worth researching in some more detail. Some initial thoughts I have for questions to consider would be: * How might this work if the value to be updated later is in a nested block rather than a top-level attribute? That would presumably require a way to refer to a specific nested block, and neste…",,,,,,Anecdotal,comment,,,,,,,,2020-12-12,github/apparentlymart,https://github.com/hashicorp/terraform/issues/27188#issuecomment-743693171,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"I don't have full answers, @apparentlymart, but some thoughts from my end-user perspective : An initial or even final implementation might simply say ""You can't do that"" with regards to nested blocks that aren't addressable, or even nested blocks altogether. Other Terraform functionality has limitations on what can be interacted with (""destroy"" provisioners come immediately to mind as an example of something with heavy restrictions), so such limitations wouldn't be unprecedented. A solution tha…",,,,,,Anecdotal,comment,,,,,,,,2020-12-12,github/okaros,https://github.com/hashicorp/terraform/issues/27188#issuecomment-743702264,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
@okaros I know it doesn't solve this particular issue but I feel in needs pointing out that the event data passed to the lambda trigger does include the Cognito user pool id.,,,,,,Anecdotal,comment,,,,,,,,2021-04-26,github/lmmattr,https://github.com/hashicorp/terraform/issues/27188#issuecomment-826795753,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"> we'll need several more examples of similar problems in order to start to analyze what they all have in common and thus how the problem might generalize I have two Okta orgs managed via https://registry.terraform.io/providers/oktadeveloper/okta and I want to set up a SAML based trust between them. Creating the resources in step 1 and 2 generates unique identifiers that must be exchanged, and cannot be known in advance. Steps: 1. Create resource okta_saml_idp in org1 2. Create resource okta_ap…",,,,,,Anecdotal,comment,,,,,,,,2021-05-05,github/jeffg-archive,https://github.com/hashicorp/terraform/issues/27188#issuecomment-832834450,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"> we'll need several more examples of similar problems in order to start to analyze what they all have in common and thus how the problem might generalize AWS Transit Gateway provides routing between multiple VPCs, replacing VPC Peering. Setting this up involves circular dependencies because the TGW must be explicitly attached to the VPCs (requiring knowledge of the `vpc_id`) while the VPCs must setup routes through the TGW (requiring knowledge of the `ec2_transit_gateway_id`). It makes a lot o…",,,,,,Anecdotal,comment,,,,,,,,2021-06-22,github/dzrtc,https://github.com/hashicorp/terraform/issues/27188#issuecomment-866168400,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"I've been working around this issue using a blue/green and dev environments for my app, but then I ran into an AWS issue that left appsync domains in a state where it was unusable for hours (seperate Terraform issue regarding Custom domain disassociation). This was the original stacks: ``` GlobalStack (route53 hosted zone, SES, IAM, ACM) | \------------------------------ | | LiveDataStack DevDataStack (databases and Cognito user pools) | | | | Blue/GreenAppStacks DevAppStack (Appsync, StepFunct…",,,,,,Anecdotal,comment,,,,,,,,2022-09-23,github/bmilesp,https://github.com/hashicorp/terraform/issues/27188#issuecomment-1256340723,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"> we'll need several more examples of similar problems in order to start to analyze what they all have in common and thus how the problem might generalize I tried to use Terraform to set up a Snowflake ""Storage Integration"" object that links to an AWS S3 bucket using the ""chanzuckerberg"" Snowflake provider from the Terraform registry in addition to the standard AWS provider. Part of the process to create the integration requires the following sequence of actions: - Create an [IAM Role](https://…",,,,,,Anecdotal,comment,,,,,,,,2022-11-17,github/griffinator76,https://github.com/hashicorp/terraform/issues/27188#issuecomment-1318011777,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"I've run another use case: trying to manage content with a series of messages, each with a ""back to top"" link which would link to the table of contents. The table of contents, of course, needs to be able to link to all the other posts. This is another instance of ""I need mutually referential identifiers"". My alternative suggestion is that two-phase created could be explicitly supported at the platform level, as there are APIs that allow reservation of resources much more cheaply than full creat…",,,,,,Anecdotal,comment,,,,,,,,2022-11-23,github/spectria-limina,https://github.com/hashicorp/terraform/issues/27188#issuecomment-1325384422,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"I'm running into this in OCI. Creating a custom route table and assigning that route table to the subnet works without issue. The problem comes when I also want to create route rules in that route table. For example, I have a subnet defined and that subnet will have an Ubuntu instance in it along with a Palo Alto firewall instance. I need a route table assigned to the subnet that makes the trust interface IP of the firewall the default gateway for the subnet. Here are the components that need t…",,,,,,Anecdotal,comment,,,,,,,,2022-12-02,github/mm-col,https://github.com/hashicorp/terraform/issues/27188#issuecomment-1335685132,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
https://github.com/hashicorp/terraform-provider-aws/pull/1824 Another valid use case is cycles in AWS security groups or prefix lists.,,,,,,Anecdotal,comment,,,,,,,,2022-12-16,github/Huang-W,https://github.com/hashicorp/terraform/issues/27188#issuecomment-1354281254,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
We bumped into this while trying to pass the invokeURL of an AWS gateway resource to a lambda as an environment variable because the gateway has endpoints that route to the lambda.,,,,,,Anecdotal,comment,,,,,,,,2023-02-28,github/BWeesy,https://github.com/hashicorp/terraform/issues/27188#issuecomment-1448609811,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"I have the same use case that jeffg-hpe posted above, and to expand on it a little, the provider can't cleanly handle this one because building it requires multiple instances of the provider, one targeting the IdP tenant, the other targeting the SP tenant each with distinct API endpoints and auth tokens. So the typical approach of adding a virtual resource to the provider that manages multiple resources under the hood doesn't work here because those resources exist in disparate environments. Hi…",,,,,,Anecdotal,comment,,,,,,,,2023-04-03,github/sgal-dm,https://github.com/hashicorp/terraform/issues/27188#issuecomment-1494986161,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"> the typical way to deal with this today is for the provider to explain to Terraform that ""update cognito user pool to add lambda trigger"" is a separate operation by representing it as a separate resource. That creates a relatively easy to explain execution model: there is only one action for each resource per plan (with the special exception of ""replace"", which is internally a combined destroy/create), and the ordering of those actions is derived from the dependencies between those resources.…",,,,,,Anecdotal,comment,,,,,,,,2023-05-11,github/patmaddox,https://github.com/hashicorp/terraform/issues/27188#issuecomment-1544624017,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"One example of this pattern that I can think of quickly is in the `hashicorp/aws` provider: There are separate resource types for `aws_s3_bucket` and `aws_s3_bucket_policy`, which allows the policy to refer to the `arn` attribute of the bucket itself when describing rules about specific sub-paths inside the bucket, which typically involves writing an ARN whose prefix is the `arn` attribute of the bucket as a whole.",,,,,,Anecdotal,comment,,,,,,,,2023-05-11,github/apparentlymart,https://github.com/hashicorp/terraform/issues/27188#issuecomment-1544771891,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"> There are separate resource types for `aws_s3_bucket` and `aws_s3_bucket_policy`, which allows the policy to refer to the `arn` attribute of the bucket itself when describing rules about specific sub-paths inside the bucket, which typically involves writing an ARN whose prefix is the `arn` attribute of the bucket as a whole. Oh, is _that_ why so many aws features have separate resource types? Does that mean that in places where there are blocks that could be separate resources, the direction …",,,,,,Anecdotal,comment,,,,,,,,2023-05-11,github/dbaynard,https://github.com/hashicorp/terraform/issues/27188#issuecomment-1544798254,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"There is a separate team responsible for the `hashicorp/aws` provider and so I don't know all of what motivates their design decisions, but in this particular case (the S3 operations) the structure with separate resource types for different features matches the structure of the underlying API, which has separate write operations for the two resource types I mentioned: [`s3:CreateBucket`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateBucket.html) for `aws_s3_bucket` and [`s3:PutBucke…",,,,,,Anecdotal,comment,,,,,,,,2023-05-12,github/apparentlymart,https://github.com/hashicorp/terraform/issues/27188#issuecomment-1544944833,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"Another use case analogous to @apparentlymart 's S3 case above: locking KMS keys to any resource that uses them with a `Resource` restriction in the key policy: ``` resource ""aws_sns_topic"" ""log_processing"" { name = ""LogProcessingTopic"" kms_master_key_id = aws_kms_key.log_processing.arn ``` with a key policy for the KMS key of: ``` data ""aws_iam_policy_document"" ""log_processing_kms_key"" { statement { actions = [ ""kms:GenerateDataKey*"", ""kms:Decrypt"" ] resources = [aws_sns_topic.log_processing.a…",,,,,,Anecdotal,comment,,,,,,,,2023-05-23,github/glerb,https://github.com/hashicorp/terraform/issues/27188#issuecomment-1558300156,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"@nibblesnbits Based on scanning @apparentlymart's comments, I would not expect this behavior to change in Terraform v1.x. This is the type of issue the team likes to leave open to generate ideas and use cases for a ""hypothetical v2."" For future viewers, if you are viewing this issue and would like to indicate your interest, please use the 👍 reaction on the issue description to upvote this issue. Thanks!",,,,,,Anecdotal,comment,,,,,,,,2023-12-13,github/crw,https://github.com/hashicorp/terraform/issues/27188#issuecomment-1854590473,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
i'll add my use case to the pile. Using the respective hashicorp AWS modules for lambda and eventbridge. Dependency exists when creating a lambda with a eventbridge rule as a trigger. The lambda needs the ARN of the rule to add the required resource permissions to the lambda so it can be invoked by the event tule. The event rule also needs the lambda function ARN to be able to create the lambda target,,,,,,Anecdotal,comment,,,,,,,,2024-05-16,github/stevemckenney,https://github.com/hashicorp/terraform/issues/27188#issuecomment-2115635360,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"Hi @stevemckenney! Thanks for sharing that feedback. Could you link to the specific modules you're referring to? I'm not aware of any HashiCorp-maintained modules for either Lambda or EventBridge, and I wasn't able to find any relevant-seeming modules in [the partner-maintained AWS modules](https://registry.terraform.io/browse/modules?partner=true&provider=aws). I'd like to be able to see exactly how those modules are configuring Lambda and EventBridge to understand how the circular dependency …",,,,,,Anecdotal,comment,,,,,,,,2024-05-16,github/apparentlymart,https://github.com/hashicorp/terraform/issues/27188#issuecomment-2115806464,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"Sorry, these lines are a bit blurry when it comes to who maintains these, it isn't hashicorp https://github.com/terraform-aws-modules/terraform-aws-lambda https://github.com/terraform-aws-modules/terraform-aws-eventbridge To your point, I may just have to call the permission resource separately after the creation via the modules. _____________________________________________________________________________________ Steve McKenney | BISSELL Homecare, Inc. | Sr. ITS Enterprise Solutions Architect …",,,,,,Anecdotal,comment,,,,,,,,2024-05-16,github/stevemckenney,https://github.com/hashicorp/terraform/issues/27188#issuecomment-2115880735,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"Thanks for those links, @stevemckenney. I'm not super familiar with these modules, but from peeping in the source code for a little while it seems like something like this might work: ```hcl module ""lambda"" { source = ""terraform-aws-modules/lambda/aws"" # ... runtime = ""..."" source_path = ""..."" # ... allowed_triggers = { for k, arn in module.events.eventbridge_rule_arns : k => { service = ""events"" source_arn = arn # ... } } } module ""events"" { source = ""terraform-aws-modules/eventbridge/aws"" # .…",,,,,,Anecdotal,comment,,,,,,,,2024-05-16,github/apparentlymart,https://github.com/hashicorp/terraform/issues/27188#issuecomment-2115932750,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"I'm having an issue where a lambda function is adding the ARN of a stepfunction as it's environment variable ``` { ""STATE_MACHINE_ARN"" = var.state_machine_arn } ``` However, the stepfunction also needs the ARN of this lambda function ``` Type = ""Task"", Resource = var.x_handler_lambda_arn, ``` How can I go about this? Is it possible to set the environment variable after the stepfunction has been created?",,,,,,Anecdotal,comment,,,,,,,,2024-06-10,github/esirK,https://github.com/hashicorp/terraform/issues/27188#issuecomment-2158005363,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"Hi @esirK, For situations like that a typical strategy would be to add some sort of indirection. That means that instead of passing the step function ARN directly to the Lambda function, you'd instead pass some information that the Lambda function can use to find the step function dynamically at runtime. Of course, you will need to be able to tolerate there being a brief period at the start of the Lambda function's life when the step function doesn't exist yet. Another possibility would be to s…",,,,,,Anecdotal,comment,,,,,,,,2024-06-10,github/apparentlymart,https://github.com/hashicorp/terraform/issues/27188#issuecomment-2158640986,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
"simplify the backend interface; promote locking to a first class citizen Hi there, Looking at promoting locking to a first class function for terraform backends. See https://github.com/hashicorp/terraform/issues/26422 Starting a draft here to allow a fail-fast, in case this is a non-starter or undesirable (this would result in a large refactor, and changes to each backend implementation). Anyways, I'm thinking that backends should support simple operations, and place the complexity of calling, …",,,,,,Anecdotal,issue,,,,,,,,2020-10-13,github/steeling,https://github.com/hashicorp/terraform/pull/26572,repo: hashicorp/terraform | keyword: lesson learned | state: open
"Better yet for the above proposal, would be adding RLock for read locks which would help parallelize those workloads that heavily use `terraform_remote_state`",,,,,,Anecdotal,comment,,,,,,,,2020-10-15,github/steeling,https://github.com/hashicorp/terraform/pull/26572#issuecomment-709511443,repo: hashicorp/terraform | issue: simplify the backend interface; promote locking to a first class citizen | keyword: lesson learned
"@steeling Sadly, we are unlikely to tackle any broad changes to the current state storage backend interface in the next 6 months to a year. Internally we are considering a careful move to a plugin-type architecture as vendor-specific (3rd party) code shouldn't be shipping with core as the product matures. This would mean a radical redesign of the way backends are used (and interface) with core and will be in coordination with other Terraform teams, including our SDK and provider teams. At the m…",,,,,,Anecdotal,comment,,,,,,,,2020-10-22,github/pkolyvas,https://github.com/hashicorp/terraform/pull/26572#issuecomment-714487504,repo: hashicorp/terraform | issue: simplify the backend interface; promote locking to a first class citizen | keyword: lesson learned
"@pkolyvas thanks for the detailed response! I'm guessing the ""plugin"" model will simply be a new interface that each plugin must conform to? If so, do you have ideas on what that interface is like, or if the one mentioned above may fit that? Happy to lend a hand in this space!",,,,,,Anecdotal,comment,,,,,,,,2020-10-28,github/steeling,https://github.com/hashicorp/terraform/pull/26572#issuecomment-718075548,repo: hashicorp/terraform | issue: simplify the backend interface; promote locking to a first class citizen | keyword: lesson learned
[![CLA assistant check](https://cla.hashicorp.com/pull/badge/signed)](https://cla.hashicorp.com/hashicorp/terraform?pullRequest=26572) <br/>All committers have signed the CLA.,,,,,,Anecdotal,comment,,,,,,,,2022-03-12,github/hashicorp-cla,https://github.com/hashicorp/terraform/pull/26572#issuecomment-1065923472,repo: hashicorp/terraform | issue: simplify the backend interface; promote locking to a first class citizen | keyword: lesson learned
"Resource name collisions in Terraform Some resources for some providers got overridden when using the same value for the `name` field by accident. It could be easily missed during review, plan and apply stage. Sometimes this fails `terraform destroy` operations. This issue has several impacts: - Failing automation scripts (CI/CD, Infrastructure as a Code, etc) during destroying resources (for some resources, like `aws_autoscaling_policy` but not for `aws_cloudwatch_metric_alarm`) - Various busi…",,,,,,Anecdotal,issue,,,,,,,,2018-07-12,github/dvishniakov,https://github.com/hashicorp/terraform/issues/18444,repo: hashicorp/terraform | keyword: lesson learned | state: open
"Hi @dvishniakov! Thanks for sharing this. Funnily enough, we were discussing this internally just yesterday. The expectation here is that each provider will do what it needs to do to preserve Terraform's core assumptions, one of which is that Terraform will never secretly take ownership of something that already exists without an explicit `terraform import`. In practice, lots of underlying APIs make this impossible to achieve 100% reliably. It should be possbile for `terraform plan` to detect a…",,,,,,Anecdotal,comment,,,,,,,,2018-07-12,github/apparentlymart,https://github.com/hashicorp/terraform/issues/18444#issuecomment-404596428,repo: hashicorp/terraform | issue: Resource name collisions in Terraform | keyword: lesson learned
"I'm bumping this as I ran into the same accident/issue today. I was lazy-pasting multiple autoscaling policies and mistakenly named two of them the same. I went to apply and found that each ASG only had one policy, depending on the last one terraform applied. D'oh! To make it worse, attempting to update the name to the correct one apparently referred to the same resource for both, causing it to only modify one in-place during the apply operation. The solution was to back out everything with des…",,,,,,Anecdotal,comment,,,,,,,,2020-03-12,github/ravenium,https://github.com/hashicorp/terraform/issues/18444#issuecomment-597980984,repo: hashicorp/terraform | issue: Resource name collisions in Terraform | keyword: lesson learned
"Terraform 1.6 want to migrate s3 backend state ### Terraform Version ```shell 1.6.0 ``` ### Terraform Configuration Files N/A ### Debug Output N/A ### Expected Behavior plan ### Actual Behavior Terraform want to migrate state between Terraform 1.5 and Terraform 1.6 (no change on my code) ``` Initializing the backend... ╷ │ Error: Backend configuration changed │ │ A change in the backend configuration has been detected, which may require migrating existing state. │ │ If you wish to attempt autom…",,,,,,Anecdotal,issue,,,,,,,,2023-10-09,github/DanielCastronovo,https://github.com/hashicorp/terraform/issues/34022,repo: hashicorp/terraform | keyword: lesson learned | state: closed
"If i want to rollback to 1.5 i've another error : ``` Error: Failed to decode current backend config │ │ The backend configuration created by the most recent run of ""terraform init"" could not be decoded: unsupported attribute ""allowed_account_ids"". The configuration may have been initialized by an earlier version that used an incompatible configuration structure. Run ""terraform init -reconfigure"" to force │ re-initialization of the backend. ```",,,,,,Anecdotal,comment,,,,,,,,2023-10-09,github/DanielCastronovo,https://github.com/hashicorp/terraform/issues/34022#issuecomment-1752703436,repo: hashicorp/terraform | issue: Terraform 1.6 want to migrate s3 backend state | keyword: lesson learned
"> If i want to rollback to 1.5 i've another error : > > ``` > Error: Failed to decode current backend config > │ > │ The backend configuration created by the most recent run of ""terraform init"" could not be decoded: unsupported attribute ""allowed_account_ids"". The configuration may have been initialized by an earlier version that used an incompatible configuration structure. Run ""terraform init -reconfigure"" to force > │ re-initialization of the backend. > ``` I got this error as well (#34023)-…",,,,,,Anecdotal,comment,,,,,,,,2023-10-09,github/automationdidact,https://github.com/hashicorp/terraform/issues/34022#issuecomment-1752884838,repo: hashicorp/terraform | issue: Terraform 1.6 want to migrate s3 backend state | keyword: lesson learned
"Hi @DanielCastronovo, I'm not clear on what the bug is here? Do the commands proposed by Terraform, such as `terraform init -reconfigure`, not work and so you are prevented from moving between the different versions of Terraform? All the diagnostics you've provided include an explanation and a recommendation for how to proceed with the state migration.",,,,,,Anecdotal,comment,,,,,,,,2023-10-09,github/liamcervante,https://github.com/hashicorp/terraform/issues/34022#issuecomment-1752973435,repo: hashicorp/terraform | issue: Terraform 1.6 want to migrate s3 backend state | keyword: lesson learned
"Hello, I have noticed the same issue when upgrading to v1.6 of terraform. I have not modified any of the code on my side as well. I am using S3 Backend and there have been a large number of changes in the backend for S3 in 1.6. Maybe this is linked to it? I could not find anything in the release note about having to reconfigure the state.",,,,,,Anecdotal,comment,,,,,,,,2023-10-09,github/lanzrein,https://github.com/hashicorp/terraform/issues/34022#issuecomment-1752999841,repo: hashicorp/terraform | issue: Terraform 1.6 want to migrate s3 backend state | keyword: lesson learned
Same here. I have terraform bots in a CI (more than 150). I must fix all of them manually. I would have appreciated a heads up.,,,,,,Anecdotal,comment,,,,,,,,2023-10-09,github/moogly81,https://github.com/hashicorp/terraform/issues/34022#issuecomment-1753014460,repo: hashicorp/terraform | issue: Terraform 1.6 want to migrate s3 backend state | keyword: lesson learned
"Yes, there were a number of changes listed in the [changelog](https://github.com/hashicorp/terraform/blob/v1.6/CHANGELOG.md#160-october-4-2023) made to the AWS S3 backend, however they don't go into much detail about the different situations you may see when upgrading. I'll mark this as documentation and we can add some more information added about the upgrade process. Thanks!",,,,,,Anecdotal,comment,,,,,,,,2023-10-09,github/jbardin,https://github.com/hashicorp/terraform/issues/34022#issuecomment-1753064103,repo: hashicorp/terraform | issue: Terraform 1.6 want to migrate s3 backend state | keyword: lesson learned
"@liamcervante i don't understand why with TF 1.6 we need to reconfigure my backend like many others persons . I doesn't changed any code on my baseline, just bump Terraform from 1.5 to 1.6. If the format of backend are changed, it's needed to specify the breaking change IMO",,,,,,Anecdotal,comment,,,,,,,,2023-10-09,github/DanielCastronovo,https://github.com/hashicorp/terraform/issues/34022#issuecomment-1753069138,repo: hashicorp/terraform | issue: Terraform 1.6 want to migrate s3 backend state | keyword: lesson learned
"We've pinned at 1.5 until we can determine what the impact of the 1.6 suggested init recommendations are. It gives 2 choices, but no guidance on why it's required or which of the two choices we're presented with is the optimum choice. I don't recall having to do such a thing in recent time, so we're being cautious. Best I could find on this topic was a reply to this post and I'm still not sure! https://discuss.hashicorp.com/t/confusing-error-message-when-terraform-backend-is-changed/32637",,,,,,Anecdotal,comment,,,,,,,,2023-10-09,github/wagnerone,https://github.com/hashicorp/terraform/issues/34022#issuecomment-1753649434,repo: hashicorp/terraform | issue: Terraform 1.6 want to migrate s3 backend state | keyword: lesson learned
"we have fielded solutions and now can not use latest anymore - pinning at 1.5.7, and hand fixing issues hoping TF team will mitigate this without impacting a lot of existing customers - sad we had to learn about a deployment breaking change by seeing it fail, we have been otherwise previously pleased with TF backward compatibility and careful about major revisions on providers",,,,,,Anecdotal,comment,,,,,,,,2023-10-13,github/ellisroll-b,https://github.com/hashicorp/terraform/issues/34022#issuecomment-1761553272,repo: hashicorp/terraform | issue: Terraform 1.6 want to migrate s3 backend state | keyword: lesson learned
"Thanks for reporting this, everyone. The changes to the S3 backend shouldn't have required a reconfiguration, so I've tagged this a `bug` again.",,,,,,Anecdotal,comment,,,,,,,,2023-10-16,github/gdavison,https://github.com/hashicorp/terraform/issues/34022#issuecomment-1764966501,repo: hashicorp/terraform | issue: Terraform 1.6 want to migrate s3 backend state | keyword: lesson learned
"Sorry for the whiplash, everyone. @jbardin was correct in https://github.com/hashicorp/terraform/issues/34022#issuecomment-1753064103. Basically, this is an expected change because of the parameters added to the S3 Backend configuration. Unfortunately, this is a limitation of how Terraform currently detects changes in backend configuration: if parameters are added to the *schema*, regardless of if they are used in the configuration, Terraform requires a reconfiguration. When starting up, Terraf…",,,,,,Anecdotal,comment,,,,,,,,2023-10-16,github/gdavison,https://github.com/hashicorp/terraform/issues/34022#issuecomment-1765396713,repo: hashicorp/terraform | issue: Terraform 1.6 want to migrate s3 backend state | keyword: lesson learned
"> Sorry for the whiplash, everyone. @jbardin was correct in [#34022 (comment)](https://github.com/hashicorp/terraform/issues/34022#issuecomment-1753064103). > > Basically, this is an expected change because of the parameters added to the S3 Backend configuration. Unfortunately, this is a limitation of how Terraform currently detects changes in backend configuration: if parameters are added to the _schema_, regardless of if they are used in the configuration, Terraform requires a reconfiguration…",,,,,,Anecdotal,comment,,,,,,,,2023-10-17,github/eaterm,https://github.com/hashicorp/terraform/issues/34022#issuecomment-1765774438,repo: hashicorp/terraform | issue: Terraform 1.6 want to migrate s3 backend state | keyword: lesson learned
"> > Sorry for the whiplash, everyone. @jbardin was correct in [#34022 (comment)](https://github.com/hashicorp/terraform/issues/34022#issuecomment-1753064103). > > Basically, this is an expected change because of the parameters added to the S3 Backend configuration. Unfortunately, this is a limitation of how Terraform currently detects changes in backend configuration: if parameters are added to the _schema_, regardless of if they are used in the configuration, Terraform requires a reconfigurati…",,,,,,Anecdotal,comment,,,,,,,,2023-10-17,github/Redempter,https://github.com/hashicorp/terraform/issues/34022#issuecomment-1766932206,repo: hashicorp/terraform | issue: Terraform 1.6 want to migrate s3 backend state | keyword: lesson learned
"We are sticking with the old version, but seriously requesting Hashicorp rethink this design decision (for instance provide a means for fielded solutions to upgrade in place without code changes, or work with new or old schemas based on versioning). Having to hand fix a pile of datacenters kinda defeats the purpose of the tool. Fully admit its our own fault for using latest TF in our pipelines. Lesson learned.",,,,,,Anecdotal,comment,,,,,,,,2023-10-17,github/ellisroll-b,https://github.com/hashicorp/terraform/issues/34022#issuecomment-1766948452,repo: hashicorp/terraform | issue: Terraform 1.6 want to migrate s3 backend state | keyword: lesson learned
1.6.2 seems to have effectively cleared this issue up for us. I unpinned 1.5 and 1.6.2 didn't utter a word on applies with existing state or re-inits (using `tf init --upgrade`).,,,,,,Anecdotal,comment,,,,,,,,2023-10-19,github/wagnerone,https://github.com/hashicorp/terraform/issues/34022#issuecomment-1771799119,repo: hashicorp/terraform | issue: Terraform 1.6 want to migrate s3 backend state | keyword: lesson learned
"> 1.6.2 seems to have effectively cleared this issue up for us. I unpinned 1.5 and 1.6.2 didn't utter a word on applies with existing state or re-inits (using `tf init --upgrade`). Same here, updating to 1.6.2 seems to resole the issue.",,,,,,Anecdotal,comment,,,,,,,,2023-10-20,github/eaterm,https://github.com/hashicorp/terraform/issues/34022#issuecomment-1772160709,repo: hashicorp/terraform | issue: Terraform 1.6 want to migrate s3 backend state | keyword: lesson learned
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2023-12-08,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/34022#issuecomment-1846442922,repo: hashicorp/terraform | issue: Terraform 1.6 want to migrate s3 backend state | keyword: lesson learned
terraform: refactor ReadData I took some lessons learned during yesterday's marathon refactoring and re-refactored the dataSource plan and apply to be functions on NodeResourceAbstractInstance. Includes mild renaming to differentiate between plan and planDataSource (I considered renaming plan to planResource - thoughts?) I also pulled the logic checking for provider metas into a single function.,,,,,,Anecdotal,issue,,,,,,,,2020-12-08,github/mildwonkey,https://github.com/hashicorp/terraform/pull/27189,repo: hashicorp/terraform | keyword: lesson learned | state: closed
# [Codecov](https://codecov.io/gh/hashicorp/terraform/pull/27189?src=pr&el=h1) Report > Merging [#27189](https://codecov.io/gh/hashicorp/terraform/pull/27189?src=pr&el=desc) (ae98022) into [master](https://codecov.io/gh/hashicorp/terraform/commit/e7aaf9e39f895e9b0423667df7df355a831cf874?el=desc) (e7aaf9e) will **decrease** coverage by `0.04%`. > The diff coverage is `64.67%`. | [Impacted Files](https://codecov.io/gh/hashicorp/terraform/pull/27189?src=pr&el=tree) | Coverage Δ | | |---|---|---| |…,,,,,,Anecdotal,comment,,,,,,,,2020-12-08,github/codecov[bot],https://github.com/hashicorp/terraform/pull/27189#issuecomment-740682816,repo: hashicorp/terraform | issue: terraform: refactor ReadData | keyword: lesson learned
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2021-01-08,github/ghost,https://github.com/hashicorp/terraform/pull/27189#issuecomment-756495990,repo: hashicorp/terraform | issue: terraform: refactor ReadData | keyword: lesson learned
"Terraform 0.13-rc1 incorrect destroy cycle (glitched on provider) Terraform is able to correctly create environment however it is unable to destroy it. Each time destroy command is executed I'm receiving ""Error: Cycle: ..."" My analysis so far shows that the problem is a provider (kubernetes/helm in my case) that is configured based on the output of a submodule (or a resource in general). It's quite easy to reproduce however posting complete code would be too verbose. My case is: 1. Create EKS o…",,,,,,Anecdotal,issue,,,,,,,,2020-07-31,github/JakubWojtowicz-TomTom,https://github.com/hashicorp/terraform/issues/25715,repo: hashicorp/terraform | keyword: lesson learned | state: closed
"@JakubWojtowicz-TomTom can Thanks for reporting this. I think this is probably a valid issue, and I'd like to reproduce it locally. To do that, I have to be able to run this and run it on my workstation without inventing any details in order to be confident we're seeing the same behavior. As the Terraform Core engineering manager I don't manage Kubernetes clusters with Terraform frequently, so me inventing these details would be a pretty significant amount of work. Can you please restate your r…",,,,,,Anecdotal,comment,,,,,,,,2020-08-06,github/danieldreier,https://github.com/hashicorp/terraform/issues/25715#issuecomment-669987651,repo: hashicorp/terraform | issue: Terraform 0.13-rc1 incorrect destroy cycle (glitched on provider) | keyword: lesson learned
"@danieldreier I've managed to reproduce the issue, but I'm pretty sure it was user fault. The scenario for reproduction looks as follows: We have 4 modules A, B, C and D. B and C depend on A. C depends on B. D consists of A, B and C. Output from D depends implicitly on output from B (output forwarded to another output) and explicitly depends on C (I guess this is the point where we had the cycle). Output from D was used to configure Kubernetes/Helm provider thus the issue was kind of obscured. …",,,,,,Anecdotal,comment,,,,,,,,2020-08-18,github/JakubWojtowicz-TomTom,https://github.com/hashicorp/terraform/issues/25715#issuecomment-675365626,repo: hashicorp/terraform | issue: Terraform 0.13-rc1 incorrect destroy cycle (glitched on provider) | keyword: lesson learned
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2020-11-13,github/ghost,https://github.com/hashicorp/terraform/issues/25715#issuecomment-726455625,repo: hashicorp/terraform | issue: Terraform 0.13-rc1 incorrect destroy cycle (glitched on provider) | keyword: lesson learned
"Deleted MX record fails refresh in DO Was having some issues with the Digital Ocean provider, so I manually deleted an MX record thinking Terraform would either recreate it or prune the state file. Bad idea, apparently. Now all Terraform actions fail because there is an error retrieving the record for the deleted API element. Lesson learned, but it would be nice to have some mechanism manual or automatic for cleaning up state. Maybe if the API call fails with a 404, the Digital Ocean provider s…",,,,,,Anecdotal,issue,,,,,,,,2014-09-09,github/ndarilek,https://github.com/hashicorp/terraform/issues/279,repo: hashicorp/terraform | keyword: lesson learned | state: closed
"This is just a bug in Terraform. I've tagged it as such. Can you give us the error logs? It'd help us. State files are getting an overhaul in 0.3 which should also help with this, but we'll keep that as a separate issue.",,,,,,Anecdotal,comment,,,,,,,,2014-09-09,github/mitchellh,https://github.com/hashicorp/terraform/issues/279#issuecomment-54990975,repo: hashicorp/terraform | issue: Deleted MX record fails refresh in DO | keyword: lesson learned
"Unreliable keybase.io installation procedure dependency <!-- Hi there, Thank you for opening an issue. Please note that we try to keep the Terraform issue tracker reserved for bug reports and feature requests. For general usage questions, please see: https://www.terraform.io/community.html. If your issue relates to a specific Terraform provider, please open it in the provider's own repository. The index of providers is at https://github.com/terraform-providers . --> ### Terraform Version <!--- …",,,,,,Anecdotal,issue,,,,,,,,2020-03-29,github/Jean-Baptiste-Lasselle,https://github.com/hashicorp/terraform/issues/24498,repo: hashicorp/terraform | keyword: lesson learned | state: closed
"Hi @Jean-Baptiste-Lasselle Thanks for filing the issue. Can you explain further what exactly the unexpected behavior is here? Is it just that the Keybase server may not be available, which interrupts your automation? If that is the issue, can I ask why you are relying on Keybase as a part of your automation here? You have a reference to the security page with the public key data, ID and fingerprint which you could verify and store, and the key is stored in other public key servers as well.",,,,,,Anecdotal,comment,,,,,,,,2020-03-30,github/jbardin,https://github.com/hashicorp/terraform/issues/24498#issuecomment-606162053,repo: hashicorp/terraform | issue: Unreliable keybase.io installation procedure dependency | keyword: lesson learned
"Hi @jbardin Thank you so much for such a quick answer. I will bring further informations, but I want to immediately give you indication : * When I experienced the issue, the Keybase.io service was up n running : I first wondered if it was keybase indeed. * But it appeared that keybase.io was up , but it seemed lke the hashicorp team had teared down /deleted the PGP public key from the hashicorp keybase.io account, as you can see on the screenshot I took immediately, to report the issue (_**a mi…",,,,,,Anecdotal,comment,,,,,,,,2020-04-01,github/Jean-Baptiste-Lasselle,https://github.com/hashicorp/terraform/issues/24498#issuecomment-606983991,repo: hashicorp/terraform | issue: Unreliable keybase.io installation procedure dependency | keyword: lesson learned
"@Jean-Baptiste-Lasselle if I understand correctly, you're trying to automate key retrieval in CI and you have been doing that using keybase, and what you're seeing is that the company key just disappeared from keybase. If I'm understanding those needs correctly, I think you can run the following in CI: ```gpg --recv-keys 51852D87348FFC4C``` This will retrieve HashiCorp's public GPG key from key servers. Alternatively, you can store the GPG key listed on the web site permanently and not re-downl…",,,,,,Anecdotal,comment,,,,,,,,2020-04-01,github/danieldreier,https://github.com/hashicorp/terraform/issues/24498#issuecomment-607485314,repo: hashicorp/terraform | issue: Unreliable keybase.io installation procedure dependency | keyword: lesson learned
"Hi Mister Dreier , Thank you so much for your answer. > > if I understand correctly, you're trying to automate key retrieval in CI and you have been doing that using keybase, and what you're seeing is that the company key just disappeared from keybase. > Now I have the most serious reference, from HashiCorp Core Team, to justifiy to my management why we no have to invest into a solution to manage a key referential, to monitor any future change of PGP public keys of all dependencies (which is a …",,,,,,Anecdotal,comment,,,,,,,,2020-04-01,github/Jean-Baptiste-Lasselle,https://github.com/hashicorp/terraform/issues/24498#issuecomment-607527546,repo: hashicorp/terraform | issue: Unreliable keybase.io installation procedure dependency | keyword: lesson learned
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2020-05-02,github/ghost,https://github.com/hashicorp/terraform/issues/24498#issuecomment-622651610,repo: hashicorp/terraform | issue: Unreliable keybase.io installation procedure dependency | keyword: lesson learned
A simplifed load balancer resource for AzureRM This commit adds a resource for the ARM load balancer. The ARM LB is quite complicated and has many different configuration options. Instead of trying to implement them all in a potential confusing user experience this resources exposes the most common uses. With it a load balancer can be configured on a single public IP or subnet which can route to a single backend pool according to one load balancing rule and 1 probe. This could be expanded to su…,,,,,,Anecdotal,issue,,,,,,,,2016-04-30,github/buzztroll,https://github.com/hashicorp/terraform/pull/6429,repo: hashicorp/terraform | keyword: lesson learned | state: closed
Here is HCL that will put 2 VMs behind the load balancer and route ssh to them: https://gist.github.com/buzztroll/5c92d312af5837f50618eef5b4c7e17b I am closing #6335 in favor of this. @aznashwan @phinze,,,,,,Anecdotal,comment,,,,,,,,2016-04-30,github/buzztroll,https://github.com/hashicorp/terraform/pull/6429#issuecomment-215997334,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
"Hi @buzztroll! I agree that the load balancer is likely to be overly complex for a single resource. In lieu of the nested configurations discussed in a previous ticket, I think this might be a good route forward to getting _something_ usable. Let me discuss with @phinze et al on Monday before merging - first impressions of the code are good, however! Thanks for your work here!",,,,,,Anecdotal,comment,,,,,,,,2016-04-30,github/jen20,https://github.com/hashicorp/terraform/pull/6429#issuecomment-215997407,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
@andreimc and @iaingblack you showed interest in a load balancer resource for azure. Would this simple flat one meet your needs (or at least some of your needs)?,,,,,,Anecdotal,comment,,,,,,,,2016-05-05,github/buzztroll,https://github.com/hashicorp/terraform/pull/6429#issuecomment-217255073,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
"I'll take it! For my use cases at least, a single subnet is fine. I'll give it a test as soon as I can. Reading the description, it says it can be configured ""according to one load balancing rule and 1 probe"". Does that mean only 1 rule can be created or can there be many? Multiple rules to allow exposing both RDP and winrm ports on a machine would be nice. But I can open all ports to a single VM from a specific IP if not.Thanks for the quick implementation, Buzztroll!",,,,,,Anecdotal,comment,,,,,,,,2016-05-08,github/iaingblack,https://github.com/hashicorp/terraform/pull/6429#issuecomment-217744907,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
"With this implementation it is 1 rule, 1 probe, 1 frontend, 1 backend, 1 everything. It wouldnt be hard to extend this to be 1 of everything but allow many rules. I have a work in progress for a more complete load balancer but I ran into some buggy issues with the Azure API (the result of the Create does not match the result of a get preformed immediately after). If the community finds it better to have this simple implementation but with many rules I can extend it to do that fairly quickly",,,,,,Anecdotal,comment,,,,,,,,2016-05-08,github/buzztroll,https://github.com/hashicorp/terraform/pull/6429#issuecomment-217745264,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
"Cool. If you can do many rules that would be great. Having one rule to expose specific ports to the public/client and another rule to expose winrm/rdp to the sysadmin would be the ideal implementation, at least from my perspective.The rest would be nice but that would meet my requirements at least. As it is, this is already very encouraging. Thanks again for working on it Buzztroll :)",,,,,,Anecdotal,comment,,,,,,,,2016-05-08,github/iaingblack,https://github.com/hashicorp/terraform/pull/6429#issuecomment-217746407,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
@iaingblack I patched it to take multiple probes and rules. Example HCL can be found here: https://gist.github.com/buzztroll/7b91efdc70194ea5bdd8fdf6ce270097 I am good with whatever incantation is easiest for hashicorp to merge. @jen20 do you have any preferences here?,,,,,,Anecdotal,comment,,,,,,,,2016-05-09,github/buzztroll,https://github.com/hashicorp/terraform/pull/6429#issuecomment-217974575,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
"I've been trying to use this, and I'm running into problems with the VMs failing to create. Here's my configuration: ``` hcl variable ""name"" { default = ""dcos"" } variable ""location"" { default = ""West US"" } variable ""environment"" { default = ""DC/OS"" } resource ""azurerm_resource_group"" ""dcos"" { name = ""${var.name}"" location = ""${var.location}"" tags { environment = ""${var.environment}"" } } resource ""azurerm_virtual_network"" ""dcos"" { name = ""${var.name}"" resource_group_name = ""${azurerm_resource_gr…",,,,,,Anecdotal,comment,,,,,,,,2016-05-20,github/Bowbaq,https://github.com/hashicorp/terraform/pull/6429#issuecomment-220614473,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
@bowbaq Thats for checking trying this out. The error messages reported in the azurerm provider are very limited. There is a patch out to help with there #6445. For now it would how if you could go into the azure portal and look at the audit trail for the resource group. There should be errors reported there in some very unfriendly looking json documents.,,,,,,Anecdotal,comment,,,,,,,,2016-05-20,github/buzztroll,https://github.com/hashicorp/terraform/pull/6429#issuecomment-220661902,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
@Bowbaq I attempted to run your tf file and got the same errors. Sadly there were no illuminating details in the azure portal. However when I doced out all of the references to the load balancer I got the same errors so I do not believe that this is related to the LB.,,,,,,Anecdotal,comment,,,,,,,,2016-05-20,github/buzztroll,https://github.com/hashicorp/terraform/pull/6429#issuecomment-220724297,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
"@Bowbaq I debugged your hcl a bit. Change the azure_virtual_machine.storage_os_disk like so: ``` storage_os_disk { name = ""os-disk${count.index}"" vhd_uri = ""${azurerm_storage_account.test.primary_blob_endpoint}${element(azurerm_storage_container.test.*.name, count.index)}/os-disk${count.index}.vhd"" caching = ""ReadWrite"" create_option = ""FromImage"" } ``` That worked for me. The uri needs to be unique across both VMs",,,,,,Anecdotal,comment,,,,,,,,2016-05-20,github/buzztroll,https://github.com/hashicorp/terraform/pull/6429#issuecomment-220743029,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
"@buzztroll I'm running into some issues updating an existing load balancer. Specifically, changing probes / rules seems to result in a 400 Bad Request every time. I think the problem is in the 2-step create then update process. I a probe gets removed, then during the first step, the existing rules end up pointing to a now removed probe, and it fails",,,,,,Anecdotal,comment,,,,,,,,2016-05-23,github/Bowbaq,https://github.com/hashicorp/terraform/pull/6429#issuecomment-220954603,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
@Bowbaq thanks for the review! I fixed the things you caught. When you remove a probe do you also remove all of the rules referencing it? I think azure requires that.,,,,,,Anecdotal,comment,,,,,,,,2016-05-23,github/buzztroll,https://github.com/hashicorp/terraform/pull/6429#issuecomment-221027025,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
So what happened is that I had a 1 to 1 probe/rule setup. Then I modified the rules to use the same probe and removed the now useless extra probes. That's when I hit the problem above.,,,,,,Anecdotal,comment,,,,,,,,2016-05-23,github/Bowbaq,https://github.com/hashicorp/terraform/pull/6429#issuecomment-221027697,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
Ok I see how that would cause a problem. The way that the ARM LB API works this has to be in 2 phases or the initial create will not have the right IDs. However I should be able to factor out the update. Thanks,,,,,,Anecdotal,comment,,,,,,,,2016-05-23,github/buzztroll,https://github.com/hashicorp/terraform/pull/6429#issuecomment-221028457,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
@Bowbaq I updated the resource to allow for probes to be deleted. Two phase is still needed but it should work now. I also added an acceptance test. Please let me know if this works for you.,,,,,,Anecdotal,comment,,,,,,,,2016-05-24,github/buzztroll,https://github.com/hashicorp/terraform/pull/6429#issuecomment-221142070,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
I'm running your code in patched version of TF together with the small diff I linked here: https://github.com/hashicorp/terraform/issues/6796#issuecomment-221181570 Everything is working great for me but I'm only executing create and delete options on the entire load balancer rather than updating rules/probes.,,,,,,Anecdotal,comment,,,,,,,,2016-05-24,github/enieuw,https://github.com/hashicorp/terraform/pull/6429#issuecomment-221248507,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
@edevil If you could test the PR and let us know what works and what doesn't that would be greatly helpful in my opinion!,,,,,,Anecdotal,comment,,,,,,,,2016-05-29,github/buzztroll,https://github.com/hashicorp/terraform/pull/6429#issuecomment-222349180,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
That greatly complicates things. There is another PR out with a more complicated load balancer that would support that however due to complications in azure it has problems mapping to terraform nicely. If you have two frontends can you use 2 load balancers instead?,,,,,,Anecdotal,comment,,,,,,,,2016-05-30,github/buzztroll,https://github.com/hashicorp/terraform/pull/6429#issuecomment-222534388,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
"I understand that my scenario is not the most common, but I have a kubernetes cluster, so I have multiple apps running on the same machines but on different ports. I cannot add the same vm (nic) to different backend address pools, so I need a single LB. And I need to have multiple public IPs in the front end pool to use rules to map IPs to different ports in the backend pool.",,,,,,Anecdotal,comment,,,,,,,,2016-05-30,github/edevil,https://github.com/hashicorp/terraform/pull/6429#issuecomment-222542453,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
"You might be able to use the same backend pool in many LBs. In any case this patch intends to have a single frontend (thus the name ""simple""). I have another PR out in a WIP that could handle your use case. Hashicorp will have to decide which direction they want to go. The simple LB is in pretty good shape now and closes a pretty big hole in the azure provider so in my opinion it should be merged and the more complicated LB can be properly designed and implemented after that.",,,,,,Anecdotal,comment,,,,,,,,2016-05-30,github/buzztroll,https://github.com/hashicorp/terraform/pull/6429#issuecomment-222542890,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
"Just to make it clear, these are the commands that I need to execute with Azure CLI for my scenario: ``` azure network public-ip create -d XXX $A_RG public-ip-guestbook $A_LOCATION azure network public-ip create -d YYY $A_RG public-ip-roamersin $A_LOCATION azure network lb create $A_RG KubeNodeLB $A_LOCATION azure network lb frontend-ip create $A_RG KubeNodeLB GuestFrontPool -i public-ip-guestbook azure network lb frontend-ip create $A_RG KubeNodeLB RoamersFrontPool -i public-ip-roamersin azure…",,,,,,Anecdotal,comment,,,,,,,,2016-05-31,github/edevil,https://github.com/hashicorp/terraform/pull/6429#issuecomment-222650832,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
"@edevil You are correct. A given backend pool can only be used in a single load balancer. Additionally, a given VM can only be in two backend pools (one attached to a public LB, one to a private LB). The ""workaround"" in some sense is that LBs can have multiple front end IP configurations. This is how I'm able to expose multiple services from the same set of VMs and a single load balancer.",,,,,,Anecdotal,comment,,,,,,,,2016-05-31,github/colemickens,https://github.com/hashicorp/terraform/pull/6429#issuecomment-222770701,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
"We may be able to work in multiple frontends to this approach to the LB or the other. However before investing more time into either I would like to get an idea on the direction that Hashicorp wants to take. The current version here works for my (and my other) use cases. If this is a good chance of being merged I would be happy to make the changes, however if the plan is to take the only approach then I do not want to invest too much more time in this one.",,,,,,Anecdotal,comment,,,,,,,,2016-05-31,github/buzztroll,https://github.com/hashicorp/terraform/pull/6429#issuecomment-222784204,repo: hashicorp/terraform | issue: A simplifed load balancer resource for AzureRM | keyword: lesson learned
"Update Application Auto Scaling to support scaling an Amazon EC2 Spot fleet. This was released as part of [AWS SDK for Go v1.4.6](https://aws.amazon.com/releasenotes/0017735862860143). I added tests to create `aws_appautoscaling_policy` and `aws_appautoscaling_target` resources using a Spot Fleet Request and they pass, _without_ updating the `aws-sdk-go` vendor dependency. Presumably there are no rules enforced to check the `ScalableDimension` or `ServiceNamespace` values. ~~**QUESTION**: Shoul…",,,,,,Anecdotal,issue,,,,,,,,2016-09-07,github/niclic,https://github.com/hashicorp/terraform/pull/8697,repo: hashicorp/terraform | keyword: lesson learned | state: closed
I think this is OK for review. Some notes: - There is an `alarms` attribute on the `aws_appautoscaling_policy` resource but it is not accounted for in `getAwsAppautoscalingPutScalingPolicyInput` or the docs for this resource. A separate PR should address this. - It might be useful to add some validation to ensure correlation between `scalable_dimension` and `service_namespace` values. This could be added to `getAwsAppautoscalingPutScalingPolicyInput` (unless there is a way to reference other at…,,,,,,Anecdotal,comment,,,,,,,,2016-09-07,github/niclic,https://github.com/hashicorp/terraform/pull/8697#issuecomment-245167236,repo: hashicorp/terraform | issue: Update Application Auto Scaling to support scaling an Amazon EC2 Spot fleet. | keyword: lesson learned
"Hi @niclic thanks for the PR here - there are some alarming changes for backward compatibility purposes - we may need to test this with 0.7.3 and then apply the changes on top and see if the tests still pass With regards to the SDK update, I will open a PR that will update that - watch out for that tomorrow Paul",,,,,,Anecdotal,comment,,,,,,,,2016-09-07,github/stack72,https://github.com/hashicorp/terraform/pull/8697#issuecomment-245443317,repo: hashicorp/terraform | issue: Update Application Auto Scaling to support scaling an Amazon EC2 Spot fleet. | keyword: lesson learned
"Hi Paul, RE: changing `scalable_dimension` and `service_namespace` from _Optional_ to _Required_ on `aws_appautoscaling_policy` and `aws_appautoscaling_target` resources. `ecs:service:DesiredCount` is still a valid value for `scalable_dimension` and `ecs` is still a valid value for `service_namespace`, if that's what you are asking in your line comments above. If this change is too much, the original default values _could_ be restored, although I think this is less than ideal. This only makes s…",,,,,,Anecdotal,comment,,,,,,,,2016-09-07,github/niclic,https://github.com/hashicorp/terraform/pull/8697#issuecomment-245452489,repo: hashicorp/terraform | issue: Update Application Auto Scaling to support scaling an Amazon EC2 Spot fleet. | keyword: lesson learned
"I ran a series of test using the test files included below. I first created the `aws_appautoscaling_policy` and `aws_appautoscaling_target` resources using `v0.7.3`. I then ran `make dev` and applied updates to the resources using `v0.7.4-dev` When no values for `scalable_dimension` and `service_namespace` are supplied, the defaults in `v0.7.3` are applied. When switching to `v0.7.4-dev` the following expected ERRORS are generated: ``` vagrant@terraform:/opt/gopath/src/github.com/hashicorp/terr…",,,,,,Anecdotal,comment,,,,,,,,2016-09-09,github/niclic,https://github.com/hashicorp/terraform/pull/8697#issuecomment-245781659,repo: hashicorp/terraform | issue: Update Application Auto Scaling to support scaling an Amazon EC2 Spot fleet. | keyword: lesson learned
"Test file for above tests. Explicit values are commented out. ``` resource ""aws_ecs_cluster"" ""foo"" { name = ""${var.name}-ecs-cluster"" } resource ""aws_ecs_task_definition"" ""foo"" { family = ""${var.name}-ecs-task-definition"" container_definitions = <<EOF [ { ""name"": ""simple-app"", ""image"": ""amazon/amazon-ecs-sample"", ""cpu"": 10, ""memory"": 500, ""essential"": true } ] EOF } resource ""aws_ecs_service"" ""foo"" { cluster = ""${aws_ecs_cluster.foo.id}"" desired_count = 1 name = ""${var.name}-ecs-service"" task_d…",,,,,,Anecdotal,comment,,,,,,,,2016-09-09,github/niclic,https://github.com/hashicorp/terraform/pull/8697#issuecomment-245781810,repo: hashicorp/terraform | issue: Update Application Auto Scaling to support scaling an Amazon EC2 Spot fleet. | keyword: lesson learned
"@stack72 To reiterate, I recommend removing the current default values, because they no longer make sense for the newly added Amazon EC2 Spot fleet requests service (or any other services that may be added in the future), even though this breaks compatibility with the previous release of these resources. If the default values stay, then they only make sense for scaling ECS services. Some sort of validation or substitution code will have to be added and maintained to `aws_appautoscaling_target` …",,,,,,Anecdotal,comment,,,,,,,,2016-09-21,github/niclic,https://github.com/hashicorp/terraform/pull/8697#issuecomment-248707915,repo: hashicorp/terraform | issue: Update Application Auto Scaling to support scaling an Amazon EC2 Spot fleet. | keyword: lesson learned
"@stack72 I'd like to resolve the conflicts with this branch and hopefully get it merged in soon. Do you have a preferred approach to doing that (merge commit vs rebase and force push)? I have made a lot of comments above, but here is the TLDR: - I suggest removing default values for `scalable_dimension` and `service_namespace` from both the `aws_appautoscaling_target` and `aws_appautoscaling_policy` resources. These values no longer make sense with the support for new services and the sdk does …",,,,,,Anecdotal,comment,,,,,,,,2016-10-27,github/niclic,https://github.com/hashicorp/terraform/pull/8697#issuecomment-256754881,repo: hashicorp/terraform | issue: Update Application Auto Scaling to support scaling an Amazon EC2 Spot fleet. | keyword: lesson learned
"To reiterate my last comment, I think this is good to go. However, since the last update, **Application Auto Scaling** now supports scaling **EMR clusters**. This means a new **scalable dimension** and **service namespace** combination can be added. This should be a very quick and simple change. Should this feature be added to this PR? It might be worth adding it here since a rebase is already needed to resolve conflicts in the shared validators files, which would need updating again to support…",,,,,,Anecdotal,comment,,,,,,,,2017-02-01,github/niclic,https://github.com/hashicorp/terraform/pull/8697#issuecomment-276692571,repo: hashicorp/terraform | issue: Update Application Auto Scaling to support scaling an Amazon EC2 Spot fleet. | keyword: lesson learned
"Hi @niclic Sorry for not getting back to you - I agree, the EMR side of that is outside of the scope of this PR. If possible, please can you rebase this 1 last time and I will give this a final review and get it merged today Thanks Paul",,,,,,Anecdotal,comment,,,,,,,,2017-02-01,github/stack72,https://github.com/hashicorp/terraform/pull/8697#issuecomment-276693305,repo: hashicorp/terraform | issue: Update Application Auto Scaling to support scaling an Amazon EC2 Spot fleet. | keyword: lesson learned
@stack72 I have rebased the commits in this PR to resolve the conflicts in `validators.go`. All checks pass so that should be good. Can you review and merge soon? I notice that work has already started on adding EMR support to **Application Auto Scaling** so this PR should definitely get merged soon to avoid any problems (cf. https://github.com/hashicorp/terraform/issues/11126).,,,,,,Anecdotal,comment,,,,,,,,2017-02-01,github/niclic,https://github.com/hashicorp/terraform/pull/8697#issuecomment-276763804,repo: hashicorp/terraform | issue: Update Application Auto Scaling to support scaling an Amazon EC2 Spot fleet. | keyword: lesson learned
"Great! One last point, it is probably a good idea to include a note in the CHANGELOG or Release Notes to indicate that there are no longer any defaults for `scalable_dimension` and `service_namespace` values?",,,,,,Anecdotal,comment,,,,,,,,2017-02-01,github/niclic,https://github.com/hashicorp/terraform/pull/8697#issuecomment-276767122,repo: hashicorp/terraform | issue: Update Application Auto Scaling to support scaling an Amazon EC2 Spot fleet. | keyword: lesson learned
Hi @niclic Thanks for all the work here - the changes now LGTM and the tests are green. You are correct about adding a NOTE to the changelog on this - I will take care of this - thanks for being so patient :) ``` % make testacc TEST=./builtin/providers/aws TESTARGS='-run=TestAccAWSAppautoScaling' ✭ ==> Checking that code complies with gofmt requirements... go generate $(go list ./... | grep -v /terraform/vendor/) 2017/02/02 09:29:18 Generated command/internal_plugin_list.go TF_ACC=1 go test ./b…,,,,,,Anecdotal,comment,,,,,,,,2017-02-02,github/stack72,https://github.com/hashicorp/terraform/pull/8697#issuecomment-276910305,repo: hashicorp/terraform | issue: Update Application Auto Scaling to support scaling an Amazon EC2 Spot fleet. | keyword: lesson learned
"Just for Google-bait in case other people are having the same problem I did with the v0.8.6 patch level release: https://github.com/hashicorp/terraform/pull/8697/commits/0b9fc76eed3e68e544a87bbddfd7731a0ac825f2 is a backwards-incompatible change that breaks all existing `aws_appautoscaling_target` resource definitions with a ""name"" attribute with the following error message: ``` Errors: * aws_appautoscaling_target.main: : invalid or unknown key: name ``` Just remove the name attribute from your…",,,,,,Anecdotal,comment,,,,,,,,2017-02-09,github/tysonmote,https://github.com/hashicorp/terraform/pull/8697#issuecomment-278797631,repo: hashicorp/terraform | issue: Update Application Auto Scaling to support scaling an Amazon EC2 Spot fleet. | keyword: lesson learned
"Thanks for the note @tysonmote The backwards incompatibilities note in the CHANGELOG should have included the `name` attribute (and probably the `arn` attribute as well) for the `aws_appautoscaling_target` resource. My bad for not being explicit, since it was called out in the PR. @stack72 Can/should the CHANGELOG for `v0.8.6` be updated to include a reference to the `aws_appautoscaling_target` backwards incompatibilities? In addition to no longer having default values for `scalable_dimension` …",,,,,,Anecdotal,comment,,,,,,,,2017-02-09,github/niclic,https://github.com/hashicorp/terraform/pull/8697#issuecomment-278809468,repo: hashicorp/terraform | issue: Update Application Auto Scaling to support scaling an Amazon EC2 Spot fleet. | keyword: lesson learned
Just found the `Deprecated` attribute on the `Schema` type. Obviously a better approach would have been to initially flag this attribute before removing it in a later release. Lesson learned.,,,,,,Anecdotal,comment,,,,,,,,2017-02-10,github/niclic,https://github.com/hashicorp/terraform/pull/8697#issuecomment-278975095,repo: hashicorp/terraform | issue: Update Application Auto Scaling to support scaling an Amazon EC2 Spot fleet. | keyword: lesson learned
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2020-04-16,github/ghost,https://github.com/hashicorp/terraform/pull/8697#issuecomment-614380873,repo: hashicorp/terraform | issue: Update Application Auto Scaling to support scaling an Amazon EC2 Spot fleet. | keyword: lesson learned
"Document $TF_WORKSPACE ### Terraform Version 0.10.2 ### Expected Behavior Since TerraForm 0.10 renamed `env` to `workspace`, I manually guessed the the environment variable `$TF_ENV` was now named `$TF_WORKSPACE`. I'd expected this to be documented and to work the same. ### Actual Behavior There is no mention of `$TF_WORKSPACE` on either the [Workspaces](https://www.terraform.io/docs/state/workspaces.html) or the [Environment Variables](https://www.terraform.io/docs/configuration/environment-va…",,,,,,Anecdotal,issue,,,,,,,,2017-08-22,github/Vinnl,https://github.com/hashicorp/terraform/issues/15874,repo: hashicorp/terraform | keyword: lesson learned | state: closed
"Hi @Vinnl! Sorry for the confusion here. This environment variable was added at the same time as the terminology shifted, so it seems like before you were using `TF_ENV` for your own purposes and by coincidence after renaming it is now conflicting with a variable Terraform itself respects. This environment variable is intended only for use in automation, so currently it's documented only as part of [the guide to running Terraform in automation](https://www.terraform.io/guides/running-terraform-…",,,,,,Anecdotal,comment,,,,,,,,2017-08-22,github/apparentlymart,https://github.com/hashicorp/terraform/issues/15874#issuecomment-324067433,repo: hashicorp/terraform | issue: Document $TF_WORKSPACE | keyword: lesson learned
"Haha OK, I guess it would've made sense for me to consider `$TF_` as a ""reserved"" namespace for TerraForm - lesson learned. And I guess `$TF_WORKSPACE` is actually documented, just not at the place I was looking for, which makes sense, as you usually don't just ""run into"" the environment variables. Thanks for the explanation, anyway. I guess I'll keep using my existing `||` script then, as I'm automatically creating new workspaces based on the branch CI is running at. Might be an interesting us…",,,,,,Anecdotal,comment,,,,,,,,2017-08-22,github/Vinnl,https://github.com/hashicorp/terraform/issues/15874#issuecomment-324079383,repo: hashicorp/terraform | issue: Document $TF_WORKSPACE | keyword: lesson learned
TF_INPUT is meant for automaiton and is describe at https://www.terraform.io/docs/configuration/environment-variables.html ... TF_WORKSPACE would probably feel at home What would it take for TF_WORKSPACE to create if it does not exist already?,,,,,,Anecdotal,comment,,,,,,,,2017-11-09,github/tomdavidson,https://github.com/hashicorp/terraform/issues/15874#issuecomment-343036219,repo: hashicorp/terraform | issue: Document $TF_WORKSPACE | keyword: lesson learned
It appears in my testing that setting `TF_WORKSPACE` to a new workspace **does** create the workspace if it doesn't already exist. I've tested this with the s3 and local backends.,,,,,,Anecdotal,comment,,,,,,,,2018-03-22,github/lkysow,https://github.com/hashicorp/terraform/issues/15874#issuecomment-375433855,repo: hashicorp/terraform | issue: Document $TF_WORKSPACE | keyword: lesson learned
"Hi @lkysow! Unfortunately that is a backend-specific implementation coincidence rather than an intended feature: I expect it happens because these backends implement both ""create"" and ""update"" as a ""put"" action, and so if you bypass Terraform's existence check (which happens in `terraform workspace select`) then the new workspace is ""created"" just as a side-effect of writing its new state, not as an explicit action. At the moment the intent is that workspace creation is an explicit action since…",,,,,,Anecdotal,comment,,,,,,,,2018-03-22,github/apparentlymart,https://github.com/hashicorp/terraform/issues/15874#issuecomment-375447707,repo: hashicorp/terraform | issue: Document $TF_WORKSPACE | keyword: lesson learned
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2020-04-04,github/ghost,https://github.com/hashicorp/terraform/issues/15874#issuecomment-608956019,repo: hashicorp/terraform | issue: Document $TF_WORKSPACE | keyword: lesson learned
"Support use cases with conditional logic It's been important from the beginning that Terraform's configuration language is declarative, which has meant that the core team has intentionally avoided adding flow-control statements like conditionals and loops to the language. But in the real world, there are still plenty of perfectly reasonable scenarios that are difficult to express in the current version of Terraform without copious amounts of duplication because of the lack of conditionals. We'd…",,,,,,Anecdotal,issue,,,,,,,,2015-04-20,github/phinze,https://github.com/hashicorp/terraform/issues/1604,repo: hashicorp/terraform | keyword: lesson learned | state: closed
Here are 2 examples: https://gist.github.com/chrisferry/780140d709bfad51038c The RDS and ELB modules have minor differences. SSL cert or no. IOPs or no.,,,,,,Anecdotal,comment,,,,,,,,2015-04-20,github/chrisferry,https://github.com/hashicorp/terraform/issues/1604#issuecomment-94577950,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"OMG, I could rant on about this issue for a long while. One litany of clear and concise use cases are found in implementing a concept as a terraform module. There are even community modules which exemplify this.. two modules for essentially the same thing, one provides an ELB, one does not. I tend to want to write terraform source as I do with Saltstack: as a giant jinja template. This affords me a whole lot of flexibility while ensuring the application (Salt) ends up with a machine-readable fo…",,,,,,Anecdotal,comment,,,,,,,,2015-04-23,github/ketzacoatl,https://github.com/hashicorp/terraform/issues/1604#issuecomment-95446638,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"This is a bit of a stretch on the topic of this issue, but a couple of times I've found myself wishing for an iteration construct to allow me to create a set of resources that each map one-to-one to an item in a list. I've found and then promptly forgotten a number of examples (having dismissed them as impossible), but one that stayed in my mind was giving EC2 instances more memorable local hostnames and then creating Route53 records for each of them. ``` js resource ""aws_instance"" ""app_server""…",,,,,,Anecdotal,comment,,,,,,,,2015-04-23,github/apparentlymart,https://github.com/hashicorp/terraform/issues/1604#issuecomment-95633423,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"When your environment always looks the same (e.g. for a long standing/running app), the declarative language of terraform is expressive enough for most needs. However, my use case (frequent spin ups/tear downs of AWS VPCs of a similar general structure but with plenty of instance/subnet variation) means that terraform is not the ""start"" of my pipeline. I need to combine some configuration, logic, and templates on the fly each time to define my desired environment before terraform can ingest it.…",,,,,,Anecdotal,comment,,,,,,,,2015-04-25,github/bgeesaman,https://github.com/hashicorp/terraform/issues/1604#issuecomment-96127893,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"And I just realized why my suggested approach is flawed in some cases. The power of terraform is using derived data at runtime as variables elsewhere. If you render separately ahead of time in some cases, you lose that. The `count=5` is an example of something you can't pre-render and then reference easily. Gah, sorry.",,,,,,Anecdotal,comment,,,,,,,,2015-04-25,github/bgeesaman,https://github.com/hashicorp/terraform/issues/1604#issuecomment-96128272,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"The power in terraform, IMHO, is that we have the flexibility to choose how much we do _before_ , _in_ TF, and after TF runs. In most cases, you need to start working with some wrapper to create the JSON you want, when the existing interpolation syntax won't get you what you want. I personally, have avoided this as I would rather keep the _before_ limited to a CI / admin who defines details in the `terraform.tfvars` file to pass to TF when it runs. At the same time, this is also where the condi…",,,,,,Anecdotal,comment,,,,,,,,2015-04-25,github/ketzacoatl,https://github.com/hashicorp/terraform/issues/1604#issuecomment-96192938,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"Last December in AMS Dockercon I had asked @mitchellh if there would be any plans to add such logic control in Terraform DSL, he explained his view on keeping Terraform as simple as possible maybe adding a little algebraic functionality (which has already been merged) and standard string operations. I am glad there are second thoughts on this, but it is a decision that needs a lot of input and real world justification, so thatnks @phinze for bringing this up. I have two real world scenarios I '…",,,,,,Anecdotal,comment,,,,,,,,2015-04-28,github/pmoust,https://github.com/hashicorp/terraform/issues/1604#issuecomment-96992222,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"Other scenario, 1. CoreOS fleet in an AWS ASG, should a terraform run detects an even number behind ASG, deregister one etcd server from the etcd cluster to enable sensible quorum. This touches the surface of current issue raised by @phinze as such functionality (run provisioners on ASG cluster nodes) is not implemented in Terraform, and it more-or-less defy the purpose of exact clones in ASG. Still it is something I have personally come across and I manage externally.",,,,,,Anecdotal,comment,,,,,,,,2015-04-28,github/pmoust,https://github.com/hashicorp/terraform/issues/1604#issuecomment-96995250,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"A simple scenario - Optionally use Atlas artifacts to deploy infrastructure - and fallback to just AMI strings (if not using atlas). Something like - ``` if ${var.atlas_enabled} { resource ""atlas_artifact"" ""machine"" { name = ""${var.atlas_artifact.name}"" type = ""aws.ami"" } } then in a resource resource ""aws_instance"" ""machine"" { if ${var.atlas_enabled} { ami = ""${atlas_artifact.machine.id}"" } else { ami = ""${var.ami_string}"" } } ```",,,,,,Anecdotal,comment,,,,,,,,2015-05-07,github/tayzlor,https://github.com/hashicorp/terraform/issues/1604#issuecomment-99759056,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
:+1: I like the idea of even a simple ``` if ${var.atlas_enabled} { } ``` There are lots of times where I'm building the same infrastructure for dev/stage/prod but don't need things in dev/stage as are needed in prod.,,,,,,Anecdotal,comment,,,,,,,,2015-05-17,github/mzupan,https://github.com/hashicorp/terraform/issues/1604#issuecomment-102783472,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"Even simpler, doesn't break the current syntax and prevents complexity (Inspired by Ansible): ``` resource ""aws_instance"" ""machine"" { when = ""${var.atlas_enabled}"" ami = ""${atlas_artifact.machine.id}"" } resource ""aws_instance"" ""machine"" { when = ""not ${var.atlas_enabled}"" ami = ""${var.ami_string}"" } ```",,,,,,Anecdotal,comment,,,,,,,,2015-05-22,github/franklinwise,https://github.com/hashicorp/terraform/issues/1604#issuecomment-104456744,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"This seems inadequate as it only supports the conditional creation of resources. In my usage, I've found myself wanting conditional expressions (not statements) several times. The use case has been creating heterogenous groups of ec2 instances. I.e., I'd like to create 12 instances where the first 4 are of type m3.large and the remaining are c4.xlarge. I've hacked around this for the time being by using a lookup table and creating an entry for each index, but it's pretty nasty. For clarificatio…",,,,,,Anecdotal,comment,,,,,,,,2015-05-28,github/rafikk,https://github.com/hashicorp/terraform/issues/1604#issuecomment-106625166,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"It might make sense to consider these use cases separately, if only for the goal of getting the simpler implemented faster than the more complicated and nuanced case. `when` / `not` as a field on all resources is a _fantastic_ start. Figuring out the rest seems to be too much to figure out in the immediate.. why block one for the other?",,,,,,Anecdotal,comment,,,,,,,,2015-05-28,github/ketzacoatl,https://github.com/hashicorp/terraform/issues/1604#issuecomment-106625757,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"Expanding on the referenced aws spot instances - I'd like to be able to express ""spin up N instances in at least M availability zones, bidding the current bid price \* X, But fallback to on-demand for any az where the bid price is > Y, or spot instance is unavailable"". That seems complex (I have implemented it via a custom ruby script now), so maybe it's less a ""we need conditionals"" argument and more a ""it would be nice if the aws provider abstracted instance types and did the ""right thing""). …",,,,,,Anecdotal,comment,,,,,,,,2015-05-30,github/bortels,https://github.com/hashicorp/terraform/issues/1604#issuecomment-107052018,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"Just to add another use case similar to rafikk's example. I'm using terraform to bring up a count of 'container instances' or 'nodes' in an AWS ECS cluster with consul running on each one. I want the first three to be run as servers but the rest to be agents. Currently I have to duplicate the resource block which isn't very neat and any changes have to be done twice which allows too much room for human error. It would be more flexible if I could do the following: `user_data = ""${if count.index …",,,,,,Anecdotal,comment,,,,,,,,2015-06-11,github/RJSzynal,https://github.com/hashicorp/terraform/issues/1604#issuecomment-111044879,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
@RJSzynal We have the same thing in our configuration. I really think support conditional logic inside of interpolation blocks is a must.,,,,,,Anecdotal,comment,,,,,,,,2015-06-16,github/rafikk,https://github.com/hashicorp/terraform/issues/1604#issuecomment-112499101,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"**tl;dr** my five cents is that I'm for the high-level idea of better conditional / looping support, but would like to stick to keeping it declarative. I like the child resource proposal by @apparentlymart (far more than the foreach, which feels too imperative). That being said, for that specific example I feel like the resource isn't really a child. What may be better is a top-level ""group"" construct, which supports `count`-style looping. I'm also +1 for some basic conditional structure that g…",,,,,,Anecdotal,comment,,,,,,,,2015-06-18,github/thegedge,https://github.com/hashicorp/terraform/issues/1604#issuecomment-113255652,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"@thegedge sorry I just noticed your response to my earlier example even though you posted it a while back. The special thing I was imagining for ""child resources"" is that they'd always have an implied dependency on their parent, so if you delete the instance then that always deletes the record along with it... perhaps ""child"" is the wrong word, but I was going for ""this thing only exists to support the thing it's nested inside"".",,,,,,Anecdotal,comment,,,,,,,,2015-07-24,github/apparentlymart,https://github.com/hashicorp/terraform/issues/1604#issuecomment-124714126,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"@apparentlymart Ah yes, that would be nice. Could that also be solved with a ""also destroy dependent resources"" flag to `terraform destroy`? Maybe modeling these things as being intimately connected (i.e., an atomic unit) would be useful in other, not immediately obvious ways too. Anyways, I'm also going to add in an example from my team, since I didn't do that in my last comment and that's what @phinze was interested in seeing! We want our app developers to build things on top of a terraformed…",,,,,,Anecdotal,comment,,,,,,,,2015-07-30,github/thegedge,https://github.com/hashicorp/terraform/issues/1604#issuecomment-126361016,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"An example I just ran into was attempting to create a list of users on AWS: ``` variable ""devops"" { default = ""user1,user2,user3"" } resource ""aws_iam_group"" ""Administrators"" { name = ""Administrators"" path = ""/"" } resource ""aws_iam_user"" ""devops"" { count = ""${length(split("","", var.devops))}"" name = ""${element(split("","", var.devops), count.index)}"" } ``` This creates a reasonable create plan ``` + aws_iam_user.users.0 arn: """" => ""<computed>"" name: """" => ""user1"" path: """" => ""/"" unique_id: """" => ""<…",,,,,,Anecdotal,comment,,,,,,,,2015-09-04,github/glenjamin,https://github.com/hashicorp/terraform/issues/1604#issuecomment-137797595,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"If you are building a module for any resource like RDS or any resource that change behaviour when additional attributes have been set then a evaluating function is required like the following which is available in cloudformation ""MyDB"" : { ""Type"" : ""AWS::RDS::DBInstance"", ""Properties"" : { ""AllocatedStorage"" : ""5"", ""DBInstanceClass"" : ""db.m1.small"", ""Engine"" : ""MySQL"", ""EngineVersion"" : ""5.5"", ""MasterUsername"" : { ""Ref"" : ""DBUser"" }, ""MasterUserPassword"" : { ""Ref"" : ""DBPassword"" }, ""DBParameterG…",,,,,,Anecdotal,comment,,,,,,,,2015-09-07,github/aavileli,https://github.com/hashicorp/terraform/issues/1604#issuecomment-138193039,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"AWS Elasticache Redis has a snapshot parameter. Sometimes we want to pass in a snapshot, and we set this param: `snapshot_arns = [""${var.redis_snapshot}""]` Other times we do not want to pass in snapshot, however if you pass in an empty string, the elasticache cluster will not be built as its an invalid request. Even using modules and count=0, we cannot get around the conditional nature of this.",,,,,,Anecdotal,comment,,,,,,,,2015-09-10,github/jonhatalla,https://github.com/hashicorp/terraform/issues/1604#issuecomment-139083449,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"The use case that I ran into today was wishing for: ``` resource ""aws_elb"" ""maybe-ssl-elb"" { listener { instance_port = 80 instance_protocol = ""http"" lb_port = 80 lb_protocol = ""http"" } if (not empty(""${var.elb_server_certificate_arn}"")) { listener { instance_port = 443 instance_protocol = ""https"" lb_port = 443 lb_protocol = ""https"" ssl_certificate_id = ""${var.elb_server_certificate_arn}"" } } } ```",,,,,,Anecdotal,comment,,,,,,,,2015-09-11,github/kara-ryli,https://github.com/hashicorp/terraform/issues/1604#issuecomment-139419596,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"I have a module which I was using for each environment ""./app"" now I've split it up into ""./app-prod"" ""./app-stage"" etc because some environments use `user_data` some do not. And the ones that do not I can't pass in a variable that says `null`. An `if` statement would allow us to do something like: ``` if ""${var.userdata}"" { user_data = ""${var.userdata}"" } ``` Of course having a `null` or something would be good too. I might be missing something obvious.",,,,,,Anecdotal,comment,,,,,,,,2015-09-17,github/davedash,https://github.com/hashicorp/terraform/issues/1604#issuecomment-141269340,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"I wanted to at least drop in an interesting way of doing `if ! x.empty y else empty` right now, which we use to choose whether or not we chef provision with a private or public IP: ``` host = ""${replace(self.private_ip, replace(var.bastion_hosts, ""/^$/"", ""/^.*$/""), """")}"" ``` If `var.bastion_hosts` is empty, we end up replacing all of `self.private_ip` with an empty string. For the connection `host` parameter, this means terraform will pick the value for us. If it's non-empty, we search for `var…",,,,,,Anecdotal,comment,,,,,,,,2015-09-18,github/thegedge,https://github.com/hashicorp/terraform/issues/1604#issuecomment-141456800,repo: hashicorp/terraform | issue: Support use cases with conditional logic | keyword: lesson learned
"Enable support for Google Cloud SQL Postgres Hi there, Thank you for opening an issue. Please note that we try to keep the Terraform issue tracker reserved for bug reports and feature requests. For general usage questions, please see: https://www.terraform.io/community.html. ### Terraform Version Terraform v0.8.8 ### Affected Resource(s) - google_sql_database_instance ### Terraform Configuration Files ```hcl resource ""google_sql_database_instance"" ""prod-us-east1-postgres-01"" { name = ""prod-us-e…",,,,,,Anecdotal,issue,,,,,,,,2017-03-12,github/samhamilton,https://github.com/hashicorp/terraform/issues/12617,repo: hashicorp/terraform | keyword: lesson learned | state: closed
"Just to be clear, the correct config has to look like this (notice that `database_version` is defined on the top level): ```hcl resource ""google_sql_database_instance"" ""prod-us-east1-postgres-01"" { name = ""prod-us-east1-postgres-01"" region = ""us-east1"" database_version = ""POSTGRES_9_6"" settings { tier = ""db-f1-micro"" disk_size = ""10"" disk_type = ""PD_SSD"" } } ```",,,,,,Anecdotal,comment,,,,,,,,2017-04-27,github/ctavan,https://github.com/hashicorp/terraform/issues/12617#issuecomment-297662764,repo: hashicorp/terraform | issue: Enable support for Google Cloud SQL Postgres | keyword: lesson learned
"This only works with shared CPU instances. Google Cloud SQL requires a custom machine instance or shared-core for PostgreSQL. For example, the following resource: ``` resource ""google_sql_database_instance"" ""development"" { name = ""development-instance"" region = ""${var.region}"" database_version = ""POSTGRES_9_6"" settings { tier = ""db-n1-standard-1"" backup_configuration { enabled = true start_time = ""03:00"" } } } ``` Will fail with the following error: ``` Error applying plan: 1 error(s) occurred:…",,,,,,Anecdotal,comment,,,,,,,,2017-05-01,github/davejlong,https://github.com/hashicorp/terraform/issues/12617#issuecomment-298395597,repo: hashicorp/terraform | issue: Enable support for Google Cloud SQL Postgres | keyword: lesson learned
"This is interesting. When I tried it last week it worked with `db-n1-standard-1` without any problems. But back then the cloud console UI also still allowed me to choose the machine types… So I guess this is what ""beta"" status means then :/. Maybe we should reopen this issue then?",,,,,,Anecdotal,comment,,,,,,,,2017-05-02,github/ctavan,https://github.com/hashicorp/terraform/issues/12617#issuecomment-298534694,repo: hashicorp/terraform | issue: Enable support for Google Cloud SQL Postgres | keyword: lesson learned
And does anyone have a clue how to specify custom machine instance types or where to find documentation on that? https://cloud.google.com/sql/docs/postgres/admin-api/v1beta4/instances isn't really helpful at the moment.,,,,,,Anecdotal,comment,,,,,,,,2017-05-02,github/ctavan,https://github.com/hashicorp/terraform/issues/12617#issuecomment-298563506,repo: hashicorp/terraform | issue: Enable support for Google Cloud SQL Postgres | keyword: lesson learned
"I see that the Cloud SDK has `--cpu` and `--memory` options: ``` gcloud beta sql instances create --help --cpu=CPU A whole number value indicating how many cores are desired in the machine. Both --cpu and --memory must be specified if a custom machine type is desired, and the --tier flag must be omitted. --memory=MEMORY A whole number value indicating how much memory is desired in the machine. A size unit should be provided (eg. 3072MiB or 9GiB) - if no units are specified, GiB is assumed. Both…",,,,,,Anecdotal,comment,,,,,,,,2017-05-02,github/ctavan,https://github.com/hashicorp/terraform/issues/12617#issuecomment-298578284,repo: hashicorp/terraform | issue: Enable support for Google Cloud SQL Postgres | keyword: lesson learned
"I could find out that cloud console uses a custom tier called `db-custom-2-5888`. ```json { ""name"": ""testest"", ""region"": ""europe-west1"", ""project"": ""my-testing-project-1234567"", ""databaseVersion"": ""POSTGRES_9_6"", ""backendType"": ""SECOND_GEN"", ""instanceType"": ""CLOUD_SQL_INSTANCE"", ""replicaNames"": [], ""settings"": { ""dataDiskType"": ""PD_SSD"", ""tier"": ""db-custom-2-5888"", ""locationPreference"": {}, ""ipConfiguration"": { ""authorizedNetworks"": [] }, ""backupConfiguration"": { ""enabled"": true, ""binaryLogEnab…",,,,,,Anecdotal,comment,,,,,,,,2017-05-02,github/ctavan,https://github.com/hashicorp/terraform/issues/12617#issuecomment-298586684,repo: hashicorp/terraform | issue: Enable support for Google Cloud SQL Postgres | keyword: lesson learned
"Haha, I just figured out that the custom tier actually encodes the amount of CPU and memory. The first number is the number of CPUs, the second number is the amount of memory expressed in MiB. I found it out by creating several instances through the web interface and observed the payload assembled by cloud console: * 2 CPU, 13GB (=13*1024=13312MiB) ram -> `db-custom-2-13312` * 1 CPU, 4GB (=4*1024=4096MiB) ram -> `db-custom-1-4096` Just tried it out with terraform and it seems to work well! So t…",,,,,,Anecdotal,comment,,,,,,,,2017-05-02,github/ctavan,https://github.com/hashicorp/terraform/issues/12617#issuecomment-298617855,repo: hashicorp/terraform | issue: Enable support for Google Cloud SQL Postgres | keyword: lesson learned
"Thanks @ctavan , the more I work with gcloud the more I'm of the opinion that AWS is far > than gcloud. This is pretty nonsensical, having to write out a tier like that.",,,,,,Anecdotal,comment,,,,,,,,2018-10-19,github/sereeth,https://github.com/hashicorp/terraform/issues/12617#issuecomment-431445860,repo: hashicorp/terraform | issue: Enable support for Google Cloud SQL Postgres | keyword: lesson learned
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2020-04-01,github/ghost,https://github.com/hashicorp/terraform/issues/12617#issuecomment-606989471,repo: hashicorp/terraform | issue: Enable support for Google Cloud SQL Postgres | keyword: lesson learned
feature request: inverse targeting / exclude Is there anything that can be done such that db_instance - RDS formed by the terraform files can be saved if we destroy the whole state?,,,,,,Anecdotal,issue,,,,,,,,2015-06-06,github/shubhambhartiya,https://github.com/hashicorp/terraform/issues/2253,repo: hashicorp/terraform | keyword: workaround | state: open
"Hi @shubhambhartiya - we have [`prevent_destroy`](https://terraform.io/docs/configuration/resources.html#prevent_destroy) which provides protection against accidental destruction, but it sounds like perhaps you're asking about ""destroy everything but this"" feature. Can you elaborate on the behavior you're looking for?",,,,,,Anecdotal,comment,,,,,,,,2015-06-07,github/phinze,https://github.com/hashicorp/terraform/issues/2253#issuecomment-109814512,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
"Considering an example. I have a set of tf files which creates vpc, subnets, ASG, sg, instances in various subnets, nat instances and databases (RDS). I want to plan in this way such that when I destroy the plan, I want the RDS to be there (VPC and subnets would be needed), rest all the things would get destroy.",,,,,,Anecdotal,comment,,,,,,,,2015-06-08,github/shubhambhartiya,https://github.com/hashicorp/terraform/issues/2253#issuecomment-109959992,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
"Ah okay I get it now. I think I'd call what you're looking for ""inverse targeting"". ``` sh # Destroy everything except aws_db_instance.foo and its dependencies terraform plan -destroy -exclude=aws_db_instance.foo ``` ^^ If that looks like what you're asking for I'll edit the title and we can track that feature request with this thread.",,,,,,Anecdotal,comment,,,,,,,,2015-06-08,github/phinze,https://github.com/hashicorp/terraform/issues/2253#issuecomment-110023949,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
Just to confirm that it would be nice to have a feature of inverted targeting like as follows: `terraform apply -target-exclude aws_ecs_service.ecs_service` Thanks.,,,,,,Anecdotal,comment,,,,,,,,2017-02-13,github/anosulchik,https://github.com/hashicorp/terraform/issues/2253#issuecomment-279507963,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
Another use case: We're importing existing AWS environments. Migrating the DB to the new subnet group is a manual step. It would be nice to provision all the subnet/security/parameter groups before updating the instance (all part of the same module),,,,,,Anecdotal,comment,,,,,,,,2017-05-15,github/beanaroo,https://github.com/hashicorp/terraform/issues/2253#issuecomment-301363941,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
"I'd also love to see this. In the meantime, I'm using a combination of `lifecycle` to ""protect"" certain resources, and targeting like so: ``` terraform plan -destroy $(for r in `terraform state list | fgrep -v resource.address.to.exclude` ; do printf ""-target ${r} ""; done) -out destroy.plan ``` Not pretty, but it does the job 🙂",,,,,,Anecdotal,comment,,,,,,,,2017-07-28,github/cmacrae,https://github.com/hashicorp/terraform/issues/2253#issuecomment-318665739,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
"Following what @anosulchik posted: > Just to confirm that it would be nice to have a feature of inverted targeting like as follows:`terraform apply -target-exclude aws_ecs_service.ecs_service.` A target and something like a -target-exclude would be great to support regexp or by name-matching similar to consul, such as: `terraform apply -target-exclude aws_ecs_service.` would match all that start with `aws_ecs_service.` or if its regexp it can be more explicit which would be ideal thanks",,,,,,Anecdotal,comment,,,,,,,,2017-08-16,github/olenm,https://github.com/hashicorp/terraform/issues/2253#issuecomment-322833445,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
"`terraform destroy -target-exclude aws_db_instance.my_rds` It would be great to have this feature. so that we can destroy everything except rds instance. It will save a significant amount of time for us if we can just destroy everything except rds resource, as rds takes around 30 minutes to create and timeout during destroy",,,,,,Anecdotal,comment,,,,,,,,2017-09-16,github/ffoysal,https://github.com/hashicorp/terraform/issues/2253#issuecomment-329937307,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
"This would be really useful, so I can destroy everything except the resources marked with `prevent_destroy`. At the moment, because of `prevent_destroy`, I comment out everything except that code and run apply instead of destroy. Very unintuitive.",,,,,,Anecdotal,comment,,,,,,,,2017-10-09,github/ColOfAbRiX,https://github.com/hashicorp/terraform/issues/2253#issuecomment-335145477,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
"+1 :) My current workaround is to taket the outout of ""terraform plan list"" , grep out all resource I wanna keep, and then create a list of -target parameters from the rest with a shell script. Another thing that would make it supereasy to destroy everything unless the things you want to keep is to destroy all resources instead of those protected by the ""prevent_destroy"" flag. Actually, in my opinion the behaviout for that flag is not ideal - if I call destroy, I want to destroy the configured …",,,,,,Anecdotal,comment,,,,,,,,2017-10-11,github/henning,https://github.com/hashicorp/terraform/issues/2253#issuecomment-335886155,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
"Still, it would be very useful to have ```terraform apply -exclude``` as sometimes your ECS cluster has changed due to Autoscaling rules and you don't want to change that, but might want to add more resources, etc",,,,,,Anecdotal,comment,,,,,,,,2017-10-26,github/laura-herrera,https://github.com/hashicorp/terraform/issues/2253#issuecomment-339620824,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
"Jumping in to provide a use case I'm trying to meet in which I'm trying to run my tf file but exclude just one resource that calls ansible, which for testing purposes do not want to worry about for the time being. Right now it seems like I have to do several ""-target"" to include what I want, and that takes a very long time considering how many resources I have in my module. So, to re-iterate what someone else suggested, something like: `terraform plan -target-exclude="""" -target-exclude=""""` Idea…",,,,,,Anecdotal,comment,,,,,,,,2017-11-09,github/idjaw,https://github.com/hashicorp/terraform/issues/2253#issuecomment-343204637,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
"Please refrain from commenting on this ticket unless you have something to help complete this ticket. Speaking of which, could anyone review https://github.com/hashicorp/terraform/pull/3366 and let me know what’s remaining?",,,,,,Anecdotal,comment,,,,,,,,2017-11-14,github/josephholsten,https://github.com/hashicorp/terraform/issues/2253#issuecomment-344334665,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
"@josephholsten https://github.com/hashicorp/terraform/pull/3366 looks great! looking forward to this, I will test run your code next week on my env and report back",,,,,,Anecdotal,comment,,,,,,,,2017-12-21,github/olenm,https://github.com/hashicorp/terraform/issues/2253#issuecomment-353230664,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
"@creativeux & others, please refrain from posting +1 comments or other comments that don’t add to the discussion. It increases noise for everyone subscribed to the issue. Use the emoji reactions instead.",,,,,,Anecdotal,comment,,,,,,,,2018-04-26,github/tdmalone,https://github.com/hashicorp/terraform/issues/2253#issuecomment-384810914,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
"I found this thread because I was learning Terraform and thought I needed this capability. But I found a workaround that no longer required me to need `exclude`. Hopefully, it will help others that might be in the same boat as me and help them find their way around Terraform. If I restructured my configurations, I am able to use `data` instead of `resource`, which then allowed me to destroy all of my resources without triggering Terraform to destroy any resource that is managed elsewhere. So fo…",,,,,,Anecdotal,comment,,,,,,,,2018-11-26,github/zhao-li,https://github.com/hashicorp/terraform/issues/2253#issuecomment-441843773,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
"I have an additional scenario to share. I have trouble with the new configuration of a VM. So I manually disconnected a public IP address using Azure Portal and do not want it to be reconnected when I recreate the machine, but at a later stage when I am happy with my tests and can be sure not to create a security hole.",,,,,,Anecdotal,comment,,,,,,,,2019-01-31,github/giuliov,https://github.com/hashicorp/terraform/issues/2253#issuecomment-459326549,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
"Another +1 Prepping for a migration. I am using a shared module between environment declarations. One component of the module writes public DNS records upon ELB creation. I want this to happen for staging/dev but ""not-quite-yet"" for production since prod is currently pointed to the current prod env as an A record. New record post-migration is an ALIAS record root module that's being called so I can't just set the current `A` root domain record to the current value. Sure I could add some conditi…",,,,,,Anecdotal,comment,,,,,,,,2019-04-17,github/emmm-dee,https://github.com/hashicorp/terraform/issues/2253#issuecomment-484257171,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
"Error: Only .tf, .tfvars, and .tftest.hcl files can be processed with terraform fmt ### Terraform Version ```shell Terraform v1.10.5 ``` ### Use Cases We have terraform files in a custom pipeline with the name tfvars.template. Such naming convention is not supported by terraform fmt. It will be useful to have a specific flag in terraform fmt to accept any file. Something like ""terraform fmt -all"". Or maybe even ""fmt"" by a specific file extension. For example: ``` terraform fmt -ext template -re…",,,,,,Anecdotal,issue,,,,,,,,2025-08-14,github/YevheniiPokhvalii,https://github.com/hashicorp/terraform/issues/37447,repo: hashicorp/terraform | keyword: workaround | state: open
"Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the 👍 reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. For a workaround, please see this comment: https://github.com/hashicorp/terraform/issues/36564#issuecomment-2679452650 Thanks again!",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/crw,https://github.com/hashicorp/terraform/issues/37447#issuecomment-3189228735,"repo: hashicorp/terraform | issue: Error: Only .tf, .tfvars, and .tftest.hcl files can be processed with terraform fmt | keyword: workaround"
"Allow private provider registries to self-host provider packages/checksums/signatures under the same credentials as the registry itself I have a remote Terraform registry that experiencing 401 error on the provider download (download_url, shasums_url and shasums_signature_url). It appears that only for those 3 downloads, the client does not add the authorization header. I have one domain for the registry API, the modules and the providers files. All the other registry API calls are executed wit…",,,,,,Anecdotal,issue,,,,,,,,2021-08-11,github/BarakHc,https://github.com/hashicorp/terraform/issues/29349,repo: hashicorp/terraform | keyword: workaround | state: open
"Hi @BarakHc! Thanks for reporting this. I guess this is the _provider_ registry equivalent of #28659, where we were discussing the same behavior for _module_ installation. Again, unfortunately this behavior is intended rather than a bug because from Terraform's perspective the registry is just a pointer to the location for the files, and the files themselves are treated as an entirely separate system. As in that other issue, I do see the benefit of having a way to treat the package files as if …",,,,,,Anecdotal,comment,,,,,,,,2021-08-16,github/apparentlymart,https://github.com/hashicorp/terraform/issues/29349#issuecomment-899881338,repo: hashicorp/terraform | issue: Allow private provider registries to self-host provider packages/checksums/signatures under the same credentials as the registry itself | keyword: workaround
Nice I'm not alone with this use case. My use case is similar but we use github releases as storage backend. Once provider is built GH [action](https://github.com/AbsaOSS/ghpages-to-tf-provider-registry) triggers Provider registry [generator](https://github.com/AbsaOSS/tf-provider-registry-generator) which publishes provider metadata into github pages. So we're fine to have registry token the same as download token. So far we're on the same page here. I think this could be a good point to start…,,,,,,Anecdotal,comment,,,,,,,,2021-09-25,github/k0da,https://github.com/hashicorp/terraform/issues/29349#issuecomment-927102742,repo: hashicorp/terraform | issue: Allow private provider registries to self-host provider packages/checksums/signatures under the same credentials as the registry itself | keyword: workaround
"Hi @aaklilu! Thanks for working on that. I'm not on the Terraform team anymore, so I'll need to leave it to someone still on the team to consider what you submitted. (FWIW, It seems like https://github.com/hashicorp/terraform/issues/36011 is effectively a duplicate of this issue and https://github.com/hashicorp/terraform/issues/28659 but framed as a problem with the documentation.)",,,,,,Anecdotal,comment,,,,,,,,2024-11-15,github/apparentlymart,https://github.com/hashicorp/terraform/issues/29349#issuecomment-2479553091,repo: hashicorp/terraform | issue: Allow private provider registries to self-host provider packages/checksums/signatures under the same credentials as the registry itself | keyword: workaround
"@aaklilu's .netrc workaround for this issue has been merged. I'm not closing this issue as I think the .netrc workaround is not a true fix for this and we might want to implement something properly in the future but I don't know what that will look like. For the time being, from Terraform v1.11 going forward you'll be able to use a .netrc file to attach credentials to follow up provider requests should that be required.",,,,,,Anecdotal,comment,,,,,,,,2024-11-21,github/liamcervante,https://github.com/hashicorp/terraform/issues/29349#issuecomment-2490655054,repo: hashicorp/terraform | issue: Allow private provider registries to self-host provider packages/checksums/signatures under the same credentials as the registry itself | keyword: workaround
"> @aaklilu's .netrc workaround for this issue has been merged. I'm not closing this issue as I think the .netrc workaround is not a true fix for this and we might want to implement something properly in the future but I don't know what that will look like. > > For the time being, from Terraform v1.11 going forward you'll be able to use a .netrc file to attach credentials to follow up provider requests should that be required. Thanks @liamcervante, I believe this is an acceptable workaround for …",,,,,,Anecdotal,comment,,,,,,,,2024-11-21,github/aaklilu,https://github.com/hashicorp/terraform/issues/29349#issuecomment-2490730055,repo: hashicorp/terraform | issue: Allow private provider registries to self-host provider packages/checksums/signatures under the same credentials as the registry itself | keyword: workaround
"FWIW what _I_ had in mind when writing my earlier comments was to extend the [Find a Provider Package](https://developer.hashicorp.com/terraform/internals/provider-registry-protocol#find-a-provider-package) response format with a new optional property like `""use_registry_host_credentials""` which can be set to `true` if the registry trusts the URLs given in `""download_url""`, `""shasums_url""`, and `""shasums_signature_url""` enough to disclose the user's registry host auth token to them. If that wer…",,,,,,Anecdotal,comment,,,,,,,,2024-11-26,github/apparentlymart,https://github.com/hashicorp/terraform/issues/29349#issuecomment-2499387934,repo: hashicorp/terraform | issue: Allow private provider registries to self-host provider packages/checksums/signatures under the same credentials as the registry itself | keyword: workaround
"@apparentlymart it seems that getterHTTPGetter (in go-getter) used to retrieve modules in getmodules, has support already for NetRC credentials. It tries to add credentials for Get and GetFile methods using addAuthFromNetrc. I haven't tested it myself, but there might be chance that with these recent changes for providers, it's also possible to obtain modules from a private registry using NetRC (HTTP basic auth). We were having issues with an identity proxy that uses oauth, so NetRC won't be us…",,,,,,Anecdotal,comment,,,,,,,,2025-03-11,github/fancybear-dev,https://github.com/hashicorp/terraform/issues/29349#issuecomment-2713608597,repo: hashicorp/terraform | issue: Allow private provider registries to self-host provider packages/checksums/signatures under the same credentials as the registry itself | keyword: workaround
"I think the interim solution (enabling .netrc) currently is incomplete for providers: - download url - DONE - thanks to @aaklilu [35843](https://github.com/hashicorp/terraform/pull/35843) - shasums url - TODO - [registry_client.go](https://github.com/hashicorp/terraform/blob/01babe4a96ef9d819b71ab2cb7f18f87ed44eb0c/internal/getproviders/registry_client.go#L329) - shasums signature url - TODO - same as above @liamcervante @aaklilu can someone help with this plz? I'm happy to contribute, just not…",,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/oefimov,https://github.com/hashicorp/terraform/issues/29349#issuecomment-3184756317,repo: hashicorp/terraform | issue: Allow private provider registries to self-host provider packages/checksums/signatures under the same credentials as the registry itself | keyword: workaround
"Creating a new workspace with `terraform workspace new -state=tf.state.default` does not work for s3 remote state. <!-- Hi there, Thank you for opening an issue. Please note that we try to keep the Terraform issue tracker reserved for bug reports and feature requests. For general usage questions, please see: https://www.terraform.io/community.html. If your issue relates to Terraform Cloud/Enterprise, please contact tf-cloud@hashicorp.support. If your issue relates to a specific Terraform provid…",,,,,,Anecdotal,issue,,,,,,,,2021-10-27,github/dnozay,https://github.com/hashicorp/terraform/issues/29819,repo: hashicorp/terraform | keyword: workaround | state: open
"Thanks for reporting this, @dnozay. For most commands `-state=...` is [a legacy option for the local backend only](https://www.terraform.io/docs/language/settings/backends/local.html#command-line-arguments), but it seems like it intentionally has a different meaning for `terraform workspace new`, because that command is handling the option inline itself rather than passing it over to the backend as other commands do: https://github.com/hashicorp/terraform/blob/de105595e2788b5614081a295268cdb759…",,,,,,Anecdotal,comment,,,,,,,,2021-10-29,github/apparentlymart,https://github.com/hashicorp/terraform/issues/29819#issuecomment-955087649,repo: hashicorp/terraform | issue: Creating a new workspace with `terraform workspace new -state=tf.state.default` does not work for s3 remote state. | keyword: workaround
"As mentioned in the repro scenario ``` terraform workspace new -state=tf.state.default ${USER}_testing terraform state list | wc -l ``` shows no resources when using s3 remote state; I've also tried with gcs, and that worked much better.",,,,,,Anecdotal,comment,,,,,,,,2021-11-02,github/dnozay,https://github.com/hashicorp/terraform/issues/29819#issuecomment-957110348,repo: hashicorp/terraform | issue: Creating a new workspace with `terraform workspace new -state=tf.state.default` does not work for s3 remote state. | keyword: workaround
This is still an issue on: ``` Terraform v1.3.1 on linux_amd64 + provider registry.terraform.io/hashicorp/aws v4.32.0 ```,,,,,,Anecdotal,comment,,,,,,,,2022-09-30,github/tsibley,https://github.com/hashicorp/terraform/issues/29819#issuecomment-1264062844,repo: hashicorp/terraform | issue: Creating a new workspace with `terraform workspace new -state=tf.state.default` does not work for s3 remote state. | keyword: workaround
I have the same problem with: ``` Terraform v1.4.0 on darwin_arm64 + provider registry.terraform.io/hashicorp/aws v4.57.1 ``` When state is stored in Azure storage account it works.,,,,,,Anecdotal,comment,,,,,,,,2023-03-09,github/mattew,https://github.com/hashicorp/terraform/issues/29819#issuecomment-1461350388,repo: hashicorp/terraform | issue: Creating a new workspace with `terraform workspace new -state=tf.state.default` does not work for s3 remote state. | keyword: workaround
"`terraform state rm` does not respect local state backend's `workspace_dir` when creating state backup ### Terraform Version ```shell Terraform v1.5.1 on linux_amd64 ``` ### Terraform Configuration Files ``` ``` ### Debug Output - ### Expected Behavior When using local state with custom `workspace_dir`, `terraform state rm` stores the state backup into the correct `workspace_dir` ### Actual Behavior `terraform state rm` tries and fails to save state backup into the default directory which does …",,,,,,Anecdotal,issue,,,,,,,,2023-06-23,github/cspotcode,https://github.com/hashicorp/terraform/issues/33426,repo: hashicorp/terraform | keyword: workaround | state: open
"Thanks for this issue report! If you are viewing this issue and would like to indicate your interest, please use the 👍 reaction on the issue description to upvote this issue. We also welcome additional reports. Thanks again!",,,,,,Anecdotal,comment,,,,,,,,2023-06-27,github/crw,https://github.com/hashicorp/terraform/issues/33426#issuecomment-1610183606,repo: hashicorp/terraform | issue: `terraform state rm` does not respect local state backend's `workspace_dir` when creating state backup | keyword: workaround
"I'm also facing this problem and I'd say this is not a ""use case"" but a bug. I export [the `TF_CLI_ARGS_init` environment variable](https://developer.hashicorp.com/terraform/cli/config/environment-variables#tf_cli_args-and-tf_cli_args_name) with `-backend-config=/path/to/local-backend-options.hcl` as value where the file contains the `path` and `workspace_dir` fields, both set to a custom location (maybe a unnecessary detail, but I do this because the custom path contains all my TF states which…",,,,,,Anecdotal,comment,,,,,,,,2024-04-24,github/svengreb,https://github.com/hashicorp/terraform/issues/33426#issuecomment-2075809854,repo: hashicorp/terraform | issue: `terraform state rm` does not respect local state backend's `workspace_dir` when creating state backup | keyword: workaround
"Feature Request: Verbose option for plan <!-- Hi there, Thank you for opening an issue. Please note that we try to keep the Terraform issue tracker reserved for bug reports and feature requests. For general usage questions, please see: https://www.terraform.io/community.html. If your issue relates to a specific Terraform provider, please open it in the provider's own repository. The index of providers is at https://github.com/terraform-providers . --> ### Current Terraform Version <!--- Run `te…",,,,,,Anecdotal,issue,,,,,,,,2021-01-19,github/gitwitheprogram,https://github.com/hashicorp/terraform/issues/27547,repo: hashicorp/terraform | keyword: workaround | state: open
"@gitwitheprogram Sorry, no real answer to this, but at least you don't need to write a wrapper. Just set the value as a prefix to the command in the same line `TF_X_CONCISE_DIFF=0 terraform plan`. This will set the environment variable just for this one shot.",,,,,,Anecdotal,comment,,,,,,,,2021-01-25,github/jgrumboe,https://github.com/hashicorp/terraform/issues/27547#issuecomment-767187662,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"True. However, I wasn't really looking for a better kludge. My point was just that having the -verbose flag here seems like a pretty standard thing. No user is going to know that they can set that environment variable without reading the release notes, as it's not mentioned in the -help output or in the documentation [here](https://www.terraform.io/docs/cli/commands/plan.html) that I can see. It seems that the intention was to not provide functionality for users to get the full output, and I wa…",,,,,,Anecdotal,comment,,,,,,,,2021-01-27,github/gitwitheprogram,https://github.com/hashicorp/terraform/issues/27547#issuecomment-768295108,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"I recently had a situation where the full output was **essential** to determining that terraform would do the correct thing before apply. ``` # module.<redacted>.aws_security_group.<redacted>-dbs must be replaced -/+ resource ""aws_security_group"" ""<redacted>-dbs"" { ~ arn = ""<redacted>"" -> (known after apply) ~ description = ""<redacted>"" -> (known after apply) # forces replacement ~ id = ""sg-<redacted>"" -> (known after apply) ~ ingress = [ - { - cidr_blocks = [ - ""10.10.96.0/19"", - ""10.10.128.0/…",,,,,,Anecdotal,comment,,,,,,,,2021-03-30,github/techdragon,https://github.com/hashicorp/terraform/issues/27547#issuecomment-810038104,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"It would also be great to add an option to expand _n_ lines of context around the line of interest, e.g. `TF_X_DIFF_LINES=2` would show 2 lines above and below the line which would like to be modified, regardless of being part of some block or not. The use case I had was changing environmental variables for AWS ECS task, which was showing only env var values and not their names, while in the same time showing surrounding blocks for other env vars which were not changing and which were not relev…",,,,,,Anecdotal,comment,,,,,,,,2021-04-13,github/bomb-on,https://github.com/hashicorp/terraform/issues/27547#issuecomment-818615987,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"Just came across this today for the first time after upgrading to Terraform v0.15.4 and it seems to me that even the `TF_X_CONCISE_DIFF` env var might be deprecated, as it doesn't seem to do anything... I just want to echo the concerns of others about not being able to see the full plan. Seeing the full plan is part of the development cycle in some cases. If I'm interacting with some new or convoluted cloud service, I'll sometimes end up creating the resources manually in the console, then craf…",,,,,,Anecdotal,comment,,,,,,,,2021-05-30,github/soapergem,https://github.com/hashicorp/terraform/issues/27547#issuecomment-850932625,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"Although the concise plan has worked wonders in preserving my sanity since it was introduced, I recently realised while working with `aws_cloudfront_distribution` that there is a valid case for having a fully verbose plan when working with such resources that have complex internal structures. Specifically changes made to this resource's `ordered_cache_behavior` blocks are hard to understand when the context does not include some identifying attribute such as the `path_pattern`. For example: ```…",,,,,,Anecdotal,comment,,,,,,,,2021-06-04,github/alkar,https://github.com/hashicorp/terraform/issues/27547#issuecomment-854692593,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"We need either a more verbose plan, or smarter one, where TF would do a better job at distinguishing repeatable blocks. I think verbose option is simpler?",,,,,,Anecdotal,comment,,,,,,,,2021-06-15,github/burner1024,https://github.com/hashicorp/terraform/issues/27547#issuecomment-861568557,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"Looking at the env var, Frankly I am suprised this was taken away. I am now in a position where terraform says there is a change, but hiding the change, due to the succint diff changes. What changed on these two resources? They have a change ""~"" but no sub attributes listed.. Am I safe to apply? At this point I want to revert back to a version of terraform that actually tells me whats going on. `` ``` # azurerm_api_management_api.non_prod_dnaapi-person[""uat""] has been changed ~ resource ""azurer…",,,,,,Anecdotal,comment,,,,,,,,2021-07-14,github/chadcarlton,https://github.com/hashicorp/terraform/issues/27547#issuecomment-880210702,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
This really helps when PLAN redirects to an approver who doesn't have access to the real resource. please enable flag for **”Objects have been changes outside of Terraform”** and **unchanged blocks hidden** new features.,,,,,,Anecdotal,comment,,,,,,,,2021-09-05,github/dduleep,https://github.com/hashicorp/terraform/issues/27547#issuecomment-913103908,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"Is there any way to see the full output now that `TF_X_CONCISE_DIFF` has been removed? A flag to specify the number of surrounding lines or attributes would also work here. For context this is with the `ordered_cache_behavior` on the `aws_cloudfront_distribution` that is causing the most issues. When adding in a new cache behaviour you have no context to know if you created the order correctly. The only workaround I have come up with is to apply the change, go check if its correct, and then adj…",,,,,,Anecdotal,comment,,,,,,,,2021-09-16,github/perobertson,https://github.com/hashicorp/terraform/issues/27547#issuecomment-921218231,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"Modifying my comment from the recently linked and closed issue. I agree we need a -verbose option. In our situation, a concise diff is problematic when updating an aws_ecs_task_definition json blob for the container_definitions variable, and that blob contains multiple containers (sidecars). It's not possible to tell which sidecar/container definition a change is in without the full diff of the full json blob or a smarter concise diff which can grok a json blob and include the relavent context.…",,,,,,Anecdotal,comment,,,,,,,,2021-10-21,github/jchristner-trustwave,https://github.com/hashicorp/terraform/issues/27547#issuecomment-948721899,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"The only thing I could do was rollback to v0.15.3...because v0.15.4 is here the ""feature""/problem about extra unnecessary info began.. According to me, all that we gotta take care of is this: whether my config and reality is matching...I don't care what the terraform records in tfstate file..and then screams when it sees a wrong order in the reality....say order in json, for instance...",,,,,,Anecdotal,comment,,,,,,,,2021-10-21,github/imaginarynik,https://github.com/hashicorp/terraform/issues/27547#issuecomment-948762608,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
I came here because I couldn't see exactly what is going to happen and only turn to closed https://github.com/hashicorp/terraform/issues/28906. I don't see a reason for hiding things and not allowing people see exact plan.,,,,,,Anecdotal,comment,,,,,,,,2021-11-04,github/vojkny,https://github.com/hashicorp/terraform/issues/27547#issuecomment-960474318,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
This issue currently (17th November 2021) has the label: `enhancement` which further confirms the ineptitude of the maintainers. The issue being discussed is clearly that changes made by Hashicorp have broken required functionality making the current version of Terraform potentially _unusable_ for resources containing repeatable blocks. There may of course be many other examples. Can this issue be given the labelling and therefore the priority it needs please?,,,,,,Anecdotal,comment,,,,,,,,2021-11-17,github/DoctorPolski,https://github.com/hashicorp/terraform/issues/27547#issuecomment-971543035,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"We have CI/CD pipelines for too sensitive network changes. Every network changes plan is going through many higher management approval processes as it is critical to the entire business. Somehow trim plan does not help out for understanding the changes example **code before update** ``` resource ""azurerm_virtual_network"" ""example"" { name = ""virtualNetwork1"" location = azurerm_resource_group.example.location resource_group_name = azurerm_resource_group.example.name address_space = [""10.0.0.0/16""…",,,,,,Anecdotal,comment,,,,,,,,2022-01-03,github/dduleep,https://github.com/hashicorp/terraform/issues/27547#issuecomment-1004103636,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"As another example of why this is critical, I just encountered the following plan today for a change to a GCP IAM policy: ``` ~ { ~ members = [ ""group:<group-name>"", - ""serviceAccount:<service-account-name>"", ] # (1 unchanged element hidden) }, ``` It's very frustrating to not be able to see that particular ""unchanged"" element, since it's quite important to know which role the service account is being removed from!",,,,,,Anecdotal,comment,,,,,,,,2022-01-14,github/dwangfaulkner-fn,https://github.com/hashicorp/terraform/issues/27547#issuecomment-1013433236,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"Another example that caused a small panic initially. An AWS account has been (correctly) suspended pending closure within the AWS console. Terraform reported this as follows: ``` # aws_organizations_organization.xxxx1_organisation has changed ~ resource ""aws_organizations_organization"" ""xxxx1_organisation"" { ~ accounts = [ # (10 unchanged elements hidden) { arn = ""arn:aws:organizations::123456789012:account/o-abcdefghij/000000000001"" email = ""Xxxx.Yyyy+nt-production-compute@xxxx1.co.uk"" id = ""0…",,,,,,Anecdotal,comment,,,,,,,,2022-01-28,github/rquadling,https://github.com/hashicorp/terraform/issues/27547#issuecomment-1024133743,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
Is there any fix to this? We can't trust our terraform code anymore because of this bug basically. I'm running Terraform version `1.1.4` and AWS Provider version `3.70.0` and still seeing this issue.,,,,,,Anecdotal,comment,,,,,,,,2022-02-02,github/piersf,https://github.com/hashicorp/terraform/issues/27547#issuecomment-1028058014,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"Hi All, Do we have any alternative or workaround to get the full Verbose Plan? `terraform state show` for remote state or any other options? It is quite hard to justify as an issue bing open long time. 😒",,,,,,Anecdotal,comment,,,,,,,,2022-02-13,github/dduleep,https://github.com/hashicorp/terraform/issues/27547#issuecomment-1038019629,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"@alisdair I'm tagging you since you are the last confirmed contributor to have seen this. Could we get anyone from Hashicorp to give a statement on this issue? As shown in multiple examples, the current diff format is creating a lot of problems. We need to be able to trust the tools we use and Terraform is eroding that trust by not being verbose enough.",,,,,,Anecdotal,comment,,,,,,,,2022-03-01,github/deiga,https://github.com/hashicorp/terraform/issues/27547#issuecomment-1055213954,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"Walking through the various comments with @alisdair, there are three categories of requests in this thread. First, the original enhancement request for a verbose plan output. Second, legitimate bugs in the current plan output code. Third, other enhancement requests. I want to start by enumerating the bugs identified in comments. ~If the original commenter or another contributor is facing one of these issues, could please open these as discrete issues following the issue format template? That wi…",,,,,,Anecdotal,comment,,,,,,,,2022-03-04,github/crw,https://github.com/hashicorp/terraform/issues/27547#issuecomment-1058712097,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"> As for the original enhancement request per the original issue, there is currently no commitment to implement a verbose plan option in the near future. It would be very difficult to implement in the current design, and so plan output will likely not significantly change without the current implementation being overhauled. This is a pretty terrible outcome to be frank. The plan diffs are a critical component that this tool revolves around, to not allow devs to see the entire plan output when d…",,,,,,Anecdotal,comment,,,,,,,,2022-03-23,github/mattsfrey,https://github.com/hashicorp/terraform/issues/27547#issuecomment-1075967178,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"> Two comments look like a similar enhancement request: that the provider should be able to set a flag on a resource attribute that marks it as an ""identifier,"" and that identifier should always be printed in the plan output. @alisdair is looking into whether or not there is an existing enhancement request for this, if not we will create one and update this issue. This would be a very welcome change that I think should help with most of the issues we see with regards to the concise diff. Howeve…",,,,,,Anecdotal,comment,,,,,,,,2022-03-23,github/alkar,https://github.com/hashicorp/terraform/issues/27547#issuecomment-1076513501,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"#30753 is tracking the inability for Terraform to display identifying information for resources or substructures in which `id` and `name` are not present or not identifying. #30685 has been merged as a fix for the bug described by five cases reported in this thread, and will be released in Terraform 1.1.8.",,,,,,Anecdotal,comment,,,,,,,,2022-03-28,github/alisdair,https://github.com/hashicorp/terraform/issues/27547#issuecomment-1080778130,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"We very much appreciate the feedback in this thread. For the time being I am going to lock it, primarily to encourage any new issues in the existing system to be reported discreetly. For those who would like to add their voice to this request, please continue to vote up the issue with the 👍 emoji on the issue description. Thanks for all the feedback on this issue, we do truly appreciate it.",,,,,,Anecdotal,comment,,,,,,,,2022-03-28,github/crw,https://github.com/hashicorp/terraform/issues/27547#issuecomment-1081237940,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
[This comment](https://github.com/hashicorp/terraform/issues/28906#issuecomment-866969227) has the best workaround I've seen: ``` terraform plan -out planfile terraform show -json planfile | jq '.resource_changes' ```,,,,,,Anecdotal,comment,,,,,,,,2025-02-13,github/eddydee123,https://github.com/hashicorp/terraform/issues/27547#issuecomment-2657246988,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"Use xdg basedir spec on linux ### Terraform Version 0.9.8 https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html TLDR: Instead of ~/.terraform.d, configuration should be in the `$XDG_CONFIG_HOME/terraform/` and cache (safely-deletable files) should be in `$XDG_CACHE_HOME/terraform`. If not defined, `$XDG_CONFIG_HOME` should be defaulted to `$HOME/.config` and `$XDG_CACHE_HOME` should be defaulted to `$HOME/.cache`. I believe everything currently in .terraform.d is consider…",,,,,,Anecdotal,issue,,,,,,,,2017-06-23,github/jleclanche,https://github.com/hashicorp/terraform/issues/15389,repo: hashicorp/terraform | keyword: workaround | state: open
"Thanks for pointing this out, @jleclanche. This seems like a reasonable idea as long as we can find a way to get there without breaking things for people already using the current paths, though should think about whether it makes sense to adopt this just for Linux (and thus have to document another platform-specific difference) or across all UNIX-flavored platforms (where it might seem more alien). I'd like to let this one soak for a little while before we take any action. Thanks again for poin…",,,,,,Anecdotal,comment,,,,,,,,2017-06-23,github/apparentlymart,https://github.com/hashicorp/terraform/issues/15389#issuecomment-310781670,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
👍 Out of curiosity is there anything that would actually *break* things if not given an upgrade path? All I see in my dir is `.terraform.d/checkpoint_cache .terraform.d/checkpoint_signature`; both of which look to be cache for update checks.,,,,,,Anecdotal,comment,,,,,,,,2017-06-23,github/jleclanche,https://github.com/hashicorp/terraform/issues/15389#issuecomment-310782038,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
"Yeah, indeed... seems worth thinking about splitting the cache-ish things from the settings-ish things too, like you said.",,,,,,Anecdotal,comment,,,,,,,,2017-06-23,github/apparentlymart,https://github.com/hashicorp/terraform/issues/15389#issuecomment-310783684,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
Is there an environment variable to customize the location of this directory? I switch between OSX and Linux and I like to use environment variables to keep the location of config files consistent.,,,,,,Anecdotal,comment,,,,,,,,2017-07-05,github/jcrben,https://github.com/hashicorp/terraform/issues/15389#issuecomment-312980436,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
"Hi again @jleclanche! Sorry for leaving this here so long. We've been adding some more stuff into this dir in the mean time, such as more config files (`.tfrc` files) and a search location for plugins, so I think we do need to make sure to have a good upgrade path here, and indeed not everything in there is cache anymore. :confounded: I do like the idea of following the relevant standards and of distinguishing config from cache, but I want to go into it with a stronger idea of how this changes …",,,,,,Anecdotal,comment,,,,,,,,2017-10-24,github/apparentlymart,https://github.com/hashicorp/terraform/issues/15389#issuecomment-338833388,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
"If you want to look at how to implement it cross-platform, I recommend looking at the QStandardPaths implementation from Qt: https://doc.qt.io/qt-5/qstandardpaths.html This follows XDG standards on Linux and other platform-specific quirks for macOS and Windows.",,,,,,Anecdotal,comment,,,,,,,,2017-10-24,github/jleclanche,https://github.com/hashicorp/terraform/issues/15389#issuecomment-338833977,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
"In the case of a fair number of my tools, such as git, if I define the XDG variables, they are used on OSX as well. See https://git-scm.com/docs/git-config#git-config---global A simpler place to start is to just let me place the entire directory into a different place.",,,,,,Anecdotal,comment,,,,,,,,2018-04-01,github/jcrben,https://github.com/hashicorp/terraform/issues/15389#issuecomment-377796689,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
"As a Mac user, I am against `~/Library/Preferences`. According to https://github.com/rust-lang/rfcs/pull/1615#issuecomment-386911319, the folder is for configuration plists for the ""user defaults"" subsystem, though I couldn't find an official source for this. Instead, there seems to be consensus in following XDG or providing an environment variable, both of which allow for neater, cleaner `$HOME`s. The actual change does not appear difficult, relative to the docs and migration. I might try subm…",,,,,,Anecdotal,comment,,,,,,,,2018-12-04,github/0az,https://github.com/hashicorp/terraform/issues/15389#issuecomment-444023674,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
"Hi all, We're intentionally not making any changes related to this right now because we're trying to wrap up development of the v0.12.0 release, which already has a large scope and so we'd prefer not to make changes to how Terraform interacts with the host system at the same time. However, I'd love to dive into this some more after that and see what makes sense here. Given that there are a few different options, I think the best way to proceed here is to come to some consensus on what behavior …",,,,,,Anecdotal,comment,,,,,,,,2018-12-04,github/apparentlymart,https://github.com/hashicorp/terraform/issues/15389#issuecomment-444206642,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
"@apparentlymart Check this out: https://wiki.archlinux.org/index.php/XDG_Base_Directory This is a list of Software Conforming to the XDG Base Directory Standard. If the Source was avail at the time in a git/svn repo, the pr/merge/commit is linked there. Ex: https://cgit.freedesktop.org/libreoffice/ure/commit/?id=a6f56f7 https://cgit.freedesktop.org/libreoffice/bootstrap/commit/?id=25bd2ee Those have OSX Examples https://src.chromium.org/viewvc/chrome/trunk/src/chrome/common/chrome_paths.cc?r1=2…",,,,,,Anecdotal,comment,,,,,,,,2018-12-27,github/rouing,https://github.com/hashicorp/terraform/issues/15389#issuecomment-450206196,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
"In reference to macOS. Since the ~/Library is by default hidden on Mac, the XDG should be used by command line applications leaving the ~/Library to native macOS applications .",,,,,,Anecdotal,comment,,,,,,,,2019-06-25,github/philoserf,https://github.com/hashicorp/terraform/issues/15389#issuecomment-505258099,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
I've got plenty of programs that are as native as terraform that output to `~/Library` and `~/Library/Application Support` * Jupyter * nvm * rustup * texlive * virtualbox * virtualenv * zsh etc. It doesn't really make sense to use the XDG standards for MacOS when MacOS already has it's own. this Go XDG library accounts for Mac & Windows: [OpenPeeDeeP/xdg](https://github.com/OpenPeeDeeP/xdg) Rust has a similar one called [dirs](https://crates.io/crates/dirs) <https://stackoverflow.com/questions/…,,,,,,Anecdotal,comment,,,,,,,,2019-06-26,github/06kellyjac,https://github.com/hashicorp/terraform/issues/15389#issuecomment-505945756,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
Three of those are gui apps or have a GUI over a cmdline executable. I guess i am out of touch with the macOS community.,,,,,,Anecdotal,comment,,,,,,,,2019-06-26,github/philoserf,https://github.com/hashicorp/terraform/issues/15389#issuecomment-506033233,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
"I honestly find the basedirs more useful because of the reduced cognitive load when performing backup and/or synchronization. Instead of cherry-picking from `~/Library`, we just have the XDG directories.",,,,,,Anecdotal,comment,,,,,,,,2019-06-26,github/0az,https://github.com/hashicorp/terraform/issues/15389#issuecomment-506050093,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
"So instead of applications on MacOS conforming to the OS's official standards they should move to match XDG?... The only reason for cognitive load is because of people not following the OS's standards. If all Linux programs followed XDG, Windows programs go to `AppData` etc, Mac go to `~/Library` etc, the only cognitive load is knowing where the standard locations are. There is more cognitive load due to the mishmash caused by ignoring the OS's standards and having them go in various different …",,,,,,Anecdotal,comment,,,,,,,,2019-06-26,github/06kellyjac,https://github.com/hashicorp/terraform/issues/15389#issuecomment-506066764,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
"Look in your ~/Library/Preferences I looked at mine. Nearly only macOS GUI apps. Where are the rest? dotfiles in ~/ or a few, that I think are well behaved, in ~/.config. among those in ~/ items provided by Apple as it ships. I return to where I started, macOS native apps in the ~/Library and command line executables following XDG.",,,,,,Anecdotal,comment,,,,,,,,2019-06-27,github/philoserf,https://github.com/hashicorp/terraform/issues/15389#issuecomment-506105019,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
Well I listed both non native apps & some cli apps that follow that spec. Additionally my ZSH setup has `XDG_CONFIG_HOME` set to `~/.config` so any cli programs that give priority to the env variable will have avoided `~/Library` all together. I'm going to start working on this to strictly respect XDG. Maybe later we can have a further chat about Mac standard dirs but even then `XDG_...` env vars would take priority. At the end of the day I just want it out of my base home dir and also split co…,,,,,,Anecdotal,comment,,,,,,,,2019-06-27,github/06kellyjac,https://github.com/hashicorp/terraform/issues/15389#issuecomment-506234349,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
In the PR I've explained the flow of priorities/preference. Please see the flows & the note in the first section and give me your thoughts. Once we've decided on the order I'll get to work on fixing the docs etc. to match the changes,,,,,,Anecdotal,comment,,,,,,,,2019-06-27,github/06kellyjac,https://github.com/hashicorp/terraform/issues/15389#issuecomment-506304898,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
"As per the spec, the alternate directories should be `~/.config/*` instead of `~/config/*`. In addition, I believe the spec mandates `$XDG_CONFIG_HOME/terraform/terraformrc` instead of `$XDG_CONFIG_HOME/terraform/.terraformrc` (note the lack of full stop in the previous). Even if it isn't mandated, all other spec implementations I'm aware of drop the ""dot"". I am strongly against `$XDG_*` taking lower priority, especially as other projects will gracefully fall back to the legacy locations.",,,,,,Anecdotal,comment,,,,,,,,2019-06-30,github/0az,https://github.com/hashicorp/terraform/issues/15389#issuecomment-507049496,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
Where did I miss the dot in `.config`? I just had a check but line 54 in `config_unix.go` has the dot. zsh keeps the dot in `.zsh` even if youre using `~/.config/zsh` as your `ZDOTDIR`. Im happy to swap it & I prefer it without the dot when outside of the home dir aesthetically but I went with the easy option of leaving it. Making it swap between dot for in home and no dot for elsewhere will take some extra logic & refactoring but I'll get on that soon. The only issue with the env var taking hi…,,,,,,Anecdotal,comment,,,,,,,,2019-06-30,github/06kellyjac,https://github.com/hashicorp/terraform/issues/15389#issuecomment-507052116,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
"macOS's difference comes up every time in issues about adhering to XDG... essentially I think there's two camps: - those that version (or otherwise curate) their dotfiles, and want everything with text configuration to be in their `$XDG_CONFIG_HOME` - those that don't, and can't see why anybody wouldn't want to use `~/Library/Application\ Suppport/*/Content/Resources/MacOS` or whatever it is The really easy resolution is simply to respect `$XDG_CONFIG_HOME` iff it exists! (resp. `$XDG_CACHE_HOM…",,,,,,Anecdotal,comment,,,,,,,,2019-07-21,github/OJFord,https://github.com/hashicorp/terraform/issues/15389#issuecomment-513576281,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
I've changed the code to only keep the `.` in-front of `terraformrc` in legacy locations & moved everything to work with the changes made in PR #22277,,,,,,Anecdotal,comment,,,,,,,,2019-08-26,github/06kellyjac,https://github.com/hashicorp/terraform/issues/15389#issuecomment-524912525,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
Is there any known workaround for this? It's very annoying to have `.terraform.d` showing up there between al my _actual_ stuff in `$HOME`.,,,,,,Anecdotal,comment,,,,,,,,2021-02-01,github/WhyNotHugo,https://github.com/hashicorp/terraform/issues/15389#issuecomment-770764560,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
Can we actually get this fixed on Linux while we continue to discuss the other platforms? The decision and solution seems really cut and dry when it comes to Linux.,,,,,,Anecdotal,comment,,,,,,,,2021-12-17,github/eternaltyro,https://github.com/hashicorp/terraform/issues/15389#issuecomment-996886268,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
"Instantiating Multiple Providers with a loop ### Current Terraform Version <!--- Run `terraform -v` to show the version, and paste the result between the ``` marks below. This will record which version was current at the time of your feature request, to help manage the request backlog. If you're not using the latest version, please check to see if something related to your request has already been implemented in a later version. --> ``` Terraform v0.11.11 ``` ### Use-cases In my current situati…",,,,,,Anecdotal,issue,,,,,,,,2019-01-07,github/JakeNeyer,https://github.com/hashicorp/terraform/issues/19932,repo: hashicorp/terraform | keyword: workaround | state: open
"With a more generic formulation, it would be great to have a ```count``` attribute in **providers**, not only to instantiate multiple times the same provider but also to choose between several **providers**: some having a zero count, some having a non-zero count.",,,,,,Anecdotal,comment,,,,,,,,2019-01-11,github/debovema,https://github.com/hashicorp/terraform/issues/19932#issuecomment-453526838,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
Has anyone ever found any kind of workaround for this kind of scenario? We have multiple AWS accounts and would like to be able to deploy modules across all accounts without having to have repeatable code for each account.,,,,,,Anecdotal,comment,,,,,,,,2019-05-23,github/hhh0505,https://github.com/hashicorp/terraform/issues/19932#issuecomment-495365845,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
@jaken551 Have you tried this with Terraform 0.12 ? Did you make any progress? I ask because I'm looking for the same functionality.,,,,,,Anecdotal,comment,,,,,,,,2019-07-06,github/sc250024,https://github.com/hashicorp/terraform/issues/19932#issuecomment-508954002,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"We are using terraform `0.12` and have the same issue when using the `terraform-provider-aws` and a few other providers. Basically this is a problem in any provider that wasnt designed to work across multiple organizations/accounts/workspaces/etc. By comparison, `terraform-provider-google` and `terraform-provider-tfe` dont really require this feature. To summarize, the ask here is to allow `provider` blocks to instantiated dynamically and be dependent on computed values from other resources, su…",,,,,,Anecdotal,comment,,,,,,,,2019-09-21,github/kuwas,https://github.com/hashicorp/terraform/issues/19932#issuecomment-533806910,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"@jaken551 @kuwas - I'd also like to see provider blocks support for_each. My use case being creating aws_ses_domain_identity resources in multiple regions. These regions being defined by a list variable i.e. ``` resource ""aws_ses_domain_identity"" ""example"" { for_each = var.ses_regions provider = each.value domain = var.ses_domain }",,,,,,Anecdotal,comment,,,,,,,,2020-01-08,github/autodeck,https://github.com/hashicorp/terraform/issues/19932#issuecomment-572078410,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"One workaround is, you need to maintain separate directory for each account and call the same terraform module by passing correct values to the modules / keep some default values. to run plan / apply in all directory use wrapper such as terragrunt. https://blog.gruntwork.io/terragrunt-how-to-keep-your-terraform-code-dry-and-maintainable-f61ae06959d8 https://davidbegin.github.io/terragrunt/use_cases/execute-terraform-commands-on-multiple-modules-at-once.html",,,,,,Anecdotal,comment,,,,,,,,2020-01-14,github/shankarsundaram,https://github.com/hashicorp/terraform/issues/19932#issuecomment-573982925,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"I'm an enterprise user and doing a for_each over the vault provider would reduce a lot of code if you could just do: ``` variable ""namespaces"" { type = set(string) default = [] description = ""Names to be created"" } resource ""vault_namespace"" ""namespace"" { for_each = var.namespaces path = each.key } provider ""vault"" { for_each = var.namespaces alias = each.key namespace = each.key } ``` instead of copy pasting the same piece for each namespace.",,,,,,Anecdotal,comment,,,,,,,,2020-09-16,github/rgevaert,https://github.com/hashicorp/terraform/issues/19932#issuecomment-693464489,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"Terraform deployment environment protection using separated Cloud Providers accounts is a great method to reduce configuration/deployment error Blast Radius. E.g. different AWS accounts can be used to make deployment environments of any kind (devel, stage, prod, sandbox), with potentially a lot of account with same ""**common configuration**"". Without a solution like for_each in provider block lot of identical code must be replicated, raising a lot the risk of typing errors on addition of each n…",,,,,,Anecdotal,comment,,,,,,,,2020-09-27,github/Roxyrob,https://github.com/hashicorp/terraform/issues/19932#issuecomment-699638233,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"+1 on this. We would like to use Terraform to deploy/manage IAM Roles in a list of AWS Accounts -- assuming a known role in each account for access. Below is an example implementation that would work for us with Terraform >v0.13 and the existing `module.for_each` -- just also need an equivalent `provider.for_each`: ```terraform ## Just some data... a list(map()) locals { aws_accounts = [ { ""aws_account_id"": ""123456789012"", ""foo_value"": ""foo"", ""bar_value"": ""bar"" }, { ""aws_account_id"": ""987654321…",,,,,,Anecdotal,comment,,,,,,,,2020-10-20,github/bryankaraffa,https://github.com/hashicorp/terraform/issues/19932#issuecomment-713080710,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"Allowing alias _as a variable_ in the provider config would also make for cleaner code: ``` variable awsEnvironments { type = list default = [ ""paas-luse1"", ""paas-lusw2"", ""gi-luse1"", ""gi-lusw2"" ] } variable awsRegion { type = map default = { paas-luse1 = ""us-east-1"" paas-lusw2 = ""us-west-2"" gi-luse1 = ""us-east-1"" gi-lusw2 = ""us-west-2"" } } variable project { type = map default = { paas-lusw2 = { localEnvironment = ""paas-luse1"", localVPCSuffix = ""paas"", peerEnvironment = ""paas-lusw2"", peerVPCSuf…",,,,,,Anecdotal,comment,,,,,,,,2020-12-07,github/DonBower,https://github.com/hashicorp/terraform/issues/19932#issuecomment-740144403,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"> @DonBower AFAIK, alias _is_ allowed in the providers config for modules It is. but not as a variable.... ``` Error: Variables not allowed on provider.tf line 27, in provider ""aws"": 27: alias = var.awsEnvironments[count.index] Variables may not be used here. ``` original comment updated....",,,,,,Anecdotal,comment,,,,,,,,2020-12-07,github/DonBower,https://github.com/hashicorp/terraform/issues/19932#issuecomment-740171607,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"@ajbouh @DonBower Yes, you can pass aliased providers into modules, but the providers and their aliases are still statically defined. Furthermore, when using `for_each` with modules, you must pass in the **same** provider instance to every module instance within the loop. I recall reading a comment from @apparentlymart on the reasoning around this current limitation. Example 1: this does not work ``` provider ""azurerm"" { for_each = toset([""one"", ""two""]) alias = each.value } module ""test"" { for_…",,,,,,Anecdotal,comment,,,,,,,,2020-12-07,github/kuwas,https://github.com/hashicorp/terraform/issues/19932#issuecomment-740190413,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"I've literally tried to this exact same thing and have been unable. ``` module ""peering"" { for_each = var.vnet_peering_config source = ""./config/network_peering"" providers = { azurerm.custom = ""azurerm.${each.value.provider_alias}"" } resource_group_name = module.resource_group.rg.name ne_virtual_network = azurerm_virtual_network.default we_virtual_network = azurerm_virtual_network.default remote_virtual_network_name = each.virtual_network_name remote_resource_group_name = each.resource_group_na…",,,,,,Anecdotal,comment,,,,,,,,2020-12-08,github/oliverlucas85,https://github.com/hashicorp/terraform/issues/19932#issuecomment-740652779,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"Any updates? Also depends from this ... For AWS provider the only way to use multi-regions for me is dublicate code many times ```tf provider ""aws"" { alias = ""euwestprovider"" region = ""eu-west-1"" ... } provider ""aws"" { alias = ""eucentralprovider"" region = ""eu-central-1"" ... } resource ""aws_instance"" ""ec2-eu-west""{ provider = ""aws.euwestprovider"" ami = ""ami-030dbca661d402413"" instance_type = ""t2.nano"" } resource ""aws_instance"" ""ec2-eu-central"" { provider = ""aws.eucentralprovider"" ami = ""ami-0ebe…",,,,,,Anecdotal,comment,,,,,,,,2021-04-09,github/augustgerro,https://github.com/hashicorp/terraform/issues/19932#issuecomment-816687083,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"+1 on the topic. Can you give us some feedback? We are using multiple accounts in my company, and it will be great to be able to loop through organization accounts to create the same resources. for_each in providers block is really needed. Thanks.",,,,,,Anecdotal,comment,,,,,,,,2021-04-12,github/ced3eals,https://github.com/hashicorp/terraform/issues/19932#issuecomment-817869607,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"Hey all 👋 - I found a ~~solution~~ workaround that works well enough to create multiple sets of S3 buckets (or any resource) across multiple regions using multiple [providers](https://www.terraform.io/docs/language/meta-arguments/module-providers.html) and [modules](https://www.terraform.io/docs/language/modules/index.html). It's not as clean as provider interpolation would be, but it **does** work. An example using two S3 buckets in _different_ regions: ``` provider ""aws"" { alias = ""use1"" regi…",,,,,,Anecdotal,comment,,,,,,,,2021-04-21,github/egeexyz,https://github.com/hashicorp/terraform/issues/19932#issuecomment-823748793,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
Big +1 to get this fixed please. Just dealing with the available regions is hard; I can't imagine having to deal with this for multiple AWS accounts.,,,,,,Anecdotal,comment,,,,,,,,2021-05-10,github/gswallow,https://github.com/hashicorp/terraform/issues/19932#issuecomment-836104254,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"With the `azurerm` module, it is not possible to create a resource group in a specific subscription unless you make the provider explicit. This limitation makes it impossible to iterate over a map or list of configuration data, when the underlying subscription is also parameterized. ```terraform provider ""azurerm"" { features {} alias = ""specific_subscription"" subscription_id = ""00000000-0000-0000-0000-000000000000"" } resource ""azurerm_resource_group"" ""foo"" { name = ""foo-rg"" location = ""centralu…",,,,,,Anecdotal,comment,,,,,,,,2021-05-13,github/NeverOddOrEven,https://github.com/hashicorp/terraform/issues/19932#issuecomment-840655296,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"I know this sounds a bit crazy, but I solved this using M4 macros. You can use `esyscmd` to do whatever you want, and then the output of that will be handled as macros. So, for instance, here's a simplified `main.tf.m4` that does something for every AWS region: ```m4 /** * ******** DO NOT EDIT THIS FILE ******** * * This file is auto-generated. If a new region has launched, please run make. */ dnl dnl You can edit this file, though. The end of the file will shell out to the AWS CLI and dnl magi…",,,,,,Anecdotal,comment,,,,,,,,2021-06-07,github/ziggythehamster,https://github.com/hashicorp/terraform/issues/19932#issuecomment-856191323,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"I dealt with it by using a shell script that calls terraform to output a map variable that contains the account structure, it then iterates over each element (account) to build a terraform environment for that environment (copy in the base code, copy in the variable structure, build a backend file so everyone has their own S3 state, etc.) and then launch a new terraform instance for every single one, capture the output and exit codes, and then dump the output from each one back to the caller an…",,,,,,Anecdotal,comment,,,,,,,,2021-06-07,github/PCjrBasic,https://github.com/hashicorp/terraform/issues/19932#issuecomment-856312490,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"I'd like to add another use cases to justify this demand. 1. When you setup an AWS Organization and follow best practices, you often want to create a centralized network account and share the VPCs via AWS RAM. You also might want to place a private route53 zone in each account you share the VPC to (e.g. apps.integration.private, database.integration.private) and associate the VPCs with the zones. Currently you need to configure a provider for each target account and pass it to a module. Neither…",,,,,,Anecdotal,comment,,,,,,,,2021-06-21,github/yves-vogl,https://github.com/hashicorp/terraform/issues/19932#issuecomment-864970638,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"@apparentlymart @jbardin Can we have an update on this? Other issues have been closed in favor of using this one for tracking, but I cannot find a progress update in the past year on this issue. Last I heard this was [planned for Terraform 1.0](https://github.com/hashicorp/terraform/issues/24476#issuecomment-700368878) (or maybe delayed until is more accurate), which of course you know is now out. We need it to generically work with an arbitrary set of accounts.",,,,,,Anecdotal,comment,,,,,,,,2021-07-07,github/Nuru,https://github.com/hashicorp/terraform/issues/19932#issuecomment-875931295,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"Hi @Nuru, Sorry, I don't have any update at the moment. While it seems like a simple request on the surface, the underlying architecture of terraform is not suited for using providers in this manner. What this essentially means is that it's going to be a very large project which must complete in resources with many other large projects. While this is definitely a desired feature, and fulfilling this use case has a high priority, I cannot say which release may be targeted for implementation.",,,,,,Anecdotal,comment,,,,,,,,2021-07-09,github/jbardin,https://github.com/hashicorp/terraform/issues/19932#issuecomment-877355744,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"(maybe) 1.12 S3 Backend (nonAWS) Regression: skip_s3_checksum ignored? ### Terraform Version ```shell 1.12.1 ``` ### Terraform Configuration Files ```terraform # Configure TF remote-state terraform { backend ""s3"" { bucket = ""tf-state-terraform-myorg"" key = ""org/prod/project/.terraform.tfstate"" region = ""somewhere"" # Required, but in our case ignored #access_key = ENV_AWS_ACCESS_KEY_ID # set in env, not via tf #secret_key = ENV_AWS_SECRET_ACCESS_KEY # set in env, not via tf skip_credentials_vali…",,,,,,Anecdotal,issue,,,,,,,,2025-06-03,github/gr-fl,https://github.com/hashicorp/terraform/issues/37203,repo: hashicorp/terraform | keyword: workaround | state: open
"I discovered something a little bit interesting. I turned _off_ the `skip_s3_checksum` flag to see what would happen, expecting a similar result. Actually, my JSON still got litered with checksums. But they are different now. Original - if `skip_s3_checksum` is TRUE: ````json 500000 { ""version"": 4, ""terraform_version"": ""1.12.1"", ... ""somejsonvalue"", 0 x-amz-checksum-crc32:K1edIQ== 1d2894 ""someotherjsonvalue"", ""somemorejsonforyou"", ... } 0 x-amz-checksum-crc32:Oy5svw== ```` If `skip_s3_checksum`…",,,,,,Anecdotal,comment,,,,,,,,2025-06-03,github/gr-fl,https://github.com/hashicorp/terraform/issues/37203#issuecomment-2937611269,repo: hashicorp/terraform | issue: (maybe) 1.12 S3 Backend (nonAWS) Regression: skip_s3_checksum ignored? | keyword: workaround
"Hi, I couldn't leave well enough alone, so I downgraded our project to 1.8. The state is now being written correctly to S3 without checksums. This was the process I used to try to downgrade our project: ```` 1. wget https://releases.hashicorp.com/terraform/1.8.2/terraform_1.8.2_linux_amd64.zip 2. unzip terraform_1.8.2_linux_amd64.zip 3. mv terraform ~/bin/terraform-1.8.2 # (in path) 4. terraform-1.8.2 version # should produce right thing 6. terraform-1.8.2 -reconfigure 7. terraform-1.8.2 -migra…",,,,,,Anecdotal,comment,,,,,,,,2025-06-03,github/gr-fl,https://github.com/hashicorp/terraform/issues/37203#issuecomment-2937721992,repo: hashicorp/terraform | issue: (maybe) 1.12 S3 Backend (nonAWS) Regression: skip_s3_checksum ignored? | keyword: workaround
"I took a quick look at this and I think this is likely due to a change in the aws-sdk-go-v2 library. From the release notes on 2025-01-15 for that package it says that the default checksum algorithm is CRC32. I've found an announcement about this change at https://github.com/aws/aws-sdk-go-v2/discussions/2960 In the S3 backend code in Terraform, it only sets the algorithm when `skipS3Checksum` in `RemoteClient` is false, and leaves it unset when true, which I'm guessing is now causing the aws-s…",,,,,,Anecdotal,comment,,,,,,,,2025-06-04,github/claytonpeters,https://github.com/hashicorp/terraform/issues/37203#issuecomment-2939245655,repo: hashicorp/terraform | issue: (maybe) 1.12 S3 Backend (nonAWS) Regression: skip_s3_checksum ignored? | keyword: workaround
"`enabled` parameter to avoid logical statements in `count` Terraform v0.12.2 ### Use-cases We often use `count` to disable a resource (above all in modules) from a boolean var, with statements like `count = var.enabled ? 1 : 0` or `count = var.enabled ? 1 : length([some list of resources or datasources])` Here are among others some code snippets from `terraform-aws-vpc` module : ``` resource ""aws_subnet"" ""private"" { count = var.create_vpc && length(var.private_subnets) > 0 ? length(var.private_…",,,,,,Anecdotal,issue,,,,,,,,2019-07-02,github/romachalm,https://github.com/hashicorp/terraform/issues/21953,repo: hashicorp/terraform | keyword: workaround | state: open
"This is in my opinion such a huge problem with what appears a ""simple"" solution. Relying on count to disable a module has a lot of bad effects from how you retrieve the output to having warnings about unsupported ""count"" from submodules that define providers themselves. What is the technical/functional obstacle of having a simple boolean switch on modules and/or resources? One can neatly separate module calls by using terraform workspace name and much more, avoiding tons of nasty hacks that are…",,,,,,Anecdotal,comment,,,,,,,,2020-09-25,github/PCatinean,https://github.com/hashicorp/terraform/issues/21953#issuecomment-699144350,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"@PCatinean The argument was that there are no clear use-cases. I fully support yours and @romachalm and here's mine: I deploy Landing Zone for 200 Products from my `landing-zone` module, which actually consists of different modules such as `network`, `key-vault`, `rbac`, `des` and so on. In some cases, deployment of a feature requires other inputs. e.g. to deploy disk encryption set `des`, I need first the Security team to deploy the HSM key into a vault that I deploy via `key-vault` of the `la…",,,,,,Anecdotal,comment,,,,,,,,2021-02-26,github/Tbohunek,https://github.com/hashicorp/terraform/issues/21953#issuecomment-786729696,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"Adding additional context @schollii provided in #27936 ---- [As recommended by @apparentlymart in https://github.com/hashicorp/hcl/issues/450, I'm moving this improvement request here.] The notion of ""nullness"" or ""existence"" must be first class in a language, and the language should minimize how much ""how"" the user needs to express. Currently in order to make resources and modules optional, we have to resort to using count = 0 or 1. Examples: ``` resource ""provider_type"" ""name"" { count = var.n…",,,,,,Anecdotal,comment,,,,,,,,2021-03-01,github/jbardin,https://github.com/hashicorp/terraform/issues/21953#issuecomment-787987116,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"Thank you @jbardin for taking the time to write that up. The sheer length of the post forced me to also take some time to read it and understand it, and you summarized it very well. How does it look now? Did the chances of this going through increase? It would help many Terraform users build better, nicer, cleaner and more flexible code, and adopt `enabling` ""as they go"" with no code modification effort (like adding `[0]` indices). On the point of chained resources, I would say it doesn't requi…",,,,,,Anecdotal,comment,,,,,,,,2021-03-22,github/Tbohunek,https://github.com/hashicorp/terraform/issues/21953#issuecomment-804357343,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"Hi @Tbohunek, don't thank me, thank @schollii, who originally did that nice writeup as feature request in the hcl repository ;) I agree it is a compelling feature, but there are many competing compelling features to consider for terraform, and we cannot prioritize them all. I cannot say when this might be considered, but it is not a minor feature and will take considerable planning to handle in a future version of terraform.",,,,,,Anecdotal,comment,,,,,,,,2021-03-22,github/jbardin,https://github.com/hashicorp/terraform/issues/21953#issuecomment-804380597,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"Thanks @schollii ! :) @jbardin if `count` could co-exist with this `enabled`, the change should be transparent to current users. If `count` would change then obviously the sooner the better. More users stuck with `count` makes such change harder up to a point where it won't matter anymore.",,,,,,Anecdotal,comment,,,,,,,,2021-03-27,github/Tbohunek,https://github.com/hashicorp/terraform/issues/21953#issuecomment-808694863,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"@jbardin This looks like a minor and backward-compatible effort though. While waiting for the big features to be prioritized, we keep struggling daily and produce hard-to-maintain unreadable code!",,,,,,Anecdotal,comment,,,,,,,,2021-04-10,github/nikolay,https://github.com/hashicorp/terraform/issues/21953#issuecomment-817212216,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"Consider Meta-Argument `condition` or `include_if` so people stop using `count= 0 or 1 ` or `for_each empty or single item` And `content` which allows passing a data structure instead of DSL When choosing a Meta-Argument consider classes with existing resources. ``` locals { something = { prevent_destroy = true } } dynamic ""lifecycle"" { condition = can(var.ENVIRONMENT) # include_if = can(var.ENVIRONMENT) content = something # as long as the map is what the resource expects in this block. i.e. i…",,,,,,Anecdotal,comment,,,,,,,,2021-08-07,github/damon-atkins,https://github.com/hashicorp/terraform/issues/21953#issuecomment-894608054,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"@damon-atkins both condition and include_if are fine by me, but I'm not sure I agree with content. Can you provide an example? I think count and enabled / condition / include_if would have to be mutually exclusive. Count means you want an array, possibly of 0 size, and the language should make it simple to handle the case where count is 0, a bit like `one()` in does. Whereas enabled / condition is never an array, the resource is either there or null, and the DSL should make it easy to deal with…",,,,,,Anecdotal,comment,,,,,,,,2021-08-07,github/schollii,https://github.com/hashicorp/terraform/issues/21953#issuecomment-894658311,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"I just ran into this when trying to create OpenStack resources, optionally using a keypair or creating one depending on a value. Unfortunately it is extremely ugly to make everything consider the keypair to be a list.",,,,,,Anecdotal,comment,,,,,,,,2021-12-10,github/tculp,https://github.com/hashicorp/terraform/issues/21953#issuecomment-991192054,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"content as show allow you to pass a data structure representing the HCL ``` locals { something = { prevent_destroy = true etc.... } } dynamic ""lifecycle"" { include_if= can(var.ENVIRONMENT) # condition = can(var.ENVIRONMENT) content = something # as long as the map is what the resource expects in this block. i.e. its passing prevent_destroy } ``` vs ``` dynamic ""lifecycle"" { include_if = can(var.ENVIRONMENT) # condition = can(var.ENVIRONMENT) prevent_destroy = true etc..... } ```",,,,,,Anecdotal,comment,,,,,,,,2021-12-13,github/damon-atkins,https://github.com/hashicorp/terraform/issues/21953#issuecomment-992159210,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"Hi all! I think there are at least four remaining design questions to be solved before something like this could be implemented: * The usual reason given to motivate this addition is the desire to be able to disable something without having to change all of the references to it. Currently if I add `count` to `aws_instance.foo` then every other reference to it must become `aws_instance.foo[count.index]` where `count.index` is `0`, or must be a carefully-guarded hard-coded reference to `aws_insta…",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/apparentlymart,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1216017283,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"@apparentlymart I think `enabled = <condition>` could work just like `count = <condition> ? 1 : 0` and people would have to use `one(aws_instance.foo[*])`. Basically, `enbaled` would be syntactic sugar and nothing more and thus fully backward-compatible. I've also seen people writing `for_each = <condition> ? [1] : []`, which could also be replaced with `enabled = <condition>`. Lastly, HCL could be extended with the comprehension-like `resource ... {} if <condition>` instead of making this an a…",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/nikolay,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1216071967,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"The best sugar of it all would be to not have to add unpredictable `.[0]` into every reference of the `counted` resource, and conditions to `outputs` (i.e. `output` would default to `null` or `[]` respectively if the resources get deployed or not).",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/Tbohunek,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1216200424,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"@Tbohunek I wish `one(<reference>[*])` could be sugared somehow. Talking about outputs - I wish they had `for_each`, `count`, and `enabled`, too!",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/nikolay,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1216261257,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"count has order issues, for_each does not, hence people have change code to use for_each. `include_if` or `enable` could be the same as commenting out the section of code. e.g. this does not exist do not include it, or I do not want a setting to be set in this account/resource. Thats why I suggest name `include_if` Sometime people create two tf files which are the same with parts of one deleted, because a flag like this does not exist. `count=0` and `for_each=[]` still create an empty item.",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/damon-atkins,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1216268318,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
To actually react on @apparentlymart's post > * enabled = aws_instance.foo != null How about `enabled = aws_instance.foo.enabled`? For `output` so far I defaulted all of them into `null` because it doesn't really matter.,,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/Tbohunek,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1216501719,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"The following could be the same, i.e. if `include_if` is false treat the following as if it is commented out. ``` dynamic ""lifecycle"" { include_if= can(var.DESTROY) # condition = can(var.DESTROY) Evaluated to TRUE, included lines prevent_destroy = var.DESTROY } ``` ``` # dynamic ""lifecycle"" { # include_if= can(var.DESTROY) # condition = can(var.DESTROY) Evaluated to FALSE, exclude lines # prevent_destroy = var.DESTROY # } ```",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/damon-atkins,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1216952936,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"We cannot add any new synthetic attributes to an expression like `aws_instance.foo`, because that's already defined to be an object of a type defined by the provider's schema. If we wanted to expose metadata about a `resource` block in addition to the actual data for that object then I expect we'd need to do it under a different namespace such as `meta.aws_instance.foo`, although I've not deeply considered the implications of doing that.",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/apparentlymart,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1217257356,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
Wouldn't the chosen attribute like `.enabled` become part of the schema for all objects? The caveat here is rather to make `.enabled` accessible on an object that doesn't actually exist.,,,,,,Anecdotal,comment,,,,,,,,2022-08-17,github/Tbohunek,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1217483758,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"This comes back to my comment about defining what exactly `aws_instance.foo` evaluates to when we have a resource with `enabled = true` or `enabled = false` set. It's true that Terraform _could_ add a synthetic additional attribute to the resource type's schema so that `.enabled` could be `true` in the case where it's enabled, but as you noted `<null object>.enabled` would fail with an error, so I don't think this design is quite right yet. (Note that this is also in the vicinity of the other p…",,,,,,Anecdotal,comment,,,,,,,,2022-08-17,github/apparentlymart,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1218239509,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"In some languages (eg Python), null is an object ie it has a type and attributes just like any other (it is called None in Python but same thing). Could something like that work? So there would be a built-in resource that represents null and has enabled = false. Every other object would have enabled = true. Also, let's not forget that the real goal is to not have to litter the code with `[0]` and code that checks for empty list. Groovy's approach to handling null should be considered : somethin…",,,,,,Anecdotal,comment,,,,,,,,2022-08-17,github/schollii,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1218526585,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
@apparentlymart or anyone why do we need to read anything back? Why just have the HCL parser ignoring a set of lines not enough to do the job? On a side note we already have `can()` which can test something exists or not.,,,,,,Anecdotal,comment,,,,,,,,2022-08-18,github/damon-atkins,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1218821517,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"Of course it depends on the task at hand but at minimum it's typical for a module to output something about the object it declared so that the caller can use it: ```hcl resource ""aws_instance"" ""example"" { count = var.enable_web_servers ? 5 : 0 # ... } output ""web_server_ip_addresses"" { value = toset(aws_instance.example[*].private_ip) } ``` My comment above was saying that a full proposal for adding any new mechanism for changing how many instances there are of a resource (even if it's just lim…",,,,,,Anecdotal,comment,,,,,,,,2022-08-18,github/apparentlymart,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1219680940,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"Any reason why output can not also have `include_if` or something. ``` resource ""aws_instance"" ""webserver"" { include_if= can(env.MAX_WEB_SERVERS) count = env.MAX_WEB_SERVERS # ... } output ""web_server_ip_addresses"" { include_if= can(aws_instance.webserver[*].private_ip) value = toset(aws_instance.webserver[*].private_ip) } ``` If `include_if` is false do not include the current block `{` `}` and children blocks",,,,,,Anecdotal,comment,,,,,,,,2022-08-19,github/damon-atkins,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1220988768,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"@damon-atkins you can already do what you describe with `one()`, no? ``` output ""web_server_ip_addresses"" { value = one(aws_instance.webserver[*].private_ip) } ``` will be null if the expression does not evaluate and will be the specified attribute of `item[0]` if there is a zeroth item. I've never sat down and tried to see if `one()` might be a suitable workaround for the problem that this current github issue attempts to address. It is certainly not a solution, but it might be a workaround.",,,,,,Anecdotal,comment,,,,,,,,2022-08-19,github/schollii,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1221169985,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
@schollii what I am suggesting can be placed anywhere as part of a block ``` layerone { include_if = ... layertwo { include_if = ... layerN { include_if = ... } } } ```,,,,,,Anecdotal,comment,,,,,,,,2022-08-20,github/damon-atkins,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1221203008,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"I think there is a much simpler answer, both for everyone here and for your team to implement @apparentlymart. > We need to decide what type of value aws_instance.foo would be both when it's enabled and when it's disabled, and that decision will in turn decide what references to that value would look like. I don't believe that is necessary for the situation where something should or should not exist. I'm going to suggest a different answer below that will satisfy everything I personally want ou…",,,,,,Anecdotal,comment,,,,,,,,2022-08-20,github/jorhett,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1221389742,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"I'm not sure why everybody's so inclined for these long and not very meaningful `include_if` and `only_if`! Do we have `include_if` or `only_if` in the `for` expressions? No! So, why can't this just be `if` if you don't like `enabled`? I've seen tons of modules, already using `enabled` in parameter along with the logic `count = var.enabled ? 1 : 0`, so, using `enabled` would match what people already use and simply replace the above with `enabled = var.enabled`. But changing HCL to have an `if`…",,,,,,Anecdotal,comment,,,,,,,,2022-08-20,github/nikolay,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1221403327,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"Backend S3 try to load aws credentials on non aws backends ! ### Terraform Version ```shell Terraform v1.12.2 + provider registry.terraform.io/scaleway/scaleway v2.57.0 ``` ### Expected Behavior i was expecting to be able to use the backend ""s3"" block and a scaleway provider and using the scw-cli credential file on the system (~/$HOME/.config/scw/config.yaml) here is my provider code : ``` terraform { required_providers { scaleway = { source = ""scaleway/scaleway"" version = ""2.57.0"" } } required…",,,,,,Anecdotal,issue,,,,,,,,2025-07-23,github/tcarecolin,https://github.com/hashicorp/terraform/issues/37366,repo: hashicorp/terraform | keyword: workaround | state: open
"Hi, thanks for filing this request. I changed it to enhancement as the report seems to be requesting specialized recognition of an S3-compatible service's configuration files. The AWS Provider team at HashiCorp supports the S3 backend but does not test against S3-compatible services. I suspect that recognizing a particular non-AWS provider's configuration scheme would be outside of the scope of support for the S3 backend, but I will leave this open for comment by the appropriate maintainers. Th…",,,,,,Anecdotal,comment,,,,,,,,2025-07-23,github/crw,https://github.com/hashicorp/terraform/issues/37366#issuecomment-3109628206,repo: hashicorp/terraform | issue: Backend S3 try to load aws credentials on non aws backends ! | keyword: workaround
"[Feature] Support for backend state with private link ### Is there an existing issue for this? - [X] I have searched the existing issues ### Community Note <!--- Please keep this note for the community ---> * Please vote on this issue by adding a :thumbsup: [reaction](https://blog.github.com/2016-03-10-add-reactions-to-pull-requests-issues-and-comments/) to the original issue to help the community and maintainers prioritize this request * Please do not leave ""+1"" or ""me too"" comments, they gene…",,,,,,Anecdotal,issue,,,,,,,,2024-02-08,github/AndreasAugustin,https://github.com/hashicorp/terraform/issues/34640,repo: hashicorp/terraform | keyword: workaround | state: open
"Thanks for taking the time to open this feature request. The Azurerm backend lives in the [hashicorp/terraform](https://github.com/hashicorp/terraform) repo, so I am going to move this feature request there.",,,,,,Anecdotal,comment,,,,,,,,2024-02-08,github/rcskosir,https://github.com/hashicorp/terraform/issues/34640#issuecomment-1934707091,repo: hashicorp/terraform | issue: [Feature] Support for backend state with private link | keyword: workaround
**Remark** Reading the code: Possibly it is already possible to use this feature if you set the `storageAccountName` parameter to `storageAccountName: <hosted_zone_prefix>.privatelink` and a documentation addon could help. The related part should be located within [https://github.com/tombuildsstuff/giovanni/blob/v0.15.1/storage/internal/endpoints/endpoints.go](https://github.com/tombuildsstuff/giovanni/blob/v0.15.1/storage/internal/endpoints/endpoints.go) ```golang // GetBlobEndpoint returns th…,,,,,,Anecdotal,comment,,,,,,,,2024-02-08,github/AndreasAugustin,https://github.com/hashicorp/terraform/issues/34640#issuecomment-1934861275,repo: hashicorp/terraform | issue: [Feature] Support for backend state with private link | keyword: workaround
"I can confirm that this is already possible. You need to do something like this: ```tf terraform { backend ""azurerm"" { use_azuread_auth = true resource_group_name = ""$($BackendResourceGroupName)"" storage_account_name = ""$($BackendStorageAccountName)"" container_name = ""$($BackendContainerName)"" key = ""$($BackendKeyName)"" } } ``` And make sure to have your env settings set: ```yaml env: ARM_TENANT_ID: $(ARM_TENANT_ID) ARM_SUBSCRIPTION_ID: $(ARM_SUBSCRIPTION_ID) ARM_CLIENT_ID: $(ARM_CLIENT_ID) ARM…",,,,,,Anecdotal,comment,,,,,,,,2024-02-16,github/Devvox93,https://github.com/hashicorp/terraform/issues/34640#issuecomment-1948332832,repo: hashicorp/terraform | issue: [Feature] Support for backend state with private link | keyword: workaround
"> I can confirm that this is already possible. You need to do something like this: > > ```terraform > terraform { > backend ""azurerm"" { > use_azuread_auth = true > resource_group_name = ""$($BackendResourceGroupName)"" > storage_account_name = ""$($BackendStorageAccountName)"" > container_name = ""$($BackendContainerName)"" > key = ""$($BackendKeyName)"" > } > } > ``` > > And make sure to have your env settings set: > > ```yaml > env: > ARM_TENANT_ID: $(ARM_TENANT_ID) > ARM_SUBSCRIPTION_ID: $(ARM_SUBSC…",,,,,,Anecdotal,comment,,,,,,,,2024-02-16,github/AndreasAugustin,https://github.com/hashicorp/terraform/issues/34640#issuecomment-1948402033,repo: hashicorp/terraform | issue: [Feature] Support for backend state with private link | keyword: workaround
"@AndreasAugustin We just use the normal storage account name. I believe Azure (almost) always handles private link routing by itself, unless you have custom DNS resolution. Could that be an issue in your case?",,,,,,Anecdotal,comment,,,,,,,,2024-02-19,github/Devvox93,https://github.com/hashicorp/terraform/issues/34640#issuecomment-1952418094,repo: hashicorp/terraform | issue: [Feature] Support for backend state with private link | keyword: workaround
"> @AndreasAugustin We just use the normal storage account name. I believe Azure (almost) always handles private link routing by itself, unless you have custom DNS resolution. Could that be an issue in your case? there are cases where the recursive DNS resolution from public to private link is not working (e.g. you only have on premise DNS forwarding in place for private links and not a recursive resolve enabled ). In those cases it is not possible to resolve the public FQDN to the private link …",,,,,,Anecdotal,comment,,,,,,,,2024-02-19,github/AndreasAugustin,https://github.com/hashicorp/terraform/issues/34640#issuecomment-1952876659,repo: hashicorp/terraform | issue: [Feature] Support for backend state with private link | keyword: workaround
"for my case `export ARM_STORAGE_BLOB_ENDPOINT: ""${{ vars.STORAGE_ACCOUNT_NAME }}.blob.core.windows.net""` solve all problems with az cli commands and with the tf backend and azurerm commands wich operate with the blobs",,,,,,Anecdotal,comment,,,,,,,,2025-06-03,github/VectorBCO,https://github.com/hashicorp/terraform/issues/34640#issuecomment-2932981376,repo: hashicorp/terraform | issue: [Feature] Support for backend state with private link | keyword: workaround
"for me its working we have set conditional forwarding zones in our internal dns service on-premise for - blob.core.windows.net to our private dns resolver in azure see extensive documentation here https://github.com/dmauser/PrivateLink/tree/master/DNS-Integration-Scenarios#43-on-premises-dns-server-conditional-forwarder-considerations the ""blob"" part is indeed hardcoded in https://github.com/hashicorp/terraform/blob/main/internal/backend/remote-state/azure/storage_client_helpers.go#L158 seems t…",,,,,,Anecdotal,comment,,,,,,,,2025-07-22,github/martijnvdp,https://github.com/hashicorp/terraform/issues/34640#issuecomment-3102414647,repo: hashicorp/terraform | issue: [Feature] Support for backend state with private link | keyword: workaround
"Azure Gov OIDC Backend Authentication Bug - Wrong Audience Used ### Terraform Version ```shell Terraform v1.5.0 on linux_amd64 + provider registry.terraform.io/hashicorp/azurerm v3.116.0 ``` ### Terraform Configuration Files **Backend Configuration:** ```terraform terraform { required_version = "">= 1.0"" required_providers { azurerm = { source = ""hashicorp/azurerm"" version = ""~> 3.0"" } } backend ""azurerm"" { # Backend configuration provided via --backend-config flags } } provider ""azurerm"" { feat…",,,,,,Anecdotal,issue,,,,,,,,2025-07-21,github/jed-exotic,https://github.com/hashicorp/terraform/issues/37353,repo: hashicorp/terraform | keyword: workaround | state: open
"Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently ### Terraform Version ```shell 1.3.2 ``` ### Terraform Configuration Files ```terraformrc plugin_cache_dir = ""$HOME/.terraform.d/plugin-cache"" ``` ### Debug Output no debug ### Expected Behavior Multiple terraform processes should work fine with the same `plugin_cache_dir` ### Actual Behavior ``` Initializing provider plugins... - Reusing previous version of hashicorp/aws from the dependency lock file - Using hashico…",,,,,,Anecdotal,issue,,,,,,,,2022-10-07,github/amkartashov,https://github.com/hashicorp/terraform/issues/31964,repo: hashicorp/terraform | keyword: workaround | state: open
"Thanks for the report. This behaviour is expected, and [described in the `plugin_cache_dir` documentation](https://www.terraform.io/cli/config/config-file#provider-plugin-cache): > Note: The plugin cache directory is not guaranteed to be concurrency safe. The provider installer's behavior in environments with multiple terraform init calls is undefined. I can't find an enhancement request describing the desired outcome of being able to use multiple instances of `terraform init` with a global plu…",,,,,,Anecdotal,comment,,,,,,,,2022-10-07,github/alisdair,https://github.com/hashicorp/terraform/issues/31964#issuecomment-1271597992,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
"If we do want to change something to make this concurrency-safe then I think a key requirement for us to navigate is ensuring we don't break anyone who currently has their cache directory on a network filesystem where e.g. filesystem-level locking may not be available or may not be reliable. We didn't explicitly document that the plugin cache directory must be on a filesystem that is only visible to the current kernel, and I've seen questions online in the past which imply that people are alrea…",,,,,,Anecdotal,comment,,,,,,,,2022-10-10,github/apparentlymart,https://github.com/hashicorp/terraform/issues/31964#issuecomment-1273710875,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
"@apparentlymart may be it'll work w/o locking? As `installFromHTTPURL` downloads to a temporary file, why `installFromLocalArchive` can't do the same? Unpack to a temporary location and then move to final path. I believe this won't break backwards compatibility. Other processes won't see half-unpacked file.",,,,,,Anecdotal,comment,,,,,,,,2022-10-11,github/amkartashov,https://github.com/hashicorp/terraform/issues/31964#issuecomment-1274465785,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
"As far as I'm aware, none of the installation methods are truly atomic today: even if we first extract into a temporary directory and then copy into the final location, there is no way to atomically copy a whole directory tree and so there will still be a window of time where another concurrent process can observe a directory that exists but doesn't yet have all of the expected files inside it, or has partial content for one file. In both of those situations the observing process will calculate…",,,,,,Anecdotal,comment,,,,,,,,2022-10-11,github/apparentlymart,https://github.com/hashicorp/terraform/issues/31964#issuecomment-1274829510,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
"On Linux (only), `renameat()` supports a `RENAME_NOREPLACE` flag which can help with atomicity surprises. Windows has `MoveFileEx` where not overwriting existing files seems to be the default. As the plugin cache is only a _cache_, failure to write any entry should not block continuing with the original operation.",,,,,,Anecdotal,comment,,,,,,,,2022-11-21,github/sftim,https://github.com/hashicorp/terraform/issues/31964#issuecomment-1322431221,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
"@jbardin over in https://github.com/hashicorp/terraform/issues/32915 I encountered the ""text file busy"" issue and you pointed to here for an upcoming fix. It happened to me again and I was able to investigate: ``` .... - Installing hashicorp/aws v4.64.0... ╷ │ Error: Failed to install provider │ │ Error while installing hashicorp/aws v4.64.0: open │ /home/mkeisler/.terraform.d/plugin-cache/registry.terraform.io/hashicorp/aws/4.64.0/linux_amd64/terraform-provider-aws_v4.64.0_x5: │ text file busy…",,,,,,Anecdotal,comment,,,,,,,,2023-04-27,github/grimm26,https://github.com/hashicorp/terraform/issues/31964#issuecomment-1526300003,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
"We can work around locking issue; What we can do is to copy files from temporary location to the target directory and create a special file (e.g. `active.true`) after copy is done - if the file is present, then we can use this directory as cache source. It is possible that more than 1 concurrent jobs will create duplicate directories with provider cache; in case we have duplicates, we can just pick the first occurrence, and delete remaining ones. Also, we will need a process to periodically sca…",,,,,,Anecdotal,comment,,,,,,,,2023-06-16,github/ns-vpanfilov,https://github.com/hashicorp/terraform/issues/31964#issuecomment-1594972245,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
"Locks (as files or dirs) do not work. 1. Just remember the state of `.terraform/` dir - which files exist and which do not 2. Init everything async 3. If something fails - remove new stuff from `.terraform/` and try once again. 4. Repeat step 3 a few times before giving up. Based on the pain of lock implementation in https://github.com/antonbabenko/pre-commit-terraform/pull/620 Also, it just works >5 times quicker, when you init 50+ dirs at once, compared to realization with lock mechanizm",,,,,,Anecdotal,comment,,,,,,,,2024-02-13,github/MaxymVlasov,https://github.com/hashicorp/terraform/issues/31964#issuecomment-1939869453,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
"A possibly naive question, but why does `terraform init` with `plugin_cache_dir` try to overwrite an existing provider installation rather than identifying that the provider is already present (i.e. stat, checksum/validate signature of existing binary)? If version/platform weren't already in the path, sure, but shouldn't the existing forced-overwrite behaviour be gated behind a `-force-update-provider` flag (or similar)? (We face the issue where an active `terraform (plan|apply)` will fail a co…",,,,,,Anecdotal,comment,,,,,,,,2024-03-19,github/isometry,https://github.com/hashicorp/terraform/issues/31964#issuecomment-2007333938,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
"> A possibly naive question, but why does `terraform init` with `plugin_cache_dir` try to overwrite an existing provider installation rather than identifying that the provider is already present (i.e. stat, checksum/validate signature of existing binary)? If version/platform weren't already in the path, sure, but shouldn't the existing forced-overwrite behaviour be gated behind a `-force-update-provider` flag (or similar)? > > (We face the issue where an active `terraform (plan|apply)` will fai…",,,,,,Anecdotal,comment,,,,,,,,2024-03-27,github/ranesagar,https://github.com/hashicorp/terraform/issues/31964#issuecomment-2023780083,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
"Not to say that re-downloading existing (vs checking their checksum and comparing with desired one) is probably costing terraform (or github) a non-insignificant amount of traffic)! For reference, `terraform providers lock` from terragrunt (on a non-trivial scenario) takes 15m to process 21 providers (for 5 platforms). It downloads some of them 2x prob due to parallel processing If we wanted to do this for the whole tree, I would say some 2h just to refresh cache and/or update providers lock - …",,,,,,Anecdotal,comment,,,,,,,,2024-03-27,github/joaocc,https://github.com/hashicorp/terraform/issues/31964#issuecomment-2023873835,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
"> If we wanted to do this for the whole tree, I would say some 2h just to refresh cache and/or update providers lock - it seems to take same time regardless of whether they are in cache or not. Would really love to see this fixed in some manner :) That's already fixed - https://github.com/hashicorp/terraform/pull/34632, it will be GA when 1.8.0 is released, now it exists only in pre-releases for 2 months.",,,,,,Anecdotal,comment,,,,,,,,2024-03-28,github/MaxymVlasov,https://github.com/hashicorp/terraform/issues/31964#issuecomment-2025082462,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
"From what I can see this: https://github.com/hashicorp/terraform/pull/33479 has been approved for months now, and from what I understand it waits to address some comments from core maintainers.",,,,,,Anecdotal,comment,,,,,,,,2024-11-19,github/wosiu,https://github.com/hashicorp/terraform/issues/31964#issuecomment-2485791765,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
"@wosiu regarding core Terraform (not including the code for the various backends), only official maintainer approval matters. Nothing has otherwise changed with the status of that PR or the reason why it is in stasis. I am not sure why GitHub allows drive-by PR approvals, although yesterday there was a big permissions overhaul so it may no longer be possible. Sorry for the confusion. If the person is not in the HashiCorp organization (as indicated by GitHub) the approval is not meaningful.",,,,,,Anecdotal,comment,,,,,,,,2024-11-19,github/crw,https://github.com/hashicorp/terraform/issues/31964#issuecomment-2486705038,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
on our project fixed by switching from terraform plugin cache dir to terragrunt cache as described here: https://terragrunt.gruntwork.io/docs/features/provider-cache-server/,,,,,,Anecdotal,comment,,,,,,,,2025-01-17,github/DmitriyStoyanov,https://github.com/hashicorp/terraform/issues/31964#issuecomment-2598562961,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
> on our project fixed by switching from terraform plugin cache dir to terragrunt cache as described here: [terragrunt.gruntwork.io/docs/features/provider-cache-server](https://terragrunt.gruntwork.io/docs/features/provider-cache-server/) I'm a long time terragrunt user anyway and tried out their provider caching as described in that link. I ended up turning it off again because it was slow and wonky. It also changes the terraform provider lock file to be exact versions rather than reflect what…,,,,,,Anecdotal,comment,,,,,,,,2025-01-19,github/grimm26,https://github.com/hashicorp/terraform/issues/31964#issuecomment-2600884749,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
> Can anyone suggest any workaround? We ended up making the TF init commands run in sequence to avoid concurrency for the downloads and then the pland and apply in parallel. The init does not take too much time. You can create a lock file or some other mechanism to check if another init is running or not.,,,,,,Anecdotal,comment,,,,,,,,2025-03-07,github/martivo,https://github.com/hashicorp/terraform/issues/31964#issuecomment-2705665792,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
You can use `flock /tmp/whatever.lock terraform init` to do that easily. [flock(1)](https://man7.org/linux/man-pages/man1/flock.1.html),,,,,,Anecdotal,comment,,,,,,,,2025-04-08,github/ricardbejarano,https://github.com/hashicorp/terraform/issues/31964#issuecomment-2787526602,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
"@ricardbejarano https://github.com/hashicorp/terraform/issues/31964#issuecomment-1939869453 We tried `flock`, it just not works for this case - https://github.com/antonbabenko/pre-commit-terraform/pull/620/commits/c17035bda0a5a83459c6d7da69fd257ca6b79bbb And our current implementation mentioned in https://github.com/hashicorp/terraform/issues/31964#issuecomment-1939869453 is buggy, as it can adds nonexistent `h1:` to lockfile - https://github.com/antonbabenko/pre-commit-terraform/issues/640#iss…",,,,,,Anecdotal,comment,,,,,,,,2025-04-10,github/MaxymVlasov,https://github.com/hashicorp/terraform/issues/31964#issuecomment-2793081615,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
"@MaxymVlasov correction: `flock` works (we use it), but it doesn't work on all filesystems. You can `flock` on a path outside the cache if you want, so long the path is within a compatible filesystem. YMMV.",,,,,,Anecdotal,comment,,,,,,,,2025-04-10,github/ricardbejarano,https://github.com/hashicorp/terraform/issues/31964#issuecomment-2794945689,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
"We're also using `flock` on our end. I only briefly looked at your linked script with flock, but one additional suggestion I'd make is to pass the `-w [timeout]` flag rather than trying to manually manage the timeout in a loop.",,,,,,Anecdotal,comment,,,,,,,,2025-04-10,github/bobziuchkovski,https://github.com/hashicorp/terraform/issues/31964#issuecomment-2795177109,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
"_(Warning: link to my own blog)_ [Here](https://www.bejarano.io/terraform-overlayfs/)'s a write up of an alternative to have concurrent-safe Terraform plugin caching using OverlayFS, until the native fix is on its way. I'm running this and it works.",,,,,,Anecdotal,comment,,,,,,,,2025-04-13,github/ricardbejarano,https://github.com/hashicorp/terraform/issues/31964#issuecomment-2799994358,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
"> As far as I'm aware, none of the installation methods are truly atomic today: even if we first extract into a temporary directory and then copy into the final location, there is no way to atomically copy a whole directory tree and so there will still be a window of time where another concurrent process can observe a directory that exists but doesn't yet have all of the expected files inside it, or has partial content for one file. In both of those situations the observing process will calcula…",,,,,,Anecdotal,comment,,,,,,,,2025-07-18,github/krzysztof-magosa,https://github.com/hashicorp/terraform/issues/31964#issuecomment-3089245593,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
"Feature Request: Builtin Function to Sort Semantic Versions UPDATE: The PR for this is here: https://github.com/hashicorp/terraform/pull/22689 ---- <!-- Hi there, Thank you for opening an issue. Please note that we try to keep the Terraform issue tracker reserved for bug reports and feature requests. For general usage questions, please see: https://www.terraform.io/community.html. If your issue relates to a specific Terraform provider, please open it in the provider's own repository. The index …",,,,,,Anecdotal,issue,,,,,,,,2019-09-04,github/vsimon,https://github.com/hashicorp/terraform/issues/22688,repo: hashicorp/terraform | keyword: workaround | state: open
"Thanks for sharing this use-case, @vsimon! One thing we'd need to think about for `sortsemver` that doesn't apply to `sort` is that semver does not define a _total order_ over all versions: if given the input `[""v1.0.0+foo"", ""v1.0.0+bar""]` then the ordering is undefined. One way we could address that is to make sure the input argument is defined as being a list (rather than a set) and then use a stable sort approach where if two versions have the same precedence then their relative ordering in …",,,,,,Anecdotal,comment,,,,,,,,2019-09-04,github/teamterraform,https://github.com/hashicorp/terraform/issues/22688#issuecomment-528109724,repo: hashicorp/terraform | issue: Feature Request: Builtin Function to Sort Semantic Versions | keyword: workaround
"Thanks for the reply. For defining a total ordering specifically for the build-metadata case, do you think this would be in-itself defining an ordering precedence outside of the spec when the spec specifically calls out that build metadata (characters following the '+' identifier) must be ignored when determining version precedence? (https://semver.org/#spec-item-10) ``` 10. Build metadata MAY be denoted by appending a plus sign and a series of dot separated identifiers immediately following th…",,,,,,Anecdotal,comment,,,,,,,,2019-09-04,github/vsimon,https://github.com/hashicorp/terraform/issues/22688#issuecomment-528126764,repo: hashicorp/terraform | issue: Feature Request: Builtin Function to Sort Semantic Versions | keyword: workaround
"Hi @vsimon, The problem is that in order to produce a flat list we need to decide on _some_ ordering for the build metadata, whether it be to order them lexically, or to preserve the order given in the input, or something else. Otherwise we'd need to have some way to indicate that two versions in the input are equivalent, which would add a lot of extra complication for what is hopefully an edge-case. On more reflection, it seems like preserving the input order in that case is probably the easie…",,,,,,Anecdotal,comment,,,,,,,,2019-09-09,github/teamterraform,https://github.com/hashicorp/terraform/issues/22688#issuecomment-529682601,repo: hashicorp/terraform | issue: Feature Request: Builtin Function to Sort Semantic Versions | keyword: workaround
"Hello, it's been awhile but I've updated this aged PR to address: 1. Using `hashicorp/go-version` and its `NewSemver` function for parsing. 2. Adding a version constraint. Constraints are defined and supported by https://godoc.org/github.com/hashicorp/go-version#Constraint 3. Using `sort.Stable` (and by extension go-version's `LessThan` function) 4. Updating tests and documentation",,,,,,Anecdotal,comment,,,,,,,,2020-08-09,github/vsimon,https://github.com/hashicorp/terraform/issues/22688#issuecomment-670988696,repo: hashicorp/terraform | issue: Feature Request: Builtin Function to Sort Semantic Versions | keyword: workaround
thanks @superbobdthm but unfortunately it's been super difficult getting a maintainer's attention. Maybe more upvotes? ping @teamterraform @apparentlymart for #22689 please 🙏,,,,,,Anecdotal,comment,,,,,,,,2021-02-19,github/vsimon,https://github.com/hashicorp/terraform/issues/22688#issuecomment-782420063,repo: hashicorp/terraform | issue: Feature Request: Builtin Function to Sort Semantic Versions | keyword: workaround
"A use case I have for this is to check that another state that a workspace depends on via Terraform remote state has the right version. With an assertion and @vsimon's addition in the PR, it would allow a component A to require that another component B that it depends on be deployed at a compatible version, and block deployment otherwise. Some of that can be mimicked using string manipulations (e.g., regexes), but that's not great. Any chance that this'll be merged? PR has been open for a long …",,,,,,Anecdotal,comment,,,,,,,,2021-06-17,github/peay,https://github.com/hashicorp/terraform/issues/22688#issuecomment-863192259,repo: hashicorp/terraform | issue: Feature Request: Builtin Function to Sort Semantic Versions | keyword: workaround
"I've found a workaround that can be used in the meantime. This can compare two semantic versions and indicate which value is larger. ``` terraform locals { version_a = split(""."", ""1.2.3"") version_b = split(""."", ""2.0.1"") version-test = [ for i, j in reverse(range(length(local.version_a))) : signum(local.version_b[i] - local.version_a[i]) * pow(10, j) ] # The version-compare value is -1, 0, or 1 if version_a is greater than, equal to or less than version_b respectively version-compare = signum(su…",,,,,,Anecdotal,comment,,,,,,,,2022-07-27,github/entscheidungsproblem,https://github.com/hashicorp/terraform/issues/22688#issuecomment-1196805096,repo: hashicorp/terraform | issue: Feature Request: Builtin Function to Sort Semantic Versions | keyword: workaround
"In case this is still relevant, I implemented this for kubernetes version comparison: ``` data ""aws_eks_cluster"" ""cluster"" { name = var.cluster_name } locals { kubernetes_1_24 = [1,24] cluster_version = [ for i in split(""."", data.aws_eks_cluster.cluster.version): tonumber(i) ] cluster_1_24_or_higher = alltrue( [ for index, num in local.cluster_version: (signum(num - local.kubernetes_1_24[index]) >= 0) ] ) } ```",,,,,,Anecdotal,comment,,,,,,,,2023-01-29,github/liad5h,https://github.com/hashicorp/terraform/issues/22688#issuecomment-1407656318,repo: hashicorp/terraform | issue: Feature Request: Builtin Function to Sort Semantic Versions | keyword: workaround
"If you want to also allow different version sizes (e.g. `14.6` compared with `14`) an improvement for https://github.com/hashicorp/terraform/issues/22688#issuecomment-1196805096 (thanks for the code) is: ```hcl version_engine = split(""."", ""14.7.1"") default_version = split(""."", ""14.6"") # The version_engine_with_default_version_comparison value is -1 (version_engine < default_version), 0 (version_engine = default_version), or 1 (version_engine > default_version) version_engine_with_default_versio…",,,,,,Anecdotal,comment,,,,,,,,2023-03-24,github/marcel-puchol-jt,https://github.com/hashicorp/terraform/issues/22688#issuecomment-1482694608,repo: hashicorp/terraform | issue: Feature Request: Builtin Function to Sort Semantic Versions | keyword: workaround
"Not sure if this should be a separate feature request, but I would just like to test if a version matches a version range. Something like: ```terraform my_local = version_matches("">= 7.0"", var.version) ? ""thing-for-higher-versions"" : ""thing-for-lower-versions"" ```",,,,,,Anecdotal,comment,,,,,,,,2023-04-27,github/tmccombs,https://github.com/hashicorp/terraform/issues/22688#issuecomment-1525986571,repo: hashicorp/terraform | issue: Feature Request: Builtin Function to Sort Semantic Versions | keyword: workaround
Any updates on this PR? Being able to leverage [hashicorp/go-version](https://godoc.org/github.com/hashicorp/go-version) in Terraform code as suggested in https://github.com/hashicorp/terraform/issues/22688#issuecomment-1525986571 would help a lot to perform some validations in Terraform modules. It would be really nice if something can be added (looks like this PR has been opened for a really long time),,,,,,Anecdotal,comment,,,,,,,,2024-01-04,github/HenriBlacksmith,https://github.com/hashicorp/terraform/issues/22688#issuecomment-1877265040,repo: hashicorp/terraform | issue: Feature Request: Builtin Function to Sort Semantic Versions | keyword: workaround
Thank you for your continued interest in this issue. [Terraform version 1.8](https://github.com/hashicorp/terraform/blob/v1.8.0-beta1/CHANGELOG.md) launches with support of provider-defined functions. It is now possible to implement your own functions! We would love to see this implemented as a provider-defined function. Please see the [provider-defined functions documentation](https://developer.hashicorp.com/terraform/plugin/framework/functions) to learn how to implement functions in your prov…,,,,,,Anecdotal,comment,,,,,,,,2024-03-06,github/crw,https://github.com/hashicorp/terraform/issues/22688#issuecomment-1982068727,repo: hashicorp/terraform | issue: Feature Request: Builtin Function to Sort Semantic Versions | keyword: workaround
"@crw Although this does make sense, I personally fear that without a common framework in place that scans, tests, and stamps providers with ""Hashicorp Conformance Tested"", this approach will lead to the grand mess of NodeJS and alike.",,,,,,Anecdotal,comment,,,,,,,,2024-03-07,github/mloskot,https://github.com/hashicorp/terraform/issues/22688#issuecomment-1983088578,repo: hashicorp/terraform | issue: Feature Request: Builtin Function to Sort Semantic Versions | keyword: workaround
"> @crw Although this does make sense, I personally fear that without a common framework in place that scans, tests, and stamps providers with ""Hashicorp Conformance Tested"", this approach will lead to the grand mess of NodeJS and alike. I see this as more HashiCorp will use that to move some deeper and specific functions into optional providers. I agree that the DIY option should be avoided when possible I expect an optional functions library from HashiCorp including those version checks for in…",,,,,,Anecdotal,comment,,,,,,,,2024-03-07,github/HenriBlacksmith,https://github.com/hashicorp/terraform/issues/22688#issuecomment-1983471467,repo: hashicorp/terraform | issue: Feature Request: Builtin Function to Sort Semantic Versions | keyword: workaround
Made a crappy TF provider to return sorted list of semver via data-source: [semvers](https://github.com/anapsix/terraform-provider-semvers),,,,,,Anecdotal,comment,,,,,,,,2024-09-01,github/anapsix,https://github.com/hashicorp/terraform/issues/22688#issuecomment-2323271039,repo: hashicorp/terraform | issue: Feature Request: Builtin Function to Sort Semantic Versions | keyword: workaround
"For reference, here is a plain Terraform expression that gives you the latest version. ```hcl variable ""versions"" { type = list(string) default = [ ""1.26.14"", ""1.27.10"", ""1.27.1"", ] } locals { versions = { for v in var.versions : v => [ for i in split(""."", v): tonumber(i) ] } latest_version = [ for k, v in local.versions : k if alltrue([ for other_k, other_v in local.versions : k == other_k || try([ for i in range(length(v)) : signum(v[i] - other_v[i]) if signum(v[i] - other_v[i]) != 0 ][0], 0)…",,,,,,Anecdotal,comment,,,,,,,,2025-07-18,github/visit1985,https://github.com/hashicorp/terraform/issues/22688#issuecomment-3089753981,repo: hashicorp/terraform | issue: Feature Request: Builtin Function to Sort Semantic Versions | keyword: workaround
"Terraform does not wait for the resource to be destroyed before creating it Hi there, I've faced this issue with `terraform` on multiple occasions with different resources. On renaming or tainting a resource terraform does not wait for the resource to be destroyed completely before attempting to create it. (the output for this is shown under `debug output`) The same if you want to rename a resource (AWS ALB). Then when we run `terraform apply` again. Everything works as expected becuase now the…",,,,,,Anecdotal,issue,,,,,,,,2020-04-18,github/ghost,https://github.com/hashicorp/terraform/issues/24704,repo: hashicorp/terraform | keyword: workaround | state: open
"Hi! Thanks for reporting this. On the surface of it, this sure looks like a valid issue, and I'd like to reproduce it locally. To do that, I have to be able to run this and run it on my workstation without inventing any details in order to be confident we're seeing the same behavior. As-is, don't have the terraform code that produced this, and so I'm stuck. In my experience, if I try to invent all the relevant details, it's a ton of work for me and it's unlikely to make for a very good reproduc…",,,,,,Anecdotal,comment,,,,,,,,2020-04-29,github/danieldreier,https://github.com/hashicorp/terraform/issues/24704#issuecomment-620930941,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
"We encoutered this problem a number of times. We now have the problem with the kubernets provider A deployment has to be recreated ``` Terraform will perform the following actions: # module.hello-world.kubernetes_deployment.app-deploy must be replaced -/+ resource ""kubernetes_deployment"" ""app-deploy"" { ``` The output of the terraform run is as follows ```` module.hello-world.kubernetes_deployment.app-deploy: Destroying... [id=moaprplatform-dev/hello-world-dpl] module.hello-world.kubernetes_depl…",,,,,,Anecdotal,comment,,,,,,,,2020-06-25,github/mark-00,https://github.com/hashicorp/terraform/issues/24704#issuecomment-649359266,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
Reference to issue filed against kubernetes provider an why this is difficult to solve https://github.com/hashicorp/terraform-provider-kubernetes/issues/697,,,,,,Anecdotal,comment,,,,,,,,2020-06-25,github/mark-00,https://github.com/hashicorp/terraform/issues/24704#issuecomment-649428581,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
We encountered this problem too when we need to recreate IAM roles. Example: We changed the path of an IAM role. This forces the recreation of the `aws_iam_role` resource. First i thought it was eventual consistency of the AWS API but rerunning the `terraform apply` command doesn't fix the problem. @danieldreier may you can reproduce this on your aws sandbox environment. Simply create an IAM role via terraform and try to change the path of the role. ## Terraform version ```bash $ terraform vers…,,,,,,Anecdotal,comment,,,,,,,,2021-11-30,github/cedricAhlers,https://github.com/hashicorp/terraform/issues/24704#issuecomment-982851481,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
"> We encountered this problem too when we need to recreate IAM roles. I faced this and, also, the same with aws_lambda_permission resources.",,,,,,Anecdotal,comment,,,,,,,,2021-12-02,github/ehrhardt,https://github.com/hashicorp/terraform/issues/24704#issuecomment-984987647,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
Hit this issue while moving `aws_iam_policy` resource into another module. Terraform doesn't wait for resource to be destroyed before creating.,,,,,,Anecdotal,comment,,,,,,,,2022-04-01,github/sla-1,https://github.com/hashicorp/terraform/issues/24704#issuecomment-1086412899,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
I faced the same issue with aws_lb. I added a subnet to the loadbalancer which forces it to recreate. But terraform trys to create new one and fails with error the load balancer exists with same name and different settings. It fails to delete the old load balancer for some reason.,,,,,,Anecdotal,comment,,,,,,,,2022-04-15,github/phanikumar1,https://github.com/hashicorp/terraform/issues/24704#issuecomment-1099813243,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
I am facing the same issue when creating security groups. I changed some configuration for the subnet & applied the changes but then it failed 👇 ![image](https://user-images.githubusercontent.com/59220026/177566145-9d3ccdc9-ad5d-402b-9403-4a117dc73585.png),,,,,,Anecdotal,comment,,,,,,,,2022-07-06,github/Parth909,https://github.com/hashicorp/terraform/issues/24704#issuecomment-1176248988,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
"same with aws_security_group. How is this issue still open after 2 years?!? initial plan and error output. this is a completely isolated security group with no attachments other than a vpc ``` Terraform will perform the following actions: # module.network.module.security_groups.aws_security_group.some-random-test will be destroyed # (because aws_security_group.some-random-test is not in configuration) - resource ""aws_security_group"" ""some-random-test"" { - arn = ""arn:aws:ec2:us-east-2:XXX:securi…",,,,,,Anecdotal,comment,,,,,,,,2022-07-06,github/shortpoet,https://github.com/hashicorp/terraform/issues/24704#issuecomment-1176282677,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
"Thanks for the high-quality reports! Running this issue past the team in triage, there is not a capability in terraform to identify that two different resources refer to the same underlying desired object. In this case, the use of [`moved` blocks](https://www.terraform.io/language/modules/develop/refactoring) would be necessary to signal to terraform that the resource is being refactored. Please let us know if you have use cases in which the use of `moved` blocks would not solve the problem.",,,,,,Anecdotal,comment,,,,,,,,2022-07-08,github/crw,https://github.com/hashicorp/terraform/issues/24704#issuecomment-1179409362,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
"I'm upgrading some ingresses from `kubernetes_ingress` to `kubernetes_ingress_v1`, which should be backward compatible. Because the resource types differ, I'm not able to use `state mv`. However because querying for v1 ingresses produces the old ingress, I always have a conflict where one gets added before the other is deleted, and the deletion seems to fail.",,,,,,Anecdotal,comment,,,,,,,,2022-07-14,github/arunv,https://github.com/hashicorp/terraform/issues/24704#issuecomment-1184981731,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
"Getting the same when recreating Cosmos DB containers: > module.databases.azurerm_cosmosdb_sql_container.cosmosDbDatabaseContainer[""ContainerName""] will be created module.databases.azurerm_cosmosdb_sql_container.cosmosDbDdatabaseAccountContainer[""ContainerName""]: Destruction complete after 30s Error:resource with the ID ""providers/Microsoft.DocumentDB/databaseAccounts/CosmosDbAccountName/sqlDatabases/CosmosDbName/containers/ContainerName"" already exists - to be managed via Terraform this resour…",,,,,,Anecdotal,comment,,,,,,,,2022-09-23,github/AliakseiKrylou,https://github.com/hashicorp/terraform/issues/24704#issuecomment-1256120270,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
"I wonder if we could have a way to tell terraform to apply a delay after a destroy operation, other than using a local shell script in preference to run upon destroy",,,,,,Anecdotal,comment,,,,,,,,2023-03-03,github/pacorreia,https://github.com/hashicorp/terraform/issues/24704#issuecomment-1453409935,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
"I cannot get my security group rule changes to apply no matter what I try. I had created my security group rules in an earlier version of the AWS provider, and the required arguments have changed. I don't want to change the names of my security groups; I simply want to recreate them with the correct, required, arguments. Unfortunately, I get the same issue that everyone else is getting, namely: ``` │ Error: creating VPC Security Group Rule │ │ with module.vpc[0].aws_vpc_security_group_ingress_r…",,,,,,Anecdotal,comment,,,,,,,,2023-07-28,github/holyoaks,https://github.com/hashicorp/terraform/issues/24704#issuecomment-1655663997,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
"I've even tried the `-replace=` option, listing every single security group rule that needs replacing. No change. Still says every security group rule `already exists`.",,,,,,Anecdotal,comment,,,,,,,,2023-07-28,github/holyoaks,https://github.com/hashicorp/terraform/issues/24704#issuecomment-1655680678,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
It would be much appreciated to have an option like `terraform apply --destroy-first` that makes sure all destructive state changes successfully finished before proceeding to update / create other resources.,,,,,,Anecdotal,comment,,,,,,,,2023-11-30,github/gsedlacz,https://github.com/hashicorp/terraform/issues/24704#issuecomment-1834252094,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
Facing the same issue while updating an EKS nodegroup where Terraform is trying to create the Nodegroup before deleting it.,,,,,,Anecdotal,comment,,,,,,,,2024-03-01,github/nandha4083,https://github.com/hashicorp/terraform/issues/24704#issuecomment-1972682828,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
"Same here with a target group: ``` ## PLAN # module.api-service.aws_alb_target_group.this must be replaced +/- resource ""aws_alb_target_group"" ""this"" { ## APPLY terraform apply -input=false .terraform/data_retention.dev.tfplan module.api-service.aws_alb_target_group.this: Creating... ╷ │ Error: error creating LB Target Group: DuplicateTargetGroupName: A target group with the same name 'dev-f7af-v3-data-retention-servi' exists, but with different settings │ status code: 400, request id: 160e71e0…",,,,,,Anecdotal,comment,,,,,,,,2024-04-18,github/Inmovilizame,https://github.com/hashicorp/terraform/issues/24704#issuecomment-2064724494,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
"Same issue with rds_proxy for me. No indication in the output of it even attempting to delete the existing rds_proxy when running the apply. My changes were to move the proxy onto different subnets :( ###PLAN: # aws_db_proxy.aurora_proxy must be replaced +/- resource ""aws_db_proxy"" ""aurora_proxy"" { ###APPLY Error: creating RDS DB Proxy: DBProxyAlreadyExistsFault: The DBProxy 'XXXXXXXXX' already exists status code: 400, request id: 885c0d94-f49a-4db6-92d5-e311cb9e91ab with aws_db_proxy.aurora_pr…",,,,,,Anecdotal,comment,,,,,,,,2024-04-24,github/neilbottomleysbg,https://github.com/hashicorp/terraform/issues/24704#issuecomment-2075158169,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
"I have a similar problem where I'm re-allocating a custom domain between two security policies on Azure FrontDoor. The plan decides it has to destroy and re-create them, however during the apply it doesn't wait for all resources to be destroyed before it starts creating. This can cause a scenario where the domain is still associated to the still-destroying policy and causes the AzureRM to throw an error. ``` ╷ │ Error: creating Front Door Security Policy: (Security Policy Name ""***"" / Profile N…",,,,,,Anecdotal,comment,,,,,,,,2024-10-16,github/ghost,https://github.com/hashicorp/terraform/issues/24704#issuecomment-2416934399,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
"Similar problem with AWS Route53 records, when two aws_route53_records share the same name and you want to create either one of them. Very easy to reproduce. main.tf: ``` resource ""aws_route53_record"" ""foo"" { count = 1 name = ""foo"" records = [ ""1.1.1.1"" ] zone_id = ""REDACTED"" type = ""A"" ttl = 30 } resource ""aws_route53_record"" ""foo2"" { count = 0 name = ""foo"" records = [ ""2.2.2.2"" ] zone_id = ""REDACTED"" type = ""A"" ttl = 30 } ``` Apply: ``` ❯ terraform apply Terraform used the selected providers …",,,,,,Anecdotal,comment,,,,,,,,2024-11-27,github/GagarinX,https://github.com/hashicorp/terraform/issues/24704#issuecomment-2503440063,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
"We are developing a Terraform provider and have encountered a similar issue related to resource replacement sequencing. **Use Case** A user creates an A record using Terraform. In a subsequent execution, they attempt to replace the A record with a CNAME record for the same FQDN. However, the backend system enforces a constraint where no other DNS record can exist for a name when creating a CNAME. **Problem** Terraform sometimes initiates the creation of the CNAME record before the A record is f…",,,,,,Anecdotal,comment,,,,,,,,2025-07-16,github/JkhatriInfobox,https://github.com/hashicorp/terraform/issues/24704#issuecomment-3076717632,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
"prevent_destroy should let you succeed Call me crazy, but I'm willing to call the current implementation of `prevent_destroy` a bug. Here is why: The current implementation of this flag prevents you from using it for 1/2 the use case. The net result is more frustration when trying to get Terraform to succeed instead of destroying your resources.. `prevent_destroy` adds to the frustration more than alleviating it. `prevent_destroy` is for these two primary use cases, right? 1) you don't want thi…",,,,,,Anecdotal,issue,,,,,,,,2015-11-12,github/ketzacoatl,https://github.com/hashicorp/terraform/issues/3874,repo: hashicorp/terraform | keyword: workaround | state: open
"Hi @ketzacoatl - thanks for opening this! Based on your description I'm certainly sympathetic to the idea that Terraform should not terminate with an error code if the user intent is to prevent resources being deleted, but I'm inclined to say that the output should indicate that resources where `prevent_destroy` was a factor in the execution should indicate this. @phinze, do you have any thoughts on this?",,,,,,Anecdotal,comment,,,,,,,,2015-11-12,github/jen20,https://github.com/hashicorp/terraform/issues/3874#issuecomment-156144124,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"Definitely sympathetic about this use-case too. I think a concern is that if Terraform fails to include one part of the update then that may have downstream impact in the dependency graph, which can be fine if you're intentionally doing it but would be confusing if Terraform just did it ""by default"". Do you think having the ability to exclude resources from plan, as proposed in #3366, would address your use-case? I'm imagining the following workflow: - Run `terraform plan` and see the error you…",,,,,,Anecdotal,comment,,,,,,,,2015-11-12,github/apparentlymart,https://github.com/hashicorp/terraform/issues/3874#issuecomment-156160003,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"I'd agree @jen20, I am primarily looking for the ability to tell TF that it does not need to quit/error out hard. Same on @apparentlymart's comment on default behavior - I agree, this is a specific use case and not meant as a default. > Do you think having the ability to exclude resources from plan, as proposed in #3366, would address your use-case? I had to re-read that a few times to make enough sense out of how that works (the doc addition helps: _Prefixing the resource with ! will exclude t…",,,,,,Anecdotal,comment,,,,,,,,2015-11-13,github/ketzacoatl,https://github.com/hashicorp/terraform/issues/3874#issuecomment-156288038,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"Would it be possible to get an additional flag when calling: terraform plan -destroy [ -keep-prevent-destroy ] I have the same problem, I have a few EIP associated with some instances. I want to be able to destroy every but keep the EIP for obvious reasons like whitelisting but I get the same kind of problem. I understand what destroy is all about, but in some cases it would be nice getting a warning saying this and that didn't get destroyed because of lifecycle.prevent_destroy = true. @ketzaco…",,,,,,Anecdotal,comment,,,,,,,,2015-11-16,github/mrfoobar1,https://github.com/hashicorp/terraform/issues/3874#issuecomment-157086868,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"+1, I need something along these lines as well. Would #3366 allow you to skip destroying a resource, but modify it instead? My specific use case is that I have a staging RDS instance I want to persist (never be destroyed), but I want the rest of my staging infrastructure to disappear. As a side effect of the staging environment disappearing, I need to modify the security groups on the RDS instance, since it is being deleted. So, if I had - two AWS instances - one rds security group - one rds in…",,,,,,Anecdotal,comment,,,,,,,,2015-11-19,github/erichmond,https://github.com/hashicorp/terraform/issues/3874#issuecomment-157945975,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"Hey folks, Good discussion here. It does sound like there's enough real world use cases to warrant a feature here. What about maintaining the current semantics of `prevent_destroy` and adding a new key called something like `skip_destroy` indicating: _any plan that would destroy this resource should be automatically modified to **not** destroy it_. Would something like this address all the needs expressed in this thread? If so, we can spec out the feature more formally and get it added.",,,,,,Anecdotal,comment,,,,,,,,2015-12-03,github/phinze,https://github.com/hashicorp/terraform/issues/3874#issuecomment-161715361,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"@phinze, that sounds good, yes.. I'd hope that in most cases, TF would be able to let the apply proceed, and let the user flag some resources as being left alone/not destroyed, and your proposal seems to provide the level of control needed, while retaining sensible semantics.",,,,,,Anecdotal,comment,,,,,,,,2015-12-03,github/ketzacoatl,https://github.com/hashicorp/terraform/issues/3874#issuecomment-161737325,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"I keep running in to this, I would like the ability for TF to only create if it does not exist and do not delete it. I would like to keep some ebs or rds data around and keep the rest of my stack as ephemeral (letting TF apply/destroy at will). Currently been doing this with different projects/directories. But it would be nice to keep the entire stack together as one piece. I too thought the prevent_destroy would not create an error and have been hacking my way around it quite a bit :(",,,,,,Anecdotal,comment,,,,,,,,2016-01-09,github/chadgrant,https://github.com/hashicorp/terraform/issues/3874#issuecomment-170273569,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
":+1: to what @phinze said. During apply, I want it to be created but ignored during destroy. Currently, I have to explicitly define the rest of the targets just to ignore 1 s3 resource.",,,,,,Anecdotal,comment,,,,,,,,2016-03-14,github/tsailiming,https://github.com/hashicorp/terraform/issues/3874#issuecomment-196392632,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"+1 - just ran into this. Another example are key pairs. I want to create (import) them if they don't exist, but if I destroy, I don't want to delete the keypair as other instances may be using the shared keypair. Is there a way around this for now?",,,,,,Anecdotal,comment,,,,,,,,2016-03-27,github/gservat,https://github.com/hashicorp/terraform/issues/3874#issuecomment-202054221,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"Yes, split your terraform project into multiple parts. Example: - base - core (like persistent data) - application stack Le dimanche 27 mars 2016, gservat notifications@github.com a écrit : > +1 - just ran into this. Another example are key pairs. I want to create > them if they don't exist, but if I destroy, I don't want to delete the > keypair as other instances may be using the shared keypair. > > Is there a way around this for now? > > — > You are receiving this because you commented. > Rep…",,,,,,Anecdotal,comment,,,,,,,,2016-03-28,github/mrfoobar1,https://github.com/hashicorp/terraform/issues/3874#issuecomment-202569646,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
Is this being looked at? I can't imagine there are many use cases that would NOT benefit from it. One example is 'Anyone using key pairs ever'.,,,,,,Anecdotal,comment,,,,,,,,2016-12-14,github/jbrown-rp,https://github.com/hashicorp/terraform/issues/3874#issuecomment-267093193,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"This is absolutely one of the banes of my life too. I've got *dozens* of resources I want to preserve from accidental overwrites - such as DynamoDB tables. A pair of flags for: - Keeping items that you prevent_destroy on (i.e. Don't delete the users from DynamoDB, ever - just skip it during a routine destroy) - Destroy, force. The flags could be something explicit like: - terraform destroy --skip-protected - terraform destroy --force-destroy-protected This would allow us to have the desired beh…",,,,,,Anecdotal,comment,,,,,,,,2017-01-04,github/steve-gray,https://github.com/hashicorp/terraform/issues/3874#issuecomment-270296400,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"Here's the use case we'd like this for: we have a module that we can use either for production (where some resources like Elastic IPs should not be accidentally deleted) or for running integration tests (where all resources should be destroyed afterwards). Because of #10730/#3116, we can't set these resources to be conditionally prevent_destroy, which would be the ideal solution. As a workaround, we'd be happy to have our integration test scripts run `terraform destroy --ignore-prevent-destroy`…",,,,,,Anecdotal,comment,,,,,,,,2017-01-19,github/glasser,https://github.com/hashicorp/terraform/issues/3874#issuecomment-273662886,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"This would defintely be a useful feature. I've been using terraform for less than a month and ran into this required feature in order to protect DNS Managed zone ... everything else in my infrastucture is transient but dealing with a new DNS zone comes with it computed ( potentially new ) Name Servers on what is a delegated zone, and this would introduce an unnecessary manual step to update the parent DNS managed zone - not to mention the DNS change time delay permeating making any auto testing…",,,,,,Anecdotal,comment,,,,,,,,2017-01-24,github/andyjcboyle,https://github.com/hashicorp/terraform/issues/3874#issuecomment-274786378,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"I'm hitting a slightly different use case with Vault. I'm not 100% sure whether this belongs here. Might be best handled in the Vault resource itself. Example: ```hcl resource ""vault_generic_secret"" ""github_auth_enable"" { path = ""sys/auth/github"" data_json = ""...some json..."" } resource ""vault_generic_secret"" ""github_auth_config"" { path = ""auth/github/config"" data_json = ""...some json..."" depends_on = [""vault_generic_secret.github_auth_enable""] } ``` The problem is that the 'auth/github/config'…",,,,,,Anecdotal,comment,,,,,,,,2017-01-25,github/kaii-zen,https://github.com/hashicorp/terraform/issues/3874#issuecomment-274996945,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"So my instance and issue would be things like rapid development and say docker_image / docker_container usage. I set `prevent_destroy = true` on the docker_image resources because I don't want terraform deleting the image from my disk so that I can rapidly destroy/create and run through development. When I set that, now I have to use a fancy scripting method to execute my targeted destroy list to destroy everything BUT the docker_image resources: ```bash TARGETS=$(for I in $(terraform state lis…",,,,,,Anecdotal,comment,,,,,,,,2017-02-24,github/mengesb,https://github.com/hashicorp/terraform/issues/3874#issuecomment-282431612,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"Any update on this. This has been open for 1.5 years and it is not fun to try to organize terraform around this shortcoming. The workaround for this is pretty ridiculous, I have a separate modules for ""persistent"", ""ephemeral"" in every project, but still need to use target or some way of skipping of and not running destroying modules that are persistent (or they spew errors).",,,,,,Anecdotal,comment,,,,,,,,2017-05-22,github/bradenwright,https://github.com/hashicorp/terraform/issues/3874#issuecomment-303233117,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"Is there any work being done on this? It feels like this feature ""prevent_destroy"" is designed as ""annoy you because you put this flag in if you want to destroy resources"" rather than... destroy what I want to destroy except for the things I don't want to destroy, notated by the ""prevent_destroy"" flag. Use case 1 in the original post seems like a silly use case because it's only designed to alert you and error out. In reality, adding prevent_destroy on a resource actually seems to mean prevent …",,,,,,Anecdotal,comment,,,,,,,,2017-06-06,github/HighwayofLife,https://github.com/hashicorp/terraform/issues/3874#issuecomment-306638305,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"To echo what @andyjcboyle said; when creating an aws_route53_zone you get a delegation set of 4 random name servers. I use the zone to define a subdomain, my domain is however not managed by terraform and I must insert the ns records manually. If I want to teardown my environment and then redeploy it (which I do often) I must manually reinsert the new name servers. It would be much nicer if I could have lifecycle flag like ignore_destroy/skip_destroy that allowed everything else in the plan to …",,,,,,Anecdotal,comment,,,,,,,,2017-07-17,github/Harrison-Miller,https://github.com/hashicorp/terraform/issues/3874#issuecomment-315911357,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"Hashicorp does not want this. It's the only explanation why it has not been implemented yet. On Mon, 17 Jul 2017 at 20:12 Verrazano <notifications@github.com> wrote: > To echo what @andyjcboyle <https://github.com/andyjcboyle> said; when > creating an aws_route53_zone you get a delegation set of 4 random name > servers. I use the zone to define a subdomain, my domain is however not > managed by terraform and I must insert the ns records manually. If I want > to tear down my environment and then…",,,,,,Anecdotal,comment,,,,,,,,2017-07-22,github/cescoferraro,https://github.com/hashicorp/terraform/issues/3874#issuecomment-317201553,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"This feature was added originally more as a ""prevent _replace_"", to avoid accidentally changing a ""forces new resource"" attribute on a critical object. Its current interaction with `terraform destroy` is not a critical part of that, so I think it would be reasonable to strike a compromise here: * If a diff contains a replacement of a `prevent_destroy` instance, that is a fatal error since there is no way to make the changes indicated without violating the constraint. * If the diff would contain…",,,,,,Anecdotal,comment,,,,,,,,2017-07-22,github/apparentlymart,https://github.com/hashicorp/terraform/issues/3874#issuecomment-317206197,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
That doesn't satisfy the use case in my comment above (which is honestly really an argument for the ability to set prevent_destroy conditionally rather than for this precise feature).,,,,,,Anecdotal,comment,,,,,,,,2017-07-23,github/glasser,https://github.com/hashicorp/terraform/issues/3874#issuecomment-317225876,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"Allow negative negative indices in slice() <!-- Hi there, Thank you for opening an issue. Please note that we try to keep the Terraform issue tracker reserved for bug reports and feature requests. For general usage questions, please see: https://www.terraform.io/community.html. If your issue relates to a specific Terraform provider, please open it in the provider's own repository. The index of providers is at https://github.com/terraform-providers . --> ### Current Terraform Version ``` Terrafo…",,,,,,Anecdotal,issue,,,,,,,,2019-06-19,github/hamstah,https://github.com/hashicorp/terraform/issues/21793,repo: hashicorp/terraform | keyword: workaround | state: open
This would be very useful. It'd also bring this function inline with the way the `substr()` function works currently which already allows a negative index.,,,,,,Anecdotal,comment,,,,,,,,2020-07-05,github/antonosmond,https://github.com/hashicorp/terraform/issues/21793#issuecomment-653888478,repo: hashicorp/terraform | issue: Allow negative negative indices in slice() | keyword: workaround
"Terraform should indeed have a negative index search to get the last element/index in list, as like this: `var.my_list[-1]` Please add this! Though it's quite clever (but not as clean and intuitive) to use the reverse function: `reverse(var.my_list)[0]`",,,,,,Anecdotal,comment,,,,,,,,2021-07-03,github/Satak,https://github.com/hashicorp/terraform/issues/21793#issuecomment-873451443,repo: hashicorp/terraform | issue: Allow negative negative indices in slice() | keyword: workaround
"Using `reverse(list)[N-1]` as a substitute for `list[-N]` is an anti-pattern as far as I'm concerned: - It is verbose - It is error prone: you have to convert indices if you want the second to last you could have `list[-2]` which matches your thought of *second* to last, whereas in the reversal you will need `reverse(list)[1]` where ""second"" is not as obvious - it is waste of computation: a new list must be created and populated via a loop - It obfuscates intent, leaving the reader to guess wha…",,,,,,Anecdotal,comment,,,,,,,,2021-07-07,github/schollii,https://github.com/hashicorp/terraform/issues/21793#issuecomment-875199525,repo: hashicorp/terraform | issue: Allow negative negative indices in slice() | keyword: workaround
"> Using reverse(list)[N-1] as a substitute for list[-N] is an anti-pattern as far as I'm concerned I agree, and would love if some author or contributor could have a look on it. I only wanted to provide a workaround for whoever stumbles into this thread and is looking for a possible solution",,,,,,Anecdotal,comment,,,,,,,,2021-07-07,github/txomon,https://github.com/hashicorp/terraform/issues/21793#issuecomment-875534495,repo: hashicorp/terraform | issue: Allow negative negative indices in slice() | keyword: workaround
"> I only wanted to provide a workaround for whoever stumbles into this thread and is looking for a possible solution The solution until then should be as suggested by the OP, since we've all seen how ""temporary workarounds"" almost always end up permanent in your code. The general pattern of OP, ie not just for replacing last item but other parts of string segmented by CHAR, is: ``` locals { split_var = split(CHAR, variable) sliced = slice(local.split_var, START, END) new_var = join(CHAR, expres…",,,,,,Anecdotal,comment,,,,,,,,2021-07-07,github/schollii,https://github.com/hashicorp/terraform/issues/21793#issuecomment-875632796,repo: hashicorp/terraform | issue: Allow negative negative indices in slice() | keyword: workaround
"Your usecase can be workarounded with `regex()`. Replace last 6 chars ``` > ""${regex(""(.*).{6}$"", ""arn:aws:secretsmanager:us-east-1:123456789012:secret:path/to/secret-name-fbghts"")[0]}??????"" ""arn:aws:secretsmanager:us-east-1:123456789012:secret:path/to/secret-name-??????"" ``` Replace everything after the last ""-"" ``` > ""${regex(""(.*-).*$"", ""arn:aws:secretsmanager:us-east-1:123456789012:secret:path/to/secret-name-fbghts"")[0]}??????"" ""arn:aws:secretsmanager:us-east-1:123456789012:secret:path/to/…",,,,,,Anecdotal,comment,,,,,,,,2021-07-13,github/zarnovican,https://github.com/hashicorp/terraform/issues/21793#issuecomment-879012342,repo: hashicorp/terraform | issue: Allow negative negative indices in slice() | keyword: workaround
Thank you for your continued interest in this issue. [Terraform version 1.8](https://github.com/hashicorp/terraform/blob/v1.8.0-beta1/CHANGELOG.md) launches with support of provider-defined functions. It is now possible to implement your own functions! We would love to see this implemented as a provider-defined function. Please see the [provider-defined functions documentation](https://developer.hashicorp.com/terraform/plugin/framework/functions) to learn how to implement functions in your prov…,,,,,,Anecdotal,comment,,,,,,,,2024-03-06,github/crw,https://github.com/hashicorp/terraform/issues/21793#issuecomment-1982068378,repo: hashicorp/terraform | issue: Allow negative negative indices in slice() | keyword: workaround
"@crw HashiCorp please make then an **official function extension provider** where the community can contribute. For big enterprise companies, that has high security and quality standards, using some random guy’s from Nebraska function extension provider is really a no go.",,,,,,Anecdotal,comment,,,,,,,,2024-03-07,github/Satak,https://github.com/hashicorp/terraform/issues/21793#issuecomment-1982372203,repo: hashicorp/terraform | issue: Allow negative negative indices in slice() | keyword: workaround
"For all that I am shocked that this issue exists and has been open for 5 years already, HCL [explicitly rejected](https://github.com/hashicorp/hcl/blob/360ae579460fab69e9939599af85c8f255a59007/ops.go#L122) the idea of using negative numbers to index from the end of a list/tuple: ``` // Some other languages allow negative indices to // select ""backwards"" from the end of the sequence, // but HCL doesn't do that in order to give better // feedback if a dynamic index is calculated // incorrectly. `…",,,,,,Anecdotal,comment,,,,,,,,2024-07-19,github/atykhyy,https://github.com/hashicorp/terraform/issues/21793#issuecomment-2238796001,repo: hashicorp/terraform | issue: Allow negative negative indices in slice() | keyword: workaround
"@cpboyd as much as I would like that feature, I have to agree with @atykhyy that introducing this now would break _existing_ code, in many cases silently. This is not acceptable. Yes languages evolve, but some fundamentals cannot change if they are going to break existing usages beyond your own code. One needs to either introduce a new function, a new operator, or new syntax so that the old behavior continues in existing code. Eg we could introduce a new function: `get_item(list, index)` since …",,,,,,Anecdotal,comment,,,,,,,,2024-07-23,github/olivers-xaxis,https://github.com/hashicorp/terraform/issues/21793#issuecomment-2244075641,repo: hashicorp/terraform | issue: Allow negative negative indices in slice() | keyword: workaround
"If you want to add a useful feature to an existing function, but there is danger that it can break existing code, then create a new function and implement this feature there. Sounds easy enough to me. What can go wrong? Example: `shift(list, count)`: return first `count` elements of `list`; if `count` is negative, then return all but first `count` elements. `pop(list, count)`: return last `count` elements of `list`; if `count` is negative, then return all but last `count` elements. p.s. impleme…",,,,,,Anecdotal,comment,,,,,,,,2025-04-28,github/shapirus,https://github.com/hashicorp/terraform/issues/21793#issuecomment-2836183578,repo: hashicorp/terraform | issue: Allow negative negative indices in slice() | keyword: workaround
You may just endup using https://developer.hashicorp.com/terraform/language/functions/basename I would have expected endname/endpath/filename as function name,,,,,,Anecdotal,comment,,,,,,,,2025-07-14,github/good92,https://github.com/hashicorp/terraform/issues/21793#issuecomment-3070255908,repo: hashicorp/terraform | issue: Allow negative negative indices in slice() | keyword: workaround
"Terraform not allowing to pass nested values (like assume_role.role_arn) to the backend.s3 configuration using cli (-backend-config=""KEY=VALUE"") and config file (-backend-config=PATH) ### Terraform Version ```shell 1.11.4 ``` ### Terraform Configuration Files ```terraform terraform { required_providers { aws = { source = ""hashicorp/aws"" version = "">= 4.33.0"" } } backend ""s3"" { bucket = """" use_lockfile="""" key="""" region="""" assume_role = { role_arn = """" external_id = """" } } required_version = "">= …",,,,,,Anecdotal,issue,,,,,,,,2025-04-21,github/Humeid-Ussene-Jocordasse,https://github.com/hashicorp/terraform/issues/36911,repo: hashicorp/terraform | keyword: workaround | state: open
"In case it's somehow useful, _a long time ago_ I wrote [`github.com/apparentlymart/go-hcl-overlay`](https://pkg.go.dev/github.com/apparentlymart/go-hcl-overlay/hcloverlay) with the goal of supporting stuff like this where a system needs to merge some command line options in with existing HCL body content. Even if that library is not directly useful as an import, I remember there being some interesting design tradeoffs in making that work which might give some ideas on similar tradeoffs to make …",,,,,,Anecdotal,comment,,,,,,,,2025-04-23,github/apparentlymart,https://github.com/hashicorp/terraform/issues/36911#issuecomment-2825000035,"repo: hashicorp/terraform | issue: Terraform not allowing to pass nested values (like assume_role.role_arn) to the backend.s3 configuration using cli (-backend-config=""KEY=VALUE"") and config file (-backend-config=PATH) | keyword: workaround"
"Hi @apparentlymart , thanks for sharing that project! I [had a go](https://github.com/hashicorp/terraform/compare/sarah/partial-config-nested-attrs-bugfix...refactor-with-hcloverlay) using it directly but it looks like attributes with `NestedType` set cannot be handled currently (I get `Invalid argument` errors for `assume_role.role_arn` etc. [from cli_args.go](https://github.com/apparentlymart/go-hcl-overlay/blob/e9dc0b303d829485dc4b8a09a1130c094b9e1893/hcloverlay/cli_args.go#L170), due to it …",,,,,,Anecdotal,comment,,,,,,,,2025-04-25,github/SarahFrench,https://github.com/hashicorp/terraform/issues/36911#issuecomment-2831004739,"repo: hashicorp/terraform | issue: Terraform not allowing to pass nested values (like assume_role.role_arn) to the backend.s3 configuration using cli (-backend-config=""KEY=VALUE"") and config file (-backend-config=PATH) | keyword: workaround"
"Ahh hmm yes I suppose I wrote this long enough ago that the idea of using the attribute syntax in a block-like way hadn't happened yet, and so my old implementation doesn't support that... it only wants to allow dotting through block types and their labels. ""Overlaying"" into attributes is actually likely to be somewhat harder than what I implemented there previously because from HCL's perspective the given expression hasn't been evaluated yet and so it doesn't have any idea what type of value i…",,,,,,Anecdotal,comment,,,,,,,,2025-04-25,github/apparentlymart,https://github.com/hashicorp/terraform/issues/36911#issuecomment-2831153163,"repo: hashicorp/terraform | issue: Terraform not allowing to pass nested values (like assume_role.role_arn) to the backend.s3 configuration using cli (-backend-config=""KEY=VALUE"") and config file (-backend-config=PATH) | keyword: workaround"
"The fact that nested structural attributes can't be declared from the CLI was a known limitation and tradeoff when the S3 backend added them to their schema, and they could only be parameterized via a config file. So in essence this is all working as intended right now. Being able to imply the attribute's existence by using the dotted traversal notation through the object in the above proposals would be unusual, and is really only going to work for simple single-nested structural attributes. If…",,,,,,Anecdotal,comment,,,,,,,,2025-04-28,github/jbardin,https://github.com/hashicorp/terraform/issues/36911#issuecomment-2836037311,"repo: hashicorp/terraform | issue: Terraform not allowing to pass nested values (like assume_role.role_arn) to the backend.s3 configuration using cli (-backend-config=""KEY=VALUE"") and config file (-backend-config=PATH) | keyword: workaround"
"Hi @jbardin, interesting thoughts... 1. I'd say that if we want to move to toward accepting HCL configurations in the CLI, just like the example you gave, it would make much sense to accept in one argument rather that multiple arguments, as terraforms is doing right now... e.g. **Not this:** ``` terraform -chdir=.iac init \ -backend=true \ -backend-config=""bucket=my-humble-bucket"" \ -backend-config=""key=my-humble-key.tfstate"" \ -backend-config=""region=af-south-1"" \ -backend-config=""use_lockfile…",,,,,,Anecdotal,comment,,,,,,,,2025-05-01,github/Humeid-Ussene-Jocordasse,https://github.com/hashicorp/terraform/issues/36911#issuecomment-2845528822,"repo: hashicorp/terraform | issue: Terraform not allowing to pass nested values (like assume_role.role_arn) to the backend.s3 configuration using cli (-backend-config=""KEY=VALUE"") and config file (-backend-config=PATH) | keyword: workaround"
"Does anyone has idea how to workaround this? I tried to convert my current style e.g. ```hcl # backend.hcl bucket = ""workloads-deployment-states"" key = ""services/sandbox/dev/terraform.tfstate"" dynamodb_table = ""workloads-deployment-locks"" region = ""eu-west-1"" max_retries = 3 role_arn = ""arn:aws:iam::0123456789:role/cicd/cicd-workloads-deploy"" session_name = ""sandbox-dev"" ``` along with init ``` terraform init -backend-config=environment/backend.hcl -input=false ``` and ENV var for External ID `…",,,,,,Anecdotal,comment,,,,,,,,2025-05-15,github/d47zm3,https://github.com/hashicorp/terraform/issues/36911#issuecomment-2883266266,"repo: hashicorp/terraform | issue: Terraform not allowing to pass nested values (like assume_role.role_arn) to the backend.s3 configuration using cli (-backend-config=""KEY=VALUE"") and config file (-backend-config=PATH) | keyword: workaround"
"Hi @d47zm3 - Currently it's only possible to override the values of `assume_role` and its nested attributes when the `-backend-config` flag supplies the path to an override file. The backend.hcl contents you've shared should work for that purpose. Overriding `assume_role` and its nested attributes is currently not possible when the `-backend-config` instead supplies a string containing a key and a value. It's expected that `-backend-config='assume_role={ external_id=""***"" }'` would not work. So…",,,,,,Anecdotal,comment,,,,,,,,2025-05-16,github/SarahFrench,https://github.com/hashicorp/terraform/issues/36911#issuecomment-2886369127,"repo: hashicorp/terraform | issue: Terraform not allowing to pass nested values (like assume_role.role_arn) to the backend.s3 configuration using cli (-backend-config=""KEY=VALUE"") and config file (-backend-config=PATH) | keyword: workaround"
I'm not keen on keeping ExternalID inside plaintext file (though people debate if it's secret...) but at least I got confirmation it's not currently possible so thanks @SarahFrench for your input 👍,,,,,,Anecdotal,comment,,,,,,,,2025-05-16,github/d47zm3,https://github.com/hashicorp/terraform/issues/36911#issuecomment-2886638563,"repo: hashicorp/terraform | issue: Terraform not allowing to pass nested values (like assume_role.role_arn) to the backend.s3 configuration using cli (-backend-config=""KEY=VALUE"") and config file (-backend-config=PATH) | keyword: workaround"
"allow optional support for matching prerelease versions in module constraints ### Terraform Version ```shell Terraform v1.11.4 on linux_amd64 ``` ### Use Cases When developing or testing modules that have prerelease versions (e.g., 2.0.0-beta.1), it is currently impossible to reference them using common version constraint operators like ```>=, ~>```, etc., because Terraform explicitly ignores prerelease versions in these comparisons. This makes prerelease testing workflows difficult and require…",,,,,,Anecdotal,issue,,,,,,,,2025-05-05,github/TimaevAlexandr,https://github.com/hashicorp/terraform/issues/36985,repo: hashicorp/terraform | keyword: workaround | state: open
"FWIW, I don't expect that implementing this would require any change to my ""go-versions"" module. Terraform could already call [`MeetingConstraintsExact`](https://pkg.go.dev/github.com/apparentlymart/go-versions@v1.0.3/versions#MeetingConstraintsExact) instead of [`MeetingConstraints`](https://pkg.go.dev/github.com/apparentlymart/go-versions@v1.0.3/versions#MeetingConstraints) in any situation where it wants to accept selecting any prerelease that otherwise meets the constraints.",,,,,,Anecdotal,comment,,,,,,,,2025-05-05,github/apparentlymart,https://github.com/hashicorp/terraform/issues/36985#issuecomment-2852587893,repo: hashicorp/terraform | issue: allow optional support for matching prerelease versions in module constraints | keyword: workaround
"@apparentlymart Thanks for your reply! it seems that some changes to the [apparentlymart/go-versions](https://github.com/apparentlymart/go-versions) might still be needed: I opened an issue about adding a string-based wrapper for ```MeetingConstraintsExact``` here: https://github.com/apparentlymart/go-versions/issues/8 Additionally, since ```MeetingConstraintsExact``` hasn't been tested yet, I believe it’s crucial to ensure its correctness and reliability before proceeding with integration",,,,,,Anecdotal,comment,,,,,,,,2025-05-06,github/TimaevAlexandr,https://github.com/hashicorp/terraform/issues/36985#issuecomment-2853363135,repo: hashicorp/terraform | issue: allow optional support for matching prerelease versions in module constraints | keyword: workaround
"While we’ve discussed some possible implementation details, I think it’s still important to first determine if this feature makes sense for the project",,,,,,Anecdotal,comment,,,,,,,,2025-05-06,github/TimaevAlexandr,https://github.com/hashicorp/terraform/issues/36985#issuecomment-2853383098,repo: hashicorp/terraform | issue: allow optional support for matching prerelease versions in module constraints | keyword: workaround
"I'm not sure if `MeetingConstraintsExact` fits most user's intention, or if we need a different definition of constraint matching. While it's true that on a timeline `2.0.0-beta1` falls _before_ `2.0.0` and therefor could be considered to match `<2.0`, versions also indicate compatibility breaks, so one cannot assume that including `2.0.0-beta1` in a config which otherwise requires `>=1.0.0` is even valid. I think for most users, opting into a prerelease means that the final GA version of that …",,,,,,Anecdotal,comment,,,,,,,,2025-05-06,github/jbardin,https://github.com/hashicorp/terraform/issues/36985#issuecomment-2854434078,repo: hashicorp/terraform | issue: allow optional support for matching prerelease versions in module constraints | keyword: workaround
"As far as UI for this goes, I don't know if environment variables are the right choice, but it does need to be more than a binary flag, because globally enabling prereleases might include any number of modules which the user does not intend to test.",,,,,,Anecdotal,comment,,,,,,,,2025-05-06,github/jbardin,https://github.com/hashicorp/terraform/issues/36985#issuecomment-2854443493,repo: hashicorp/terraform | issue: allow optional support for matching prerelease versions in module constraints | keyword: workaround
"FWIW, I agree with @jbardin that this doesn't ""feel right"" to me, and that's a big part of why go-versions has the default behavior it does: Semver says that prereleases have lower precedence than their corresponding final release, and that rule makes sense when you're only trying to answer ""which of these two versions is newer?"", but doesn't make much sense when it comes to resolving version constraints (which is not something semver says much about). My view has been that prereleases are esse…",,,,,,Anecdotal,comment,,,,,,,,2025-05-06,github/apparentlymart,https://github.com/hashicorp/terraform/issues/36985#issuecomment-2854785426,repo: hashicorp/terraform | issue: allow optional support for matching prerelease versions in module constraints | keyword: workaround
"Back when I wrote go-versions I did some research into how other similar systems were handling prereleases and adopted something similar, but I just noticed that Rust's Cargo has a different compromise that might work for Terraform too: > Cargo allows “newer” pre-releases to be used automatically. For example, if `1.0.0-beta` is published, then a requirement `foo = ""1.0.0-alpha""` will allow updating to the beta version. Note that this only works on the same release version, `foo = ""1.0.0-alpha""…",,,,,,Anecdotal,comment,,,,,,,,2025-05-06,github/apparentlymart,https://github.com/hashicorp/terraform/issues/36985#issuecomment-2856350483,repo: hashicorp/terraform | issue: allow optional support for matching prerelease versions in module constraints | keyword: workaround
"I can't remember what the original need for the second go-versions package was, but FWIW we wrote `hashicorp/go-version` to handle pre-release matching exactly as @apparentlymart described above. A constraint of `>=1.0.0-a` would include matching releases, with pre-releases which lexically sort >= `a`, and the above list of examples starting with `>= 1.1.0-beta.2` all pass and fail correctly using `hashicorp/go-version`. That package was written to mirror the popular ruby-gems versioning system…",,,,,,Anecdotal,comment,,,,,,,,2025-05-07,github/jbardin,https://github.com/hashicorp/terraform/issues/36985#issuecomment-2858657629,repo: hashicorp/terraform | issue: allow optional support for matching prerelease versions in module constraints | keyword: workaround
"My recollection is that `hashicorp/go-version` did not initially have any special treatment of prerelease versions at all, and so it would happily e.g. install `1.1.0-beta.1` given the version constraint `>= 1.0.0`, which I believe is the cause of https://github.com/hashicorp/web-unified-docs/issues/724. We tried to rally around updating go-version but since it's a shared library there were other users of it that objected to changing it at first, and so we swapped in my library as a workaround …",,,,,,Anecdotal,comment,,,,,,,,2025-05-08,github/apparentlymart,https://github.com/hashicorp/terraform/issues/36985#issuecomment-2864608242,repo: hashicorp/terraform | issue: allow optional support for matching prerelease versions in module constraints | keyword: workaround
"Thank you for the extra research @apparentlymart! I didn't have the timeline quite right, but this makes more sense now how we ended up here. I'll have to check out the linked issue too 👍",,,,,,Anecdotal,comment,,,,,,,,2025-05-09,github/jbardin,https://github.com/hashicorp/terraform/issues/36985#issuecomment-2866228476,repo: hashicorp/terraform | issue: allow optional support for matching prerelease versions in module constraints | keyword: workaround
"Since it's difficult to choose the best technical direction at this stage, it might make sense to first focus on what the intended user experience should look like — and use that to narrow down the implementation options as @jbardin mentioned > As far as UI for this goes, I don't know if environment variables are the right choice, but it does need to be more than a binary flag, because globally enabling prereleases might include any number of modules which the user does not intend to test. I ex…",,,,,,Anecdotal,comment,,,,,,,,2025-05-28,github/TimaevAlexandr,https://github.com/hashicorp/terraform/issues/36985#issuecomment-2917757431,repo: hashicorp/terraform | issue: allow optional support for matching prerelease versions in module constraints | keyword: workaround
Regression due to #31237: Local paths not valid return values any more ### Terraform Version Working: v1.2.2 Broken: ``` Terraform v1.2.3 on darwin_arm64 + provider registry.terraform.io/hashicorp/archive v2.2.0 + provider registry.terraform.io/hashicorp/aws v4.21.0 + provider registry.terraform.io/hashicorp/random v3.3.2 Your version of Terraform is out of date! The latest version is 1.2.4. You can update by downloading from https://www.terraform.io/downloads.html ``` Same behavior in v1.2.4. …,,,,,,Anecdotal,issue,,,,,,,,2022-07-08,github/dr-yd,https://github.com/hashicorp/terraform/issues/31404,repo: hashicorp/terraform | keyword: workaround | state: closed
"Hi @dr-yd, Thanks for filing the issue. I'm not sure what exactly has changed from that commit, mostly because I surprised that the use case you've shown previously worked. The [registry protocol](https://www.terraform.io/internals/module-registry-protocol) was defined such that a path returned in `X-Terraform-Get` from an http URL is considered a relative path to the first request. > The value of X-Terraform-Get may instead be a relative URL, indicated by beginning with `/`, `./` or `../`, in …",,,,,,Anecdotal,comment,,,,,,,,2022-07-08,github/jbardin,https://github.com/hashicorp/terraform/issues/31404#issuecomment-1178990550,repo: hashicorp/terraform | issue: Regression due to #31237: Local paths not valid return values any more | keyword: workaround
"Hi @jbardin , thanks for commenting. Interesting to see that we were using an unintended functionality - I just followed the sentence from the documentation that I quoted and it worked. You answer is a bit alarming. Does it mean that switching to git (== SSH) protocol is against your intentions as well? That would be a huge problem for us. Background: We have a central `modules` repo and a number of project repos that use it. It's not rare that we have to introduce breaking changes to the modul…",,,,,,Anecdotal,comment,,,,,,,,2022-07-11,github/dr-yd,https://github.com/hashicorp/terraform/issues/31404#issuecomment-1180195577,repo: hashicorp/terraform | issue: Regression due to #31237: Local paths not valid return values any more | keyword: workaround
"Oh yes, while arbitrary protocol switching is something that is not desired for security reasons (having a remote system force the loading of any local file could be combined with other attack vectors), we did try to maintain the documented protocol combinations for compatibility. Switching to `git` should therefor work, which appears to be the case from my local testing.",,,,,,Anecdotal,comment,,,,,,,,2022-07-11,github/jbardin,https://github.com/hashicorp/terraform/issues/31404#issuecomment-1180431444,repo: hashicorp/terraform | issue: Regression due to #31237: Local paths not valid return values any more | keyword: workaround
"Ah, that's great to hear - so at least the pipelines won't break in the future. For local development, there are workarounds (e.g. running a webserver locally and just serving local files from there) but if you come to the conclusion that there should be some official way to reference local files, that would of course be preferable from my POV. Since it's not a pressing issue, maybe you can find the time to discuss this and update the thread for one of the next releases. Thanks!",,,,,,Anecdotal,comment,,,,,,,,2022-07-12,github/dr-yd,https://github.com/hashicorp/terraform/issues/31404#issuecomment-1181837275,repo: hashicorp/terraform | issue: Regression due to #31237: Local paths not valid return values any more | keyword: workaround
"Hi @jbardin ! We've recently bumped into some issues when attempting to make a protocol switch from `http` to `git` and were curious around how you got that local testing done to allow switching to `git`. For context, we're hosting vanity redirects through a static website towards GitHub sources that hold our Terraform modules. The `url` referenced in our `X-Terraform-Get` headers is a Git/SSH format (`git@github.com:<org>/<repo>.git`) and we're getting the same `invalid source string` error ba…",,,,,,Anecdotal,comment,,,,,,,,2022-10-18,github/Andres-Lu,https://github.com/hashicorp/terraform/issues/31404#issuecomment-1282694454,repo: hashicorp/terraform | issue: Regression due to #31237: Local paths not valid return values any more | keyword: workaround
"@Andres-Lu For git, the response must be something like x-terraform-get: git::ssh://git@git.example.com/terraform/modules.git//vpc?ref=v1.2.3 ---- Re: Topic, I only recently noticed that the `file://` protocol actually _is_ included and still usable in TF 1.3.2. Only returning a naked filename is a problem. I didn't even check in the light of @jbardin 's response and I'm not sure if that's slated for removal so I'm leaving the issue open. For now, I'm just really happy to have 1.3.",,,,,,Anecdotal,comment,,,,,,,,2022-10-18,github/dr-yd,https://github.com/hashicorp/terraform/issues/31404#issuecomment-1282701022,repo: hashicorp/terraform | issue: Regression due to #31237: Local paths not valid return values any more | keyword: workaround
apt install terraform @ sid ### Terraform Version ```shell not applicable ``` ### Use Cases Wishing to use terraform. ### Attempted Solutions Several retries of what is documented at https://www.terraform.io/downloads But got today every time ```text ... Ign:3 https://apt.releases.hashicorp.com sid InRelease Err:4 https://apt.releases.hashicorp.com sid Release 404 Not Found [IP: 18.65.39.12 443] Reading package lists... Done E: The repository 'https://apt.releases.hashicorp.com sid Release' doe…,,,,,,Anecdotal,issue,,,,,,,,2022-09-19,github/stappersg,https://github.com/hashicorp/terraform/issues/31816,repo: hashicorp/terraform | keyword: workaround | state: closed
"Hi, it looks like you are using the unstable branch of Debian. Currently we build for the following Debian distros: Jessie, Stretch, Buster, Bullseye, Bookworm. I would imagine if you looked into mixing stable and unstable apt repositories, you would find a way to install Terraform on an unstable distribution of Debian. Probably you need to change: `sid` to `bullseye` or `bookworm` in `/etc/apt/sources.list.d/hashicorp.list`, but I have not tested this myself.",,,,,,Anecdotal,comment,,,,,,,,2022-09-19,github/crw,https://github.com/hashicorp/terraform/issues/31816#issuecomment-1251573313,repo: hashicorp/terraform | issue: apt install terraform @ sid | keyword: workaround
"> Hi, it looks like you are using the unstable branch of Debian. Yes. ```text $ lsb_release -cs sid $ ``` > Currently we build for the following Debian distros: Jessie, Stretch, Buster, Bullseye, Bookworm. So I tried `bookworm` ```text $ curl --silent https://apt.releases.hashicorp.com/dists/bookworm/main/binary-amd64/Packages.xz \ | xmllint --format - <?xml version=""1.0"" encoding=""UTF-8""?> <Error> <Code>NoSuchKey</Code> <Message>The specified key does not exist.</Message> <Key>dists/bookworm/m…",,,,,,Anecdotal,comment,,,,,,,,2022-09-19,github/stappersg,https://github.com/hashicorp/terraform/issues/31816#issuecomment-1251633270,repo: hashicorp/terraform | issue: apt install terraform @ sid | keyword: workaround
"Hi @stappersg, In the meantime you could install Terraform manually. The CLI only consists of a single binary, and the most common installation method is to download that binary directly from [releases.hashicorp.com/terraform](https://releases.hashicorp.com/terraform/).",,,,,,Anecdotal,comment,,,,,,,,2022-09-20,github/jbardin,https://github.com/hashicorp/terraform/issues/31816#issuecomment-1252313259,repo: hashicorp/terraform | issue: apt install terraform @ sid | keyword: workaround
"> Should I retitle the issue into apt install terraform @ sid? That makes sense. One outcome from this issue is that the release engineering team plans to update this page (https://www.hashicorp.com/official-packaging-guide) with the list of supported distributions. To set expectations, I do not imagine we will decide to support sid/unstable in the near future. The workarounds are fairly straight-forward and it does not make sense for us to officially support the product on an unstable linux di…",,,,,,Anecdotal,comment,,,,,,,,2022-09-20,github/crw,https://github.com/hashicorp/terraform/issues/31816#issuecomment-1252606928,repo: hashicorp/terraform | issue: apt install terraform @ sid | keyword: workaround
In the meantime we do have [some Terraform-specific documentation about APT repositories](https://www.terraform.io/cli/install/apt) which lists which distributions we support. I'm noticing though that the page is no longer linked from anywhere useful. It was formally linked from our downloads page but that link got lost in the redesign of the download page which removed the side navigation bar in favor of the more-compact package manager selection tabs. --- We should probably discuss this more …,,,,,,Anecdotal,comment,,,,,,,,2022-09-20,github/apparentlymart,https://github.com/hashicorp/terraform/issues/31816#issuecomment-1252731047,repo: hashicorp/terraform | issue: apt install terraform @ sid | keyword: workaround
"Notes for the docs team: I flagged this issue due to the docs concerns, but we should probably open a new issue for any docs changes we choose to make, and remove the docs label from this one once we've decided what to do next. :)",,,,,,Anecdotal,comment,,,,,,,,2022-09-22,github/crw,https://github.com/hashicorp/terraform/issues/31816#issuecomment-1255655721,repo: hashicorp/terraform | issue: apt install terraform @ sid | keyword: workaround
"Terraform init is extremely slow Terraform init takes forever and it seems like it is due to provider download. This extremely slow behaviour is consistent for the last 2 days. Working like that is hellish as I'm using terragrunt which runs the `terraform init` command often in my case. Drilling down on the trace logs, I've seen that the download of https://releases.hashicorp.com/terraform-provider-aws/3.22.0/terraform-provider-aws_3.22.0_darwin_amd64.zip takes usually 1-5 minutes minimum. When…",,,,,,Anecdotal,issue,,,,,,,,2020-12-29,github/AlmogCohen,https://github.com/hashicorp/terraform/issues/27379,repo: hashicorp/terraform | keyword: workaround | state: closed
"Here is another trace log example from a project that is includes a little more pieces https://gist.github.com/AlmogCohen/19495e839bc4894d8084e9ab191e23f6 A billion of things are being downloaded, but mostly when it is requested from the hashicorp releases host, the download takes forever. The download of `https://releases.hashicorp.com/terraform-provider-aws/2.70.0/terraform-provider-aws_2.70.0_darwin_amd64.zip` took almost a minute. Download of `https://releases.hashicorp.com/terraform-provid…",,,,,,Anecdotal,comment,,,,,,,,2020-12-29,github/AlmogCohen,https://github.com/hashicorp/terraform/issues/27379#issuecomment-752077640,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
"Thanks for reporting this! You're right that it shouldn't be so slow. I'm running on a mac and it's not that slow for me. Can you try and do an install of terraform via homebrew? Homebrew's Terraform has cgo enabled, and I'm wondering if you're stalling on DNS resolution.",,,,,,Anecdotal,comment,,,,,,,,2020-12-29,github/danieldreier,https://github.com/hashicorp/terraform/issues/27379#issuecomment-752258528,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
"Thanks! I appreciate your quick (and affirming) response! I'm pretty sure I'm using the homebrew version as I've upgraded from Terraform v0.12 a few days ago...the last homebrew command I have in history is `brew reinstall terraform@0.13`. If I had DNS issues...won't it be affecting the browser download speed as well? For all modules I tried, the browser speed was like 20 times faster than the `terraform init` for that specific module. EDIT: I've just verified locally, the terraform init is sti…",,,,,,Anecdotal,comment,,,,,,,,2020-12-30,github/AlmogCohen,https://github.com/hashicorp/terraform/issues/27379#issuecomment-752347091,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
"same issue here, terrafrom version is 0.13.5, installed via homebrew hang on aws provider for very long time(10 minutes+) ``` - Installing -/aws v3.22.0... ```",,,,,,Anecdotal,comment,,,,,,,,2020-12-30,github/zzn01,https://github.com/hashicorp/terraform/issues/27379#issuecomment-752393749,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
"@AlmogCohen the reason I wanted to look into DNS issues is that golang uses its own DNS resolver rather than the native one, and this has caused issues in Terraform (and many other projects) that can currently only be worked around with a cgo-enabled build. However, if you're not seeing it on a homebrew build then that's probably not the issue. Most of the Terraform Core engineering team is out until next week, so I'm just doing basic triage of urgent issues. As other folks run into this - plea…",,,,,,Anecdotal,comment,,,,,,,,2020-12-30,github/danieldreier,https://github.com/hashicorp/terraform/issues/27379#issuecomment-752720887,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
"I went and tried to reproduce this again on my mac with the following trivial config: ```terraform terraform { required_providers { aws = { source = ""hashicorp/aws"" version = ""~> 3.0"" } } } # Configure the AWS Provider provider ""aws"" { region = ""us-east-1"" } ``` My init took about 7 seconds: ``` time terraform init Initializing the backend... Initializing provider plugins... - Finding hashicorp/aws versions matching ""~> 3.0""... - Installing hashicorp/aws v3.22.0... - Installed hashicorp/aws v3.…",,,,,,Anecdotal,comment,,,,,,,,2020-12-30,github/danieldreier,https://github.com/hashicorp/terraform/issues/27379#issuecomment-752723131,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
"I copied your trivial config into `main.tf` and run `time terraform init` and this is the result: ``` Initializing the backend... Initializing provider plugins... - Finding hashicorp/aws versions matching ""~> 3.0""... - Installing hashicorp/aws v3.22.0... - Installed hashicorp/aws v3.22.0 (signed by HashiCorp) Terraform has been successfully initialized! You may now begin working with Terraform. Try running ""terraform plan"" to see any changes that are required for your infrastructure. All Terraf…",,,,,,Anecdotal,comment,,,,,,,,2020-12-31,github/AlmogCohen,https://github.com/hashicorp/terraform/issues/27379#issuecomment-752862919,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
"@AlmogCohen Thanks for running the timings and providing additional data.`terraform init` will take longer than the GET request alone. One suggestion I have if you're running init often (not a workflow that is currently optimized), do make sure that if you can you're using [provider cache settings](https://www.terraform.io/docs/commands/cli-config.html#provider-plugin-cache). This should help with regard to your workflow. On the final comments with timing, I see @danieldreier's report with ~7 s…",,,,,,Anecdotal,comment,,,,,,,,2021-01-04,github/pselle,https://github.com/hashicorp/terraform/issues/27379#issuecomment-754100075,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
"@pselle Thanks for your comment. I'll look into enabling the cache later this week and respond here. Regardless, it is important to note that the original problem still stands in place. The timing was **1 minute** (and not 5 seconds like you said). If you'll take a closer look, you can see that the 5s timing is of **wget**. Terraform init is still 12 times slower, and from previous logs you could see that 95% of the time spent on terraform init is due to the resource download of the aws provide…",,,,,,Anecdotal,comment,,,,,,,,2021-01-05,github/AlmogCohen,https://github.com/hashicorp/terraform/issues/27379#issuecomment-754453595,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
"I have the same problem on Windows. Checking with Wireshark, I can see that the releases.hashicorp.com server starts off by accepting a TLSv1.2 connection but then gets stuck returning messages saying ""ignored unknown record"". It took so long that I killed it. I was just doing the learn-terraform-aws-instance tutorial. I was able to create infrastructure once, but never again after that. My provider has a short MTU of 1472 bytes. I think this requires servers to accept ICMP messages saying that…",,,,,,Anecdotal,comment,,,,,,,,2021-01-08,github/future-media,https://github.com/hashicorp/terraform/issues/27379#issuecomment-756655072,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
"Although learn-terraform-aws-instance tutorial fails completely on my office connection as described above, it works just fine when executed from Windows Server hosted by a different provider. Other Windows PCs in my office also fail, but someone was able to get it to work on Linux. So it's the combination of my office connection + Windows that fails.",,,,,,Anecdotal,comment,,,,,,,,2021-01-11,github/future-media,https://github.com/hashicorp/terraform/issues/27379#issuecomment-757620103,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
"Something that also aggravates this issue is the fact that apparently, if you run multiple consecutive `terraform init` commands without any changes in the used providers nor their versions, Terraform will still proceed to install them each time: ```bash $ terraform init Initializing modules... Initializing the backend... Initializing provider plugins... - Reusing previous version of hashicorp/random from the dependency lock file - Reusing previous version of hashicorp/local from the dependency…",,,,,,Anecdotal,comment,,,,,,,,2021-01-11,github/lmserrano,https://github.com/hashicorp/terraform/issues/27379#issuecomment-757914995,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
"I'm using terraform 0.14.5 on ubuntu. I'm experiencing the same thing. `terraform init` takes a lot of time. here is my` main.tf`: ``` provider ""kubernetes"" { } resource ""kubernetes_deployment"" ""backend"" { metadata { name = ""backend"" labels = { app = ""backend"" } } spec { replicas = 3 selector { match_labels = { app = ""backend"" } } template { metadata { labels = { app = ""backend"" } } spec { container { image = ""IMAGE"" name = ""backend"" port { container_port = 8000 } } } } } } ```",,,,,,Anecdotal,comment,,,,,,,,2021-02-02,github/HsnVahedi,https://github.com/hashicorp/terraform/issues/27379#issuecomment-771916338,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
"@gevial and @HsnVahedi , if you can, try updating to 0.14.6. It has a nice set of improvements related with init, especially for what concerns default caching config.",,,,,,Anecdotal,comment,,,,,,,,2021-03-26,github/lmserrano,https://github.com/hashicorp/terraform/issues/27379#issuecomment-808328769,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
"It has just stuck on my mac: ``` Initializing the backend... Initializing provider plugins... - Finding hashicorp/aws versions matching ""~> 3.27""... - Installing hashicorp/aws v3.38.0... ``` ``` terraform { backend ""s3"" { bucket = ""boss-mr-terraform-state-dev"" key = ""obs"" dynamodb_table = ""terraform-lock"" region = ""us-east-1"" } required_providers { aws = { source = ""hashicorp/aws"" version = ""~> 3.27"" } } required_version = "">= 0.14.9"" } provider ""aws"" { region = ""us-east-1"" } ``` ``` ▶ terrafor…",,,,,,Anecdotal,comment,,,,,,,,2021-05-06,github/martavoi,https://github.com/hashicorp/terraform/issues/27379#issuecomment-833777542,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
"for me as well most of the time. could be that the server where the providers are hosted is slow or some other networking issue. I already enabled the plugin cache, but unfortunately i think gitlab-runner running in kubernetes do not have a shared cache.",,,,,,Anecdotal,comment,,,,,,,,2021-05-06,github/lsascha,https://github.com/hashicorp/terraform/issues/27379#issuecomment-833820300,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
"I found the issue for me. My corporate IT installed a network security product a few weeks ago called Zscaler. It had not caused any trouble before so I didn’t think about it. However, I also noticed than when I would watch streaming video lately that I could never get an HD stream, only low quality streams. I disabled Zscaler and it fixed both issues, video streaming and TF init plug-in downloads.",,,,,,Anecdotal,comment,,,,,,,,2021-05-14,github/scastria,https://github.com/hashicorp/terraform/issues/27379#issuecomment-841016648,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
Any updates here? having the same issues... downloading from providers is extremely slow while everything else network-wise seems fine...,,,,,,Anecdotal,comment,,,,,,,,2021-09-30,github/marceloboeira,https://github.com/hashicorp/terraform/issues/27379#issuecomment-931325483,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
"If there's a mirror in China for developers will be very nice, turn on the VPN, but the bellow error message is often encountered. ```text terraform init Initializing the backend... Initializing provider plugins... - Finding hashicorp/azurerm versions matching "">= 2.75.0""... - Finding aztfmod/azurecaf versions matching ""1.2.10""... - Installing hashicorp/azurerm v2.93.1... ╷ │ Error: Failed to install provider │ │ Error while installing hashicorp/azurerm v2.93.1: Get │ ""https://releases.hashicor…",,,,,,Anecdotal,comment,,,,,,,,2022-01-27,github/moarychan,https://github.com/hashicorp/terraform/issues/27379#issuecomment-1023167965,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
A year later and this issue still exists in latest version of terraform. Can someone provide a status update?? ![bump](https://user-images.githubusercontent.com/4675662/153973452-4e0e17a4-6ecb-4283-957b-b4ece1daa1b4.gif),,,,,,Anecdotal,comment,,,,,,,,2022-02-15,github/lots0logs,https://github.com/hashicorp/terraform/issues/27379#issuecomment-1039743948,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
"Lambda source code hash changing each time I perform terraform plan/apply through Jenkins despite not having any changes in the lambda code ### Terraform Version ```shell 1.10.5 ``` ### Terraform Configuration Files data ""archive_file"" ""ec2_start"" { type = ""zip"" source_file = ""${path.module}/ec2_start_code/lambda_function.py"" output_path = ""ec2start.zip"" } resource ""aws_lambda_function"" ""ec2_start_lambda"" { function_name = ""ec2-start-lambda"" filename = ""ec2start.zip"" source_code_hash = data.arc…",,,,,,Anecdotal,issue,,,,,,,,2025-08-07,github/jasin-devops,https://github.com/hashicorp/terraform/issues/37414,repo: hashicorp/terraform | keyword: workaround | state: closed
"Hello, We use GitHub issues for tracking bugs and enhancements, rather than for questions. While we can sometimes help with certain simple problems here, it's better to use [the community forum](https://discuss.hashicorp.com/c/terraform-core) where there are more people ready to help. I suggest creating a complete, minimal, reproducible example if you want to ask for assistance, since it's not clear why you could be getting a different hash value if you are not changing the input file. Thanks!",,,,,,Anecdotal,comment,,,,,,,,2025-08-07,github/jbardin,https://github.com/hashicorp/terraform/issues/37414#issuecomment-3164565892,repo: hashicorp/terraform | issue: Lambda source code hash changing each time I perform terraform plan/apply through Jenkins despite not having any changes in the lambda code | keyword: workaround
"Support specifying provider configurations (e.g., region) in import blocks ### Terraform Version ```shell Terraform v1.12.2 on windows_amd64 + provider registry.terraform.io/hashicorp/aws v6.4.0 ``` ### Use Cases Terraform's import blocks are a powerful addition for managing resource imports declaratively. However, they currently lack support for specifying provider configurations—such as region or alias—which significantly limits their usability in multi-region or multi-account setups. This li…",,,,,,Anecdotal,issue,,,,,,,,2025-08-01,github/franklinmike,https://github.com/hashicorp/terraform/issues/37393,repo: hashicorp/terraform | keyword: workaround | state: closed
"Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the 👍 reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/crw,https://github.com/hashicorp/terraform/issues/37393#issuecomment-3145456833,"repo: hashicorp/terraform | issue: Support specifying provider configurations (e.g., region) in import blocks | keyword: workaround"
"Hi @franklinmike, The import process must target a resource within the configuration, which itself must have a provider defined. The exception to that rule is when the import process is being used to create a configuration, in which case the `import` block can take a `provider` argument that it will map into the generated configuration. Under your attempted solution, you have already specified that the `aws_s3_bucket.example` resource will use the `aws.us_west_2` provider. If the import process…",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/jbardin,https://github.com/hashicorp/terraform/issues/37393#issuecomment-3145692631,"repo: hashicorp/terraform | issue: Support specifying provider configurations (e.g., region) in import blocks | keyword: workaround"
"Hi @jbardin Let me see if I can provide more clarity. I'm using Terraform AWS Provider v6 and deploying resources across multiple regions using modules and for_each. I do not configure provider aliases. Instead, I pass region = var.region to each resource, and Terraform correctly provisions them in the intended regions. However, when I attempt to import resources using the new import block, Terraform defaults to the root provider, which is configured for a single region. Since I'm not using ali…",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/franklinmike,https://github.com/hashicorp/terraform/issues/37393#issuecomment-3145781513,"repo: hashicorp/terraform | issue: Support specifying provider configurations (e.g., region) in import blocks | keyword: workaround"
"Thanks @franklinmike, I think I see where the confusion is. There is no other ""default"" provider to be used, it is the same provider regardless. Also, a ""region"" attribute has no meaning to Terraform, so it's not something we would add to the `import` block directly. The provider must be able to locate a resource instance only by the values given in the `ImportResourceState` RPC. If more information is required to locate a specific resource, then the provider must either accept that information…",,,,,,Anecdotal,comment,,,,,,,,2025-08-02,github/jbardin,https://github.com/hashicorp/terraform/issues/37393#issuecomment-3146780016,"repo: hashicorp/terraform | issue: Support specifying provider configurations (e.g., region) in import blocks | keyword: workaround"
"@franklinmike As of Terraform AWS Provider **v6.0.0** the `region` value _can_ be specified as part of the traditional import ID string by appending `@<region>`: ``` import { to = aws_s3_bucket.example[""west""] id = ""my-west-bucket@us-west-2"" } ``` This is [poorly documented](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/guides/enhanced-region-support#how-region-works) right now. I've added an [issue](https://github.com/hashicorp/terraform-provider-aws/issues/43733) to explic…",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/ewbankkit,https://github.com/hashicorp/terraform/issues/37393#issuecomment-3160411482,"repo: hashicorp/terraform | issue: Support specifying provider configurations (e.g., region) in import blocks | keyword: workaround"
"Route53 Resolver Endpoint Hash Function Prevents Multiple IPs per Subnet When IP Address is Auto-Assigned ### Terraform Version ```shell Terraform v1.9.8 on darwin_arm64 + provider registry.terraform.io/hashicorp/aws v6.7.0 Tested across multiple versions: - AWS Provider v5.0.0 through v6.7.0 (all affected) - Terraform v1.1.9 through v1.9.8 (all affected) ``` ### Terraform Configuration Files ```terraform terraform { required_providers { aws = { source = ""hashicorp/aws"" version = ""~> 5.0"" } } }…",,,,,,Anecdotal,issue,,,,,,,,2025-08-05,github/ffreitas-te,https://github.com/hashicorp/terraform/issues/37404,repo: hashicorp/terraform | keyword: workaround | state: closed
"Hello @ffreitas-te, this appears to be an issue or question with the AWS provider, not with Terraform itself. You can see existing issues and file a new one in their repository here: https://github.com/terraform-providers/terraform-provider-aws/issues. If you have questions about Terraform or the AWS provider, it's better to use the [community forum](https://discuss.hashicorp.com/c/terraform-providers/tf-aws/33) where there are more people ready to help. The GitHub issues here are monitored onl…",,,,,,Anecdotal,comment,,,,,,,,2025-08-05,github/crw,https://github.com/hashicorp/terraform/issues/37404#issuecomment-3156592738,repo: hashicorp/terraform | issue: Route53 Resolver Endpoint Hash Function Prevents Multiple IPs per Subnet When IP Address is Auto-Assigned | keyword: workaround
"Move `stacksplugin` cache to `.terraform.d` ## Description This PR moves the cached stacksplugin binary location to `.terraform.d` by default, with the option to override this location with the argument `-plugin-cache-dir`. If this location is overriden, then the path provided is stored in `.terraform/.stackspluginpath` of the current working directory for future reference by the subcommand. The thought behind this was to be proactive with providing a workaround in the event any user specific i…",,,,,,Anecdotal,issue,,,,,,,,2025-07-01,github/Maed223,https://github.com/hashicorp/terraform/pull/37283,repo: hashicorp/terraform | keyword: workaround | state: closed
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-08-02,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/37283#issuecomment-3146139862,repo: hashicorp/terraform | issue: Move `stacksplugin` cache to `.terraform.d` | keyword: workaround
"When using resource targeting, don't fetch data sources if not needed for targeted resources ### Terraform Version ```shell 1.12.2 ``` ### Use Cases Faster terraform plans when using resource targeting. If a project has hundreds of `data` sources, it can take a long time to load them. If I'm using `-target` to target a single resource, which doesn't use any of these `data` sources, it needs to wait for all the `data` sources to be loaded. This can make a simple `terraform plan -target=X` go fro…",,,,,,Anecdotal,issue,,,,,,,,2025-07-31,github/jsm,https://github.com/hashicorp/terraform/issues/37388,repo: hashicorp/terraform | keyword: workaround | state: closed
"Thanks for this feature request! If you are viewing this issue and would like to indicate your interest, please use the 👍 reaction on the issue description to upvote this issue. We also welcome additional use case descriptions. Thanks again!",,,,,,Anecdotal,comment,,,,,,,,2025-07-31,github/crw,https://github.com/hashicorp/terraform/issues/37388#issuecomment-3141545139,"repo: hashicorp/terraform | issue: When using resource targeting, don't fetch data sources if not needed for targeted resources | keyword: workaround"
"Hi @jsm, Terraform does not read data sources which are not required by the plan, so I suspect something else is happening here. With the following config: ``` data ""external"" ""example"" { program = [""bash"", ""-c"", ""sleep 10 && echo {}""] } resource ""terraform_data"" ""a"" { } ``` I get the plan output immediately if I only target the `terraform_data` resource, and there is no indication of the `external` data source being evaluated at all. ``` % terraform plan -target terraform_data.a Terraform used…",,,,,,Anecdotal,comment,,,,,,,,2025-07-31,github/jbardin,https://github.com/hashicorp/terraform/issues/37388#issuecomment-3141589647,"repo: hashicorp/terraform | issue: When using resource targeting, don't fetch data sources if not needed for targeted resources | keyword: workaround"
"> When using resource targeting, and building a graph of all dependencies, find all data dependencies. Instead of loading all data sources, only load those determined to be downstream dependencies of targeted resources. Since the proposal spelled out here is exactly what Terraform already does, I'm going to close the issue. If there is a case where data sources are being read which are not dependencies of the target, then that would need to be isolated and addressed outside of a general enhance…",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/jbardin,https://github.com/hashicorp/terraform/issues/37388#issuecomment-3144627886,"repo: hashicorp/terraform | issue: When using resource targeting, don't fetch data sources if not needed for targeted resources | keyword: workaround"
"Crash/Strange behaviour when modifying merged object ### Terraform Version ```shell Terraform v1.12.2 on linux_amd64 + provider registry.terraform.io/hashicorp/azuread v3.3.0 + provider registry.terraform.io/hashicorp/azurerm v3.116.0 ``` ### Terraform Configuration Files ```terraform locals { aks_functions = merge( { function1 = { aks_project = ""proj"", type = ""instance"" } }, { function2 = { aks_project = ""proj"" } }, { function3 = { aks_project = ""proj"", type = ""regional"" } }, { function4 = { u…",,,,,,Anecdotal,issue,,,,,,,,2025-06-27,github/danlumb01,https://github.com/hashicorp/terraform/issues/37279,repo: hashicorp/terraform | keyword: workaround | state: closed
"Hi @danlumb01, thanks for filing this. When I try to replicate this, I do see the error you mentioned on my first plan / apply operation. Is it possible that you created the initial `azurerm_user_assigned_identity` before adding the `terraform_data` resource? It makes sense to me that the `for_each` value does contain unknowns because it holds the values for the computed `id` attributes that will be unknown before those resources are created. I don't have an explanation as to why it might have …",,,,,,Anecdotal,comment,,,,,,,,2025-06-27,github/liamcervante,https://github.com/hashicorp/terraform/issues/37279#issuecomment-3013113413,repo: hashicorp/terraform | issue: Crash/Strange behaviour when modifying merged object | keyword: workaround
"I think you can refactor things to make this work without the problem though - you just want to move computation of what is or isn't enabled to an earlier point. ``` locals { aks_functions = merge( { function1 = { aks_project = ""proj"", type = ""instance"" } }, { function2 = { aks_project = ""proj"" } }, { function3 = { aks_project = ""proj"", type = ""regional"", enable_mi = false } }, { function4 = { use_org_name = true, aks_project = ""proj"", type = ""instance"", enable_mi = true, mi_access_types = [""sq…",,,,,,Anecdotal,comment,,,,,,,,2025-06-27,github/liamcervante,https://github.com/hashicorp/terraform/issues/37279#issuecomment-3013122778,repo: hashicorp/terraform | issue: Crash/Strange behaviour when modifying merged object | keyword: workaround
"I have closed this issue, but if you verify that the `terraform_data` resource definitely existed, and referenced unknown values in the for each, when the resources were originally recreated I can reopen and investigate further. Thanks!",,,,,,Anecdotal,comment,,,,,,,,2025-06-27,github/liamcervante,https://github.com/hashicorp/terraform/issues/37279#issuecomment-3013125953,repo: hashicorp/terraform | issue: Crash/Strange behaviour when modifying merged object | keyword: workaround
"Hi @liamcervante , thanks for the response, I appreciate it. That refactor makes sense, thanks for the suggestion. The behaviour you've described also makes sense as well, we just weren't originally seeing the error that was provided by the `terraform_data` resource in our real infrastructure... Obviously we're not using `terraform_data` in our actual code - we are in fact creating variables in Azure DevOps with the `azdo` provider. For example: ```terraform locals { # Managed identity client I…",,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/danlumb01,https://github.com/hashicorp/terraform/issues/37279#issuecomment-3018475312,repo: hashicorp/terraform | issue: Crash/Strange behaviour when modifying merged object | keyword: workaround
"The core of the problem is the unknown values going into any `for_each`. You should try and avoid that at all costs. In your original case, what was probably happening is that since the `for_each` in the `dynamic ""variable""` block was evaluating to unknown (or contained unknown values), the plan was saying ""okay, I don't know what's going to be in these variables, so I'll mark all that currently exist as being deleted and the whole thing replaced by a (known after apply) list of new variables"".…",,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/liamcervante,https://github.com/hashicorp/terraform/issues/37279#issuecomment-3018549702,repo: hashicorp/terraform | issue: Crash/Strange behaviour when modifying merged object | keyword: workaround
"Thanks @liamcervante, I have refactored so I'm not directly iterating over anything that has unknown values in it. I think potentially a slight amendment to the error messaging in this case may be helpful? As I understand it, in this case my values were unknown until apply time, but my keys were actually fully known since they were just built from a string and a variable. Perhaps I'm misunderstanding but the error I referred to above does specifically mention keys - `The ""for_each"" map includes…",,,,,,Anecdotal,comment,,,,,,,,2025-07-01,github/danlumb01,https://github.com/hashicorp/terraform/issues/37279#issuecomment-3023328329,repo: hashicorp/terraform | issue: Crash/Strange behaviour when modifying merged object | keyword: workaround
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/37279#issuecomment-3141974509,repo: hashicorp/terraform | issue: Crash/Strange behaviour when modifying merged object | keyword: workaround
"Provider registry.terraform.io/hashicorp/aws v6.1.0 does not have a package available for your current platform, darwin_arm64. ### Terraform Version ```shell terraform -version Terraform v1.12.2 on darwin_arm64 ``` ### Terraform Configuration Files n/a ### Debug Output n/a ### Expected Behavior terraform plan worked an hour ago. ### Actual Behavior ╷ │ Error: Incompatible provider version │ │ Provider registry.terraform.io/hashicorp/aws v6.1.0 does not have a package available for your current …",,,,,,Anecdotal,issue,,,,,,,,2025-06-30,github/prowlaiii,https://github.com/hashicorp/terraform/issues/37282,repo: hashicorp/terraform | keyword: workaround | state: closed
"Workaround: I pinned the provider to 6.0.0 and it ran ok, but I have a lot of stacks and don't want to do that.. ``` terraform { required_providers { aws = { source = ""hashicorp/aws"" version = ""= 6.0.0"" } } } ```",,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/prowlaiii,https://github.com/hashicorp/terraform/issues/37282#issuecomment-3019297648,"repo: hashicorp/terraform | issue: Provider registry.terraform.io/hashicorp/aws v6.1.0 does not have a package available for your current platform, darwin_arm64. | keyword: workaround"
"Ah - it looks like it has been reported, https://github.com/hashicorp/terraform-provider-aws/issues/43213 I did a search...",,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/prowlaiii,https://github.com/hashicorp/terraform/issues/37282#issuecomment-3019333301,"repo: hashicorp/terraform | issue: Provider registry.terraform.io/hashicorp/aws v6.1.0 does not have a package available for your current platform, darwin_arm64. | keyword: workaround"
"Hello, This appears to be an issue or question with the AWS provider, not with Terraform itself. You can see existing issues and file a new one in their repository here: https://github.com/hashicorp/terraform-provider-aws/issues. If you have questions about Terraform or the AWS provider, it's better to use [the community forum](https://discuss.hashicorp.com/c/terraform-providers/tf-aws/33) where there are more people ready to help. The GitHub issues here are monitored only by a few core maintai…",,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/jbardin,https://github.com/hashicorp/terraform/issues/37282#issuecomment-3019405864,"repo: hashicorp/terraform | issue: Provider registry.terraform.io/hashicorp/aws v6.1.0 does not have a package available for your current platform, darwin_arm64. | keyword: workaround"
"Just FYI there is an internal incident open for this issue, I would recommend following on the issue suggested above: https://github.com/hashicorp/terraform-provider-aws/issues/43213",,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/crw,https://github.com/hashicorp/terraform/issues/37282#issuecomment-3020259899,"repo: hashicorp/terraform | issue: Provider registry.terraform.io/hashicorp/aws v6.1.0 does not have a package available for your current platform, darwin_arm64. | keyword: workaround"
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/37282#issuecomment-3141974438,"repo: hashicorp/terraform | issue: Provider registry.terraform.io/hashicorp/aws v6.1.0 does not have a package available for your current platform, darwin_arm64. | keyword: workaround"
hexdecode function addition Added functionality for hex decode. Im a bit of a novice so please let me know what ive missed Please see my related issue here: https://github.com/hashicorp/terraform/issues/26163 _____________________________________________________________________ Update: i have changed the output to return a base64 encoded representation rather than the raw binary as per @apparentlymart comment on my issue.,,,,,,Anecdotal,issue,,,,,,,,2020-09-11,github/akingscote,https://github.com/hashicorp/terraform/pull/26214,repo: hashicorp/terraform | keyword: workaround | state: closed
"[![CLA assistant check](https://cla.hashicorp.com/pull/badge/not_signed)](https://cla.hashicorp.com/hashicorp/terraform?pullRequest=26214) Thank you for your submission! We require that all contributors sign our Contributor License Agreement (""CLA"") before we can accept the contribution. [Read and sign the agreement](https://cla.hashicorp.com/hashicorp/terraform?pullRequest=26214) [Learn more about why HashiCorp requires a CLA and what the CLA includes](https://www.hashicorp.com/cla) --- **ashl…",,,,,,Anecdotal,comment,,,,,,,,2020-09-11,github/hashicorp-cla,https://github.com/hashicorp/terraform/pull/26214#issuecomment-690913473,repo: hashicorp/terraform | issue: hexdecode function addition | keyword: workaround
# [Codecov](https://codecov.io/gh/hashicorp/terraform/pull/26214?src=pr&el=h1) Report > Merging [#26214](https://codecov.io/gh/hashicorp/terraform/pull/26214?src=pr&el=desc) into [master](https://codecov.io/gh/hashicorp/terraform/commit/0babd493aee4580d428056530629f4c5c22ac2ed?el=desc) will **increase** coverage by `0.00%`. > The diff coverage is `77.77%`. | [Impacted Files](https://codecov.io/gh/hashicorp/terraform/pull/26214?src=pr&el=tree) | Coverage Δ | | |---|---|---| | [lang/funcs/encodin…,,,,,,Anecdotal,comment,,,,,,,,2020-09-11,github/codecov[bot],https://github.com/hashicorp/terraform/pull/26214#issuecomment-690918204,repo: hashicorp/terraform | issue: hexdecode function addition | keyword: workaround
"## Changelog Warning Currently this PR would target a v1.13 release. Please add a changelog entry for in the .changes/v1.13 folder, or discuss which release you'd like to target with your reviewer. If you believe this change does not need a changelog entry, please add the 'no-changelog-needed' label.",,,,,,Anecdotal,comment,,,,,,,,2025-05-19,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/26214#issuecomment-2891276155,repo: hashicorp/terraform | issue: hexdecode function addition | keyword: workaround
"Thank you for your continued interest in this issue. Given the age of this pull request, I am going to close it. This seems to be the type of function that the team developed provider-defined functions to solve (and the azure provider did add the relevant functionality for the original use case.) I will provide further information on provider-defined functions below to aid in future implementation efforts. Please see the [provider-defined functions documentation](https://developer.hashicorp.com…",,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/crw,https://github.com/hashicorp/terraform/pull/26214#issuecomment-3020228642,repo: hashicorp/terraform | issue: hexdecode function addition | keyword: workaround
"@crw yeah, it was just a shout of disappointment when I realized there is absolutely no way to generate MySQL style hash (`SHA1(hex2bin(SHA1(password)))`), my usecase is to store it in k8s secret for usage with clickhouse-operator. 1. There is some crypto functions and even `sha1()` but it returns hex string. `random_password` could return a number of different hashes too, but it's not. 2. I tried to transform hex string into byte string, but a. no `ord()` or `printf '%c' analogs in `format` to…",,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/baznikin,https://github.com/hashicorp/terraform/pull/26214#issuecomment-3020726807,repo: hashicorp/terraform | issue: hexdecode function addition | keyword: workaround
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-07-31,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/26214#issuecomment-3138370634,repo: hashicorp/terraform | issue: hexdecode function addition | keyword: workaround
"Can not move resource ### Terraform Version ```shell $ terraform --version Terraform v1.11.4 on darwin_arm64 ``` ### Terraform Configuration Files Just rename resource from from name into another. I checked terraform.state file and it does not have `""module.private-cloud.aws_s3_bucket.iguides-s3""` resource. Only `dependencies`: ``` ""dependencies"": [ ""data.external.env"", ""module.private-cloud.aws_s3_bucket.iguides-s3"" ] ``` ### Debug Output ``` # module.private-cloud.aws_s3_bucket.iguides-s3 has…",,,,,,Anecdotal,issue,,,,,,,,2025-06-09,github/EugenKon,https://github.com/hashicorp/terraform/issues/37224,repo: hashicorp/terraform | keyword: workaround | state: closed
"Hi @EugenKon, State refactoring operations can only affect the state, they cannot make API calls because we can't start planning until after the state has been updated to match the configuration. If there is a resource instance at the destination address, terraform won't know whether that represents a real life resource or not, it only has the state to compare with. While `moved` blocks take effect during a Terraform plan, they will still have the same limitation because the state refactoring o…",,,,,,Anecdotal,comment,,,,,,,,2025-06-10,github/jbardin,https://github.com/hashicorp/terraform/issues/37224#issuecomment-2957410011,repo: hashicorp/terraform | issue: Can not move resource | keyword: workaround
"FWIW, I think the `moved` block variation of this would be treated as a no-op (possibly with a warning?) rather than an error. The rule for `moved` blocks was that the validity of such a block must be decided only based on configuration and not on state, so that a shared module containing a `moved` block can be instantiated many times with different states without the module author having to test every possible situation. So `moved` works by moving the resource instances as far as possible alon…",,,,,,Anecdotal,comment,,,,,,,,2025-06-10,github/apparentlymart,https://github.com/hashicorp/terraform/issues/37224#issuecomment-2957429702,repo: hashicorp/terraform | issue: Can not move resource | keyword: workaround
"Yes, in this case the `moved` block would produce a warning like ``` │ Terraform tried to adjust resource instance addresses in the prior state based on change information recorded in the configuration, but some adjustments did │ not succeed due to existing objects already at the intended addresses ``` while planning to destroy the old instance which no longer has configuration",,,,,,Anecdotal,comment,,,,,,,,2025-06-10,github/jbardin,https://github.com/hashicorp/terraform/issues/37224#issuecomment-2959748049,repo: hashicorp/terraform | issue: Can not move resource | keyword: workaround
"Closing since we have a working solution for now. Exceptional situations will still tend to require use of the CLI, and it's not always going to be a single command to resolve every possible discrepancy.",,,,,,Anecdotal,comment,,,,,,,,2025-06-23,github/jbardin,https://github.com/hashicorp/terraform/issues/37224#issuecomment-2997233601,repo: hashicorp/terraform | issue: Can not move resource | keyword: workaround
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-07-24,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/37224#issuecomment-3111750424,repo: hashicorp/terraform | issue: Can not move resource | keyword: workaround
"ignore_changes for set arguments We are using custom headers to increase security in the communication between cloudfront and origin, as recommended by [aws](https://aws.amazon.com/about-aws/whats-new/2016/01/amazon-cloudfront-adds-new-origin-security-features/). The custom header is modified by a process outside of Terraform, so we need to add `ignore_changes` to `custom_headers`. <!--- If you are running into one of these scenarios, we recommend opening an issue in the [Terraform core reposit…",,,,,,Anecdotal,issue,,,,,,,,2020-09-23,github/marlenepereira,https://github.com/hashicorp/terraform/issues/26359,repo: hashicorp/terraform | keyword: workaround | state: closed
"I apologize for the stupid question, but shouldn't you just be doing `ignore_changes = [ origin.custom_header ]`? the `.0` is only for when you're creating multiple instances of that block, which it doesn't seem like you are doing.",,,,,,Anecdotal,comment,,,,,,,,2020-09-28,github/nexxai,https://github.com/hashicorp/terraform/issues/26359#issuecomment-700174814,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
"@nexxai, blocks are always contained within their respective data type, and must be addressed accordingly. This means that a List element would always require and index, and a Set element is not addressable. In older versions of Terraform, individual resources with a count of 1 could be referenced either way, but that behavior was also deprecated to allow for more strict handling of data types.",,,,,,Anecdotal,comment,,,,,,,,2020-10-05,github/jbardin,https://github.com/hashicorp/terraform/issues/26359#issuecomment-703648831,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
"It seems like this particular problem has a root cause which has no clear solution, but that there are also some specific situations which are in principle simpler to handle but still require being able to describe something that isn't describable in the language today. The root problem here is that _by definition_ elements of a set doesn't have any key or index to look them up by. Set elements are defined by their contents, but that isn't helpful in a situation like `ignore_changes` where the …",,,,,,Anecdotal,comment,,,,,,,,2022-03-11,github/apparentlymart,https://github.com/hashicorp/terraform/issues/26359#issuecomment-1064652196,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
"We just ran into the same problem, but for our use case it would be totally sufficient if all origins (just one exists) would be ignored, so we tried ```terraform resource ""aws_cloudfront_distribution"" ""distro"" { // … lifecycle { ignore_changes = [origin,etag,last_modified_time] } // … } ``` ![Untitled 5](https://user-images.githubusercontent.com/110414/157835907-ec8e5772-8ddd-4ec6-aa31-7e7506fc3349.jpg) but we still get changes",,,,,,Anecdotal,comment,,,,,,,,2022-03-11,github/jigfox,https://github.com/hashicorp/terraform/issues/26359#issuecomment-1064911325,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
"`ignore_changes` is for ignoring changes _in the configuration_, not for ignoring changes in the remote system. That particular message is reporting that if you apply this change then the Terraform state will be updated to record these updated values. If this is a situation where you are expecting particular values to change outside of Terraform because your Terraform configuration isn't attempting to manage them, then #28803 is the issue to follow for that.",,,,,,Anecdotal,comment,,,,,,,,2022-03-11,github/apparentlymart,https://github.com/hashicorp/terraform/issues/26359#issuecomment-1065315780,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
"Would it not be possible to allow for expression with an if clause when ignoring changes? It could still be limited to not allow function calls or other problematic stuff. Another solution I can see would be to allow using lifecycle directive inside of all blocks, so you could have it inside the origin block and the reference there would be local.",,,,,,Anecdotal,comment,,,,,,,,2022-05-04,github/xremming,https://github.com/hashicorp/terraform/issues/26359#issuecomment-1117647987,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
"For context, we have the exact same issue with the [ibm_database](https://registry.terraform.io/providers/IBM-Cloud/ibm/latest/docs/resources/database) resource, IBM decided to use a set for dynamic disc allocation, it changes over time, and we cannot ignore it. See group->disk->allocation_mb.",,,,,,Anecdotal,comment,,,,,,,,2022-07-08,github/lra,https://github.com/hashicorp/terraform/issues/26359#issuecomment-1179381913,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
"One more joining the party here, affecting `aws_lb_listener` (and presumably `aws_lb_listener_rule`) When configuring the default_action of an `aws_lb_listener` resource with an action of `forward` to multiple `target_group` objects, like so, it becomes impossible to ignore changes to the target group routing weight because the target groups are defined as a set. ```hcl resource ""aws_lb_listener"" ""test"" { (...) default_action { type = ""forward"" forward { target_group { arn = foo weight = 100 } …",,,,,,Anecdotal,comment,,,,,,,,2022-09-14,github/a-abella,https://github.com/hashicorp/terraform/issues/26359#issuecomment-1247223672,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
"it seems there's a recent change on the said ECS Blue/Green deployments with CodeDeploy. Previously CodeDeploy will detach the inactive target group so we can just ignore `default_action[0].target_group_arn` in terraform. Lately CodeDeploy leaves both target group attached at the end of deployments, so we need to ignore the 2 target group blocks. How do you ignore the entire forward block @a-abella?",,,,,,Anecdotal,comment,,,,,,,,2022-10-14,github/salvianreynaldi,https://github.com/hashicorp/terraform/issues/26359#issuecomment-1278520604,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
"@salvianreynaldi I don't know about whatever resource you're using, but on aws_lb_listener to ignore the forward rule default_action I think we did: ```hcl ignore_changes = [ default_action[0].forward[0] ] ```",,,,,,Anecdotal,comment,,,,,,,,2022-10-14,github/a-abella,https://github.com/hashicorp/terraform/issues/26359#issuecomment-1279201903,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
"Sets do not have individually addressable items but it still makes sense to want to ignore changes to all of them. This is also a pattern appearing in list items, wanting to ignore changes to all of them. A syntax such as the following would solve both ``` ignore_changes = [ list_or_set[*].attribute_to_ignore ] ```",,,,,,Anecdotal,comment,,,,,,,,2023-03-01,github/dtheodor,https://github.com/hashicorp/terraform/issues/26359#issuecomment-1450805026,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
"> Sets do not have individually addressable items but it still makes sense to want to ignore changes to all of them. This is also a pattern appearing in list items, wanting to ignore changes to all of them. A syntax such as the following would solve both > > ``` > ignore_changes = [ > list_or_set[*].attribute_to_ignore > ] > ``` This is exactly what is needed.",,,,,,Anecdotal,comment,,,,,,,,2023-08-18,github/fardarter,https://github.com/hashicorp/terraform/issues/26359#issuecomment-1683557296,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
Currently I had to put a rotated secrets in custom header in AWS Cloudfront distribution origin. Best effort support of `*` in sets would be nice: ```hcl ignore_changes = [ origin[*].custom_header[*].value ] ```,,,,,,Anecdotal,comment,,,,,,,,2023-10-02,github/fdaugan,https://github.com/hashicorp/terraform/issues/26359#issuecomment-1743684339,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
"> It seems like this particular problem has a root cause which has no clear solution, but that there are also some specific situations which are in principle simpler to handle but still require being able to describe something that isn't describable in the language today. > > The root problem here is that _by definition_ elements of a set doesn't have any key or index to look them up by. Set elements are defined by their contents, but that isn't helpful in a situation like `ignore_changes` wher…",,,,,,Anecdotal,comment,,,,,,,,2024-01-08,github/instaclustr-wenbodu,https://github.com/hashicorp/terraform/issues/26359#issuecomment-1880260828,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
"> Sets do not have individually addressable items but it still makes sense to want to ignore changes to all of them. This is also a pattern appearing in list items, wanting to ignore changes to all of them. A syntax such as the following would solve both > > ``` > ignore_changes = [ > list_or_set[*].attribute_to_ignore > ] > ``` This not work with some resources it showing below error ``` A single static variable reference is required: only attribute access and │ indexing with constant keys. No…",,,,,,Anecdotal,comment,,,,,,,,2024-02-13,github/pranit-p,https://github.com/hashicorp/terraform/issues/26359#issuecomment-1940456666,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
Running into this problem with Fastly provider trying to ignore programmatic changes to a Dictionary done outside of Terraform.,,,,,,Anecdotal,comment,,,,,,,,2024-04-17,github/medialab-aaron,https://github.com/hashicorp/terraform/issues/26359#issuecomment-2061314696,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
Same issue when I try to ignore changes on Resource `aws_codebuild_project` and block `file_system_locations` `mount_options`,,,,,,Anecdotal,comment,,,,,,,,2024-04-18,github/adv4000,https://github.com/hashicorp/terraform/issues/26359#issuecomment-2064268450,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
"I have the same problem for azurerm, trying to ignore notification.contact_groups for Resource azurerm_consumption_budget_subscription",,,,,,Anecdotal,comment,,,,,,,,2024-05-31,github/tivoodoo,https://github.com/hashicorp/terraform/issues/26359#issuecomment-2141540203,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
Also have the same issue as reported by @a-abella . I have a blue-green deployment scenario and it's the deployment platform that sets the weights on the ALB listener rule. Would like to only ignore weight changes as he is trying to do...,,,,,,Anecdotal,comment,,,,,,,,2025-01-08,github/nunofernandes,https://github.com/hashicorp/terraform/issues/26359#issuecomment-2578078480,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
"Hello, While the focus of this issue was primarily about a syntax for referring to set object attributes, what it failed to address was the logic required to calculate the desired config after taking into account `ignore_changes` over multiple set elements. That change logic unfortunately is where this proposal starts to fail. Because sets are unordered, deduplicated, and the set elements are only defined by their intrinsic value, there is no universal way to correlate multiple changes of simil…",,,,,,Anecdotal,comment,,,,,,,,2025-06-16,github/jbardin,https://github.com/hashicorp/terraform/issues/26359#issuecomment-2977033194,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
Thank you for the explanation. What is the concrete solution to this issue? Should we advocate for the provider maintainers to stop using a set and replace them with a map?,,,,,,Anecdotal,comment,,,,,,,,2025-06-16,github/lra,https://github.com/hashicorp/terraform/issues/26359#issuecomment-2977111760,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
"@lra, it of course depends on the situation, but in general if a user ever needs to directly reference object attributes within a collection, a set is not the optimal choice. If the provider can use a different data structure, lists and maps both allow for the bidirectional identification of objects which Terraform needs to track changes to those objects.",,,,,,Anecdotal,comment,,,,,,,,2025-06-16,github/jbardin,https://github.com/hashicorp/terraform/issues/26359#issuecomment-2977280728,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-07-17,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/26359#issuecomment-3082230649,repo: hashicorp/terraform | issue: ignore_changes for set arguments | keyword: workaround
"don't validate a resource block when there is a static `count = 0` ### Terraform Version ```shell v1.11.3 ``` ### Terraform Configuration Files ``` locals { cdn_ssl_san = [""""] } resource ""aws_acm_certificate"" ""acm-ssl-cdn"" { count = 0 domain_name = local.cdn_ssl_subject_domain subject_alternative_names = local.cdn_ssl_san validation_method = ""DNS"" # https://docs.aws.amazon.com/acm/latest/userguide/acm-public-certificates.html#request-public-cli key_algorithm = ""EC_prime256v1"" lifecycle { create…",,,,,,Anecdotal,issue,,,,,,,,2025-03-26,github/EugenKon,https://github.com/hashicorp/terraform/issues/36776,repo: hashicorp/terraform | keyword: workaround | state: closed
"Using a static `count=0` like this is akin to attempting to comment out a resource block, though as you've seen Terraform will still require it to be valid, because it's part of the configuration and Terraform doesn't expect to find completely unused configuration. No, you can't use `count` and `for_each` at the same time, the concepts are mutually exclusive. Other than your example here which kind of makes sense, Terraform wouldn't be able to expand the resource into an ordered series of insta…",,,,,,Anecdotal,comment,,,,,,,,2025-03-27,github/jbardin,https://github.com/hashicorp/terraform/issues/36776#issuecomment-2756049317,repo: hashicorp/terraform | issue: don't validate a resource block when there is a static `count = 0` | keyword: workaround
"The real config is a bit more complex. I just simplified it here. The block depends on the configuration and looks like: ``` count = local.someconfig == ""xxxx""? 1 : 0 ``` Where `local.someconfig` depends on `somconfig` at `file.yaml`. Thus I can not just comment out it =(.",,,,,,Anecdotal,comment,,,,,,,,2025-03-27,github/EugenKon,https://github.com/hashicorp/terraform/issues/36776#issuecomment-2758009575,repo: hashicorp/terraform | issue: don't validate a resource block when there is a static `count = 0` | keyword: workaround
"I think that shows why it's not possible to skip in general, which is why I assumed that we could reduce this to `count=0`. Validation happens statically, using unknown values for all inputs to make sure the configuration is as broadly valid as possible given any set of inputs. What this means is that expansion can't be taken into account during validation, and we treat it as an unknown value in order to validate the configuration block with the provider. If `count` were entirely static there m…",,,,,,Anecdotal,comment,,,,,,,,2025-03-27,github/jbardin,https://github.com/hashicorp/terraform/issues/36776#issuecomment-2758154254,repo: hashicorp/terraform | issue: don't validate a resource block when there is a static `count = 0` | keyword: workaround
"I tried different solution here without success. The idea was to move all these configuration files into separate module and disable it. Unfortunately this does not work. I expected that TF just skip TF files at the disabled module, but TF unfortunately still process them. Here is minimized part of configuration: ```hcl locals { enable_cdn = local.cluster.domain.cdn_ssl_domains == """" } module ""cdn"" { count = local.enable_cdn ? 1 : 0 source = ""./modules/cdn"" project_name = local.project_name } m…",,,,,,Anecdotal,comment,,,,,,,,2025-04-02,github/EugenKon,https://github.com/hashicorp/terraform/issues/36776#issuecomment-2773755339,repo: hashicorp/terraform | issue: don't validate a resource block when there is a static `count = 0` | keyword: workaround
"That is still the same fundamental issue, expansion is not handled during validation. Adding a module exposes a slightly different version of the situation though, because modules themselves are not a single entity within Terraform, they are really just a namespace for the resources they contain, so the configuration for the resources exists regardless of whether the module itself is expanded.",,,,,,Anecdotal,comment,,,,,,,,2025-04-02,github/jbardin,https://github.com/hashicorp/terraform/issues/36776#issuecomment-2773797490,repo: hashicorp/terraform | issue: don't validate a resource block when there is a static `count = 0` | keyword: workaround
"Since this is not a request for skipping a hard-coded `count = 0`, and in order to pass validation the config must be entirely valid regardless of what `count` evaluates to, I'm marking this as working as designed.",,,,,,Anecdotal,comment,,,,,,,,2025-05-16,github/jbardin,https://github.com/hashicorp/terraform/issues/36776#issuecomment-2886870980,repo: hashicorp/terraform | issue: don't validate a resource block when there is a static `count = 0` | keyword: workaround
"I am not sure how to make it valid if application does not have that part configured. Eg. `domain_name` is not passed. @jbardin Do you mean to change configuration to look like the next? ``` resource ""aws_acm_certificate"" ""acm-ssl-cdn"" { count = var.domain_name == """"? 0 : 1 domain_name = var.domain_name == """" ? ""example.com"" : var.domain_name ... ``` `example.com` usage in production environment will look very strange.",,,,,,Anecdotal,comment,,,,,,,,2025-05-20,github/EugenKon,https://github.com/hashicorp/terraform/issues/36776#issuecomment-2894457801,repo: hashicorp/terraform | issue: don't validate a resource block when there is a static `count = 0` | keyword: workaround
"> Since this is not a request for skipping a hard-coded `count = 0`, and in order to pass validation the config must be entirely valid regardless of what `count` evaluates to, I'm marking this as working as designed. I must say then that there is an error in the implementation of the design because the validate command it is actually giving different result depending on the count. Here an example of 2 resources, in where the one depends on the other. ``` locals { deploy_test_vm = false } resour…",,,,,,Anecdotal,comment,,,,,,,,2025-06-05,github/CorrenSoft,https://github.com/hashicorp/terraform/issues/36776#issuecomment-2945761391,repo: hashicorp/terraform | issue: don't validate a resource block when there is a static `count = 0` | keyword: workaround
"@EugenKon In case that may be useful to you, i came up with 3 possible solutions of my use case. 1. The one that you mentioned in your message, that is repeating the condition used in the count on each relevant property providing a value. Example: `network_interface_ids = local.deploy_test_vm ? local.test_vm_nic.id : null` 2. Using the try function, only applicable when using ""objects"" that may not exists. Example: `= [try(local.test_vm_nic.id, null)]`. But this basically removing the validatio…",,,,,,Anecdotal,comment,,,,,,,,2025-06-05,github/CorrenSoft,https://github.com/hashicorp/terraform/issues/36776#issuecomment-2945836963,repo: hashicorp/terraform | issue: don't validate a resource block when there is a static `count = 0` | keyword: workaround
> I must say then that there is an error in the implementation of the design I agree with you. These workarounds should not be on user's shoulders. It should be easily fixed on terraform side: `if count is 0 then ignore this section`.,,,,,,Anecdotal,comment,,,,,,,,2025-06-05,github/EugenKon,https://github.com/hashicorp/terraform/issues/36776#issuecomment-2946081794,repo: hashicorp/terraform | issue: don't validate a resource block when there is a static `count = 0` | keyword: workaround
"I have got a similar problem with conditional data and resource blocks and experienced this use-case to be a huge issue a couple of times on different occassions. Supposing a person wants to implement different modes of running deployments: Mode A - runs module X Mode B - runs module Y Yet, at some point one might want to reference either module X or module Y, e.g. the person implemented different service in the middle, but it integrates with something else subsequently. The operator may **cond…",,,,,,Anecdotal,comment,,,,,,,,2025-06-08,github/Matthew0x,https://github.com/hashicorp/terraform/issues/36776#issuecomment-2953947389,repo: hashicorp/terraform | issue: don't validate a resource block when there is a static `count = 0` | keyword: workaround
"The goal of the validation is to ensure the configuration is broadly compatible with _any_ possible input values. Since `var.something` would be unknown during validation, as it could be any value of the correct type, then there is no way to conditionally validate the rest of the resource block. What's being discussed here is more closely related to #21953. I don't think there's anything to add over there, it's been discussed in great depth and there's a lot of background to cover. It constitut…",,,,,,Anecdotal,comment,,,,,,,,2025-06-09,github/jbardin,https://github.com/hashicorp/terraform/issues/36776#issuecomment-2955656768,repo: hashicorp/terraform | issue: don't validate a resource block when there is a static `count = 0` | keyword: workaround
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-07-10,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/36776#issuecomment-3055067575,repo: hashicorp/terraform | issue: don't validate a resource block when there is a static `count = 0` | keyword: workaround
"Make default value typed instead of `null`, in object-variable optional attributes ### Terraform Version ```shell Terraform v1.9.8 on linux_amd64 ``` ### Use Cases I want to easily manipulate an optional attribute with no default value, in my object-variable: ```hcl # main.tf # Define an object-variable, with an optional attribute of type `list(string)` that has no default value variable ""this"" { type = object({ list_attribute = optional(list(string)) # --> will get set to null }) default = {} …",,,,,,Anecdotal,issue,,,,,,,,2025-06-09,github/elouanKeryell-Even,https://github.com/hashicorp/terraform/issues/37223,repo: hashicorp/terraform | keyword: workaround | state: closed
"Hi @elouanKeryell-Even, The [optional attribute](https://developer.hashicorp.com/terraform/language/expressions/type-constraints#optional-object-type-attributes) feature already allows for default values, which you have shown in the second example. Null is meaningful in Terraform configuration, indicating the absence of a value. We cannot make an optional attribute always take the zero value, because there would be no way for a user to indicate the absence of that value, a feature which is used…",,,,,,Anecdotal,comment,,,,,,,,2025-06-09,github/jbardin,https://github.com/hashicorp/terraform/issues/37223#issuecomment-2955688849,"repo: hashicorp/terraform | issue: Make default value typed instead of `null`, in object-variable optional attributes | keyword: workaround"
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-07-10,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/37223#issuecomment-3055067437,"repo: hashicorp/terraform | issue: Make default value typed instead of `null`, in object-variable optional attributes | keyword: workaround"
"Storing sensitive values in state files #309 was the first change in Terraform that I could find that moved to store sensitive values in state files, in this case the `password` value for Amazon RDS. This was a bit of a surprise for me, as previously I've been sharing our state files publicly. I can't do that now, and feel pretty nervous about the idea of storing state files in version control at all (and definitely can't put them on github or anything). If Terraform is going to store secrets, …",,,,,,Anecdotal,issue,,,,,,,,2014-10-28,github/seanherron,https://github.com/hashicorp/terraform/issues/516,repo: hashicorp/terraform | keyword: workaround | state: closed
"See #874. I changed the RDS provider to store an SHA1 hash of the password. That said, I'm not sure I'd agree that it's Terraform's responsibility to protect data in the state file. Things other than passwords can be sensitive: for example if I had a security group restricting SSH access to a particular set of hosts, I wouldn't want the world to know which IP they need to spoof to gain access. The state file can be protected orthogonally: you can not put it on github, you can put it in a privat…",,,,,,Anecdotal,comment,,,,,,,,2015-01-28,github/bitglue,https://github.com/hashicorp/terraform/issues/516#issuecomment-71772765,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"Just want to give my opinion on this topic. I do think Terraform should address this issue. I think it will increase the usefulness and ease of use of Terraform. Some examples from other projects: Ansible has [vaults](http://docs.ansible.com/playbooks_vault.html), and on Travis CI you can [encrypt informaton in the `.travis.yml` file](http://docs.travis-ci.com/user/encryption-keys/).",,,,,,Anecdotal,comment,,,,,,,,2015-03-17,github/dentarg,https://github.com/hashicorp/terraform/issues/516#issuecomment-82393512,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
Ansible vaults is a feature I often want in other devops tools. Protecting these details is not as easy as protecting the state file.. what about using consul or Atlas as a remote/backend store? +1 on this,,,,,,Anecdotal,comment,,,,,,,,2015-03-28,github/ketzacoatl,https://github.com/hashicorp/terraform/issues/516#issuecomment-87169626,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"I just want to point out that, according to official documentation, storing the state file in version control is a best practice: https://www.terraform.io/intro/getting-started/build.html > Terraform also put some state into the terraform.tfstate file by default. This state file is extremely > important; it maps various resource metadata to actual resource IDs so that Terraform knows what > it is managing. This file must be saved and distributed to anyone who might run Terraform. **We > recomme…",,,,,,Anecdotal,comment,,,,,,,,2015-05-28,github/dayer4b,https://github.com/hashicorp/terraform/issues/516#issuecomment-106611574,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
:+1: on this idea -- it would be enough for our case to allow configuration of server-side encryption for S3 buckets. Any thoughts on implementing that?,,,,,,Anecdotal,comment,,,,,,,,2015-06-18,github/hobbeswalsh,https://github.com/hashicorp/terraform/issues/516#issuecomment-113320180,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"At the risk of adding scope to this discussion, I think another way to think of this is that Terraform's current architecture is based on a faulty assumption: Terraform assumes that all provider configuration is sensitive and that all resource configuration _isn't_ sensitive. That is wrong in both directions: - Several resources now take passwords as inputs or produce secret values as outputs. In this issue we see the RDS `password` as one example. The potential Vault provider discussed in #222…",,,,,,Anecdotal,comment,,,,,,,,2015-09-11,github/apparentlymart,https://github.com/hashicorp/terraform/issues/516#issuecomment-139688086,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"Hi, any progress on that? Terraform 0.6.3 still stores raw passwords in the state file. Also, as a related issue, if you do not want to keep passwords in configuration, you can create variable without default value. But, this will force you to pass this variable every time you run `plan`/`apply`, even if you're not going to change resource that has this password. I think, it would be nice to separate sensitive stuff from other attributes, so it will: - be stored as sha1 or smth in state file - …",,,,,,Anecdotal,comment,,,,,,,,2015-09-18,github/little-arhat,https://github.com/hashicorp/terraform/issues/516#issuecomment-141424057,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
A good solution for this would be useful for us as well - we're manually configuring certain things to keep them out of the `tfstate` file in the meantime.,,,,,,Anecdotal,comment,,,,,,,,2015-10-19,github/mwarkentin,https://github.com/hashicorp/terraform/issues/516#issuecomment-149285299,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"So as I slowly cobble together another clean-sheet infra with Terraform I see this problem still exists, and this issue is almost exactly 1 year old. What is the thinking in regards to solving this? the ability to mark specific attributes within a resource as sensitive and storing SHA1 or SHA2 hashes of their values in the state for comparison? I see [this comment](https://github.com/hashicorp/terraform/issues/2221#issuecomment-151827879) on a related ticket, does that mean that using Vault wil…",,,,,,Anecdotal,comment,,,,,,,,2015-10-30,github/ascendantlogic,https://github.com/hashicorp/terraform/issues/516#issuecomment-152594825,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"Moving secrets to vault, and using consul-template or integration with other custom solutions you have for CM certainly helps for a lot of cases, but completely avoiding secrets in TF or ending up in TF state is not always reasonable.",,,,,,Anecdotal,comment,,,,,,,,2015-10-30,github/ketzacoatl,https://github.com/hashicorp/terraform/issues/516#issuecomment-152605787,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"Sure, in this particular case I don't want to manually manage RDS but I don't want the PW in the state in cleartext, regardless of where I keep it. I'm sure this is a somewhat common issue. Maybe an overarching ability to mark arbitrary attributes as sensitive is shooting for the moon but a good start would be anything that is obviously sensitive, such as passwords.",,,,,,Anecdotal,comment,,,,,,,,2015-10-30,github/ascendantlogic,https://github.com/hashicorp/terraform/issues/516#issuecomment-152606522,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"Would it be feasible to open up state handling to plugins? The standard could be to store it in files, like it is currently done. Other options could be Vault, S3, Atlas, etc. That way this issue can be dealt with appropriately based on the use-case.",,,,,,Anecdotal,comment,,,,,,,,2015-11-26,github/jfuechsl,https://github.com/hashicorp/terraform/issues/516#issuecomment-159904387,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"I just got tripped up by this as well, as the docs explicitly tell you to store `.tfstate` files in version control, which is problematic if passwords and other secrets end up in the `.tfstate` files. At the bare minimum, the docs should be updated with a massive warning about this. Beyond that, there seem to be a few options: 1. Offer some way to mark variables as secret and either ensure they never get stored in `.tfstate` files or store them in a hashed form. 2. Encrypt the entire `.tfstate`…",,,,,,Anecdotal,comment,,,,,,,,2015-12-12,github/brikis98,https://github.com/hashicorp/terraform/issues/516#issuecomment-164104937,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"One thing to consider around this is output. When you create a resource with secrets (key pair, access keys, db password, etc.), you likely want to show the secret in question at least once (possibly in the stdout of the first run, as `output` do) Currently output are also stored in plain text in the `.tfstate`, and can be retrieved later with `terraform output`. One possible solution would be a mechanism to only show the secrets once, then not store them at all and not show them again (like AW…",,,,,,Anecdotal,comment,,,,,,,,2016-01-05,github/ejoubaud,https://github.com/hashicorp/terraform/issues/516#issuecomment-168928829,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
To get around this for now in my production RDS I just created the instance with a password of `changeme1234` and then went to the console and manually changed the PW.,,,,,,Anecdotal,comment,,,,,,,,2016-01-19,github/ascendantlogic,https://github.com/hashicorp/terraform/issues/516#issuecomment-172901643,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"+1 I notice also that Redshift is imminently going to be supported on terraform, and the same mistakes are being made all over again: ``` master_password - (Required) Password for the master DB user. Note that this may show up in logs, and it will be stored in the state file ``` ""Applications should not transmit or store passwords in unencrypted form"" Page 77 - ISO27001 : https://books.google.co.uk/books?id=Ur1lviHCd-4C&pg=PA77&dq=no+password+unencrypted+disk+iso27001&hl=en&sa=X&ved=0ahUKEwibuM…",,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/gtmtech,https://github.com/hashicorp/terraform/issues/516#issuecomment-173921491,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"@gtmtech a hash isn't cryptographically safe either because it can be reversed. The right solution here is something that can store the value securely, doing anything else IMO would be a waste of energy.",,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/johnrengelman,https://github.com/hashicorp/terraform/issues/516#issuecomment-173926752,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"@gtmtech, if this is a blocker, can you put in a goof password on first run, and then manually update it, as @ascendantlogic notes above? While not ""clean"", and while it ""gives you something to do"", that seems like a reasonable middle ground, no?",,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/ketzacoatl,https://github.com/hashicorp/terraform/issues/516#issuecomment-173932236,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"@johnrengelman forgive me if I am misunderstanding, but I thought hashes were, by definition, one way. Or at least any reasonable use of one to add some level of protection to secrets necessitates the use of a one-way?",,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/ascendantlogic,https://github.com/hashicorp/terraform/issues/516#issuecomment-173933339,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"So problems with the comments above are: - A goof password will mean every time we terraform plan it will advise us it wants to rewire the password - not a great experience when we're always after clean terraform states, but you're right its probably the only workaround we have right now so thanks for the suggestion... - A hash is one-way, and yes if you use SHA1, or even SHA256 you deserve everything you get, but there are other hash functions that are pretty secure - they are one way, and in …",,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/gtmtech,https://github.com/hashicorp/terraform/issues/516#issuecomment-173975311,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"@ascendantlogic ah yeah, sorry, don't know where my brain was this morning. I'm blaming that fact that I hadn't had coffee yet.",,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/johnrengelman,https://github.com/hashicorp/terraform/issues/516#issuecomment-173976634,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"I would recommend bcrypt or even scrypt for the password hashing. In the meantime, a goof password with `ignore_changes=[""master_password""]` will suffice so long as I can get terraform to NOT store it in the state file. A meta_parameter to accomplish not storing a particular atribute would be an alternative to having to do the hashing work - either one could accomplish the end goal",,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/gtmtech,https://github.com/hashicorp/terraform/issues/516#issuecomment-173977805,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
@gtmtech A goof password should not cause terraform states to complain every time. As long as the password in the tf files and the password in the state file are the same terraform should be happy. I created a RDS database using a password in the TF files and after creation went into the TF file and changed it to XXXXX. I also went into the state file and change it to XXXXX. terraform plan and terraform apply are happy.,,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/sstarcher,https://github.com/hashicorp/terraform/issues/516#issuecomment-173977978,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"@sstarcher interesting, what happens when you terraform refresh, does it not override your XXXXXing out of the password?",,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/gtmtech,https://github.com/hashicorp/terraform/issues/516#issuecomment-173978538,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
@gtmtech nothing happens it continues to work happily. The state file still contains XXXXX as terraform has no way of knowing what the actual database password is. I just tested to confirm the following results in no change and my password in the tf state is still XXXXX which is not my actual password - terraform refresh - terraform plan,,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/sstarcher,https://github.com/hashicorp/terraform/issues/516#issuecomment-173979431,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"@gtmtech that is not what happens. I modified the password after the fact in the console and TF is perfectly happy. I think it only wants to change the PW on the RDS resource if the value in the `tf` file doesn't match the `tfstate` file, the AWS API does not return the RDS password for TF to compare against the `tfstate`, that would be madness.",,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/ascendantlogic,https://github.com/hashicorp/terraform/issues/516#issuecomment-173980227,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"Add lifecycle arguments for resource creation and destruction order ### Terraform Version ```shell 1.12.0 ``` ### Use Cases There are times when create and destroy order matters in terraform, while update does not. For example consider following code: ``` resource ""github_repository"" ""this"" { name = “foo” description = “my “repo visibility = ""private"" } resource ""github_repository_file"" ""code_owners"" { repository = github_repository.this.name branch = “masteR” file = "".github/CODEOWNERS"" conten…",,,,,,Anecdotal,issue,,,,,,,,2025-05-29,github/asheynkmantyler,https://github.com/hashicorp/terraform/issues/37183,repo: hashicorp/terraform | keyword: workaround | state: closed
"Hi @asheynkmantyler, The dependency order in Terraform must be static, because it is determined before evaluation can begin. This means resources must be designed such that they work with this pre-determined order of operations. Aside from Terraform needing the dependency order before we can even get to the point of the provider telling us what the plan for the resource is, the dependency changes you are proposing aren't entirely local to two resources in question. Adding/removing/reversing tha…",,,,,,Anecdotal,comment,,,,,,,,2025-05-30,github/jbardin,https://github.com/hashicorp/terraform/issues/37183#issuecomment-2922130105,repo: hashicorp/terraform | issue: Add lifecycle arguments for resource creation and destruction order | keyword: workaround
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/37183#issuecomment-3017566772,repo: hashicorp/terraform | issue: Add lifecycle arguments for resource creation and destruction order | keyword: workaround
"lang/funcs: add template function The `templatefile` function is great way ton render template in a consistent way. But so far, we don't have the ability to render dynamic templates in the same way and are forced to use the template_file data source. Here is our use case https://github.com/terraform-aws-modules/terraform-aws-eks/issues/882#issuecomment-629281750. This `template` (we can recall it `templatestring` if needed) function is more generic and take a template as a string and try to ren…",,,,,,Anecdotal,issue,,,,,,,,2020-05-17,github/barryib,https://github.com/hashicorp/terraform/pull/24978,repo: hashicorp/terraform | keyword: workaround | state: closed
[![CLA assistant check](https://cla.hashicorp.com/pull/badge/signed)](https://cla.hashicorp.com/hashicorp/terraform?pullRequest=24978) <br/>All committers have signed the CLA.,,,,,,Anecdotal,comment,,,,,,,,2020-05-17,github/hashicorp-cla,https://github.com/hashicorp/terraform/pull/24978#issuecomment-629779562,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: workaround
# [Codecov](https://codecov.io/gh/hashicorp/terraform/pull/24978?src=pr&el=h1) Report > Merging [#24978](https://codecov.io/gh/hashicorp/terraform/pull/24978?src=pr&el=desc) (d86c215) into [master](https://codecov.io/gh/hashicorp/terraform/commit/be263151e776e84688f1b0648084a1a6f280d78a?el=desc) (be26315) will **increase** coverage by `0.03%`. > The diff coverage is `88.67%`. | [Impacted Files](https://codecov.io/gh/hashicorp/terraform/pull/24978?src=pr&el=tree) | Coverage Δ | | |---|---|---| |…,,,,,,Anecdotal,comment,,,,,,,,2020-05-17,github/codecov[bot],https://github.com/hashicorp/terraform/pull/24978#issuecomment-629780072,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: workaround
Hi @barryib! Thanks for working on this. Not having _dynamic_ templates was an intentional designed decision for `templatefile` after seeing how much confusion and frustration the design of [the `template_file` data source](https://www.terraform.io/docs/providers/template/d/file.html) had caused: folks would frequently get the double-escaping wrong and then be unsure how to interpret the error messages that would result due to them being returned at the Terraform layer instead of at the templat…,,,,,,Anecdotal,comment,,,,,,,,2020-05-18,github/apparentlymart,https://github.com/hashicorp/terraform/pull/24978#issuecomment-630337484,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: workaround
Hi @apparentlymart Thanks for your answer. > Not having dynamic templates was an intentional designed decision for templatefile after seeing how much confusion and frustration the design of the template_file data source had caused: folks would frequently get the double-escaping wrong and then be unsure how to interpret the error messages that would result due to them being returned at the Terraform layer instead of at the template rendering layer. I'm know getting some explanation about why we …,,,,,,Anecdotal,comment,,,,,,,,2020-05-19,github/barryib,https://github.com/hashicorp/terraform/pull/24978#issuecomment-631062552,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: workaround
"Thanks for sharing those additional details, @barryib. If I'm understanding correctly the issue you had with `template_file`, I think it may be addressed by the change in #24904 which is planned to be in the forthcoming Terraform 0.13. That change will allow data sources to be read during the plan phase as long as there's enough information during plan to fully evaluate their configuration. That requirement (that the arguments be known at plan time) is the same as the constraint on the evaluati…",,,,,,Anecdotal,comment,,,,,,,,2020-05-19,github/apparentlymart,https://github.com/hashicorp/terraform/pull/24978#issuecomment-631080890,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: workaround
"@apparentlymart Indeed https://github.com/hashicorp/terraform/pull/24904 would improve the template_file datasource consistency in Terraform 0.13. Thanks for sharing this information. > With that said, I'd like to wait and see how the situation changes with #24904 merged. If that resolves the problem of Terraform pessimistically marking the template result as unknown during planning then I expect we'd choose to resolve this by changing the recommendation in the template_file docs to make an exc…",,,,,,Anecdotal,comment,,,,,,,,2020-05-20,github/barryib,https://github.com/hashicorp/terraform/pull/24978#issuecomment-631733766,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: workaround
"I have not yet tested #24904 with your module and likely will not have time to in the very immediate term, because that PR belongs to a project that other members of the team are working on and so my development focus is currently elsewhere. However, if you'd like to test yourself in the meantime you could potentially build Terraform yourself from the `master` branch, or if you wait a few weeks then Terraform 0.13.0-beta1 will be released and you could test against that. --- That cycle error su…",,,,,,Anecdotal,comment,,,,,,,,2020-05-20,github/apparentlymart,https://github.com/hashicorp/terraform/pull/24978#issuecomment-631747822,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: workaround
"I'll wait Terraform 0.13.0-beta1 for tests. But it'll be for my personal knowledge, because users are still far from using it in production. > The requirement here is that the `value` expression for `output ""user_data_vars""` should not refer to anything that is derived from `var.worker_groups_launch_template`. Modules themselves are not nodes in the dependency graph -- the individual outputs and variables are -- so a `module` block with an argument referring back to itself is valid as long as t…",,,,,,Anecdotal,comment,,,,,,,,2020-05-20,github/barryib,https://github.com/hashicorp/terraform/pull/24978#issuecomment-631798894,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: workaround
"Hello @apparentlymart, it sounds like the template_file data source is now archived https://github.com/hashicorp/terraform-provider-template and superseded by the templatefile function. With that we loose the ability dynamically generate templates (because the template file must be present before terraform runs) Shouldn't we reconsider this PR ?",,,,,,Anecdotal,comment,,,,,,,,2020-09-22,github/barryib,https://github.com/hashicorp/terraform/pull/24978#issuecomment-696545034,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: workaround
"@barryib Since this likely pinged people on-thread, I'll share some information about what's going on with my assignment here for some clarity :) The Terraform team recently took some time to evaluate some popular PRs, and this was one of them. What I'll be doing is evaluating whether or not to recommend this function addition move forward, before (or if at all) recommending updating this pull request to not-conflict with the latest Terraform codebase. There's no timeline attached to this, but …",,,,,,Anecdotal,comment,,,,,,,,2020-11-04,github/pselle,https://github.com/hashicorp/terraform/pull/24978#issuecomment-721867351,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: workaround
Thank you so much @pselle for sharing this information. Really appreciated. I'll continue to update this PR and resolve conflict until a decision is made. Thanks again for your answer. PS: All my pings were about to get more information on the current decisions for this PR. My principal concern was to know if I should continue to work on this and keep updating this PR. And you answered that.,,,,,,Anecdotal,comment,,,,,,,,2020-11-04,github/barryib,https://github.com/hashicorp/terraform/pull/24978#issuecomment-721886085,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: workaround
Just as an additional input for the decision making: Looks like template provider has issues with the brand new terraform 0.14. I've got this issue while passing sensitive variable to template_file: https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/issues/761 Provider is archived so I am not even sure if it is going to be resolved (or even should be resolved).,,,,,,Anecdotal,comment,,,,,,,,2020-12-08,github/nick4fake,https://github.com/hashicorp/terraform/pull/24978#issuecomment-741151774,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: workaround
"Hi, will this PR go somewhere ? Since the template datasource depreciate, it'll be nice to continue to support dynamic templating with this template function.",,,,,,Anecdotal,comment,,,,,,,,2021-05-04,github/barryib,https://github.com/hashicorp/terraform/pull/24978#issuecomment-832197096,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: workaround
"@barryib thank you for contributing this! @pselle, I appreciate you taking to push this PR. Has there been an update from the terraform core team?",,,,,,Anecdotal,comment,,,,,,,,2021-05-12,github/nitrocode,https://github.com/hashicorp/terraform/pull/24978#issuecomment-839880646,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: workaround
"## Changelog Warning Currently this PR would target a v1.13 release. Please add a changelog entry for in the .changes/v1.13 folder, or discuss which release you'd like to target with your reviewer. If you believe this change does not need a changelog entry, please add the 'no-changelog-needed' label.",,,,,,Anecdotal,comment,,,,,,,,2025-05-19,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/24978#issuecomment-2891219668,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: workaround
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-06-27,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/24978#issuecomment-3011151329,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: workaround
"Optional keys in object variables are automatically null during evaluation instead of being errors when left undefined ### Terraform Version ```shell Terraform v1.10.3 on darwin_arm64 ``` ### Terraform Configuration Files ```terraform variable ""enable_auto_scaling"" { default = null type = bool } variable ""node_pool_node_count"" { default = null type = number } variable ""worker_nodepool_names"" { default = [ ""greenz1"", ""greenz2"", ""bluez1"", ""bluez2"" ] nullable = false type = set(string) } variable …",,,,,,Anecdotal,issue,,,,,,,,2025-05-16,github/tspearconquest,https://github.com/hashicorp/terraform/issues/37069,repo: hashicorp/terraform | keyword: workaround | state: closed
"To add to this, when I convert everything to locals, the code functions properly. This leads me to conclude that the use of optional keys in objects are responsible for the incorrect behavior.",,,,,,Anecdotal,comment,,,,,,,,2025-05-16,github/tspearconquest,https://github.com/hashicorp/terraform/issues/37069#issuecomment-2885414763,repo: hashicorp/terraform | issue: Optional keys in object variables are automatically null during evaluation instead of being errors when left undefined | keyword: workaround
"Upon some further testing, I was able to get `try()` to throw an error. If I specify a key that is not defined in the object variable's definition within the `try()` function, then it gives me: ``` │ Error: Error in function call │ │ on locals.tf line 52, in locals: │ 52: node_count = !coalesce(local.enable_auto_scaling[key], false) ? coalesce(try(local.worker_nodepool_config_overrides[key].node_pool_node_count), var.node_pool_node_count) : null │ ├──────────────── │ │ local.worker_nodepool_con…",,,,,,Anecdotal,comment,,,,,,,,2025-05-16,github/tspearconquest,https://github.com/hashicorp/terraform/issues/37069#issuecomment-2885542425,repo: hashicorp/terraform | issue: Optional keys in object variables are automatically null during evaluation instead of being errors when left undefined | keyword: workaround
"Hi @tspearconquest, thanks for filing this. Can you explain a bit more why updating the `enable_auto_scaling` local with the `coalesce` function didn't work for you? I can make the rest of the configuration work if I update it to the following: ``` enable_auto_scaling = { for key in var.worker_nodepool_names : key => coalesce(try(var.worker_nodepool_config_overrides[key].enable_auto_scaling, null), var.enable_auto_scaling, false) } ``` I then see the local variables like this: ``` Changes to Ou…",,,,,,Anecdotal,comment,,,,,,,,2025-05-16,github/liamcervante,https://github.com/hashicorp/terraform/issues/37069#issuecomment-2885920135,repo: hashicorp/terraform | issue: Optional keys in object variables are automatically null during evaluation instead of being errors when left undefined | keyword: workaround
"Hi, It's not that this doesn't _work_; it gives the correct _output_ but the _behavior_ is still incorrect when taking real configuration into account. Update the code as follows: ``` enable_auto_scaling = { for key in var.worker_nodepool_names : key => try(var.worker_nodepool_config_overrides[key].enable_auto_scaling) } #worker_nodepool_configs = { # for key in var.worker_nodepool_names : key => { # node_count = !coalesce(local.enable_auto_scaling[key], false) ? coalesce(try(var.worker_nodepoo…",,,,,,Anecdotal,comment,,,,,,,,2025-05-16,github/tspearconquest,https://github.com/hashicorp/terraform/issues/37069#issuecomment-2886375683,repo: hashicorp/terraform | issue: Optional keys in object variables are automatically null during evaluation instead of being errors when left undefined | keyword: workaround
"Hi @tspearconquest, thanks for the update. In this case then Terraform is working as designed. Optional values within input variables are set to null if no values are provided for them. It is not invalid or an error to then retrieve those null values, so the `try` function will simply return them.",,,,,,Anecdotal,comment,,,,,,,,2025-05-16,github/liamcervante,https://github.com/hashicorp/terraform/issues/37069#issuecomment-2886383684,repo: hashicorp/terraform | issue: Optional keys in object variables are automatically null during evaluation instead of being errors when left undefined | keyword: workaround
"I was just typing an addendum when your comment came in. > Furthermore, if you update the code as follows, you can never get the `local.enable_auto_scaling` to evaluate the default value of `var.enable_auto_scaling`: > > ``` > variable ""enable_auto_scaling"" { > default = true > type = bool > } > > ... > > enable_auto_scaling = { > for key in var.worker_nodepool_names : key => try(var.worker_nodepool_config_overrides[key].enable_auto_scaling, var.enable_auto_scaling) > } > ``` It's unfortunate t…",,,,,,Anecdotal,comment,,,,,,,,2025-05-16,github/tspearconquest,https://github.com/hashicorp/terraform/issues/37069#issuecomment-2886405307,repo: hashicorp/terraform | issue: Optional keys in object variables are automatically null during evaluation instead of being errors when left undefined | keyword: workaround
"Hi @tspearconquest, optional attributes are just syntactic sugar - there is no such thing as an undefined value within Terraform. Not setting an optional attribute and explicitly setting it to null is the same thing from Terraform's perspective. The same thing is true for providers - explicitly setting `null` for an optional attribute in a resource is the same as not setting that attribute. The following resources are functionally equivalent by the time the data is eventually passed to the prov…",,,,,,Anecdotal,comment,,,,,,,,2025-05-16,github/liamcervante,https://github.com/hashicorp/terraform/issues/37069#issuecomment-2886433723,repo: hashicorp/terraform | issue: Optional keys in object variables are automatically null during evaluation instead of being errors when left undefined | keyword: workaround
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-06-16,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/37069#issuecomment-2974926773,repo: hashicorp/terraform | issue: Optional keys in object variables are automatically null during evaluation instead of being errors when left undefined | keyword: workaround
"Can't push terraform state to Hetzer object storage ### Terraform Version ```shell v1.6.0, v.1.7.0, v1.11.4 ``` ### Terraform Configuration Files ```terraform ...terraform config... ``` terraform { backend ""s3"" { bucket = ""my-bucket"" key = ""terraform.tfstate"" region = ""eu-central"" endpoint = ""https://nbg1.your-objectstorage.com"" skip_credentials_validation = true skip_region_validation = true skip_metadata_api_check = true force_path_style = true } } ### Debug Output │ Error: Retrieving AWS acc…",,,,,,Anecdotal,issue,,,,,,,,2025-04-27,github/anhbuiquocit,https://github.com/hashicorp/terraform/issues/36924,repo: hashicorp/terraform | keyword: workaround | state: closed
"Hi @anhbuiquocit, The S3 backend does not guarantee compatibility with clones of the S3 API, and uses only the standard AWS provided SDK for connectivity. Given that this is present since v1.6 perhaps some of the existing issues have information which can help solve you problem? e.g. #33983 or #34053",,,,,,Anecdotal,comment,,,,,,,,2025-04-27,github/jbardin,https://github.com/hashicorp/terraform/issues/36924#issuecomment-2833596878,repo: hashicorp/terraform | issue: Can't push terraform state to Hetzer object storage | keyword: workaround
"Closing for now, since we have existing issues about non-aws compatibility, like the above, https://github.com/hashicorp/terraform/issues/34086 or https://github.com/hashicorp/terraform/issues/36704. [The community forums](https://discuss.hashicorp.com/c/terraform-core) are always a resource for community support as well.",,,,,,Anecdotal,comment,,,,,,,,2025-05-01,github/jbardin,https://github.com/hashicorp/terraform/issues/36924#issuecomment-2844755903,repo: hashicorp/terraform | issue: Can't push terraform state to Hetzer object storage | keyword: workaround
"Thanks @jbardin for your support. I resolved this issue by using terrafrom v1.6.6 with this backend config: terraform { backend ""s3"" { bucket = ""bucket-name"" key = ""terraform.tfstate"" region = ""your-region"" endpoints = { s3 = ""https://your-endpoint"" } skip_credentials_validation = true skip_region_validation = true skip_metadata_api_check = true force_path_style = true skip_s3_checksum = true skip_requesting_account_id = true } } note that: It only work with terraofmr v1.6.6 because I tried a f…",,,,,,Anecdotal,comment,,,,,,,,2025-05-02,github/anhbuiquocit,https://github.com/hashicorp/terraform/issues/36924#issuecomment-2846879537,repo: hashicorp/terraform | issue: Can't push terraform state to Hetzer object storage | keyword: workaround
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-06-02,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/36924#issuecomment-2928459439,repo: hashicorp/terraform | issue: Can't push terraform state to Hetzer object storage | keyword: workaround
feature request: inverse targeting / exclude Is there anything that can be done such that db_instance - RDS formed by the terraform files can be saved if we destroy the whole state?,,,,,,Anecdotal,issue,,,,,,,,2015-06-06,github/shubhambhartiya,https://github.com/hashicorp/terraform/issues/2253,repo: hashicorp/terraform | keyword: gotcha | state: open
"Hi @shubhambhartiya - we have [`prevent_destroy`](https://terraform.io/docs/configuration/resources.html#prevent_destroy) which provides protection against accidental destruction, but it sounds like perhaps you're asking about ""destroy everything but this"" feature. Can you elaborate on the behavior you're looking for?",,,,,,Anecdotal,comment,,,,,,,,2015-06-07,github/phinze,https://github.com/hashicorp/terraform/issues/2253#issuecomment-109814512,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
"Considering an example. I have a set of tf files which creates vpc, subnets, ASG, sg, instances in various subnets, nat instances and databases (RDS). I want to plan in this way such that when I destroy the plan, I want the RDS to be there (VPC and subnets would be needed), rest all the things would get destroy.",,,,,,Anecdotal,comment,,,,,,,,2015-06-08,github/shubhambhartiya,https://github.com/hashicorp/terraform/issues/2253#issuecomment-109959992,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
"Ah okay I get it now. I think I'd call what you're looking for ""inverse targeting"". ``` sh # Destroy everything except aws_db_instance.foo and its dependencies terraform plan -destroy -exclude=aws_db_instance.foo ``` ^^ If that looks like what you're asking for I'll edit the title and we can track that feature request with this thread.",,,,,,Anecdotal,comment,,,,,,,,2015-06-08,github/phinze,https://github.com/hashicorp/terraform/issues/2253#issuecomment-110023949,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
Just to confirm that it would be nice to have a feature of inverted targeting like as follows: `terraform apply -target-exclude aws_ecs_service.ecs_service` Thanks.,,,,,,Anecdotal,comment,,,,,,,,2017-02-13,github/anosulchik,https://github.com/hashicorp/terraform/issues/2253#issuecomment-279507963,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
Another use case: We're importing existing AWS environments. Migrating the DB to the new subnet group is a manual step. It would be nice to provision all the subnet/security/parameter groups before updating the instance (all part of the same module),,,,,,Anecdotal,comment,,,,,,,,2017-05-15,github/beanaroo,https://github.com/hashicorp/terraform/issues/2253#issuecomment-301363941,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
"I'd also love to see this. In the meantime, I'm using a combination of `lifecycle` to ""protect"" certain resources, and targeting like so: ``` terraform plan -destroy $(for r in `terraform state list | fgrep -v resource.address.to.exclude` ; do printf ""-target ${r} ""; done) -out destroy.plan ``` Not pretty, but it does the job 🙂",,,,,,Anecdotal,comment,,,,,,,,2017-07-28,github/cmacrae,https://github.com/hashicorp/terraform/issues/2253#issuecomment-318665739,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
"Following what @anosulchik posted: > Just to confirm that it would be nice to have a feature of inverted targeting like as follows:`terraform apply -target-exclude aws_ecs_service.ecs_service.` A target and something like a -target-exclude would be great to support regexp or by name-matching similar to consul, such as: `terraform apply -target-exclude aws_ecs_service.` would match all that start with `aws_ecs_service.` or if its regexp it can be more explicit which would be ideal thanks",,,,,,Anecdotal,comment,,,,,,,,2017-08-16,github/olenm,https://github.com/hashicorp/terraform/issues/2253#issuecomment-322833445,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
"`terraform destroy -target-exclude aws_db_instance.my_rds` It would be great to have this feature. so that we can destroy everything except rds instance. It will save a significant amount of time for us if we can just destroy everything except rds resource, as rds takes around 30 minutes to create and timeout during destroy",,,,,,Anecdotal,comment,,,,,,,,2017-09-16,github/ffoysal,https://github.com/hashicorp/terraform/issues/2253#issuecomment-329937307,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
"This would be really useful, so I can destroy everything except the resources marked with `prevent_destroy`. At the moment, because of `prevent_destroy`, I comment out everything except that code and run apply instead of destroy. Very unintuitive.",,,,,,Anecdotal,comment,,,,,,,,2017-10-09,github/ColOfAbRiX,https://github.com/hashicorp/terraform/issues/2253#issuecomment-335145477,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
"+1 :) My current workaround is to taket the outout of ""terraform plan list"" , grep out all resource I wanna keep, and then create a list of -target parameters from the rest with a shell script. Another thing that would make it supereasy to destroy everything unless the things you want to keep is to destroy all resources instead of those protected by the ""prevent_destroy"" flag. Actually, in my opinion the behaviout for that flag is not ideal - if I call destroy, I want to destroy the configured …",,,,,,Anecdotal,comment,,,,,,,,2017-10-11,github/henning,https://github.com/hashicorp/terraform/issues/2253#issuecomment-335886155,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
"Still, it would be very useful to have ```terraform apply -exclude``` as sometimes your ECS cluster has changed due to Autoscaling rules and you don't want to change that, but might want to add more resources, etc",,,,,,Anecdotal,comment,,,,,,,,2017-10-26,github/laura-herrera,https://github.com/hashicorp/terraform/issues/2253#issuecomment-339620824,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
"Jumping in to provide a use case I'm trying to meet in which I'm trying to run my tf file but exclude just one resource that calls ansible, which for testing purposes do not want to worry about for the time being. Right now it seems like I have to do several ""-target"" to include what I want, and that takes a very long time considering how many resources I have in my module. So, to re-iterate what someone else suggested, something like: `terraform plan -target-exclude="""" -target-exclude=""""` Idea…",,,,,,Anecdotal,comment,,,,,,,,2017-11-09,github/idjaw,https://github.com/hashicorp/terraform/issues/2253#issuecomment-343204637,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
"Please refrain from commenting on this ticket unless you have something to help complete this ticket. Speaking of which, could anyone review https://github.com/hashicorp/terraform/pull/3366 and let me know what’s remaining?",,,,,,Anecdotal,comment,,,,,,,,2017-11-14,github/josephholsten,https://github.com/hashicorp/terraform/issues/2253#issuecomment-344334665,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
"@josephholsten https://github.com/hashicorp/terraform/pull/3366 looks great! looking forward to this, I will test run your code next week on my env and report back",,,,,,Anecdotal,comment,,,,,,,,2017-12-21,github/olenm,https://github.com/hashicorp/terraform/issues/2253#issuecomment-353230664,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
"@creativeux & others, please refrain from posting +1 comments or other comments that don’t add to the discussion. It increases noise for everyone subscribed to the issue. Use the emoji reactions instead.",,,,,,Anecdotal,comment,,,,,,,,2018-04-26,github/tdmalone,https://github.com/hashicorp/terraform/issues/2253#issuecomment-384810914,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
"I found this thread because I was learning Terraform and thought I needed this capability. But I found a workaround that no longer required me to need `exclude`. Hopefully, it will help others that might be in the same boat as me and help them find their way around Terraform. If I restructured my configurations, I am able to use `data` instead of `resource`, which then allowed me to destroy all of my resources without triggering Terraform to destroy any resource that is managed elsewhere. So fo…",,,,,,Anecdotal,comment,,,,,,,,2018-11-26,github/zhao-li,https://github.com/hashicorp/terraform/issues/2253#issuecomment-441843773,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
"I have an additional scenario to share. I have trouble with the new configuration of a VM. So I manually disconnected a public IP address using Azure Portal and do not want it to be reconnected when I recreate the machine, but at a later stage when I am happy with my tests and can be sure not to create a security hole.",,,,,,Anecdotal,comment,,,,,,,,2019-01-31,github/giuliov,https://github.com/hashicorp/terraform/issues/2253#issuecomment-459326549,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
"Another +1 Prepping for a migration. I am using a shared module between environment declarations. One component of the module writes public DNS records upon ELB creation. I want this to happen for staging/dev but ""not-quite-yet"" for production since prod is currently pointed to the current prod env as an A record. New record post-migration is an ALIAS record root module that's being called so I can't just set the current `A` root domain record to the current value. Sure I could add some conditi…",,,,,,Anecdotal,comment,,,,,,,,2019-04-17,github/emmm-dee,https://github.com/hashicorp/terraform/issues/2253#issuecomment-484257171,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
"Ability to pass providers to modules in for_each ### Use-cases <!--- In order to properly evaluate a feature request, it is necessary to understand the use-cases for it. Please describe below the _end goal_ you are trying to achieve that has led you to request this feature. Please keep this section focused on the problem and not on the suggested solution. We'll get to that in a moment, below! --> I'd like to be able to provision the same set of resources in multiple regions a `for_each` on a mo…",,,,,,Anecdotal,issue,,,,,,,,2020-03-26,github/mightyguava,https://github.com/hashicorp/terraform/issues/24476,repo: hashicorp/terraform | keyword: gotcha | state: open
"Hello! :robot: This issue seems to be covering the same problem or request as #9448, so we're going to close it just to consolidate the discussion over there. Thanks!",,,,,,Anecdotal,comment,,,,,,,,2020-03-27,github/hashibot,https://github.com/hashicorp/terraform/issues/24476#issuecomment-604994867,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"Hey there @mightyguava & @jspiro, I'm going to re-open the issue as I agree that the concerns are not the same. I did rename it for clarity; to distinguish this request from instantiating providers with `for_each`.",,,,,,Anecdotal,comment,,,,,,,,2020-03-30,github/pkolyvas,https://github.com/hashicorp/terraform/issues/24476#issuecomment-606047618,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"@mightyguava I ran into the same abstraction issue with the `azurerm` provider. My goal was to automate multiple azure subscriptions and keep the code DRY as possible. Since I have to use Service Principals for auth with the `azurerm` provider, each subscription requires a separate provider declaration. I have ended up using `terragrunt`'s `generate` function (https://terragrunt.gruntwork.io/docs/reference/config-blocks-and-attributes/#generate) ```bash . ├── dev │ └── terragrunt.hcl ├── module…",,,,,,Anecdotal,comment,,,,,,,,2020-04-25,github/s1mark,https://github.com/hashicorp/terraform/issues/24476#issuecomment-619450972,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"First of all, thanks for the great work adding iteration and depends_on for modules - both are going to be really useful and I wished for them so many times back during 0.11 days when we were building the majority of our config. In addition to each.key, I'd expect to be able to freely use maps with for_each and have each.&lt;property&gt; be a provider. This would require the ability to assign a provider ""instance"" to a local or list/map members. For example: ``` locals { modules_vars = { instan…",,,,,,Anecdotal,comment,,,,,,,,2020-08-25,github/vivanov-dp,https://github.com/hashicorp/terraform/issues/24476#issuecomment-679859179,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"While not strictly the same as #9448 I think they might be solved together. First, like @vivanov-dp said, thanks for adding the `for_each` support for modules. I had been expecting it for a long time. However I had not realized that provider configuration in modules was deprecated. Here is my use case: * I use terraform to manage the list of AWS accounts I have in my organization * When I create a new account (from a variable list), I then want to provision it with a few common standard resourc…",,,,,,Anecdotal,comment,,,,,,,,2020-09-04,github/gbataille,https://github.com/hashicorp/terraform/issues/24476#issuecomment-686968681,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"I personally think that inline provider declaration, which honors the module `for_each` or `count` is the cleanest solution: ``` module ""some_module"" { source = ""./some-module"" for_each = local.modules_elements provider ""provider1"" { ... } provider ""provider2"" { ... } var1 = each.var1 var2 = each.var2 } ``` Ideally, this would support `dynamic` for providers as well. Another option is to add `for_each` for `provider` as well along these lines: ``` provider ""aws"" { for_each = var.regions region …",,,,,,Anecdotal,comment,,,,,,,,2020-09-25,github/nikolay,https://github.com/hashicorp/terraform/issues/24476#issuecomment-698677004,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"@nikolay That can do the job, but why creating new providers for each module invocation ? Even if it is ""for free"" in terms of performance, which I don't really know, there are a bunch of properties to configure the provider and this approach would require to put them all into `local.modules_elements` and list them all in each provider declaration in each module invocation. You can't really declare an AWS provider just by setting the region. It requires a `profile`, or `access_key`&`secret_key`…",,,,,,Anecdotal,comment,,,,,,,,2020-09-25,github/vivanov-dp,https://github.com/hashicorp/terraform/issues/24476#issuecomment-698803143,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"@vivanov-dp This was pseudocode just to illustrate my point, which was that the logic of how the provider should be initialized could be encapsulated in the module. I can't think of a situation where the instantiation of a provider would be an expensive operation. Also, providers with `assume_role` have session information, which may not make sense to be reused across different modules, but will happen due to natural laziness if we have to create too many aliases.",,,,,,Anecdotal,comment,,,,,,,,2020-09-25,github/nikolay,https://github.com/hashicorp/terraform/issues/24476#issuecomment-698806084,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"@nikolay What I understand is that you propose to have this: ``` locals { modules_vars = { instance_1 = { var1 = ... var2 = ... region = ... profile = ... role_arn = ... } instance_2 = { var1 = ... var2 = ... region = ... profile = ... role_arn = ... } } } module ""some_module"" { source = ""./some-module"" for_each = local.modules_vars provider ""aws"" { region = each.region profile = each.profile assume_role { role_arn = each.role_arn } } var1 = each.var1 var2 = each.var2 } ``` instead of: ``` prov…",,,,,,Anecdotal,comment,,,,,,,,2020-09-25,github/vivanov-dp,https://github.com/hashicorp/terraform/issues/24476#issuecomment-698825661,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"A use case for me would be to configure a dynamic provider based on output from a module using `for_each` such as creating multiple kubernetes clusters (`foo`) and optionally applying resources (`bar`) ```hcl module ""foo"" { source = ""./foo"" for_each = var.foo_things var1 = each.key var2 = each.values.something } module ""bar"" { source = ""./bar"" for_each = { for k, v in var.bar_things : k => v if v.add_bar_to_foo == true } provider ""some_provider"" { config1 = module.foo[each.values.foo_thing].out…",,,,,,Anecdotal,comment,,,,,,,,2020-09-25,github/jon-walton,https://github.com/hashicorp/terraform/issues/24476#issuecomment-698831672,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"@vivanov-dp The ideal approach is to have identical code and only data, which varies between environment and clusters within the environment. Right now, almost everything has `for_each`/`count` except providers.",,,,,,Anecdotal,comment,,,,,,,,2020-09-26,github/nikolay,https://github.com/hashicorp/terraform/issues/24476#issuecomment-699375147,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"> @jon-walton Your example is identical to mine, but I illustrated if the module needs more than a single provider. My example illustrates the provider config being supplied to a module is set by the output of another module which also uses `for_each`",,,,,,Anecdotal,comment,,,,,,,,2020-09-26,github/jon-walton,https://github.com/hashicorp/terraform/issues/24476#issuecomment-699443102,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"@nikolay Sure, having `for_each` for providers sounds logical and natural and I fully support it, I believe it deserves its own feature request",,,,,,Anecdotal,comment,,,,,,,,2020-09-26,github/vivanov-dp,https://github.com/hashicorp/terraform/issues/24476#issuecomment-699447307,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"@jon-walton Fair enough, we need dynamic providers - one way or another. Right now providers and outputs are the only two static resources in Terraform.",,,,,,Anecdotal,comment,,,,,,,,2020-09-28,github/nikolay,https://github.com/hashicorp/terraform/issues/24476#issuecomment-700131550,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"Hi all! Thanks for the interesting discussion here. It feels to me that both this issue and #9448 are covering the same underlying use-case, which I would describe as: the ability to dynamically declare and use zero or more provider configurations based on data determined at runtime. These various proposals all have in common a single underlying design constraint: unlike most other concepts in Terraform, provider configurations must be available for operations on resources that belong to them, …",,,,,,Anecdotal,comment,,,,,,,,2020-09-29,github/apparentlymart,https://github.com/hashicorp/terraform/issues/24476#issuecomment-700368878,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"@apparentlymart Having `providerconfig(aws)` is a bit limiting as you can't pass the dynamic index from a TFC variable or `terraform.tfvars.json` file. The easiest and probably quickest to implement it just to allow something like `provider.aws[var.provider_alias]` - you still have static providers, just dynamic references to them.",,,,,,Anecdotal,comment,,,,,,,,2020-09-29,github/nikolay,https://github.com/hashicorp/terraform/issues/24476#issuecomment-700413697,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"I refer to the [blog announcement](https://www.hashicorp.com/blog/terraform-0-13-brings-powerful-meta-arguments-to-modular-workflows) for TF 0.13 with this block of code: ``` variable ""project_id"" { type = string } variable ""regions"" { type = map(object({ region = string network = string subnetwork = string ip_range_pods = string ip_range_services = string })) } module ""kubernetes_cluster"" { source = ""terraform-google-modules/kubernetes-engine/google"" for_each = var.regions project_id = var.pro…",,,,,,Anecdotal,comment,,,,,,,,2020-10-12,github/cregkly,https://github.com/hashicorp/terraform/issues/24476#issuecomment-706802751,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"@cregkly This example is with Google cloud - the provider instance is not constrained within the region with Google, so you don't need multiple provider instances to use different regions - resources have 'region' properties themselves",,,,,,Anecdotal,comment,,,,,,,,2020-10-12,github/vivanov-dp,https://github.com/hashicorp/terraform/issues/24476#issuecomment-706961509,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"> @cregkly This example is with Google cloud - the provider instance is not constrained within the region with Google, so you don't need multiple provider instances to use different regions - resources have 'region' properties themselves And I quote the original post: > I'd like to be able to provision the same set of resources in multiple regions a for_each on a module. However, looping over providers (which are tied to regions) is currently not supported. And then they gave a google cloud exa…",,,,,,Anecdotal,comment,,,,,,,,2020-10-12,github/cregkly,https://github.com/hashicorp/terraform/issues/24476#issuecomment-707344139,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"> @cregkly Yes, but we're talking about providers here, not modules. Ability to pass **providers** to **modules** in for_each",,,,,,Anecdotal,comment,,,,,,,,2020-10-12,github/cregkly,https://github.com/hashicorp/terraform/issues/24476#issuecomment-707344834,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"@apparentlymart Can you guys put a better example up on the blog post about TF 13 then? It uses the example of for_each over regions with google cloud. Naturally it is the first thing I wanted to try out with in AWS, then it turns out it can't be done. At the very least link to the something that explains why this works with Google Cloud and not others like AWS. I appreciate you insights and transparency on the development to version 1.",,,,,,Anecdotal,comment,,,,,,,,2020-10-12,github/cregkly,https://github.com/hashicorp/terraform/issues/24476#issuecomment-707361678,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"I think the person who wrote that blog post was motivated to find an existing registry module with a relatively simple interface so that the module's own complexity wouldn't overwhelm the article with module-specific complexity. The point of it is just to be a generic (but working) example of what the syntax looks like for marketing purposes, not to be documentation. In general I'd suggest thinking of HashiCorp blog posts as being more ""notification that the thing exists"" than ""guide/example on…",,,,,,Anecdotal,comment,,,,,,,,2020-10-12,github/apparentlymart,https://github.com/hashicorp/terraform/issues/24476#issuecomment-707379710,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"I updated the blog post a while ago, but I am waiting for another team to push the changes live. It looks like our blogging platform was updated between the release of 0.13 and today. The replaced example is designed to signal the `for_each` feature without misleading users to believing they can copy paste code and use it as is. I apologize for the delay in getting this remediated. Update: I went back to check and the blog post has been updated.",,,,,,Anecdotal,comment,,,,,,,,2020-10-15,github/pkolyvas,https://github.com/hashicorp/terraform/issues/24476#issuecomment-708816449,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
Our use-case is the multi account setup where we deploy stuff like IAM roles for monitoring permission to all accounts and do have a centrally Grafana that does collect these data. Looks like currently there is no way to handle this without an addon like terragrunt? The following would be an example on how this could be handled if you require the provider to stay on root level. But this also requires to have the `for_each` available on `providers`. ```hcl # A list of AWS accounts that also migh…,,,,,,Anecdotal,comment,,,,,,,,2020-10-15,github/timmjd,https://github.com/hashicorp/terraform/issues/24476#issuecomment-709070083,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"We have the same use case as https://github.com/hashicorp/terraform/issues/24476#issuecomment-709070083 for AWS account bootstrap (has to iterate by each provider) ```terraform module ""account"" { for_each = local.accounts something = each.value.something providers ""aws"" { aws = aws[each.key] } } ```",,,,,,Anecdotal,comment,,,,,,,,2020-10-15,github/rjudin,https://github.com/hashicorp/terraform/issues/24476#issuecomment-709167395,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"file provisioner behaves surprisingly when target folder not present ### Terraform Version 0.10.7 ### Terraform Configuration Files ```hcl resource ""aws_instance"" ""master"" { ... connection { type = ""ssh"" host = ""${aws_instance.master.public_dns}"" agent = false user = ""ubuntu"" private_key = ""${file(var.private_key_file)}"" } provisioner ""file"" { source = ""registry/config.yaml"" destination = ""/home/ubuntu/registry/config.yaml"" } tags = { Name = ""swarm-manager"" } } ``` ### Expected Behavior Either …",,,,,,Anecdotal,issue,,,,,,,,2017-10-12,github/schmidlop,https://github.com/hashicorp/terraform/issues/16330,repo: hashicorp/terraform | keyword: gotcha | state: open
"Even more fun, if you try to create `/home/ubuntu/registry/config.yaml` and `/home/ubuntu/registry/file2.txt` the content of the latter ends up in `/home/ubuntu/registry`. I guess that isn't all that surprising.",,,,,,Anecdotal,comment,,,,,,,,2019-01-04,github/kcd83,https://github.com/hashicorp/terraform/issues/16330#issuecomment-451349850,repo: hashicorp/terraform | issue: file provisioner behaves surprisingly when target folder not present | keyword: gotcha
"wasted a some good hours today going through my whole configuration again and again, believing I did something fundamentally wrong when my config file was being written into the directory name rather than on the full path. I gave up occasionally and decided to search the web and found this issue... I am quite sad and depressed right now and my neck hurts because of the stress, specially because [in the docs](https://www.terraform.io/docs/provisioners/file.html#directory-uploads) it is actually …",,,,,,Anecdotal,comment,,,,,,,,2019-07-02,github/gchamon,https://github.com/hashicorp/terraform/issues/16330#issuecomment-507858885,repo: hashicorp/terraform | issue: file provisioner behaves surprisingly when target folder not present | keyword: gotcha
"I tested what is in the documentation. If I understood correctly, I could just use ``` provisioner ""file"" { source = ""foo/"" destination = ""/home/user/bar"" } ``` with `foo` containing `baz`, that `bar` would be created in the remote and `baz` contents would be uploaded to `/home/user/bar/baz`: > If the source is /foo (no trailing slash), and the destination is /tmp, then the contents of /foo on the local machine will be uploaded to /tmp/foo on the remote machine. The foo directory on the remote …",,,,,,Anecdotal,comment,,,,,,,,2019-07-02,github/gchamon,https://github.com/hashicorp/terraform/issues/16330#issuecomment-507865433,repo: hashicorp/terraform | issue: file provisioner behaves surprisingly when target folder not present | keyword: gotcha
"Run into this while trying to create a `kubeconfig` file on a remote VM and worked around like this: ``` resource ""null_resource"" ""aksvmkubeconfig"" { #https://github.com/hashicorp/terraform/issues/16330 provisioner ""remote-exec"" { connection { host = azurerm_public_ip.aksvm.ip_address type = ""ssh"" user = var.admin_username private_key = tls_private_key.sshkey.private_key_pem } inline = [ ""mkdir /home/ubuntu/.kube/"" ] } provisioner ""file"" { connection { host = azurerm_public_ip.aksvm.ip_address …",,,,,,Anecdotal,comment,,,,,,,,2020-02-23,github/ams0,https://github.com/hashicorp/terraform/issues/16330#issuecomment-590086388,repo: hashicorp/terraform | issue: file provisioner behaves surprisingly when target folder not present | keyword: gotcha
"so instead of safely creating the ~/.ssh dir, and putting a single ssh key's pub file into authorized_keys, the easiest workaround, is to copy my entire ~/.ssh dir? ``` resource ""null_resource"" ""controllerpi"" { connection { type = ""ssh"" user = var.initial_user password = var.initial_password host = ""10.10.10.129"" } provisioner ""file"" { #TODO: this is an aweful workaround to https://github.com/hashicorp/terraform/issues/16330 # source = ""~/.ssh/id_rsa.pub"" # destination = ""/home/pi/.ssh/authoriz…",,,,,,Anecdotal,comment,,,,,,,,2020-04-04,github/SvenDowideit,https://github.com/hashicorp/terraform/issues/16330#issuecomment-608970920,repo: hashicorp/terraform | issue: file provisioner behaves surprisingly when target folder not present | keyword: gotcha
"Hi all! The weird behavior here unfortunately seems to belong to the `scp` program on the target system, rather than Terraform itself. The `file` provisioner is acting as a `scp` client to upload the file to the target system, so exactly what happens on the other end is under the server's control. I produced the problem outside of Terraform by pretending to be a `scp` client right at my terminal: * Run `scp -vt target_dir` to start the scp server. Terraform would normally create an SSH session …",,,,,,Anecdotal,comment,,,,,,,,2022-09-30,github/apparentlymart,https://github.com/hashicorp/terraform/issues/16330#issuecomment-1262963761,repo: hashicorp/terraform | issue: file provisioner behaves surprisingly when target folder not present | keyword: gotcha
"terraform needs a way to obviate color everywhere, globally, at once The color output makes the output illegible to many people. There needs to be a way to turn it off globally. I'm aware of the -no-color option on some commands but that's insufficient. I forget. It's too long. it only works on some commands. I'm also aware of TERM, but I don't want TERM broken. I still use TERM for my curses based apps like vi, emacs, etc. I just want a way to be able to read the output from terraform.",,,,,,Anecdotal,issue,,,,,,,,2019-12-17,github/pixleyr,https://github.com/hashicorp/terraform/issues/23708,repo: hashicorp/terraform | keyword: gotcha | state: open
"Hello @pixleyr! Would using an environment variable work for your use case, and if not, for which use cases does it not? You mentioned that -no-color doesn't work on all commands, so which ones are you still seeing this with? Here's more info on setting the environment variable: https://www.terraform.io/docs/commands/environment-variables.html#tf_cli_args-and-tf_cli_args_name, you would want `TF_CLI_ARGS=""-no-color""`",,,,,,Anecdotal,comment,,,,,,,,2020-01-07,github/pselle,https://github.com/hashicorp/terraform/issues/23708#issuecomment-571676112,"repo: hashicorp/terraform | issue: terraform needs a way to obviate color everywhere, globally, at once | keyword: gotcha"
"You might consider making `-no-color` the default, with -color being the optional flag. The escape characters don't even do anything in my use case except disrupt attempts to pipe the output. (I just lost a lot of time trying to figure out why I couldn't pipe the `terraform show` output to a bash variable; see [here](https://unix.stackexchange.com/questions/564265/how-to-use-eval-to-set-variables-from-a-string-in-linux?noredirect=1#comment1049258_564265).)",,,,,,Anecdotal,comment,,,,,,,,2020-01-27,github/MagnusBrzenk,https://github.com/hashicorp/terraform/issues/23708#issuecomment-578587673,"repo: hashicorp/terraform | issue: terraform needs a way to obviate color everywhere, globally, at once | keyword: gotcha"
"A growing number of applications support the [NO_COLOR](https://no-color.org) environment variable (if present, regardless of value, color output should be disabled) and it'd be nice to add terraform to that list.",,,,,,Anecdotal,comment,,,,,,,,2022-04-22,github/guildencrantz,https://github.com/hashicorp/terraform/issues/23708#issuecomment-1105867945,"repo: hashicorp/terraform | issue: terraform needs a way to obviate color everywhere, globally, at once | keyword: gotcha"
"Is the intent of the `NO_COLOR` convention to disable formatting sequences entirely, or only to disable the color sequences in particular? Terraform uses both the color codes and bold/underline codes, and it isn't clear to me from the description on the website whether the bold/underline codes should still be emitted. It seems like there are two use cases here which overlap but are not the same: * Using Terraform in a terminal which doesn't present color in a way that is legible to its user. * …",,,,,,Anecdotal,comment,,,,,,,,2022-05-24,github/apparentlymart,https://github.com/hashicorp/terraform/issues/23708#issuecomment-1136396802,"repo: hashicorp/terraform | issue: terraform needs a way to obviate color everywhere, globally, at once | keyword: gotcha"
"I would scope this to the visual accessibility issue as originally reported. If NO_COLOR is to be accepted as the requirements (which seems reasonable), it is scoped to ANSI color output.",,,,,,Anecdotal,comment,,,,,,,,2022-05-24,github/crw,https://github.com/hashicorp/terraform/issues/23708#issuecomment-1136487326,"repo: hashicorp/terraform | issue: terraform needs a way to obviate color everywhere, globally, at once | keyword: gotcha"
"Is there any update on this? NO_COLOR is definitely a rising standard, and would make shutting off colors much easier. Currently, it is incredibly hard to read the Terraform outputs in a terminal that does not support ANSI colors.",,,,,,Anecdotal,comment,,,,,,,,2023-02-15,github/kflathers-chwy,https://github.com/hashicorp/terraform/issues/23708#issuecomment-1432015163,"repo: hashicorp/terraform | issue: terraform needs a way to obviate color everywhere, globally, at once | keyword: gotcha"
"I think there's still some ambiguity here about what's best to do. A relatively easy thing to implement would be for us to treat `NO_COLOR` as synonymous with the existing `-no-color` command line option, which (despite the name) actually means to disable _all_ terminal escape sequences, including the ones that activate bold and underline. That would make it serve as a way to specify ""I'm not running in a terminal at all!"" and therefore make the output be plain text, suitable for display in sys…",,,,,,Anecdotal,comment,,,,,,,,2023-02-22,github/apparentlymart,https://github.com/hashicorp/terraform/issues/23708#issuecomment-1439306458,"repo: hashicorp/terraform | issue: terraform needs a way to obviate color everywhere, globally, at once | keyword: gotcha"
"Confirmed that GitHub Actions do not run in a tty: https://github.com/actions/runner/issues/241 After some further research I've learned about a separate convention of using `FORCE_COLOR` to produce color codes even when the output stream isn't a tty, which is apparently how some applications are working around GitHub Actions supporting formatting sequences but not actually running programs in a terminal. Some discussion on that here: https://github.com/chalk/supports-color/issues/106. There's …",,,,,,Anecdotal,comment,,,,,,,,2023-02-22,github/apparentlymart,https://github.com/hashicorp/terraform/issues/23708#issuecomment-1439325175,"repo: hashicorp/terraform | issue: terraform needs a way to obviate color everywhere, globally, at once | keyword: gotcha"
"It would be really nice if the terraform could detect the output mode and change behaviour accordingly. Similar to `ls --color=auto` behaviour when run in the terminal - it produces colour output, when run in a headless mode or with output redirected to a file ```bash ls --color=auto >output.txt ``` It simply outputs only text, with no escape characters or other formatting.",,,,,,Anecdotal,comment,,,,,,,,2025-02-12,github/stumyp,https://github.com/hashicorp/terraform/issues/23708#issuecomment-2654213975,"repo: hashicorp/terraform | issue: terraform needs a way to obviate color everywhere, globally, at once | keyword: gotcha"
"cloudplugin: use terminal.Streams to write command output <!-- Describe in detail the changes you are proposing, and the rationale. See the contributing guide: https://github.com/hashicorp/terraform/blob/main/.github/CONTRIBUTING.md --> Use the command meta's `Streams` object to write output rather than pass stdout/stderr file descriptor between the command and the grpc client. This has two notable benefits: - `terminal.Streams` is the desired interface commands should use to write output to st…",,,,,,Anecdotal,issue,,,,,,,,2023-12-07,github/sebasslash,https://github.com/hashicorp/terraform/pull/34375,repo: hashicorp/terraform | keyword: gotcha | state: open
"All right, so I smoke tested this alongside the version -json plugin PR, and it works! Things I don't quite fully understand yet: - Hey, are there times when we _shouldn't_ be word-wrapping the stdout? like with -json, for example. (Remember that newline literals are forbidden inside a json string value.) - What can we do to get around the ""diags pretty-printed to stdout"" scenario when doing json output?",,,,,,Anecdotal,comment,,,,,,,,2023-12-15,github/nfagerlund,https://github.com/hashicorp/terraform/pull/34375#issuecomment-1857107666,repo: hashicorp/terraform | issue: cloudplugin: use terminal.Streams to write command output | keyword: gotcha
"Agreed that it seems risky to assume that all output from the plugin ought to be naively wrapped by the client. Do you think we could instead send some metadata about stdout (whether it's a terminal at all, and what its width is if so) in the initial request, and then have the plugin do the wrapping itself? (That does mean that the plugin wouldn't be able to respond properly if the terminal gets resized while it's already running, but Terraform's already not responding to terminal resizing in m…",,,,,,Anecdotal,comment,,,,,,,,2024-03-14,github/apparentlymart,https://github.com/hashicorp/terraform/pull/34375#issuecomment-1998607985,repo: hashicorp/terraform | issue: cloudplugin: use terminal.Streams to write command output | keyword: gotcha
"Replace Function Accepts Map of Replacements <!-- Hi there, Thank you for opening an issue. Please note that we try to keep the Terraform issue tracker reserved for bug reports and feature requests. For general usage questions, please see: https://www.terraform.io/community.html. For feature requests concerning Terraform Cloud/Enterprise, please contact tf-cloud@hashicorp.support If your issue relates to a specific Terraform provider, please open it in the provider's own repository. The index o…",,,,,,Anecdotal,issue,,,,,,,,2022-07-04,github/GabrielEisenberg,https://github.com/hashicorp/terraform/issues/31367,repo: hashicorp/terraform | keyword: gotcha | state: open
"Hi @GabrielEisenberg! Thanks for sharing this use-case. I think this proposal raises some interesting questions about order of operations: multiple patterns can potentially overlap one another in ways that would lead to a different outcome depending on what order you process the patterns in and whether you apply each pattern to the entire string in turn or scan the string only once and perform a replacement each time one pattern matches. I'm sure there are other permutations too, such as choosi…",,,,,,Anecdotal,comment,,,,,,,,2022-07-05,github/apparentlymart,https://github.com/hashicorp/terraform/issues/31367#issuecomment-1175147607,repo: hashicorp/terraform | issue: Replace Function Accepts Map of Replacements | keyword: gotcha
"It's a pleasure and thank you for the very interesting feedback @apparentlymart! After speaking to a colleague on this, our thoughts are that one could simply apply the replacements in the logical order that they are specified. In the example above, one would first replace `.py` with `""""`, then `.scala` with `""""` and then `""/""` with `""-""`. Exactly as the chaining does. However, if the order of replacements is done poorly, the onus is on the user and they will need to conform to the way in which…",,,,,,Anecdotal,comment,,,,,,,,2022-07-07,github/GabrielEisenberg,https://github.com/hashicorp/terraform/issues/31367#issuecomment-1177351854,repo: hashicorp/terraform | issue: Replace Function Accepts Map of Replacements | keyword: gotcha
"Hi @GabrielEisenberg! Thanks for the additional context. Since a mapping is not an ordered data type in the Terraform language, the exact syntax you proposed here would not give any information about what order to perform the operations in, but if just behaving as if it were multiple calls to the function feeding each output into the next input then indeed something _like_ that using either a variable number of individual arguments (because arguments are inherently ordered) or a single argument…",,,,,,Anecdotal,comment,,,,,,,,2022-07-07,github/apparentlymart,https://github.com/hashicorp/terraform/issues/31367#issuecomment-1177729845,repo: hashicorp/terraform | issue: Replace Function Accepts Map of Replacements | keyword: gotcha
"## Ordered Replacement Regarding the issue with maps not being ordered; lists are, so a simple fix for the example would be: ``` resource ""resource"" ""resource"" { name = replace(local.script, [ ["""", "".py"", "".scala""] [""-"", ""/""] ] ) } ``` Here the value to be replaced is each item in each list's tail, whilst the head is the replacement value. ## Gotcha Of course, as @apparentlymart describes, this does still hide potential issues that are slightly more obvious when doing multiple `replace` stateme…",,,,,,Anecdotal,comment,,,,,,,,2024-01-26,github/JohnLBevan,https://github.com/hashicorp/terraform/issues/31367#issuecomment-1912544845,repo: hashicorp/terraform | issue: Replace Function Accepts Map of Replacements | keyword: gotcha
"`terraform output` flag `-raw` should behave like `-json` when state file is empty ### Terraform Version ```shell $ terraform version Terraform v1.3.6 on linux_amd64 ``` ### Terraform Configuration Files No configuration file needed. ### Debug Output Using the `-raw` flag prints a warning to stdout and exit with code 0: ```shell $ terraform output -raw my_output Warning: No outputs found The state file either has no outputs defined, or all the defined outputs are empty. Please define an output …",,,,,,Anecdotal,issue,,,,,,,,2022-12-13,github/xaviermignot,https://github.com/hashicorp/terraform/issues/32384,repo: hashicorp/terraform | keyword: gotcha | state: open
"Thanks for this well-written issue. The `-raw` flag was added in v0.15. In this case the inconsistency in behaviour is between `terraform output -json NAME` and `terraform output NAME` (any flags other than `-json`). The former is intended to be a machine-readable format. The human-readable format, on the other hand, has an optional `-raw` flag intended for use in shell scripts, i.e. also a machine readable format. This slight ambiguity is behind some of `terraform output`'s unexpected behaviou…",,,,,,Anecdotal,comment,,,,,,,,2022-12-13,github/kmoe,https://github.com/hashicorp/terraform/issues/32384#issuecomment-1348525313,repo: hashicorp/terraform | issue: `terraform output` flag `-raw` should behave like `-json` when state file is empty | keyword: gotcha
"If it's currently printing the warning to stdout (rather than stderr) then I guess that situation is _effectively_ useless for the machine-readable use case this was intended for (easy interpolation into a shell script). Therefore I think it could be defensible to change the behavior in spite of the compatibility promises here, because the current behavior isn't useful and therefore is unlikely to be a real compatibility constraint. Our intent when implementing this was for it to be essentially…",,,,,,Anecdotal,comment,,,,,,,,2022-12-13,github/apparentlymart,https://github.com/hashicorp/terraform/issues/32384#issuecomment-1348932019,repo: hashicorp/terraform | issue: `terraform output` flag `-raw` should behave like `-json` when state file is empty | keyword: gotcha
"Thanks for your answers guys, I totally understand your constraints and your commitment not to introduce breaking changes for the upcoming 1.x releases. Personally, I have noticed this while working on a small demo, nothing important. I used @kmoe's suggestion to work my way around and ended up with something like this: ```shell terraform show -json | jq -r '.values.outputs.my_output.value // ""defaultValue""' ``` I needed to either get the output value or fallback to a default, so finally I did …",,,,,,Anecdotal,comment,,,,,,,,2022-12-14,github/xaviermignot,https://github.com/hashicorp/terraform/issues/32384#issuecomment-1350352353,repo: hashicorp/terraform | issue: `terraform output` flag `-raw` should behave like `-json` when state file is empty | keyword: gotcha
"I have ran into this several times and it's definitely an unexpected gotcha to debug and see a variable now containing Terraform error texts. Both on initial run when the state file is empty and when there is state but the specific property I'm looking for is not found, I would like a mode which just returns an empty string so the shell script can use operators like -z or -n to evaluate the state of resources. Perhaps an `-ignore-errors` or `-quiet` flag could be added to `terraform output` to …",,,,,,Anecdotal,comment,,,,,,,,2023-01-13,github/hallvors,https://github.com/hashicorp/terraform/issues/32384#issuecomment-1382430486,repo: hashicorp/terraform | issue: `terraform output` flag `-raw` should behave like `-json` when state file is empty | keyword: gotcha
"This is also a problem when you try to follow the advice on resource targeting: > Instead of using -target as a means to operate on isolated portions of very large configurations, prefer instead to break large configurations into several smaller configurations that can each be independently applied. [Data sources](https://developer.hashicorp.com/terraform/language/data-sources) can be used to access information about resources created in other configurations, allowing a complex system architect…",,,,,,Anecdotal,comment,,,,,,,,2023-01-13,github/hallvors,https://github.com/hashicorp/terraform/issues/32384#issuecomment-1382443381,repo: hashicorp/terraform | issue: `terraform output` flag `-raw` should behave like `-json` when state file is empty | keyword: gotcha
"Module repository short reference & version pinning support <!-- Hi there, Thank you for opening an issue. Please note that we try to keep the Terraform issue tracker reserved for bug reports and feature requests. For general usage questions, please see: https://www.terraform.io/community.html. If your issue relates to a specific Terraform provider, please open it in the provider's own repository. The index of providers is at https://github.com/terraform-providers . --> ### Current Terraform Ve…",,,,,,Anecdotal,issue,,,,,,,,2020-06-23,github/WilliamABradley,https://github.com/hashicorp/terraform/issues/25343,repo: hashicorp/terraform | keyword: gotcha | state: open
"Hi @WilliamABradley, Thanks for sharing this proposal. We'll consider the idea of adding some indirection to avoid writing down the same source address multiple times. It's currently intentional that each instance of a module can have a separate version than the others, to allow for use-cases like rolling out a new version of a module alongside an old version to avoid downtime, etc, so we'll need to think a bit more about centralizing the _version constraints_ in that way. (Providers are differ…",,,,,,Anecdotal,comment,,,,,,,,2020-06-25,github/apparentlymart,https://github.com/hashicorp/terraform/issues/25343#issuecomment-649149976,repo: hashicorp/terraform | issue: Module repository short reference & version pinning support | keyword: gotcha
"There isn't enough friction in that situation :) So the verbose method works well enough. Notice in the above proposal that there are 2 different versions of the module with a different alias, that should solve the versioning issue, to some degree. Perhaps to solve the versioning issue, stick with using refs: ```tf terraform { module_sources = { devops_modules = { source = ""github.com/crimson-education/crimson-devops-modules?ref=1.0"" } devops_modules_legacy = { source = ""github.com/crimson-educ…",,,,,,Anecdotal,comment,,,,,,,,2020-06-25,github/WilliamABradley,https://github.com/hashicorp/terraform/issues/25343#issuecomment-649166734,repo: hashicorp/terraform | issue: Module repository short reference & version pinning support | keyword: gotcha
Add resource identity documentation This PR adds some basic information about the new `identity` attribute in import blocks. This will be used by the Terraform Search workflow and is not expected to be filled in manually (but still possible though).,,,,,,Anecdotal,issue,,,,,,,,2025-05-07,github/dbanck,https://github.com/hashicorp/terraform/pull/36994,repo: hashicorp/terraform | keyword: gotcha | state: closed
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-06-20,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/36994#issuecomment-2989621875,repo: hashicorp/terraform | issue: Add resource identity documentation | keyword: gotcha
"Update docs for indent() (fix #29508) As `indent()` function behaves weirdly on final newline character, I added document to clarify the situation.",,,,,,Anecdotal,issue,,,,,,,,2021-09-04,github/tmshn,https://github.com/hashicorp/terraform/pull/29525,repo: hashicorp/terraform | keyword: gotcha | state: closed
[![CLA assistant check](https://cla.hashicorp.com/pull/badge/signed)](https://cla.hashicorp.com/hashicorp/terraform?pullRequest=29525) <br/>All committers have signed the CLA.,,,,,,Anecdotal,comment,,,,,,,,2021-09-09,github/hashicorp-cla,https://github.com/hashicorp/terraform/pull/29525#issuecomment-915912073,repo: hashicorp/terraform | issue: Update docs for indent() (fix #29508) | keyword: gotcha
"Thanks for this submission! Between the time this was submitted and now, we have migrated our docs to a new format, which resulted in all of the files being renamed. This change would now need to be made to https://github.com/hashicorp/terraform/blob/main/website/docs/language/functions/indent.mdx. However, before making that change, I am including @laurapacilio to review whether we would be likely to accept this change. Thanks again for your submission and your patience! We appreciate it!",,,,,,Anecdotal,comment,,,,,,,,2022-05-21,github/crw,https://github.com/hashicorp/terraform/pull/29525#issuecomment-1133475027,repo: hashicorp/terraform | issue: Update docs for indent() (fix #29508) | keyword: gotcha
"I would be willing to review and include this if @tmshn is willing to reopen this PR with the new website files! This seems like a ""gotcha"" that it would be nice to help users avoid.",,,,,,Anecdotal,comment,,,,,,,,2022-05-24,github/laurapacilio,https://github.com/hashicorp/terraform/pull/29525#issuecomment-1136447375,repo: hashicorp/terraform | issue: Update docs for indent() (fix #29508) | keyword: gotcha
"As mentioned, this PR will need rebasing to reflect new filename/structure. If you are still interested in contributing this change feel free to open a new PR. I'm going to close this one as it has been without reaction for a couple of years now.",,,,,,Anecdotal,comment,,,,,,,,2025-05-13,github/radeksimko,https://github.com/hashicorp/terraform/pull/29525#issuecomment-2876095462,repo: hashicorp/terraform | issue: Update docs for indent() (fix #29508) | keyword: gotcha
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-06-13,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/29525#issuecomment-2968791601,repo: hashicorp/terraform | issue: Update docs for indent() (fix #29508) | keyword: gotcha
Store resource identities in state (TF-23255) This PR implements the two new RPCs for resource identities and stores resource identities along with the resource attributes in state. The new `GetResourceIdentitySchemas` RPC call is used to fetch all resource identity schemas for a provider. We fetch the identity schemas during the `GetProviderSchema` workflow and combine the results. In that way we end up with a resource type schema that contains everything we need later on: https://github.com/h…,,,,,,Anecdotal,issue,,,,,,,,2025-02-10,github/dbanck,https://github.com/hashicorp/terraform/pull/36464,repo: hashicorp/terraform | keyword: gotcha | state: closed
The equivalence tests failed. Please investigate [here](https://github.com/hashicorp/terraform/actions/runs/13246261336).,,,,,,Anecdotal,comment,,,,,,,,2025-02-10,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/36464#issuecomment-2648687372,repo: hashicorp/terraform | issue: Store resource identities in state (TF-23255) | keyword: gotcha
The equivalence tests failed. Please investigate [here](https://github.com/hashicorp/terraform/actions/runs/13259152393).,,,,,,Anecdotal,comment,,,,,,,,2025-02-11,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/36464#issuecomment-2650180752,repo: hashicorp/terraform | issue: Store resource identities in state (TF-23255) | keyword: gotcha
The equivalence tests failed. Please investigate [here](https://github.com/hashicorp/terraform/actions/runs/13286092779).,,,,,,Anecdotal,comment,,,,,,,,2025-02-12,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/36464#issuecomment-2653667789,repo: hashicorp/terraform | issue: Store resource identities in state (TF-23255) | keyword: gotcha
The equivalence tests failed. Please investigate [here](https://github.com/hashicorp/terraform/actions/runs/13286575738).,,,,,,Anecdotal,comment,,,,,,,,2025-02-12,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/36464#issuecomment-2653730966,repo: hashicorp/terraform | issue: Store resource identities in state (TF-23255) | keyword: gotcha
The equivalence tests failed. Please investigate [here](https://github.com/hashicorp/terraform/actions/runs/13286780121).,,,,,,Anecdotal,comment,,,,,,,,2025-02-12,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/36464#issuecomment-2653756646,repo: hashicorp/terraform | issue: Store resource identities in state (TF-23255) | keyword: gotcha
The equivalence tests failed. Please investigate [here](https://github.com/hashicorp/terraform/actions/runs/13308705755).,,,,,,Anecdotal,comment,,,,,,,,2025-02-13,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/36464#issuecomment-2656596699,repo: hashicorp/terraform | issue: Store resource identities in state (TF-23255) | keyword: gotcha
The equivalence tests failed. Please investigate [here](https://github.com/hashicorp/terraform/actions/runs/13308919013).,,,,,,Anecdotal,comment,,,,,,,,2025-02-13,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/36464#issuecomment-2656624662,repo: hashicorp/terraform | issue: Store resource identities in state (TF-23255) | keyword: gotcha
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-04-11,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/36464#issuecomment-2795677941,repo: hashicorp/terraform | issue: Store resource identities in state (TF-23255) | keyword: gotcha
"stacks: ensure providers that should not be configured cannot be This PR adds additional protections around the providers created by the stacks runtime and used within Terraform Core. Essentially, Terraform Core should not be calling configure on any of the providers that it has access to during a stacks operation. Either a provider should never be configured as it only used for offline operations, or it should already have been configured by Stacks before being passed into Terraform Core. We e…",,,,,,Anecdotal,issue,,,,,,,,2024-08-22,github/liamcervante,https://github.com/hashicorp/terraform/pull/35624,repo: hashicorp/terraform | keyword: gotcha | state: closed
"Reminder for the merging maintainer: if this is a user-visible change, please update the changelog on the appropriate release branch.",,,,,,Anecdotal,comment,,,,,,,,2024-08-23,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/35624#issuecomment-2306462571,repo: hashicorp/terraform | issue: stacks: ensure providers that should not be configured cannot be | keyword: gotcha
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2024-09-23,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/35624#issuecomment-2367121851,repo: hashicorp/terraform | issue: stacks: ensure providers that should not be configured cannot be | keyword: gotcha
"Issue with linux installation on Linux Mint 22.1 ""Xia"" Trying to install Terraform. I am using Linux Mint, just had new release codenamed ""Xia"" According to packaging guide, Ubuntu codenames are supported. https://www.hashicorp.com/en/official-packaging-guide?product_intent=terraform Installation instructions on https://developer.hashicorp.com/terraform/install is: ```shell wget -O - https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg…",,,,,,Anecdotal,issue,,,,,,,,2025-02-11,github/OTonGitHub,https://github.com/hashicorp/terraform/issues/36466,repo: hashicorp/terraform | keyword: gotcha | state: closed
"I should note that I did ""fix"" the problem by editing the > /etc/apt/sources.list.d/hashicorp.list file by changing this single line in it ```deb [arch=amd64 signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com xia main``` from `xia` to `noble`, as linux mint xia based on ubuntu noble. And works fine. don't see why this should be necessary. Or at least not mentioned clearly in the instructions.",,,,,,Anecdotal,comment,,,,,,,,2025-02-11,github/OTonGitHub,https://github.com/hashicorp/terraform/issues/36466#issuecomment-2650725161,"repo: hashicorp/terraform | issue: Issue with linux installation on Linux Mint 22.1 ""Xia"" | keyword: gotcha"
"> According to packaging guide, **Ubuntu codenames** are supported. `xia` represents a Linux Mint codename as far as I can tell, not Ubuntu. i.e. there is no Ubuntu xia. I am not aware of plans for supporting codenames for any particular Ubuntu derivatives such as Mint but I will ask the team that is responsible for maintaining our Linux packaging pipelines. https://www.hashicorp.com/en/official-packaging-guide?product_intent=terraform ^ as for instructions - the page which you also linked does…",,,,,,Anecdotal,comment,,,,,,,,2025-02-11,github/radeksimko,https://github.com/hashicorp/terraform/issues/36466#issuecomment-2650779831,"repo: hashicorp/terraform | issue: Issue with linux installation on Linux Mint 22.1 ""Xia"" | keyword: gotcha"
"@radeksimko gotcha, I don't think there is much that can be one from the particular distro side. So I hope the team maybe move on from grabing codename by running `$(lsb_release -cs)` and instead go for something that will work on more systems without breaking anything else My suggestion is, instead of using the existing: ```echo ""deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main""``` do:…",,,,,,Anecdotal,comment,,,,,,,,2025-02-13,github/OTonGitHub,https://github.com/hashicorp/terraform/issues/36466#issuecomment-2655566636,"repo: hashicorp/terraform | issue: Issue with linux installation on Linux Mint 22.1 ""Xia"" | keyword: gotcha"
Thanks for the suggestion - I like the idea. I will pass it on to the team responsible for maintaining the packaging guide to see if they have any strong opinions.,,,,,,Anecdotal,comment,,,,,,,,2025-02-13,github/radeksimko,https://github.com/hashicorp/terraform/issues/36466#issuecomment-2655993665,"repo: hashicorp/terraform | issue: Issue with linux installation on Linux Mint 22.1 ""Xia"" | keyword: gotcha"
The [guide](https://www.hashicorp.com/en/official-packaging-guide) has now been updated. Thank you for the suggestion. ![Image](https://github.com/user-attachments/assets/e83a7b23-6b43-40f1-97c5-454aed600900),,,,,,Anecdotal,comment,,,,,,,,2025-02-21,github/radeksimko,https://github.com/hashicorp/terraform/issues/36466#issuecomment-2674119191,"repo: hashicorp/terraform | issue: Issue with linux installation on Linux Mint 22.1 ""Xia"" | keyword: gotcha"
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2025-03-24,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/36466#issuecomment-2746712728,"repo: hashicorp/terraform | issue: Issue with linux installation on Linux Mint 22.1 ""Xia"" | keyword: gotcha"
"Request: Enable simple/native way to read .env files ### Current Terraform Version ``` Terraform v0.12.19 ``` ### Use-cases It's standard for code bases to have .env files, but I am not aware of any way to read their contents into terraform scripts. ### Attempted Solutions There is the option for `terraform.tfvars`, but this requires my code base to maintain two secret-variable files (and their templates). ### Proposal At minimum, instead of requiring that the `-var-file` flag take only files n…",,,,,,Anecdotal,issue,,,,,,,,2020-01-21,github/MagnusBrzenk,https://github.com/hashicorp/terraform/issues/23906,repo: hashicorp/terraform | keyword: gotcha | state: closed
"Hi @MagnusBrzenk! Could you provide a little more information about exactly what file format you mean when you say "".env files""? If you know a link to the documentation for the file format you're talking about, that would be helpful to understand what it would take to include a parser for it in Terraform.",,,,,,Anecdotal,comment,,,,,,,,2020-01-23,github/apparentlymart,https://github.com/hashicorp/terraform/issues/23906#issuecomment-577782741,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"I am not aware of any official standard for `.env` files. The *idea* of a `.env` file is that it stores variables that will be made available to a (linux?) shell when sourced. So, in theory, the syntax of a `.env` file will be such as to not error when sourcing from e.g. a bash shell. E.g.: ``` # Comment aaa=AAA export bbb=BBB ``` However, my impression is that, in practice, the acceptable syntax for a `.env` file just depends on the conventions chosen by the most popular/robust library used by…",,,,,,Anecdotal,comment,,,,,,,,2020-01-24,github/MagnusBrzenk,https://github.com/hashicorp/terraform/issues/23906#issuecomment-578000543,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"Thanks for the extra context, @MagnusBrzenk! In the interests of gathering as much context as possible: you mentioned that you're currently having to maintain two different definitions of environment variables, so I assume that means you already have something else in your system that understands this file format you're describing and so compatibility with that other system would be the most important thing to meet your use-case. Given that, are you able to identify that system so we can use it…",,,,,,Anecdotal,comment,,,,,,,,2020-01-24,github/apparentlymart,https://github.com/hashicorp/terraform/issues/23906#issuecomment-578268634,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"Sure. For example, when I make front-end applications involving static assets (html, css, js), I always have a `.env` file in order to, at minimum, build and deploy those assets to github pages. Rather than hardcoding my username and repo in the code, I pull them out into a .env file. Here is an example [repo](https://github.com/MagnusBrzenk/ng7-material-boilerplate) and its [`.env` file](https://github.com/MagnusBrzenk/ng7-material-boilerplate/blob/master/.env-template). And if my web app need…",,,,,,Anecdotal,comment,,,,,,,,2020-01-24,github/MagnusBrzenk,https://github.com/hashicorp/terraform/issues/23906#issuecomment-578307773,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"Thanks for all that extra context, @MagnusBrzenk! It's definitely helpful. With that said, it seems like the main requirement here is to identify a subset of [standard POSIX shell syntax](https://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html) that is sufficient to get standard shells to export a set of variables when it's executed as a script, and that this hypothetical Terraform-side parser can reliably interpret and get identical names and values to what a standard shell w…",,,,,,Anecdotal,comment,,,,,,,,2020-01-24,github/apparentlymart,https://github.com/hashicorp/terraform/issues/23906#issuecomment-578346503,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"Having lost a day down this rabbit hole before finding this issue ("".env"" is very hard to search for), I'd like to share some extra information. The most popular libraries for this appear to be the already mentioned javascript library, as well as these: - https://github.com/theskumar/python-dotenv for Python - https://github.com/bkeepers/dotenv for Ruby - https://github.com/joho/godotenv for Go (which is MIT licensed and usable as a library by other Go programs) @apparentlymart If you're intere…",,,,,,Anecdotal,comment,,,,,,,,2020-04-26,github/techdragon,https://github.com/hashicorp/terraform/issues/23906#issuecomment-619592366,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"Would this capability not first depend on `${env.VAR_NAME}` to be implemented so that the loaded `.env` values can be referenced? As @techdragon pointed out, terraform currently expects variables to be prefixed with `TF_VAR_`. From https://github.com/hashicorp/terraform/pull/1621: > In the future, we'd still like to add ${env.NAME} syntax to the configuration, but the way we'd like to do that represents significantly more engineering effort. This is a nice stop-gap solution to support env vars …",,,,,,Anecdotal,comment,,,,,,,,2020-06-10,github/beanaroo,https://github.com/hashicorp/terraform/issues/23906#issuecomment-641730981,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"FYI -- here's the approach I've been taking with terraform ""as is"" that satisfies me. [https://stackoverflow.com/a/60870365/8620332](https://stackoverflow.com/a/60870365/8620332)",,,,,,Anecdotal,comment,,,,,,,,2020-06-10,github/MagnusBrzenk,https://github.com/hashicorp/terraform/issues/23906#issuecomment-642078160,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"A very similar request was made in https://github.com/hashicorp/terraform-provider-kubernetes/issues/889. We have a couple of Kubernetes resources (config_map and secret) where it makes sense to read the data field directly from a dotenv. I proposed a hacky workaround where you can parse a dotenv file (with some gotchas) using the existing functions built into Terraform, but I'd also like to propose that we add a function to help with this. I've opened https://github.com/hashicorp/terraform/pul…",,,,,,Anecdotal,comment,,,,,,,,2020-06-30,github/jrhouston,https://github.com/hashicorp/terraform/issues/23906#issuecomment-651729578,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"To answer some of the questions since my last comment, I want to clarify that I am not expecting that Terraform will do any automatic processing of environment files, but rather that we might consider offering a new built in function -- similar to the existing `jsondecode`, `yamldecode`, `csvdecode` functions -- so that you can write a configuration that will explicitly read settings from a file and take some action based on them, such as populating the set of environment variables associated w…",,,,,,Anecdotal,comment,,,,,,,,2020-07-01,github/apparentlymart,https://github.com/hashicorp/terraform/issues/23906#issuecomment-652652884,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"@apparentlymart asked about a system that uses .env files they might currently have to maintain. I am surprised no one mentioned docker. (Well k8 is close.). But right now we have our dev environment using docker-compose with .env files. We are moving to terraform everywhere else. I recently added terraform to our ""dev"" environment to configure Localstack which launch as another container docker-compose. All of our other services there use .env files. I found this thread looking for a way to ge…",,,,,,Anecdotal,comment,,,,,,,,2020-11-19,github/rayjlinden,https://github.com/hashicorp/terraform/issues/23906#issuecomment-730147216,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"The suggestion in [@apparentlymart's comment](https://github.com/hashicorp/terraform/issues/23906#issuecomment-652652884), this is exactly what I need. My use case is sharing Terraform tfvars files with a Terragrunt subproject. Given a file `fred.tfvars`, ``` # fred.tfvars foo = 1 bar=barney ``` allows me to write some Terraform ``` locals { fred_vars = tfvarsdecode(file(""/tmp/fred.tfvars"")) my_bar = fred_vars.bar // barney my_foo = fred_vars.foo // ""1"" } ``` File format is easy-ish and there i…",,,,,,Anecdotal,comment,,,,,,,,2021-07-13,github/dcfsc,https://github.com/hashicorp/terraform/issues/23906#issuecomment-879153704,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"I think you might be missing the point on `.env` files and that pull request (#25433), if merged, will virtually destroy any chance of Terraform providing `.env` support consistent to the way everyone else does it. The one thing about `.env` that many people often forget is that `.env` files MUST be omitted from source control as they can potentially contain sensitive information. They are not meant to be used as just another source of configuration options, but they are meant to mimic a virtua…",,,,,,Anecdotal,comment,,,,,,,,2021-11-03,github/ionut-botizan,https://github.com/hashicorp/terraform/issues/23906#issuecomment-958757914,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"In my case, I am not using them as configuration for a specific use case, but as components of a Terragrunt stack. Being able to share configurations there by reading .env format files directly into a Terraform object would eliminate keeping one copy in YAML and one (with the same data) in .env format and eliminating dual maintenance. `tfvarsdecode(file(""/tmp/fred.tfvars""))` would allow me to read vars from a file in a specific format (like `yamldecode` and `jsondecode`), where there is no ""cur…",,,,,,Anecdotal,comment,,,,,,,,2021-11-03,github/dcfsc,https://github.com/hashicorp/terraform/issues/23906#issuecomment-959139232,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"> In my case [...] There's the problem. The `.env` functionality, if implemented in Terraform, should not fix a handful of people's use cases. Instead, it should focus on how it is most commonly used in other segments of the industry, especially considering that Terraform is often used as a part of a bigger whole, in conjunction with other technologies. It should not deviate in the way it handles things that are already common practice. By all means, merge this feature that allows people to loa…",,,,,,Anecdotal,comment,,,,,,,,2021-11-03,github/ionut-botizan,https://github.com/hashicorp/terraform/issues/23906#issuecomment-959300077,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"This might be your use of env files. We use them to define public names of services in each environment and it very much needs to be checked in. We also support env files that are not checked and optional fir overrides. (Secrets, of course are never checked in use a different mechanism.) Nothing wrong with your use case. But it certainly is not what everybody else is doing… On Wed, Nov 3, 2021 at 2:00 AM Ionuț Botizan ***@***.***> wrote: > I think you might be missing the point on .env files an…",,,,,,Anecdotal,comment,,,,,,,,2021-11-03,github/rayjlinden,https://github.com/hashicorp/terraform/issues/23906#issuecomment-959553015,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"> This might be your use of env files. It's not *my* use case, but the most common use case as you'll discover if you'll do a simple search on Google for things like ""ignore .env"", ""commit .env"", ""exclude .env from source control"", etc. Besides that, it's in the name... `.env`, as in `environment`, not as in `config`. > We use them to define public names of services in each environment and it very much needs to be checked in. Since, as you clearly say it, the env vars you define are used for se…",,,,,,Anecdotal,comment,,,,,,,,2021-11-03,github/ionut-botizan,https://github.com/hashicorp/terraform/issues/23906#issuecomment-959757247,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"> I think you might be missing the point on .env files and that pull request (#25433), if merged, will virtually destroy any chance of Terraform providing .env support consistent to the way everyone else does it. I see what you're saying here. There is indeed a reasonably established pattern on using `.env` files in an application to populate (usually local, dev) environment-specific configuration options at runtime like usernames and passwords, and it is unwise to commit (or encourage people t…",,,,,,Anecdotal,comment,,,,,,,,2021-11-03,github/jrhouston,https://github.com/hashicorp/terraform/issues/23906#issuecomment-959821740,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"> The addition of the function also doesn't propose any opinions or practices on whether or not the file should be committed, or even if it should come from a file at all. It is just a function that parses a string containing this notation into a map.. Agree on ""doesn't propose any opinions"". Which is why my proposal was `tfvarsdecode` since the .tfvars is a known Terraform format. Just says, ""take a .tfvars file, convert into a Terraform object I can work with.""",,,,,,Anecdotal,comment,,,,,,,,2021-11-03,github/dcfsc,https://github.com/hashicorp/terraform/issues/23906#issuecomment-959825694,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"You are expressing a methodology you read as a religion all must follow. Real life is not that clean. Our env files are not checked into where the application code lives. They are checked into a repo that defines our MANY environments. When you define infrastructure as code it is good to use version control for that as well. When you drive all of your environment changes through git for both version control and an automated deploy process you have to check the stuff in. In the end, an env file …",,,,,,Anecdotal,comment,,,,,,,,2021-11-03,github/rayjlinden,https://github.com/hashicorp/terraform/issues/23906#issuecomment-959829794,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"> > The addition of the function also doesn't propose any opinions or practices on whether or not the file should be committed, or even if it should come from a file at all. It is just a function that parses a string containing this notation into a map.. > > Agree on ""doesn't propose any opinions"". Which is why my proposal was `tfvarsdecode` since the .tfvars is a known Terraform format. Just says, ""take a .tfvars file, convert into a Terraform object I can work with."" The `.tfvars` format is a…",,,,,,Anecdotal,comment,,,,,,,,2021-11-03,github/jrhouston,https://github.com/hashicorp/terraform/issues/23906#issuecomment-959832056,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"> As explained above, though, we are not interested in adding any sort of implicit behaviour for loading `.env` files when Terraform runs, as that is what `.tfvars` files are for. Rather we want to add a convenience function for parsing this particular notation. I will add that there is no authoritative standard here. It is an emergent notation many projects have converged on so I don't think we can be absolutist on its use – I think the use-cases already mentioned are valid. > > The addition o…",,,,,,Anecdotal,comment,,,,,,,,2021-11-03,github/ionut-botizan,https://github.com/hashicorp/terraform/issues/23906#issuecomment-960205122,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"My favoured way to read/load `.env` files is to have the shell automatically do it, since I personally believe the environment variables should be completely external|transparent to whatever code I am running. In my case, I find that the OMZ dotenv plugin works perfectly. https://github.com/ohmyzsh/ohmyzsh/blob/master/plugins/dotenv/dotenv.plugin.zsh Simple to use: ``` $ omz plugin enable dotenv omz::plugin::enable: plugins enabled: dotenv. dotenv: found '.env' file. Source it? ([Y]es/[n]o/[a]l…",,,,,,Anecdotal,comment,,,,,,,,2022-11-26,github/crbn60,https://github.com/hashicorp/terraform/issues/23906#issuecomment-1328077859,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"You can also expose the values from a .env file as Terraform local values. I use this method to share configuration between terraform.backend config file and the actual Terraform code. ```hcl locals { dot_env_regex = ""(?m:^\\s*([^#\\s]\\S*)\\s*=\\s*[\""']?(.*[^\""'\\s])[\""']?\\s*$)"" dot_env = {for tuple in regexall(local.dot_env_regex, file("".env"")) : tuple[0] => sensitive(tuple[1])} external_id = local.dot_env[""AWS_EXTERNAL_ID""] } ``` https://stackoverflow.com/a/72888812/2127315",,,,,,Anecdotal,comment,,,,,,,,2023-02-02,github/cedricbastin,https://github.com/hashicorp/terraform/issues/23906#issuecomment-1413973319,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"@cedricbastin are you sure that regex pattern you provided is correct? When I test it none of my env variables are picked up. And also when I test it using regex101.com nothing gets selected either. Your solution works when I use it with the simpler `(.*?)=(.*)` pattern as described in that SO question, but not with your more complex one. But I'm guessing your complex one covers more use cases so I am hoping to get it working.",,,,,,Anecdotal,comment,,,,,,,,2024-01-18,github/BenJackGill,https://github.com/hashicorp/terraform/issues/23906#issuecomment-1897871720,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"> ```terraform > dot_env_regex = ""(?m:^\\s*([^#\\s]\\S*)\\s*=\\s*[\""']?(.*[^\""'\\s])[\""']?\\s*$)"" > ``` I am using it on this file [here](https://raw.githubusercontent.com/docker-mailserver/docker-mailserver/master/mailserver.env) and it doesn't work.",,,,,,Anecdotal,comment,,,,,,,,2024-01-24,github/moritzschmitz-oviva,https://github.com/hashicorp/terraform/issues/23906#issuecomment-1908736129,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
Thank you for your continued interest in this issue. [Terraform version 1.8](https://github.com/hashicorp/terraform/blob/v1.8.0-beta1/CHANGELOG.md) launches with support of provider-defined functions. It is now possible to implement your own functions! We would love to see this implemented as a provider-defined function. Please see the [provider-defined functions documentation](https://developer.hashicorp.com/terraform/plugin/framework/functions) to learn how to implement functions in your prov…,,,,,,Anecdotal,comment,,,,,,,,2024-03-07,github/crw,https://github.com/hashicorp/terraform/issues/23906#issuecomment-1982082177,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"Reviewing this issue again some time later, it seems like there isn't really a consensus on what exactly this function ought to do, or whether it should even be a function at all, and I also share the concern that if we implement something that isn't compatible with existing practices then that's likely worse than implementing nothing at all. With all of that said, I did notice the mention along the way that kubectl for Kubernetes has a _specific_ implementation of an ""env file"" format and ther…",,,,,,Anecdotal,comment,,,,,,,,2024-05-30,github/apparentlymart,https://github.com/hashicorp/terraform/issues/23906#issuecomment-2138471754,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"depends_on should should defer interpolation <!-- Hi there, Thank you for opening an issue. Please note that we try to keep the Terraform issue tracker reserved for bug reports and feature requests. For general usage questions, please see: https://www.terraform.io/community.html. If your issue relates to a specific Terraform provider, please open it in the provider's own repository. The index of providers is at https://github.com/terraform-providers . --> ### Current Terraform Version <!--- Run…",,,,,,Anecdotal,issue,,,,,,,,2019-07-11,github/mutt13y,https://github.com/hashicorp/terraform/issues/22036,repo: hashicorp/terraform | keyword: gotcha | state: closed
"Hi @mutt13y, The various functions with `file` in their names that read files from disk are intended for use with files that are delivered as part of the configuration, such as being checked in to version control alongside the `.tf` files that reference them. They are _not_ for files that are generated during the `terraform apply` step. We generally recommend against using Terraform to generate temporary artifacts locally, since that isn't really what it is for. We offer the facilities to do so…",,,,,,Anecdotal,comment,,,,,,,,2019-07-11,github/apparentlymart,https://github.com/hashicorp/terraform/issues/22036#issuecomment-510558357,repo: hashicorp/terraform | issue: depends_on should should defer interpolation | keyword: gotcha
"Hi @apparentlymart, I take your point, I am just wondering what the use case is for the local_exec provisioner if it is not executed first (or last). If there is a command that needs to be executed locally I think mostly you would want to run it before the apply or after it. So perhaps an option on local_exec to control when it runs could be an option ? Stuart",,,,,,Anecdotal,comment,,,,,,,,2019-07-12,github/mutt13y,https://github.com/hashicorp/terraform/issues/22036#issuecomment-510770415,repo: hashicorp/terraform | issue: depends_on should should defer interpolation | keyword: gotcha
"Provisioners in general are a sort of ""last resort"" feature for doing small fixups after an object is created that don't otherwise fit into Terraform's declarative model. For example, in some environments it's impractical to customize machine images so that compute instances can immediately start their work on boot, and so provisioners can fill that gap by allowing last-moment initialization to happen on the remote host. As an example for `local-exec` in particular, it is sometimes used to run …",,,,,,Anecdotal,comment,,,,,,,,2019-07-12,github/apparentlymart,https://github.com/hashicorp/terraform/issues/22036#issuecomment-510943871,repo: hashicorp/terraform | issue: depends_on should should defer interpolation | keyword: gotcha
"I would also like to see this kind of behaviour added. We currently use the null_resource and local-exec provisioner to copy some files from a GCS bucket to the local machine. The content of these files which are being copied are used in a later step to create some kubernetes secrets. Although we specify that the kubernetes secret resource is dependent on the local-exec command, it doesn't wait for the local-exec to finish. This results in an error that the file to create the kubernetes secret …",,,,,,Anecdotal,comment,,,,,,,,2019-09-03,github/MikeBlomm,https://github.com/hashicorp/terraform/issues/22036#issuecomment-527347713,repo: hashicorp/terraform | issue: depends_on should should defer interpolation | keyword: gotcha
"I ended up writing a Makefile, you could use concourse as a better alternative. I think that if you need something done before or after the apply it is reasonable to use some other tooling. Would it make more sense to use vault for your secrets ? I am sure the vault provider will have proper dependancies. I do end up wondering what the actual use case for the local-exec is if we cant control when it runs.",,,,,,Anecdotal,comment,,,,,,,,2019-09-03,github/mutt13y,https://github.com/hashicorp/terraform/issues/22036#issuecomment-527365696,repo: hashicorp/terraform | issue: depends_on should should defer interpolation | keyword: gotcha
"Well as I read the above comment it is only intended for small fixes, not for actual resource creation. We looked at using Vault, but it currently is overkill to set up and maintain a whole client/server application just for our secrets.",,,,,,Anecdotal,comment,,,,,,,,2019-09-03,github/MikeBlomm,https://github.com/hashicorp/terraform/issues/22036#issuecomment-527376156,repo: hashicorp/terraform | issue: depends_on should should defer interpolation | keyword: gotcha
"There's going to be a lot of people wanting this use case when working with lambda, exactly as the OP is doing. The original request: > interpolation functions should not run until the depended_on resource has completed seems pretty fair and straightforward to me. ED: Although, I notice `source_code_hash` seems to be optional for `aws_lambda_function`. The documentation says it's: > (Optional) Used to trigger updates. I have no idea what ""Used to trigger updates"" is supposed to mean but my terr…",,,,,,Anecdotal,comment,,,,,,,,2019-09-20,github/subos2008,https://github.com/hashicorp/terraform/issues/22036#issuecomment-533541776,repo: hashicorp/terraform | issue: depends_on should should defer interpolation | keyword: gotcha
"I personally think that the expectation that developers should run additional build steps, in an infrastructure repository, besides `terraform apply` to be a bit ugly, perhaps unpolished. I'd prefer to keep `terraform` as the only application that performs tasks in the repo before pushing changes to infrastructure. The syntax of `depends_on`, and the examples showing how its used, definitely leads one to believe it could be used for exactly this - what the OP (and myself) is after. I feel like …",,,,,,Anecdotal,comment,,,,,,,,2020-04-09,github/perry-mitchell,https://github.com/hashicorp/terraform/issues/22036#issuecomment-611362398,repo: hashicorp/terraform | issue: depends_on should should defer interpolation | keyword: gotcha
"I'd like to add an example use case to this old issue for posterity sake. We have a reusable module for deploying Lambda functions that can take either a list of source files, a directory of source files, or an S3 bucket and key to download the prepackaged source files. The module uses `depends_on` and `data.local_file` to provide the lambda resource with any of the options above for source files. It works, but the source_code_hash variable is the trickiest part due to the issue at hand. Snippe…",,,,,,Anecdotal,comment,,,,,,,,2022-08-31,github/zachwhaley,https://github.com/hashicorp/terraform/issues/22036#issuecomment-1233289528,repo: hashicorp/terraform | issue: depends_on should should defer interpolation | keyword: gotcha
"Hi @zachwhaley! Thanks for sharing that use-case and partial workaround. I think the reason your workaround causes the configuration to be non-converging is because `base64sha256` means ""calculate a SHA256 hash of this string and then base64 encode it"", rather than ""decode this base64-encoded string and then generate a SHA256 hash of the result"". The result is therefore syntactically valid (it's a base64-encoded SHA256 hash) but it's not _semantically_ valid: it's a hash of the wrong source con…",,,,,,Anecdotal,comment,,,,,,,,2022-08-31,github/apparentlymart,https://github.com/hashicorp/terraform/issues/22036#issuecomment-1233324723,repo: hashicorp/terraform | issue: depends_on should should defer interpolation | keyword: gotcha
"Returning to this much later: I notice that [my `hashicorp/local` proposal](https://github.com/hashicorp/terraform-provider-local/issues/137) was partially accepted upstream and so the provider's [`local_file` data source](https://registry.terraform.io/providers/hashicorp/local/latest/docs/data-sources/file) now exports a similar set of checksums as are supported by Terraform's builtin file checksum functions. Resources (which includes ""data resources"", declared by `data` blocks) are how we mod…",,,,,,Anecdotal,comment,,,,,,,,2024-05-30,github/apparentlymart,https://github.com/hashicorp/terraform/issues/22036#issuecomment-2140171969,repo: hashicorp/terraform | issue: depends_on should should defer interpolation | keyword: gotcha
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2024-06-30,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/22036#issuecomment-2198405584,repo: hashicorp/terraform | issue: depends_on should should defer interpolation | keyword: gotcha
"Proposal: State Encryption Currently we have several resources that retrieve or generate secrets, and for any where these secrets are used to populate other resources or configure other providers these secrets must necessarily be stored in the state. Such resources include: - `aws_db_cluster` (password attribute) - `azurerm_virtual_machine` (machine login passwords) - `tls_private_key` - `vault_generic_secret` (both managed resource and data source) (#9158) - ...and many other resources that us…",,,,,,Anecdotal,issue,,,,,,,,2016-10-24,github/apparentlymart,https://github.com/hashicorp/terraform/issues/9556,repo: hashicorp/terraform | keyword: gotcha | state: closed
"what is the status here ? From my point of view, there should be no link between config parameters (passwords) and state files. I still see a need to have the state file encrypted. I also think there should be NO password in the state file, but at least a pointer to where to find the password. I also don't understant people ""sharing"" the state file... If you have a need to share something, maybe that's something to be added to Terraform. The state file is an ""internal"" view of the currently run…",,,,,,Anecdotal,comment,,,,,,,,2017-02-06,github/prune998,https://github.com/hashicorp/terraform/issues/9556#issuecomment-277709229,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"The forthcoming version 0.9 contains some reworking of Terraform's handling of states that will, amongst other things, make this easier to implement in a future release. I can't say exactly when that will be (I don't have visibility into the official roadmap) but the technical blockers on this will be much diminished once 0.9 is released. --- I suppose it's worth noting that the usage examples in my original proposal here are no longer valid with the changes in 0.9. Instead of configuring encry…",,,,,,Anecdotal,comment,,,,,,,,2017-02-06,github/apparentlymart,https://github.com/hashicorp/terraform/issues/9556#issuecomment-277847071,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"@mkuzmin in principle that is possible, but I've seen the Vault team recommend against storing non-trivially-sized things in Vault's generic backend, and instead to use the transit backend to encrypt for storage elsewhere. That recommendation is what this design was based on.",,,,,,Anecdotal,comment,,,,,,,,2017-02-07,github/apparentlymart,https://github.com/hashicorp/terraform/issues/9556#issuecomment-278045191,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
@apparentlymart https://github.com/hashicorp/terraform/issues/9556#issuecomment-277847071 is this supported in 0.9 release,,,,,,Anecdotal,comment,,,,,,,,2017-03-21,github/frezbo,https://github.com/hashicorp/terraform/issues/9556#issuecomment-288162157,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"as discussed at Hashidays NY with @phinze I'm also interested in this feature of encrypting state before saving it to remote location. There are already open source projects that allow to encrypt JSON files with AWS KMS keys: https://github.com/agilebits/sm https://github.com/mozilla/sops Manual workflow could be: - save state locally, encrypt - upload to remote state on second machine: - fetch encrypted file - decrypt and import as local state.",,,,,,Anecdotal,comment,,,,,,,,2017-05-21,github/stumyp,https://github.com/hashicorp/terraform/issues/9556#issuecomment-302913160,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
i was considering writing a consul http proxy that you could use as a consul backend for tf. encrypted/decrypted all the values through vault transit. not sure if it's worth it now. depends on the timing. (can i get it ballparked at all without calling in a support contract?) i like the new direction for encryption provider. it's the same adapter pattern but internalized and simplified. the consul sharing works (should work - good idea!) for my team. but consul could be (or seem like) a barrier…,,,,,,Anecdotal,comment,,,,,,,,2017-05-26,github/automaticgiant,https://github.com/hashicorp/terraform/issues/9556#issuecomment-304156905,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"I'd like to see this feature implemented in core, along with other encryption efforts. In the mean time, I came across this tool: [terrahelp](https://github.com/opencredo/terrahelp)",,,,,,Anecdotal,comment,,,,,,,,2017-11-09,github/mcameron,https://github.com/hashicorp/terraform/issues/9556#issuecomment-343123567,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
@apparentlymart What's the status on this? Would this get accepted into terraform if I would implement it? Or are there any technical blockers on this?,,,,,,Anecdotal,comment,,,,,,,,2018-09-14,github/simonre,https://github.com/hashicorp/terraform/issues/9556#issuecomment-421252774,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"Hi @simonre! The architecture of ""backends"" in Terraform changed significantly since I originally proposed this (which was before I was a HashiCorp employee), so I expect we'll need to do another round of design work before deciding what is the right thing to do here. There has also been some disagreement in subsequent discussions about whether whole-state encryption is actually what's needed here, or whether encryption of specific sensitive values is actually the requirement: whole-state encry…",,,,,,Anecdotal,comment,,,,,,,,2018-09-14,github/apparentlymart,https://github.com/hashicorp/terraform/issues/9556#issuecomment-421421504,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"Hey, has there been any progress on this issue? We've been using the Consul backend and would like to start integrating our Terraform workflows with Vault, but leaking secrets to the unencrypted Consul backend more or less makes this a no-go.",,,,,,Anecdotal,comment,,,,,,,,2019-07-12,github/eripa,https://github.com/hashicorp/terraform/issues/9556#issuecomment-510950273,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"Thanks for the detailed proposal @apparentlymart, I know this has been open a while. I have a few things to add that may be of some use. Restricting access to cloud storage of choice is probably the best way to get at rest encryption and RBAC controls. There are still situations where bring-your-own-key still makes sense (e.g. non-cloud usage or app.terraform.io). Also, some companies want KEK support on all storage or on highly sensitive storage (which TF state is). A solution could leverage t…",,,,,,Anecdotal,comment,,,,,,,,2019-10-16,github/mbrancato,https://github.com/hashicorp/terraform/issues/9556#issuecomment-542496389,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"The problem with at-rest encryption provided by some cloud provider storage services like Azure Storage Accounts is that it is transparent encryption. When the encryption/decryption is transparently handled by the cloud provider it really just becomes a proxy for RBAC. While transparent at-rest encryption provides protection against some threats/risks (eg hard drive is stolen or lost from the cloud provider's facility), it provides no protection against much more common threats (eg trusted oper…",,,,,,Anecdotal,comment,,,,,,,,2019-10-16,github/jamesrcounts,https://github.com/hashicorp/terraform/issues/9556#issuecomment-542694978,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"Given that a lot of my focus recently has been on compliance, building in the ability to encrypt and decrypt the tfstate with a given kms key say, doesn't seem like it would be a lot of work and massively reduce the need for wrapper scripts. However, I do see a need for a user to be able to decrypt this state file manually in order to perform state surgery. I'd love to see this feature in terraform.",,,,,,Anecdotal,comment,,,,,,,,2020-02-07,github/sasq31,https://github.com/hashicorp/terraform/issues/9556#issuecomment-583381613,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"I agree with @jamesrcounts. I would essentially want a good way for the entire tfstate file to be an encrypted blob, because storing inside an S3 bucket isn't enough. The S3 bucket could be compromised by a phishing attack or a malicious insider. Essentially having to work around the unencrypted blob through permission controls. I know https://github.com/opencredo/terrahelp exists, that kind of, makes the encryption of the tfstate a 2 step thing, but I don't see why it couldn't be combined with…",,,,,,Anecdotal,comment,,,,,,,,2020-02-28,github/michaelasper,https://github.com/hashicorp/terraform/issues/9556#issuecomment-592663237,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"I have implemented a possible solution that should be able to transparently encrypt state for all remote storage backend providers, see draft PR https://github.com/hashicorp/terraform/pull/28278. Successfully tested it with the azurerm backend today.",,,,,,Anecdotal,comment,,,,,,,,2021-04-06,github/StephanHCB,https://github.com/hashicorp/terraform/issues/9556#issuecomment-814252558,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"> I have implemented a possible solution that should be able to transparently encrypt state for all remote storage backend providers, see draft PR #28278. Successfully tested it with the azurerm backend today. @StephanHCB nice work!",,,,,,Anecdotal,comment,,,,,,,,2021-04-06,github/binlab,https://github.com/hashicorp/terraform/issues/9556#issuecomment-814388705,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
Following up on the discussion on https://github.com/hashicorp/terraform/pull/21558 and a suggestion by @allantargino: **proposal how to implement client side remote state encryption** (with prototype code) The central element of the proposal is the introduction of a **state crypto provider** that transparently encrypts the statefile contents before they are sent to the remote backend. ``` // StateCryptoProvider is the interface that must be implemented for a transparent remote state // encrypt…,,,,,,Anecdotal,comment,,,,,,,,2021-04-25,github/StephanHCB,https://github.com/hashicorp/terraform/issues/9556#issuecomment-826341937,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"I am working on a project for a large financial institution and we do use Terraform. However, everytime when it comes to security meetings, I have a hard time explaining to them that Terraform does not come with Terraform statefile encryption. Even though we can use Azure Storage encryption to bypass this problem, it only migrates the risk to Azure RBAC as @jamesrcounts put it. It comes to me as a surprise that statefile encryption has not been prioritised before, even though enterprise clients…",,,,,,Anecdotal,comment,,,,,,,,2021-08-04,github/byteknacker,https://github.com/hashicorp/terraform/issues/9556#issuecomment-892400844,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
Any news on my pull request that implements remote state encryption? https://github.com/hashicorp/terraform/pull/28603 @jbardin you joined the PR. Is there something I am supposed to be doing/can do? It's now been over two months of no reaction from maintainers.,,,,,,Anecdotal,comment,,,,,,,,2021-08-06,github/StephanHCB,https://github.com/hashicorp/terraform/issues/9556#issuecomment-894193929,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"@StephanHCB I love your PR, it's far better than what I could have come up with. I've Approved it, but I'm not sure if that's at all meaningful, since I'm not a maintainer.",,,,,,Anecdotal,comment,,,,,,,,2021-08-16,github/maludwig,https://github.com/hashicorp/terraform/issues/9556#issuecomment-899730022,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"As @stumyp suggested a nice JSON encryption popular project exist [Mozilla SOPS](https://github.com/mozilla/sops) allowing encryption of defined values leaving tfstate json syntactically untached for backends. SOPS is a go language open source project supporting encryption using AWS KMS, GCP KMS, Azure Key Vault, age, and PGP (and at the same time more than one to make decryption robust). SOPS is already integrated with Terraform as resource provider ([sops Provider](https://registry.terraform.…",,,,,,Anecdotal,comment,,,,,,,,2022-01-13,github/Roxyrob,https://github.com/hashicorp/terraform/issues/9556#issuecomment-1011607151,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"@Roxyrob I think you make a good point. But I don't think it matters much because if I were to venture a guess I would say that in Hashicorp's philosophy they ""trust"" cloud providers. To a degree I can understand that point of view; If you are already running everything in their cloud (VM's, etc.) encrypting your statefile before uploading it isn't going to give you that much extra security. So from their point of view it might help against such one of a kind Amazon incident but other then that…",,,,,,Anecdotal,comment,,,,,,,,2022-01-13,github/siepkes,https://github.com/hashicorp/terraform/issues/9556#issuecomment-1011927157,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"@siepkes I undertand what you suppose but I think data security is primary concern for every tools and Terraform is a so great one that cannot neglet also if there isn't a simple implementation solution. Probably I'm wrong but IMHO any piece of data especially if sensible by nature as many data in tfstate are or potentially can be, should be managed following `security by design` and `security by default` principles. Data Security cannot be leaved to external factors. Without this basic assumpt…",,,,,,Anecdotal,comment,,,,,,,,2022-01-13,github/Roxyrob,https://github.com/hashicorp/terraform/issues/9556#issuecomment-1012167517,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"The issue is that ALL of these are making a ""copy"" of credentials. That is just wrong right out of the gate. Especially when it's using something like vault - it should be storing a reference to the credentials to be loaded when applied. If Terraform were operating as a client-server type implementation, where the state had to be independently accessed by a service on some other box in order to apply the state - then yeah, I could understand the current behavior -- but it's being 100% driven by…",,,,,,Anecdotal,comment,,,,,,,,2022-01-13,github/nneul,https://github.com/hashicorp/terraform/issues/9556#issuecomment-1012193394,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"@FernandoMiguel consul or every other solution does not change the context. Tfstate is always in cleartext somewhere, and someone can access the file and so secrets inside (at least if you do not take all on a server in a private room detached from networks and always watched). Sops like logic instead allow you to save JSON file (and so potentially tfstate json too) only with values (all or some) encrypted using e.g. AWS KMS CMK. Such an approach increse security (and probably sufficient risk m…",,,,,,Anecdotal,comment,,,,,,,,2022-01-13,github/Roxyrob,https://github.com/hashicorp/terraform/issues/9556#issuecomment-1012271024,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"@nneul a tfstate cleartext problem mitigation can be reached if we do not undermining probably basic principles of Terraform behavior and also: having encrypted value in tfatste file (or in other better solution for that purpose - like consul, and so on) can be potencially useful (to share between different terraform configs or different DevOps tools in the pipeline). I think that in cloud era we cannot avoid that secrets can be ""a little out of control"" (e.g. we will never have total control o…",,,,,,Anecdotal,comment,,,,,,,,2022-01-13,github/Roxyrob,https://github.com/hashicorp/terraform/issues/9556#issuecomment-1012287825,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"Frankly, I'm quite shocked and surprised that Hashicorp haven't placed a higher priority on providing a mechanism for encryption of the tfstate at rest. In many organisations, these state files contain the 'keys to the kingdom' and a comprehensive map of their infrastructure and should be considered highly sensitive. Knowing that most large companies use Terraform, and that there's a good chance that many/most will use the Amazon S3 backend, if I were intent on compromising one of them I would …",,,,,,Anecdotal,comment,,,,,,,,2022-01-15,github/rossigee,https://github.com/hashicorp/terraform/issues/9556#issuecomment-1013569833,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"Add Warning for Deprecated Modules in `Init` (Second Revision) [Jira Ticket](https://hashicorp.atlassian.net/browse/TF-12601?atlOrigin=eyJpIjoiZGJlMWU1MzZiMWQ1NGRjNjkzNTQzOTY1MjI4NjA4YTQiLCJwIjoiaiJ9) ## Target Release 1.9.x ## Draft CHANGELOG entry ### ENHANCEMENTS - `terraform init`: Displays warnings when modules marked as deprecated are in use. ## Description Module deprecations warnings are surfaced upon running `terraform init`. This includes both cases of where the module does, or does n…",,,,,,Anecdotal,issue,,,,,,,,2024-05-22,github/Maed223,https://github.com/hashicorp/terraform/pull/35231,repo: hashicorp/terraform | keyword: gotcha | state: closed
"@uturunku1 > * When there is multiple diagnostic warnings (that include deprecation and non-deprecation warnings), all the deprecation warnings show up intertwined with non-deprecation warnings, like: deprecation diagnostic, then non-deprecation diagnostic, another deprecation diagnostic, another non-deprecation diagnostic, etc. I kind of get why that's happening. But I wonder if there is any way that we could line up all the deprecation diagnostics, one after the other, rather than having them…",,,,,,Anecdotal,comment,,,,,,,,2024-05-23,github/Maed223,https://github.com/hashicorp/terraform/pull/35231#issuecomment-2127333975,repo: hashicorp/terraform | issue: Add Warning for Deprecated Modules in `Init` (Second Revision) | keyword: gotcha
"@uturunku1 @Maed223 Even though we can't be explicit about the name of a module that contains a deprecation, the source code location that was added gives a hint: ╷ │ Warning: Module version 1.0.0 of A is deprecated │ │ on .terraform/modules/example-module/main.tf line 1: │ 1: module ""A"" { │ ╵ This warning shows that module A is a dependency of ""example-module""",,,,,,Anecdotal,comment,,,,,,,,2024-05-23,github/brandonc,https://github.com/hashicorp/terraform/pull/35231#issuecomment-2127507139,repo: hashicorp/terraform | issue: Add Warning for Deprecated Modules in `Init` (Second Revision) | keyword: gotcha
"@uturunku1 @Maed223 As far as the interlaced warnings go, they ultimately end up being grouped by the module in question. You may see a deprecation warning for a module followed by the warnings generated by that specific module. Seems fine.",,,,,,Anecdotal,comment,,,,,,,,2024-05-23,github/brandonc,https://github.com/hashicorp/terraform/pull/35231#issuecomment-2127511970,repo: hashicorp/terraform | issue: Add Warning for Deprecated Modules in `Init` (Second Revision) | keyword: gotcha
"@Maed223 Adjusting the ordering of Diagnostics: Thanks! I appreciate if you give it a quick try. This is non-blocking, so if you quickly realize that a solution for this would be complex, then no problem, we'll move forward with what we have. Denoting Nested Dependency Deprecations: Got it, thanks for clarifying! Though the line code reference in the diagnostics may force the user to think a little before they realize is a non root module, is at least a way to help them infer the origin of the …",,,,,,Anecdotal,comment,,,,,,,,2024-05-23,github/uturunku1,https://github.com/hashicorp/terraform/pull/35231#issuecomment-2127806802,repo: hashicorp/terraform | issue: Add Warning for Deprecated Modules in `Init` (Second Revision) | keyword: gotcha
"Reminder for the merging maintainer: if this is a user-visible change, please update the changelog on the appropriate release branch.",,,,,,Anecdotal,comment,,,,,,,,2024-05-24,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/35231#issuecomment-2128284866,repo: hashicorp/terraform | issue: Add Warning for Deprecated Modules in `Init` (Second Revision) | keyword: gotcha
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2024-06-24,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/35231#issuecomment-2185455416,repo: hashicorp/terraform | issue: Add Warning for Deprecated Modules in `Init` (Second Revision) | keyword: gotcha
Add Warning for Deprecated Modules in `Init` [Jira Ticket](https://hashicorp.atlassian.net/browse/TF-12601?atlOrigin=eyJpIjoiZGJlMWU1MzZiMWQ1NGRjNjkzNTQzOTY1MjI4NjA4YTQiLCJwIjoiaiJ9) ## Target Release 1.8.x ## Draft CHANGELOG entry ### ENHANCEMENTS - `terraform init`: Displays a warning when deprecated modules are in use. ## Description All module deprecation info is consolidated into and displayed as a single warning upon running `terraform init`. This includes both cases of where the module d…,,,,,,Anecdotal,issue,,,,,,,,2024-04-03,github/Maed223,https://github.com/hashicorp/terraform/pull/34943,repo: hashicorp/terraform | keyword: gotcha | state: closed
"@nfagerlund– just to address some of your points ✍️ **TFE Version of this warning** - The json output for init was added by @Uk1288 [in this PR](https://github.com/hashicorp/terraform/pull/34886), is that what you are referring to with ""json log format""? Otherwise [we have ongoing work in the agent](https://github.com/hashicorp/tfc-agent/pull/677) to bring it all together. **Passing deprecations as a return value separate from `configs.Config`** - `configs.Config` seemed to me to represent info…",,,,,,Anecdotal,comment,,,,,,,,2024-05-10,github/Maed223,https://github.com/hashicorp/terraform/pull/34943#issuecomment-2104916741,repo: hashicorp/terraform | issue: Add Warning for Deprecated Modules in `Init` | keyword: gotcha
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2024-06-22,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/34943#issuecomment-2183697082,repo: hashicorp/terraform | issue: Add Warning for Deprecated Modules in `Init` | keyword: gotcha
"terraform init -json This PR makes changes to allow the support of json output for terraform init cmd. The output would be similar to that generated by terraform plan -json. New command option: `terraform init -json` **NOTE**: since terraform cannot ask for interactive approval when -json is set, an error is returned if both -migrate-state and -json options are set ## Target Release <!-- In normal circumstances we only target changes at the upcoming minor release, or as a patch to the current m…",,,,,,Anecdotal,issue,,,,,,,,2024-03-26,github/Uk1288,https://github.com/hashicorp/terraform/pull/34886,repo: hashicorp/terraform | keyword: gotcha | state: closed
"Just curious, how did you get that output shown in your ""JSON View (terraform init -json) - with error"" screenshot? When I reproduce that same error, I get a different output: <img width=""1035"" alt=""Screenshot 2024-04-01 at 9 17 48 PM"" src=""https://github.com/hashicorp/terraform/assets/21225410/fc2f0a1f-fc60-437c-bd3f-a80fac2bb07b"">",,,,,,Anecdotal,comment,,,,,,,,2024-04-02,github/uturunku1,https://github.com/hashicorp/terraform/pull/34886#issuecomment-2031053671,repo: hashicorp/terraform | issue: terraform init -json | keyword: gotcha
"> Just curious, how did you get that output shown in your ""JSON View (terraform init -json) - with error"" screenshot? When I reproduce that same error, I get a different output: > > <img alt=""Screenshot 2024-04-01 at 9 17 48 PM"" width=""1035"" src=""https://private-user-images.githubusercontent.com/21225410/318642679-fc2f0a1f-fc60-437c-bd3f-a80fac2bb07b.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3M…",,,,,,Anecdotal,comment,,,,,,,,2024-04-02,github/Uk1288,https://github.com/hashicorp/terraform/pull/34886#issuecomment-2032776977,repo: hashicorp/terraform | issue: terraform init -json | keyword: gotcha
"@Uk1288 I don't think you were able to upload the screenshot you intended to. But , anyways, I also had added both cloud and backend config initializers in the same main.tf file, and yet I got a different output than yours 😄 . That's why I was confused.",,,,,,Anecdotal,comment,,,,,,,,2024-04-04,github/uturunku1,https://github.com/hashicorp/terraform/pull/34886#issuecomment-2037976904,repo: hashicorp/terraform | issue: terraform init -json | keyword: gotcha
"Reminder for the merging maintainer: if this is a user-visible change, please update the changelog on the appropriate release branch.",,,,,,Anecdotal,comment,,,,,,,,2024-04-17,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/34886#issuecomment-2061846606,repo: hashicorp/terraform | issue: terraform init -json | keyword: gotcha
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2024-05-18,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/34886#issuecomment-2118589202,repo: hashicorp/terraform | issue: terraform init -json | keyword: gotcha
"`terraform test` Consistently Crashing ### Terraform Version ```shell Terraform v1.7.0 on darwin_arm64 + provider registry.terraform.io/hashicorp/aws v5.34.0 + provider registry.terraform.io/hashicorp/random v3.6.0 ``` ### Terraform Configuration Files **main.tftest.hcl** ```terraform # Instructions: Create basic tests # Configure the AWS provider ""aws"" { region = ""us-east-1"" } # Global Testing Variables - Define variables to be used in all tests here. You can overwrite these varibles by defini…",,,,,,Anecdotal,issue,,,,,,,,2024-01-28,github/novekm,https://github.com/hashicorp/terraform/issues/34584,repo: hashicorp/terraform | keyword: gotcha | state: closed
"For additional context, as shown in the config I am using `locals` that are defined in a `locals.tf` file that's in the same `tests` directory as `main.tftest.hcl`. Here's the tree: ``` ├── backend.tf ├── codebuild.tf ├── codecommit.tf ├── codepipeline.tf ├── data.tf ├── iam.tf ├── locals.tf ├── s3.tf ├── tests │ ├── README.md │ ├── core.tftest.hcl │ ├── locals.tf │ └── main.tftest.hcl └── variables.tf ```",,,,,,Anecdotal,comment,,,,,,,,2024-01-28,github/novekm,https://github.com/hashicorp/terraform/issues/34584#issuecomment-1913417173,repo: hashicorp/terraform | issue: `terraform test` Consistently Crashing | keyword: gotcha
"More info - if I replace all the locals with hardcoded strings (e.g. `""my-repo""` instead of `local.test_workload_1_repository_name`, it doesn't crash. Are locals not supported, or am I referencing them incorrectly?",,,,,,Anecdotal,comment,,,,,,,,2024-01-28,github/novekm,https://github.com/hashicorp/terraform/issues/34584#issuecomment-1913430431,repo: hashicorp/terraform | issue: `terraform test` Consistently Crashing | keyword: gotcha
"Hi @novekm, unfortunately you can't reference configuration locals from within the test file. The test files and their variables are evaluated outside of the Terraform context that the locals and other parts of the configuration are available within. Locals and internal variables could rely on parts of the configuration that are only available during / after a plan or apply operation, while the test file has to make all the inputs available at the beginning of each operation, so that kind of wo…",,,,,,Anecdotal,comment,,,,,,,,2024-01-29,github/liamcervante,https://github.com/hashicorp/terraform/issues/34584#issuecomment-1915072783,repo: hashicorp/terraform | issue: `terraform test` Consistently Crashing | keyword: gotcha
> locals that are defined in a locals.tf file that's in the same tests directory as main.tftest.hcl. Terraform test would pick up variables defined within a `terraform.tfvars` [variable file](https://developer.hashicorp.com/terraform/language/values/variables#variable-definitions-tfvars-files) defined within the testing directory rather than a normal `.tf` file. Unfortunately you can't reference other variables from the global variables block yet. That is the feature request from the issue you'…,,,,,,Anecdotal,comment,,,,,,,,2024-01-29,github/liamcervante,https://github.com/hashicorp/terraform/issues/34584#issuecomment-1915080970,repo: hashicorp/terraform | issue: `terraform test` Consistently Crashing | keyword: gotcha
"Ok gotcha, thank you! So to recap - variables are fine as long as they're in a `terraform.tfvars` variable file that resides in the `tests` directory, but `locals.tf` in the same directory will not work (current feature request you mentioned?). Or, do you mean the feature request is to add support to reference say `variables.tf`, `dev.auto.tfvars`, `prod.auto.tfvars`, from the global variables block? I'm creating an AWS workshop about how to use this new feature of Terraform within the context …",,,,,,Anecdotal,comment,,,,,,,,2024-01-29,github/novekm,https://github.com/hashicorp/terraform/issues/34584#issuecomment-1915555180,repo: hashicorp/terraform | issue: `terraform test` Consistently Crashing | keyword: gotcha
"I'll try and summarise things a bit more verbosely here. Hopefully it's helpful! Referencing `local` blocks from within variable definitions within Terraform test files will never be supported. Terraform will not load `.tf` files during the initialisation of a test file, instead the test files are initialised first with all their data loaded and then provided into regular Terraform files during the execution of a `run` block. `terraform test` can already load variables defined directly in `.tfv…",,,,,,Anecdotal,comment,,,,,,,,2024-01-30,github/liamcervante,https://github.com/hashicorp/terraform/issues/34584#issuecomment-1916555416,repo: hashicorp/terraform | issue: `terraform test` Consistently Crashing | keyword: gotcha
"Hi @liamcervante sorry the delay. Yes, I have tested in v1.7.2 and this is now resolved. It throws a correct error mentioning that the local cannot be used: ``` │ Error: Variables not allowed │ │ on tests/main.tftest.hcl line 75, in variables: │ 75: name = local.tf_module_validation_module_aws_tf_cicd_codepipeline_pipeline_name │ │ Variables may not be used here. ╵ run ""e2e_test""... skip tests/main.tftest.hcl... tearing down tests/main.tftest.hcl... fail ``` Thank you so much for all of the hel…",,,,,,Anecdotal,comment,,,,,,,,2024-02-01,github/novekm,https://github.com/hashicorp/terraform/issues/34584#issuecomment-1921739840,repo: hashicorp/terraform | issue: `terraform test` Consistently Crashing | keyword: gotcha
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2024-03-03,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/34584#issuecomment-1974978514,repo: hashicorp/terraform | issue: `terraform test` Consistently Crashing | keyword: gotcha
"Switch from golang.org/x/crypto/openpgp to github.com/ProtonMail/go-crypto This replaces `golang.org/x/crypto/openpgp` with `github.com/ProtonMail/go-crypto`, since the former has been deprecated (see https://github.com/golang/go/issues/44226), and the latter is a (mostly) backwards-compatible fork. I'm not a cryptographer and haven't audited [`github.com/ProtonMail/go-crypto`](https://github.com/ProtonMail/go-crypto) for correctness, but it's already used in several other HashiCorp projects (h…",,,,,,Anecdotal,issue,,,,,,,,2022-10-20,github/MasonM,https://github.com/hashicorp/terraform/pull/32056,repo: hashicorp/terraform | keyword: gotcha | state: closed
[![CLA assistant check](https://cla.hashicorp.com/pull/badge/signed)](https://cla.hashicorp.com/hashicorp/terraform?pullRequest=32056) <br/>All committers have signed the CLA.,,,,,,Anecdotal,comment,,,,,,,,2022-10-20,github/hashicorp-cla,https://github.com/hashicorp/terraform/pull/32056#issuecomment-1286202044,repo: hashicorp/terraform | issue: Switch from golang.org/x/crypto/openpgp to github.com/ProtonMail/go-crypto | keyword: gotcha
"@MasonM this was on the list to triage in October and got missed - human error, my apologies. I will bring this back into the queue and also run this past our security team. Thanks!",,,,,,Anecdotal,comment,,,,,,,,2023-01-03,github/crw,https://github.com/hashicorp/terraform/pull/32056#issuecomment-1370053166,repo: hashicorp/terraform | issue: Switch from golang.org/x/crypto/openpgp to github.com/ProtonMail/go-crypto | keyword: gotcha
"@MasonM, the feedback from the security team is as follows. Although there are no strong objections to using the `ProtonMail/go-crypto`, there are also no strong motivators to make that change at this time. Adding support for ECC introduces an additional ""attack surface,"" meaning if it is not a product requirement, the balance of caution leads us to not make a change. Do you have a specific use case for the ECC key feature being used with private registries? If so, I can make the ""product requi…",,,,,,Anecdotal,comment,,,,,,,,2023-01-25,github/crw,https://github.com/hashicorp/terraform/pull/32056#issuecomment-1402906963,repo: hashicorp/terraform | issue: Switch from golang.org/x/crypto/openpgp to github.com/ProtonMail/go-crypto | keyword: gotcha
"@crw Thanks for getting back to me. The use case I have is integrating with a private Terraform registry at my workplace. The registry uses a custom implementation of the [Provider Registry Protocol](https://developer.hashicorp.com/terraform/internals/provider-registry-protocol), similar to https://github.com/outsideris/citizen. The registry protocol requires a GPG signing key, but it doesn't specify it has to be an RSA key. So, when I went to publish a provider to the registry, I used an ECC k…",,,,,,Anecdotal,comment,,,,,,,,2023-01-25,github/MasonM,https://github.com/hashicorp/terraform/pull/32056#issuecomment-1404075722,repo: hashicorp/terraform | issue: Switch from golang.org/x/crypto/openpgp to github.com/ProtonMail/go-crypto | keyword: gotcha
"There was a duplicate PR for this entered last week: https://github.com/hashicorp/terraform/pull/33131 The one difference is that instead of removing the `TestNewSignatureAuthentication_success` test, it mocks out the clock to avoid signature expiration issues. Personally, I think it's better to just delete it, since it's duplicating `TestSignatureAuthentication_success`, but I'm happy to copy over that to this PR if anyone disagrees.",,,,,,Anecdotal,comment,,,,,,,,2023-05-10,github/MasonM,https://github.com/hashicorp/terraform/pull/32056#issuecomment-1542532857,repo: hashicorp/terraform | issue: Switch from golang.org/x/crypto/openpgp to github.com/ProtonMail/go-crypto | keyword: gotcha
"@crw I know your Security team has already weighed in on this (per https://github.com/hashicorp/terraform/pull/32056#issuecomment-1402906963), but does the additional context provided in https://github.com/hashicorp/terraform/pull/33131#issuecomment-1531749336 move the needle at all? We (the Go Security team) would really like to see people moving away from golang.org/x/crypto/openpgp for the good of the ecosystem, but also for selfish reasons as we are forced to carry internal patches for this.",,,,,,Anecdotal,comment,,,,,,,,2023-06-20,github/rolandshoemaker,https://github.com/hashicorp/terraform/pull/32056#issuecomment-1599215968,repo: hashicorp/terraform | issue: Switch from golang.org/x/crypto/openpgp to github.com/ProtonMail/go-crypto | keyword: gotcha
"Hi @rolandshoemaker, thanks for that additional context. > but also for selfish reasons as we are forced to carry internal patches for this. This is also important context. I'll re-raise with the team.",,,,,,Anecdotal,comment,,,,,,,,2023-06-20,github/crw,https://github.com/hashicorp/terraform/pull/32056#issuecomment-1599356048,repo: hashicorp/terraform | issue: Switch from golang.org/x/crypto/openpgp to github.com/ProtonMail/go-crypto | keyword: gotcha
"Hi @MasonM, would you mind pulling the latest from main and resolving the conflicts in go.mod and go.sum. I will then look into merging this!",,,,,,Anecdotal,comment,,,,,,,,2023-06-21,github/liamcervante,https://github.com/hashicorp/terraform/pull/32056#issuecomment-1600829720,repo: hashicorp/terraform | issue: Switch from golang.org/x/crypto/openpgp to github.com/ProtonMail/go-crypto | keyword: gotcha
"Actually looking at the two PRs, I think it's easier for me to create a new one that merges both together as both carry parts that I think are useful. I've raised #33406 with that in mind, and we'll close everything out in there. Thanks for raising this with us, we really do appreciate the support!",,,,,,Anecdotal,comment,,,,,,,,2023-06-21,github/liamcervante,https://github.com/hashicorp/terraform/pull/32056#issuecomment-1601002013,repo: hashicorp/terraform | issue: Switch from golang.org/x/crypto/openpgp to github.com/ProtonMail/go-crypto | keyword: gotcha
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2023-12-14,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/32056#issuecomment-1854995579,repo: hashicorp/terraform | issue: Switch from golang.org/x/crypto/openpgp to github.com/ProtonMail/go-crypto | keyword: gotcha
"Error: key must not start with '/' (Error should print key) I will happily commit some code when i can, but this block of code, should emit the key that its failing on allowing a person to search source code. I'm in the process of a migration from terragrunt to terraform, which should not matter other than that is how i found it. Since it was not used terragrunt may have done something with the `data ""terraform_remote_state""` block that was not called anywhere. Not sure, but adding the key to t…",,,,,,Anecdotal,issue,,,,,,,,2021-03-10,github/jeffdyke,https://github.com/hashicorp/terraform/issues/28031,repo: hashicorp/terraform | keyword: gotcha | state: closed
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2023-12-10,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/28031#issuecomment-1848834382,repo: hashicorp/terraform | issue: Error: key must not start with '/' (Error should print key) | keyword: gotcha
"Allow providers defined in test files to reference variables and other providers ### Terraform Version ```shell Terraform v1.6.0 ``` ### Use Cases In a regular Terraform file it is possible to write something like this: ```hcl provider ""vault"" {} data ""vault_generic_secret"" ""aws_creds"" { path = ""some/path/to/secret"" } provider ""aws"" { access_key_id = data.vault_generic_secret.vault_secrets_automation.data[""aws_access_key_id""] secret_access_key = data.vault_generic_secret.vault_secrets_automatio…",,,,,,Anecdotal,issue,,,,,,,,2023-10-06,github/liamcervante,https://github.com/hashicorp/terraform/issues/34007,repo: hashicorp/terraform | keyword: gotcha | state: closed
"Will this also permit referencing resource attributes or module outputs in provider definitions, or only input variables? My primary use case for `terraform test` will be running fully self-contained local integration tests for my infrastructure, where provider configuration values (for the module under test) may depend on outputs of a module or resource (from the local mock infrastructure). For example, say I have a Terraform module that deploys an AWS EC2 instance running an application, and …",,,,,,Anecdotal,comment,,,,,,,,2023-10-16,github/zanecodes,https://github.com/hashicorp/terraform/issues/34007#issuecomment-1764950177,repo: hashicorp/terraform | issue: Allow providers defined in test files to reference variables and other providers | keyword: gotcha
"Hi @zanecodes, the original scope of the this ticket was to include variables and data blocks only in the provider configurations. Currently we can still just initialise the providers at the start of the test file. Your proposal would require us to interleave the run blocks with the providers to work out a run ordering. It's not impossible, but it increases the complexity more than something I think we should do as part of this ticket. As such, I'll file a separate issue to track this as an ext…",,,,,,Anecdotal,comment,,,,,,,,2023-10-17,github/liamcervante,https://github.com/hashicorp/terraform/issues/34007#issuecomment-1765842585,repo: hashicorp/terraform | issue: Allow providers defined in test files to reference variables and other providers | keyword: gotcha
"Gotcha, I was mostly just curious about the scope of this ticket and whether the merged pull request would implement said functionality. This feature will already be extremely useful, and I understand the interleaved use case is a bit unusual. Much appreciated!",,,,,,Anecdotal,comment,,,,,,,,2023-10-17,github/zanecodes,https://github.com/hashicorp/terraform/issues/34007#issuecomment-1766689116,repo: hashicorp/terraform | issue: Allow providers defined in test files to reference variables and other providers | keyword: gotcha
"**Call out for testing!** We just released a alpha for [v1.7](https://github.com/hashicorp/terraform/releases/tag/v1.7.0-alpha20231108) yesterday. In the alpha you can reference variables and outputs from run blocks in your provider configurations. This should allow you to create a setup module that uses one provider, load credentials or configuration you need for a second provider from the first provider, and then pass the required data into the second provider. For example: ```hcl # testing/s…",,,,,,Anecdotal,comment,,,,,,,,2023-11-09,github/liamcervante,https://github.com/hashicorp/terraform/issues/34007#issuecomment-1803337608,repo: hashicorp/terraform | issue: Allow providers defined in test files to reference variables and other providers | keyword: gotcha
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2023-12-10,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/34007#issuecomment-1848833948,repo: hashicorp/terraform | issue: Allow providers defined in test files to reference variables and other providers | keyword: gotcha
"AWS Verified Access Resources created on my Default account ### Terraform Version ```shell Terraform v1.5.7 on linux_amd64 + provider registry.terraform.io/hashicorp/archive v2.4.0 + provider registry.terraform.io/hashicorp/aws v4.45.0 + provider registry.terraform.io/hashicorp/awscc v0.61.0 ``` ### Terraform Configuration Files ``` terraform { required_providers { aws = { source = ""hashicorp/aws"" version = ""4.45.0"" } } provider ""aws"" { profile = ""corp_superadmin"" region = ""us-east-1"" } resourc…",,,,,,Anecdotal,issue,,,,,,,,2023-10-03,github/logansidwell,https://github.com/hashicorp/terraform/issues/33968,repo: hashicorp/terraform | keyword: gotcha | state: closed
"Hello @logansidwell, this appears to be an issue or question with the AWS provider, not with Terraform itself. You can see existing issues and file a new one in their repository here: https://github.com/terraform-providers/terraform-provider-aws/issues. If you have questions about Terraform or the AWS provider, it's better to use the [community forum](https://discuss.hashicorp.com/c/terraform-providers/tf-aws/33) where there are more people ready to help. The GitHub issues here are monitored on…",,,,,,Anecdotal,comment,,,,,,,,2023-10-03,github/crw,https://github.com/hashicorp/terraform/issues/33968#issuecomment-1745639578,repo: hashicorp/terraform | issue: AWS Verified Access Resources created on my Default account | keyword: gotcha
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2023-12-09,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/33968#issuecomment-1848087486,repo: hashicorp/terraform | issue: AWS Verified Access Resources created on my Default account | keyword: gotcha
"Unable to add entire variable to dynamic content block I am attempting to set dynamic `lifecycle_rules` for an s3 bucket. Most attributes are optional. ### Terraform Version ``` Terraform v0.14.2 ``` ### Terraform Configuration Files ```terraform locals { lifecycle_rules = [ { id = ""test1"" abort_incomplete_multipart_upload_days = 7 } ] } resource ""aws_s3_bucket"" ""action_dist"" { bucket = var.distribution_bucket_name acl = ""private"" force_destroy = true tags = var.tags dynamic ""lifecycle_rule"" { …",,,,,,Anecdotal,issue,,,,,,,,2021-01-26,github/mcaulifn,https://github.com/hashicorp/terraform/issues/27600,repo: hashicorp/terraform | keyword: gotcha | state: closed
"Hi @mcaulifn, With the language as currently designed, Terraform is working as intended here: nested blocks are fixed structures with a required set of arguments, not a dynamic data structure you can just assign a value to. With that in mind, I've relabeled this as a feature request. Restating your report as a use-case, I understand it as follows: I want to be able to dynamically populate a nested block from the contents of an object value, without having to specify each of the arguments indepe…",,,,,,Anecdotal,comment,,,,,,,,2021-01-28,github/apparentlymart,https://github.com/hashicorp/terraform/issues/27600#issuecomment-769449348,repo: hashicorp/terraform | issue: Unable to add entire variable to dynamic content block | keyword: gotcha
"Not sure if this is exactly the same, but seems quite related when reading your explanation @apparentlymart . Any unknown value inside of dynamic blocks makes the entire block ""disappear"" it seems. I'm attempting to dynamically populate a security group with security group rules in one module, based on output from one or several other modules which create resources, including this security group. So right now it requires **two runs of terraform**, one to let the security group get created so th…",,,,,,Anecdotal,comment,,,,,,,,2022-05-04,github/frimik,https://github.com/hashicorp/terraform/issues/27600#issuecomment-1117958531,repo: hashicorp/terraform | issue: Unable to add entire variable to dynamic content block | keyword: gotcha
"Hi @frimik, The behavior you are describing with unknown values is not familiar to me as you've described it here. I think it would be best to start a new issue for it and complete the issue template so we can see more about what's going on there and how to reproduce it. If it does turn out to be a restatement of this feature request then we can always merge the topics back together later. (If you do create an issue, please link to this one from the issue template section that calls for referen…",,,,,,Anecdotal,comment,,,,,,,,2022-05-05,github/apparentlymart,https://github.com/hashicorp/terraform/issues/27600#issuecomment-1118547293,repo: hashicorp/terraform | issue: Unable to add entire variable to dynamic content block | keyword: gotcha
Hello! Hope you are all doing fine. I was wondering if there was any news on the implementation of this feature request? Thank you!,,,,,,Anecdotal,comment,,,,,,,,2023-05-16,github/pfif,https://github.com/hashicorp/terraform/issues/27600#issuecomment-1549337009,repo: hashicorp/terraform | issue: Unable to add entire variable to dynamic content block | keyword: gotcha
"Hi all, The current treatment of nested blocks cannot change because lots of existing provider logic relies on the guarantee that a nested block will always appear as a known value with a specific data structure. Therefore this feature request is infeasible to meet and so I'm going to close it. Newer providers written with the Terraform plugin framework have the option to ignore the nested block concept entirely and use normal arguments for everything, including objects and collections of objec…",,,,,,Anecdotal,comment,,,,,,,,2023-05-16,github/apparentlymart,https://github.com/hashicorp/terraform/issues/27600#issuecomment-1549782563,repo: hashicorp/terraform | issue: Unable to add entire variable to dynamic content block | keyword: gotcha
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2023-06-16,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/27600#issuecomment-1593984635,repo: hashicorp/terraform | issue: Unable to add entire variable to dynamic content block | keyword: gotcha
"Emit warnings for certain run events in cloud backend The cloud backend, which communicates with TFC like APIs, can create runs which may have one or more configuration parameters altered. These alterations are emitted as run-events on the run so that API clients can consume and display them to users. This commit adds a step in plan operation to query the run-events once a run is created and then emit specific run-event descriptions to the console as warnings for the user. [Jira](https://hashic…",,,,,,Anecdotal,issue,,,,,,,,2023-04-12,github/glennsarti,https://github.com/hashicorp/terraform/pull/33020,repo: hashicorp/terraform | keyword: gotcha | state: closed
"> I don't think the user would have an easy way to silence these warnings, unless they upgrade, no? Correct, but this also matches the UX on the TFC web side as well. The flip side is, if we did silence the warning then users could be confused why their policies aren't being checked or runs aren't being blocked with failed run tasks. So I guess the question is, should these be diagnostic warnings? It's hard to say, they're not really diagnostic information, they're platform messages. But I'll d…",,,,,,Anecdotal,comment,,,,,,,,2023-04-13,github/glennsarti,https://github.com/hashicorp/terraform/pull/33020#issuecomment-1506179920,repo: hashicorp/terraform | issue: Emit warnings for certain run events in cloud backend | keyword: gotcha
> Though I am wondering if there could a time when the amount of warning lines we output could get a little loud for the user. Maybe? Although these particular warning only appear when you hit an **exceptional** scenario which will be rare.,,,,,,Anecdotal,comment,,,,,,,,2023-04-13,github/glennsarti,https://github.com/hashicorp/terraform/pull/33020#issuecomment-1506506671,repo: hashicorp/terraform | issue: Emit warnings for certain run events in cloud backend | keyword: gotcha
"> > I don't think the user would have an easy way to silence these warnings, unless they upgrade, no? > > Correct, but this also matches the UX on the TFC web side as well. The flip side is, if we did silence the warning then users could be confused why their policies aren't being checked or runs aren't being blocked with failed run tasks. > > So I guess the question is, should these be diagnostic warnings? It's hard to say, they're not really diagnostic information, they're platform messages. …",,,,,,Anecdotal,comment,,,,,,,,2023-04-13,github/uturunku1,https://github.com/hashicorp/terraform/pull/33020#issuecomment-1507244372,repo: hashicorp/terraform | issue: Emit warnings for certain run events in cloud backend | keyword: gotcha
"> FYI : @uturunku1 I don't have merge rights so I can't do much once its approved :-) Gotcha! Yes, I am happy to merge this on your behalf. So you are done making changes, right? @glennsarti",,,,,,Anecdotal,comment,,,,,,,,2023-04-14,github/uturunku1,https://github.com/hashicorp/terraform/pull/33020#issuecomment-1508859563,repo: hashicorp/terraform | issue: Emit warnings for certain run events in cloud backend | keyword: gotcha
"Reminder for the merging maintainer: if this is a user-visible change, please update the changelog on the appropriate release branch.",,,,,,Anecdotal,comment,,,,,,,,2023-04-17,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/33020#issuecomment-1511639897,repo: hashicorp/terraform | issue: Emit warnings for certain run events in cloud backend | keyword: gotcha
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2023-05-18,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/33020#issuecomment-1552309223,repo: hashicorp/terraform | issue: Emit warnings for certain run events in cloud backend | keyword: gotcha
"Subtle issue representing `null` as unset value, resulting in deprecation warning ### Terraform Version ```shell ❯ terraform version Terraform v1.2.8 on linux_amd64 + provider registry.terraform.io/hashicorp/aws v4.28.0 ``` ``` ### Terraform Configuration Files ```terraform variable ""enable_classiclink"" { type = bool default = null } resource ""aws_vpc"" ""main"" { cidr_block = ""10.0.0.0/16"" enable_classiclink = var.enable_classiclink } ``` ### Debug Output not comfortable posting this ### Expected…",,,,,,Anecdotal,issue,,,,,,,,2022-09-01,github/lorengordon,https://github.com/hashicorp/terraform/issues/31730,repo: hashicorp/terraform | keyword: gotcha | state: closed
"If you set the option to `null` directly on the resource attribute, then there is no deprecation warning: ``` resource ""aws_vpc"" ""main"" { cidr_block = ""10.0.0.0/16"" enable_classiclink = null } ```",,,,,,Anecdotal,comment,,,,,,,,2022-09-01,github/lorengordon,https://github.com/hashicorp/terraform/issues/31730#issuecomment-1234733460,"repo: hashicorp/terraform | issue: Subtle issue representing `null` as unset value, resulting in deprecation warning | keyword: gotcha"
"Attempting to conditionally set `enable_classiclink` fails to work around it, and still generates the deprecation warning: ``` resource ""aws_vpc"" ""main"" { cidr_block = ""10.0.0.0/16"" enable_classiclink = var.enable_classiclink == true ? true : null } ```",,,,,,Anecdotal,comment,,,,,,,,2022-09-01,github/lorengordon,https://github.com/hashicorp/terraform/issues/31730#issuecomment-1234735461,"repo: hashicorp/terraform | issue: Subtle issue representing `null` as unset value, resulting in deprecation warning | keyword: gotcha"
"This also fails to avoid the deprecation warning: ``` variable ""enable_classiclink"" { type = bool default = false nullable = false } resource ""aws_vpc"" ""main"" { cidr_block = ""10.0.0.0/16"" enable_classiclink = var.enable_classiclink ? true : null } ```",,,,,,Anecdotal,comment,,,,,,,,2022-09-01,github/lorengordon,https://github.com/hashicorp/terraform/issues/31730#issuecomment-1234738842,"repo: hashicorp/terraform | issue: Subtle issue representing `null` as unset value, resulting in deprecation warning | keyword: gotcha"
"Hi @lorengordon, Thanks for reporting the issue. As you can see from the example where `null` is assigned directly, it's not likely a problem with the fact there is an assignment at all, rather the issue is that the value is coming from a variable which may not be known at the point of validation. This means the problem is on the provider side, rather than within Terraform core. The fact that there is a deprecation message with the warning also indicates the provider is creating the diagnostic …",,,,,,Anecdotal,comment,,,,,,,,2022-09-01,github/jbardin,https://github.com/hashicorp/terraform/issues/31730#issuecomment-1234745811,"repo: hashicorp/terraform | issue: Subtle issue representing `null` as unset value, resulting in deprecation warning | keyword: gotcha"
"I hate to at people, but I was mostly opening the issue at the prompting of @apparentlymart, and the discussion from the slack thread I linked. I'm rather confused with the explanation, as there are no dynamic or unknown values here. The `null` assignment is entirely known. Why would a known `null` value be different when coming from a variable default or user input, vs when assigned directly to a resource attribute in the config? It seems like these two things should behave exactly the same, w…",,,,,,Anecdotal,comment,,,,,,,,2022-09-01,github/lorengordon,https://github.com/hashicorp/terraform/issues/31730#issuecomment-1234784392,"repo: hashicorp/terraform | issue: Subtle issue representing `null` as unset value, resulting in deprecation warning | keyword: gotcha"
"OK, I see the confusion now. Config validation is done statically, which means variables are inherently unknown at that point since nothing has yet been evaluated. Most validation is done within Terraform core, but deprecated attributes are not handled within Terraform, those are determined by the provider, so in this case the provider can only see that the attribute has been set in the config when the unknown variable is assigned. I would say the irregularity here is that we don't currently wa…",,,,,,Anecdotal,comment,,,,,,,,2022-09-01,github/jbardin,https://github.com/hashicorp/terraform/issues/31730#issuecomment-1234849929,"repo: hashicorp/terraform | issue: Subtle issue representing `null` as unset value, resulting in deprecation warning | keyword: gotcha"
"Gotcha, that makes more sense. It would certainly be rather nice to be able to avoid the deprecation warning just by passing `null` to the argument, but having consistent behavior would help reduce confusion either way, and make it more clear what users need to do here.",,,,,,Anecdotal,comment,,,,,,,,2022-09-01,github/lorengordon,https://github.com/hashicorp/terraform/issues/31730#issuecomment-1234861740,"repo: hashicorp/terraform | issue: Subtle issue representing `null` as unset value, resulting in deprecation warning | keyword: gotcha"
"Thanks for reporting this @lorengordon, and thanks @jbardin for looking into the details. When we were discussing it (in HangOps Slack) I knew _something_ wasn't right here but I wasn't sure what, so I asked Loren to open this issue so we would have a chance to look into it. Unfortunately I've been away from code most of the afternoon so I wasn't able to look deeper immediately, but I'm glad to see the confirmation that the deprecation warning logic does indeed still live inside the provider, s…",,,,,,Anecdotal,comment,,,,,,,,2022-09-02,github/apparentlymart,https://github.com/hashicorp/terraform/issues/31730#issuecomment-1234961537,"repo: hashicorp/terraform | issue: Subtle issue representing `null` as unset value, resulting in deprecation warning | keyword: gotcha"
"There are a couple of things going on here in the SDK. Firstly, in the code posted by @apparentlymart, the conditional is the wrong way around. Currently it says that if the value is _not_ wholly known, then the deprecation warning should be shown. This mistake seems to be due to a series of refactors over the years that changed the purpose of the conditional. Secondly, even when this is corrected, later code at https://github.com/hashicorp/terraform-plugin-sdk/blob/fa8d313665945816f6eb6c79182e…",,,,,,Anecdotal,comment,,,,,,,,2022-09-02,github/kmoe,https://github.com/hashicorp/terraform/issues/31730#issuecomment-1235314880,"repo: hashicorp/terraform | issue: Subtle issue representing `null` as unset value, resulting in deprecation warning | keyword: gotcha"
"The idea of a deprecation warning in this context is not only to communicate that the attribute won't be used in the future by the provider, but also to indicate that having that attribute in the configuration will be invalid after a future release. Without the deprecation warning a user is more likely to be surprised after an update that the configuration is no longer valid at all. If we are going to remove this check from the SDKs, perhaps it's a good time to more completely move the validati…",,,,,,Anecdotal,comment,,,,,,,,2022-09-02,github/jbardin,https://github.com/hashicorp/terraform/issues/31730#issuecomment-1235495700,"repo: hashicorp/terraform | issue: Subtle issue representing `null` as unset value, resulting in deprecation warning | keyword: gotcha"
"> The idea of a deprecation warning in this context is not only to communicate that the attribute won't be used in the future by the provider, but also to indicate that having that attribute in the configuration will be invalid after a future release. Wouldn't this fall under a breaking change in the provider when the attribute is fully removed? Its not common for breaking changes in the providers to show up as warnings, but maybe I don't fully understand when something is slated to show a warn…",,,,,,Anecdotal,comment,,,,,,,,2022-09-02,github/bryantbiggs,https://github.com/hashicorp/terraform/issues/31730#issuecomment-1235578785,"repo: hashicorp/terraform | issue: Subtle issue representing `null` as unset value, resulting in deprecation warning | keyword: gotcha"
"The Terraform AWS Provider follows the [documented best practices](https://www.terraform.io/plugin/sdkv2/best-practices/deprecations#provider-attribute-removal) for attribute removal, so the removal of `enable_classiclink` will take place, as a documented breaking change, in a future (most likely the next) major version of the provider.",,,,,,Anecdotal,comment,,,,,,,,2022-09-02,github/ewbankkit,https://github.com/hashicorp/terraform/issues/31730#issuecomment-1235705765,"repo: hashicorp/terraform | issue: Subtle issue representing `null` as unset value, resulting in deprecation warning | keyword: gotcha"
"I think an important subtlety here is that this particular argument should only be unknown _during validation_. Once we get to the planning phase it will be known and so the provider will get a chance to validated it again and can return the warning there. That means that `terraform validate` alone would not raise it, but `terraform plan` (which includes an implicit validation call with more information) _would_ raise it. Although it isn't true in this case, in some cases a value isn't even kno…",,,,,,Anecdotal,comment,,,,,,,,2022-09-06,github/apparentlymart,https://github.com/hashicorp/terraform/issues/31730#issuecomment-1238303926,"repo: hashicorp/terraform | issue: Subtle issue representing `null` as unset value, resulting in deprecation warning | keyword: gotcha"
"> but it's less helpful to bother every user of the module about it because they typically aren't empowered to respond to the warning anyway. It also means that the shared module must make a breaking change in advance of the provider making the breaking change in order to quiet the warning This is essential the crux of the issue. Users do not full understand what is controlled at the module level vs the provider level vs Terraform core, but the module is typically what they interact with direct…",,,,,,Anecdotal,comment,,,,,,,,2022-09-06,github/bryantbiggs,https://github.com/hashicorp/terraform/issues/31730#issuecomment-1238385037,"repo: hashicorp/terraform | issue: Subtle issue representing `null` as unset value, resulting in deprecation warning | keyword: gotcha"
"This is a tricky situation where we have a specific example of a problem at hand but any change we make to address it must necessarily be a _general_ change that would affect all providers, because the deprecation mechanism belongs to the SDK rather than to the provider's own logic. My personal take here is that the appropriate compromise is to change this validation rule to behave the same as all other validation rules (skipping emitting any errors or warnings for the unknown value) even thoug…",,,,,,Anecdotal,comment,,,,,,,,2022-09-06,github/apparentlymart,https://github.com/hashicorp/terraform/issues/31730#issuecomment-1238442900,"repo: hashicorp/terraform | issue: Subtle issue representing `null` as unset value, resulting in deprecation warning | keyword: gotcha"
">End-users of a module do not need (or want) to see warnings or errors related to parts of a module they are not currently using, and more generally I think do not want to see warnings that they cannot directly act upon. In this specific case, that means that only users who have explicitly used the enable_classiclink variable would see the warnings related to it, and they can manage their own migration away from this deprecated feature by treating it as if it were a deprecated feature of the mo…",,,,,,Anecdotal,comment,,,,,,,,2022-09-06,github/lorengordon,https://github.com/hashicorp/terraform/issues/31730#issuecomment-1238521775,"repo: hashicorp/terraform | issue: Subtle issue representing `null` as unset value, resulting in deprecation warning | keyword: gotcha"
"terraform-plugin-sdk v2.22.0 and terraform-plugin-framework v0.12.0, when released, will skip the deprecation warning diagnostic on unknown values. Practitioners will receive this behavior when providers with deprecated attributes update to those sdk/framework releases, cut their own release, and practitioners upgrade to that provider release with the sdk/framework dependency update.",,,,,,Anecdotal,comment,,,,,,,,2022-09-06,github/bflad,https://github.com/hashicorp/terraform/issues/31730#issuecomment-1238549069,"repo: hashicorp/terraform | issue: Subtle issue representing `null` as unset value, resulting in deprecation warning | keyword: gotcha"
"From @bflad's comment it seems that there isn't really anything left to do in this repository in response to this issue. Because each individual provider team will need to separately upgrade to a newer SDK version and make a release before this will take effect I cannot promise any particular time this will be addressed in general, but I can at least see that the latest release of `hashicorp/aws` depends on SDK v2.22.0 and should therefore have the aforementioned fix, and thus the specific argu…",,,,,,Anecdotal,comment,,,,,,,,2022-09-28,github/apparentlymart,https://github.com/hashicorp/terraform/issues/31730#issuecomment-1260252683,"repo: hashicorp/terraform | issue: Subtle issue representing `null` as unset value, resulting in deprecation warning | keyword: gotcha"
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2022-10-28,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/31730#issuecomment-1294372525,"repo: hashicorp/terraform | issue: Subtle issue representing `null` as unset value, resulting in deprecation warning | keyword: gotcha"
"Document config in plan file for http backend This attempts to document the storage of backend configuration in plan files when the config was provided with either `-backend-config` or directly written in HCL. This only changes the docs for the HTTP backend which is where [I discovered this when using gitlab managed Terraform state](https://gitlab.com/gitlab-org/gitlab/-/issues/338482), but a more global location may be better suited such as the backend overview page or a page on the plan comma…",,,,,,Anecdotal,issue,,,,,,,,2021-09-03,github/njdart,https://github.com/hashicorp/terraform/pull/29519,repo: hashicorp/terraform | keyword: gotcha | state: closed
[![CLA assistant check](https://cla.hashicorp.com/pull/badge/signed)](https://cla.hashicorp.com/hashicorp/terraform?pullRequest=29519) <br/>All committers have signed the CLA.,,,,,,Anecdotal,comment,,,,,,,,2021-09-03,github/hashicorp-cla,https://github.com/hashicorp/terraform/pull/29519#issuecomment-912587851,repo: hashicorp/terraform | issue: Document config in plan file for http backend | keyword: gotcha
"Thanks for this submission! Between the time this was submitted and now, we have migrated our docs to a new format, which resulted in all of the files being renamed. This change would now need to be made to https://github.com/hashicorp/terraform/blob/main/website/docs/language/settings/backends/http.mdx. However, before making that change, I am including @laurapacilio to review whether we would be likely to accept this change. Thanks again for your submission and your patience! We appreciate it!",,,,,,Anecdotal,comment,,,,,,,,2022-05-21,github/crw,https://github.com/hashicorp/terraform/pull/29519#issuecomment-1133473762,repo: hashicorp/terraform | issue: Document config in plan file for http backend | keyword: gotcha
"Hello! This is an incredibly useful addition, and I'm very sorry that we didn't review it before we migrated the content! @njdart would you be willing to re-open this PR with these changes in the new .mdx files? If so, I'd be happy to review it and get it merged in. I would also be willing to help open the PR if necessary. I agree that better docs are required here to help prevent folks from ""gotchas"" associated with how config is stored.",,,,,,Anecdotal,comment,,,,,,,,2022-06-07,github/laurapacilio,https://github.com/hashicorp/terraform/pull/29519#issuecomment-1148888174,repo: hashicorp/terraform | issue: Document config in plan file for http backend | keyword: gotcha
"Hi @njdart - Your work sparked a great conversation between me and engineering on this topic. We realized that this warning actually applies to many of the backends, beyond just HTTP. So I opened a separate PR (inspired by this one!) where we give users a bit more context about the consequences of hardcoding credentials, etc. And then we added a note on all relevant backend pages to warn users about this and link them back to that explanation for more details. https://github.com/hashicorp/terra…",,,,,,Anecdotal,comment,,,,,,,,2022-06-09,github/laurapacilio,https://github.com/hashicorp/terraform/pull/29519#issuecomment-1151491678,repo: hashicorp/terraform | issue: Document config in plan file for http backend | keyword: gotcha
"I'm going to lock this pull request because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active contributions. If you have found a problem that seems related to this change, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2022-07-12,github/github-actions[bot],https://github.com/hashicorp/terraform/pull/29519#issuecomment-1181248870,repo: hashicorp/terraform | issue: Document config in plan file for http backend | keyword: gotcha
"Support SSH over HTTP Proxy There are some discussions about ssh proxy support through bastion box: https://github.com/hashicorp/terraform/issues/1709 But my request is different. We are behind the corporate proxy, I need to deploy an ec2 instance into vpc. Refer the terraform example https://github.com/hashicorp/terraform/blob/master/examples/aws-two-tier/main.tf But in our current environment, we don't have bastion box, or if I need to deploy the first bastion box. I stuck at the proxy settin…",,,,,,Anecdotal,issue,,,,,,,,2016-01-06,github/ozbillwang,https://github.com/hashicorp/terraform/issues/4523,repo: hashicorp/terraform | keyword: pro tip | state: closed
"An update. After the ec2 instance is created by terraform, I can login with ~/.ssh/config setting, such as `ssh 54.26.2.6:22` ``` $ cat ~/.ssh/config Host ec2* ProxyCommand nc -X connect -x proxy_server:3128 %h %p User ubuntu IdentityFile ~/.ssh/ec2.pem Host 5* ProxyCommand nc -X connect -x proxy_server:3128 %h %p User ubuntu IdentityFile ~/.ssh/ec2.pem ``` But run `terraform apply`, it gives up at `remote-exec`",,,,,,Anecdotal,comment,,,,,,,,2016-01-06,github/ozbillwang,https://github.com/hashicorp/terraform/issues/4523#issuecomment-169199841,repo: hashicorp/terraform | issue: Support SSH over HTTP Proxy | keyword: pro tip
"Hi @SydOps, Terraform uses Go's `crypto/ssh` package, which does not do anything with the OpenSSH config file. It also doesn't support the OpenSSH config options like `ProxyCommand` etc. It looks like what you're looking for is HTTP proxy support similar to what's described in this article: https://www.perkin.org.uk/posts/ssh-via-http-proxy-in-osx.html We'd have to investigate how this might be possible via Go. Any expertise from the community here would be welcome! In the meantime, I'll update…",,,,,,Anecdotal,comment,,,,,,,,2016-01-06,github/phinze,https://github.com/hashicorp/terraform/issues/4523#issuecomment-169363286,repo: hashicorp/terraform | issue: Support SSH over HTTP Proxy | keyword: pro tip
"Thanks, @phinze May not related with this topic. Any chance or how hard to switch from Go's `crypto/ssh` package to `OpenSSH`, which is more mature and popular tool.",,,,,,Anecdotal,comment,,,,,,,,2016-01-06,github/ozbillwang,https://github.com/hashicorp/terraform/issues/4523#issuecomment-169486558,repo: hashicorp/terraform | issue: Support SSH over HTTP Proxy | keyword: pro tip
"Just ran into this issue, although I'm using [corkscrew](http://agroman.net/corkscrew/) to poke through the proxy. I can SSH to the provisioned hosts, but Terraform can't. Resisting the urge to rant about language-specific reimplementations of tools that don't implement all of said tools features... :speak_no_evil:",,,,,,Anecdotal,comment,,,,,,,,2016-12-22,github/bodgit,https://github.com/hashicorp/terraform/issues/4523#issuecomment-268763654,repo: hashicorp/terraform | issue: Support SSH over HTTP Proxy | keyword: pro tip
"+1 We are running terraform into a ad-hoc vm, but would be awesome to avoid that by running it directly from our Jenkins (connecting using ssh over HTTP Proxy). Hopefully this will be there!",,,,,,Anecdotal,comment,,,,,,,,2017-05-18,github/koalalorenzo,https://github.com/hashicorp/terraform/issues/4523#issuecomment-302446720,repo: hashicorp/terraform | issue: Support SSH over HTTP Proxy | keyword: pro tip
"+1 Same issue here. SSH via corkscrew works, but Terraform provisioners don't. I also agree with @bodgit about implementing things differently that are already there. But that seems to be the Go way anyways.. \*cough\* openssl \*cough\*. Any news on this topic?",,,,,,Anecdotal,comment,,,,,,,,2017-08-24,github/Crapworks,https://github.com/hashicorp/terraform/issues/4523#issuecomment-324586843,repo: hashicorp/terraform | issue: Support SSH over HTTP Proxy | keyword: pro tip
"+1, I've just started to run recently terraform in OCI ( Oracle Cloud Infrastructure) and I ran into this 1st issue myself. I know many customers are behind proxies and therefore they can **ssh** strictly **over http proxy** so it will be nice to include it into _provisioner ""remote-exec"" connection_ > options = ""ProxyCommand=nc -X connect -x proxyServer:proxyPort %h %p"" It will be nice to have it there already.",,,,,,Anecdotal,comment,,,,,,,,2017-10-17,github/edivaserman,https://github.com/hashicorp/terraform/issues/4523#issuecomment-337126941,repo: hashicorp/terraform | issue: Support SSH over HTTP Proxy | keyword: pro tip
"How about an option (either in configuration or as a command line argument) to use the local ssh binary instead of the native Go implementation? This is how Docker Machine solves the same problem (actually, Docker Machine will use the local ssh binary unless explicitly instructed to use crypto/ssh via the --native-ssh option). https://docs.docker.com/machine/reference/ssh/",,,,,,Anecdotal,comment,,,,,,,,2017-12-06,github/dangregorysony,https://github.com/hashicorp/terraform/issues/4523#issuecomment-349686963,repo: hashicorp/terraform | issue: Support SSH over HTTP Proxy | keyword: pro tip
+1 This is a blocker for using Terraform behind our corporate proxy. For provisioning I always have to change connection to a mobile hotspot.,,,,,,Anecdotal,comment,,,,,,,,2018-11-27,github/nodomain,https://github.com/hashicorp/terraform/issues/4523#issuecomment-441965848,repo: hashicorp/terraform | issue: Support SSH over HTTP Proxy | keyword: pro tip
"Hi all, I have implemented support for SSH over HTTP proxy like below. https://github.com/htamakos/terraform/commit/dd9f7075f3b68217ad17c141fb243107ba343486 How about this? I'll submit a PR about support for SSH over HTTP proxy.",,,,,,Anecdotal,comment,,,,,,,,2020-04-08,github/htamakos,https://github.com/hashicorp/terraform/issues/4523#issuecomment-610784454,repo: hashicorp/terraform | issue: Support SSH over HTTP Proxy | keyword: pro tip
"We just found out that terraform doesn't use the proxycommand set inside the ssh config files. Therefore we can't deploy an application we are working on, since the deployment server is behind a corporate proxy and SSH is not allowed directly. It would be extremely helpful to have PR #24611 finalized, since it looks like it could work that way.",,,,,,Anecdotal,comment,,,,,,,,2021-01-25,github/dnperfors,https://github.com/hashicorp/terraform/issues/4523#issuecomment-766644015,repo: hashicorp/terraform | issue: Support SSH over HTTP Proxy | keyword: pro tip
"This has been supported since Terraform v1.3.0. See https://developer.hashicorp.com/terraform/language/resources/provisioners/connection#connection-through-a-http-proxy-with-ssh for how to use. Pro-tip: Contrary to what the docs say, it also supports socks5.",,,,,,Anecdotal,comment,,,,,,,,2023-10-03,github/sorenisanerd,https://github.com/hashicorp/terraform/issues/4523#issuecomment-1745565562,repo: hashicorp/terraform | issue: Support SSH over HTTP Proxy | keyword: pro tip
"Thanks for the updates, @sorenisanerd. After several years, I no longer need the proxy (it's now a transparent proxy). If this feature has been implemented, I'm fine with closing it.",,,,,,Anecdotal,comment,,,,,,,,2023-10-04,github/ozbillwang,https://github.com/hashicorp/terraform/issues/4523#issuecomment-1746219484,repo: hashicorp/terraform | issue: Support SSH over HTTP Proxy | keyword: pro tip
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2023-12-09,github/github-actions[bot],https://github.com/hashicorp/terraform/issues/4523#issuecomment-1848087464,repo: hashicorp/terraform | issue: Support SSH over HTTP Proxy | keyword: pro tip
helper/schema: allow Schema attrs to be Deprecated/Removed Deprecated fields show a customizable warning message to the user when they are used in a Terraform config. This is a tool that provider authors can use for user feedback as they evolve their Schemas. Removed fields show a customizable error message to the user when they are used in a Terraform config. This is a tool that provider authors can use for user feedback as they evolve their Schemas. fixes #957,,,,,,Anecdotal,issue,,,,,,,,2015-03-05,github/phinze,https://github.com/hashicorp/terraform/pull/1134,repo: hashicorp/terraform | keyword: pro tip | state: closed
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2020-05-04,github/ghost,https://github.com/hashicorp/terraform/pull/1134#issuecomment-623226449,repo: hashicorp/terraform | issue: helper/schema: allow Schema attrs to be Deprecated/Removed | keyword: pro tip
"StreamSpecification added to AWS DynamoDB Table Added optional StreamSpecification to AWS DynamoDB Table - not claiming any proficiency with Go or Terraform, but I modeled it straight after health_check in aws_elb. Please let me know if there's anything that I can do to improve on this PR. Thanks for creating what's basically seems to be a wicked tool! So far, I'm certainly enjoying the speed of Terraform compared to what we've previously struggled with (Ansible, CloudFormation)",,,,,,Anecdotal,issue,,,,,,,,2015-08-11,github/hngkr,https://github.com/hashicorp/terraform/pull/2975,repo: hashicorp/terraform | keyword: pro tip | state: closed
"I've left some small comments but be sure to take them with a pinch of salt since I'm not a core Terraform contributor and I'm still learning this codebase myself. In particular, I think you've actually followed the existing pattern in this code well but the pre-existing implementation doesn't seem to be following the usual conventions, so it's unlike what I'm accustomed to from reading other resource implementations. As a general Go pro-tip: be sure to use `go fmt` to reformat your code to con…",,,,,,Anecdotal,comment,,,,,,,,2015-08-13,github/apparentlymart,https://github.com/hashicorp/terraform/pull/2975#issuecomment-130499923,repo: hashicorp/terraform | issue: StreamSpecification added to AWS DynamoDB Table | keyword: pro tip
"I seem to have accidentially closed the pull request, but now I'm trying to reopen it - now as a single commit. I believe that I've followed up on the great comments from @apparentlymart Still learning Go and still learning the whole GitHub collaboration process.",,,,,,Anecdotal,comment,,,,,,,,2015-08-25,github/hngkr,https://github.com/hashicorp/terraform/pull/2975#issuecomment-134605758,repo: hashicorp/terraform | issue: StreamSpecification added to AWS DynamoDB Table | keyword: pro tip
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2020-05-01,github/ghost,https://github.com/hashicorp/terraform/pull/2975#issuecomment-622211771,repo: hashicorp/terraform | issue: StreamSpecification added to AWS DynamoDB Table | keyword: pro tip
"Remove the AWS Access Key and Secret Access keys from the`consul_agent_self` data source. Pro tip: update consul! I was using an old branch of Consul, not `origin/master`. Hat tip to @slackpad for pointing out that these keys were removed. Insta-merge to `0-8-stable` after a review.",,,,,,Anecdotal,issue,,,,,,,,2017-02-17,github/sean-,https://github.com/hashicorp/terraform/pull/12061,repo: hashicorp/terraform | keyword: pro tip | state: closed
"I'm going to lock this issue because it has been closed for _30 days_ ⏳. This helps our maintainers find and focus on the active issues. If you have found a problem that seems similar to this, please open a new issue and complete the issue template so we can capture all the details necessary to investigate further.",,,,,,Anecdotal,comment,,,,,,,,2020-04-16,github/ghost,https://github.com/hashicorp/terraform/pull/12061#issuecomment-614376704,repo: hashicorp/terraform | issue: Remove the AWS Access Key and Secret Access keys from the`consul_agent_self` data source. | keyword: pro tip
