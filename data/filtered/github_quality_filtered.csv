description,rationale,use_case,impact_area,transferability_score,actionability_rating,evidence_strength,type_(form),tag_(application),unique?,role,function,company,industry,country,date,source_(interview_#/_name),link,notes
"> UPD: import `Variable` from `airflow.models` instead of `airflow.sdk` fixes the problem but doesn't seem like a best practice solution. Yeah, that still works as it's falling back to the direct DB access based approach",,,,,,Anecdotal,comment,,,,,,,,2025-06-18,github/ashb,https://github.com/apache/airflow/issues/51816#issuecomment-2984241505,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: best practice
"> FYI, it's not clear what is best practices for build constraints, it might be the case you can pin them all, alternatively you might just be able to put upper bounds or ranges on them. Yep I know. It's just our choice and we have good reasons for Airlfow to do so.",,,,,,Anecdotal,comment,,,,,,,,2025-08-12,github/potiuk,https://github.com/apache/airflow/issues/54394#issuecomment-3180159412,repo: apache/airflow | issue: Add support to `--build-constraint(s)` flag for our constraint preparation | keyword: best practice
"Hi @potiuk amd @amoghrajesh , In development (using Breeze), we can access the value if we export AIRFLOW__CORE__TEMPLATE_SEARCHPATH=""<path>"" inside the Breeze terminal. That works fine. Also, i have added support for setting default path when Breeze starts — via CLI option like breeze start-airflow --template-searchpath ""<path>"" to make it easier instead of exporting every time. For **Production**, I tried passing the variable as an environment variable in the docker run command, and it worked…",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/Brunda10,https://github.com/apache/airflow/issues/53972#issuecomment-3159742446,repo: apache/airflow | issue: Support a global template_searchpath config option in Airflow | keyword: best practice
Hey @plovegro I was able to fix this issue. The code for the dag remains the same however the code for the plugin and airflow's config file need to be changed a little. I was able to get the webhook to trigger successfully by changing the plugin code to the following: ```python import logging import requests from airflow.plugins_manager import AirflowPlugin from airflow.listeners import hookimpl # It's best practice to get a logger instance instead of using print() log = logging.getLogger(__nam…,,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/tanujdargan,https://github.com/apache/airflow/issues/53162#issuecomment-3096005490,repo: apache/airflow | issue: Listeners not firing on task success | keyword: best practice
> Hey [@plovegro](https://github.com/plovegro) I was able to fix this issue. The code for the dag remains the same however the code for the plugin and airflow's config file need to be changed a little. > > I was able to get the webhook to trigger successfully by changing the plugin code to the following: > > import logging > import requests > from airflow.plugins_manager import AirflowPlugin > from airflow.listeners import hookimpl > > # It's best practice to get a logger instance instead of us…,,,,,,Anecdotal,comment,,,,,,,,2025-07-28,github/tanujdargan,https://github.com/apache/airflow/issues/53162#issuecomment-3126147817,repo: apache/airflow | issue: Listeners not firing on task success | keyword: best practice
"i18n: Replaced hard-coded ""no {modelName} found"" message in DataTable ## Why introduce `noItemsFound` as an improved default fallback For pages that currently lack a customized noRowsMessage, DataTable will now display a fully translated message. For example, instead of an erroneous ""No 任務實例 found"" (a mix of English and Chinese), the UI will correctly render a message like ""找不到任務實例"". ## Why is the `noRowsMessage` prop still crucial Relying solely on this generic template with modelName interpol…",,,,,,Anecdotal,issue,,,,,,,,2025-05-29,github/RoyLee1224,https://github.com/apache/airflow/pull/51209,repo: apache/airflow | keyword: best practice | state: closed
"> Adding dag_reserialize back. Why is this needed? Because the dag is not yet serialized? If so, should we add wait in the test instead?",,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/Lee-W,https://github.com/apache/airflow/pull/47433#issuecomment-2710346460,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"> > Adding dag_reserialize back. > > Why is this needed? Because the dag is not yet serialized? Yes, explicitly running `airflow dags reserialize` can resolve the issue described in https://github.com/apache/airflow/pull/47433#issuecomment-2708289346 (where triggering a new `dagRun` results in a 404, indicating that `DagModel.is_active` is `False`). > If so, should we add a wait in the test instead? I believe explicitly running `reserialize` is more precise than waiting for a fixed duration, wh…",,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/jason810496,https://github.com/apache/airflow/pull/47433#issuecomment-2710365025,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"> We can't merge anything that has a high chance of breaking the CI because it's hard to get all tests green at the same time (cause they are too flaky). > > We need something more stable. > > (or this will be hard to manage for other PRs that need to run these tests) Yep, what I meant was maybe we can extend the timeout time. but would like to confirm whether it was set that way for a reason",,,,,,Anecdotal,comment,,,,,,,,2025-03-10,github/Lee-W,https://github.com/apache/airflow/pull/47433#issuecomment-2710916494,repo: apache/airflow | issue: AIP-84 | Add Auth for Dags | keyword: lesson learned
"Not only thoughts. There is an issue for actually spliting the provider : https://github.com/apache/airflow/issues/15933 - but this one is complex because of common parts so maintaining such split provider would be difficult to maintain (we learned a lot about it when we added `common.sql`. However- when it comes to extras, it could be a better solution indeed, I have not thought about it, but it might actually make it much easier for users and would let us pick and choose which extras in googl…",,,,,,Anecdotal,comment,,,,,,,,2023-03-04,github/potiuk,https://github.com/apache/airflow/issues/27292#issuecomment-1454842888,repo: apache/airflow | issue: Support for Python 3.11 for Google Provider (upgrading all dependencies) | keyword: lesson learned
"@Jorricks thank you for the kind words. I agree, SLAs could/should be made more efficient. I found this change easier/simpler to make & test, possibly because it's a problem I've tackled many times in my ""real"" job. Also, as you note, I think this change defends against a range of possible problems, rather than just SLAs. I may try to improve SLA firing in the future, but I will be upfront about my motivations; I'm not looking to be ""The SLA guy"", as I simply don't have the spare time. If I end…",,,,,,Anecdotal,comment,,,,,,,,2022-08-10,github/argibbs,https://github.com/apache/airflow/pull/25489#issuecomment-1211303956,repo: apache/airflow | issue: Dag processor manager queue split (fixes SLAs) | keyword: lesson learned
"[webui] DAG Backfills broken on Firefox ### Apache Airflow version 3.0.4 ### If ""Other Airflow 2 version"" selected, which one? _No response_ ### What happened? the DAG backfill ui doesn't work on firefox browser(desktop). on firefox(desktop) we only have date picker. no timer picker ### What you think should happen instead? make the ui work. if not, there should be kind of notification for supported browser or workaround for the unsupported bowsers ### How to reproduce after pick a date (from) …",,,,,,Anecdotal,issue,,,,,,,,2025-08-13,github/obarisk,https://github.com/apache/airflow/issues/54429,repo: apache/airflow | keyword: workaround | state: open
> [@hbc-acai](https://github.com/hbc-acai) were you ever able to figure out a workaround? I used the DELETE to temporarily work around the issue. Then we upgraded to 2.10.4 and the issue never happened again,,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/hbc-acai,https://github.com/apache/airflow/issues/45186#issuecomment-3185778577,repo: apache/airflow | issue: Task Runner Dails to Update rendered_task_instance_fields | keyword: workaround
"> UPD: import `Variable` from `airflow.models` instead of `airflow.sdk` fixes the problem but doesn't seem like a best practice solution. Yeah, that still works as it's falling back to the direct DB access based approach",,,,,,Anecdotal,comment,,,,,,,,2025-06-18,github/ashb,https://github.com/apache/airflow/issues/51816#issuecomment-2984241505,repo: apache/airflow | issue: ImportError: cannot import name 'SUPERVISOR_COMMS' with dag.test() | keyword: workaround
"I'm really excited to hear that LocalStack is being added to Breeze! That said, I've seen many setups in the industry that combine Breeze with MinIO. I was wondering — was there a specific reason for choosing LocalStack over MinIO in this case? Was LocalStack chosen because other AWS services needed to be tested as well, beyond just S3?",,,,,,Anecdotal,comment,,,,,,,,2025-08-07,github/kyungjunleeme,https://github.com/apache/airflow/pull/54050#issuecomment-3162249011,repo: apache/airflow | issue: Add `localstack` Breeze integration | keyword: workaround
"> Honestly I don’t feel this is too worthwhile to fix. This is just a temporary glitch. Maybe somewhere down the line we can allow each dag bundle to set its refresh interval instead—and a bundle that’s cheap to refresh, such as a local directory, can simply set it very low, even 0. That's the plan of AIP-66 which still needs to be completed https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=294816356 But maybe a good (and simple) solution will be to have a possibility to ""clear"" …",,,,,,Anecdotal,comment,,,,,,,,2025-07-30,github/potiuk,https://github.com/apache/airflow/issues/45227#issuecomment-3136348513,repo: apache/airflow | issue: DAG Import Errors message is dangling in web-interface | keyword: workaround
[v3-0-test] Upgrade to prek 0.0.29 (#54621) This upgrades prek to **just released** 0.0.29 and removes the workaround implemented in #54613 to add build-essential to the image that is used to build distributions - because prek 0.0.29 comes with all the necessary binary wheels/platforms that remove the need to build in in our Linux ARM image (issue https://github.com/j178/prek/issues/451 has been fixed). (cherry picked from commit a4fa2182004034ca27d4d43b973fdd4de7f99d5a) <!-- Licensed to the Ap…,,,,,,Anecdotal,issue,,,,,,,,2025-08-18,github/potiuk,https://github.com/apache/airflow/pull/54624,repo: apache/airflow | keyword: workaround | state: closed
Upgrade to prek 0.0.29 This upgrades prek to **just released** 0.0.29 and removes the workaround implemented in #54613 to add build-essential to the image that is used to build distributions - because prek 0.0.29 comes with all the necessary binary wheels/platforms that remove the need to build in in our Linux ARM image (issue https://github.com/j178/prek/issues/451 has been fixed). <!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the N…,,,,,,Anecdotal,issue,,,,,,,,2025-08-18,github/potiuk,https://github.com/apache/airflow/pull/54621,repo: apache/airflow | keyword: workaround | state: closed
"@ashb looks like eager loading the attributes is the solution. Still need to implement and test this out to confirm that it works. Is there a reason why you want this in a separate PR? I would be eager loading attributes that are only used in this PR, so I thought it would be more appropriate to add that code in this PR itself. For example, `ti.dag_model.relative_fileloc` is one of the attributes I would be eager loading, and this attribute is directly used in the code I am adding in this PR.",,,,,,Anecdotal,comment,,,,,,,,2025-07-29,github/karenbraganz,https://github.com/apache/airflow/pull/53435#issuecomment-3133937981,repo: apache/airflow | issue: Allow failure callbacks for stuck in queued TIs that fail | keyword: workaround
"I was able to find a workaround for the `DetachedInstanceError` that does not involve eager loading. Right before the `TaskCallbackRequest` is created (where the error occurs), I added a condition to check whether the ti is detached. If it is, it will be merged into the session. I found that this prevents the issue from occurring without having to eager load any attributes. I have implemented this in my latest commit. Please let me know if there are any objections to this workaround.",,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/karenbraganz,https://github.com/apache/airflow/pull/53435#issuecomment-3152130914,repo: apache/airflow | issue: Allow failure callbacks for stuck in queued TIs that fail | keyword: workaround
"If you use the ""Token URL"" `api/v1/applications/token` then I think that will deal with the 404 at least, but instead I get a 401. I've created an issue https://github.com/airbytehq/airbyte-api-python-sdk/issues/112 to track, since the core problem seems to be from the airbyte-api SDK, rather than from the provider. Alternatively, the airbyte-api SDK _does_ seem to work with username/password auth, so possibly the change could still be made here in the provider, with the option to auth using us…",,,,,,Anecdotal,comment,,,,,,,,2024-11-20,github/amardatar,https://github.com/apache/airflow/issues/42520#issuecomment-2489072269,repo: apache/airflow | issue: [Airbyte] Airflow Airbyte provider 4.0 not allowing disabled auth and does not work with auth | keyword: workaround
"For those who are encountering this issue, a quickfix I did was to use the `apache-airflow-providers-airbyte` package version 3.0.0 instead of version >=4.0.0. With this version authentication is not necessary.",,,,,,Anecdotal,comment,,,,,,,,2025-06-23,github/NahidOulmi,https://github.com/apache/airflow/issues/42520#issuecomment-2996404050,repo: apache/airflow | issue: [Airbyte] Airflow Airbyte provider 4.0 not allowing disabled auth and does not work with auth | keyword: workaround
✅ Workaround to Resolve the Issue I implemented the following solution to restrict the /dag_stats endpoint to only return data for DAGs that a user has access to: ✅ Attach the Filter to Endpoint Dependencies I added the permission filter as a dependency to the FastAPI route using readable_dags_filter: ReadableDagsFilterDep. This ensures that only permitted DAGs are considered during the request lifecycle. ✅ Apply DAG Filter to All Queries I updated the queries inside the endpoint to filter by p…,,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/suhail-zemoso,https://github.com/apache/airflow/issues/53938#issuecomment-3150219985,repo: apache/airflow | issue: UI - Home screen does not filter based on logged in user's role's access to dags | keyword: workaround
"Resolve OOM When Reading Large Logs in Webserver related issue: #45079 related PR: #45129 related discussion on slack: https://apache-airflow.slack.com/archives/CCZRF2U5A/p1736767159693839 ## Why In short, this PR aims to eliminate OOM issues by: - Replacing full log sorting with a **K-Way Merge** - Making the entire log reading path **streamable** (using `yield` generators instead of returning a list of strings) More detailed reasoning is already described in the linked issue. Due to too many …",,,,,,Anecdotal,issue,,,,,,,,2025-04-20,github/jason810496,https://github.com/apache/airflow/pull/49470,repo: apache/airflow | keyword: workaround | state: closed
"> Nice, thanks for the benchmark, the memory improvement seems promising. > > Do you know why it looks like the total response time is much longer after refactoring? Is that inherent to the stream solution? Also I'm not sure we care that much because the user will be able to see logs after the first chunck retrieval, wich might still be faster. Yes, that's correct — the longer total response time is due to the use of [StreamingResponse](https://fastapi.tiangolo.com/advanced/custom-response/#str…",,,,,,Anecdotal,comment,,,,,,,,2025-06-02,github/jason810496,https://github.com/apache/airflow/pull/49470#issuecomment-2930184615,repo: apache/airflow | issue: Resolve OOM When Reading Large Logs in Webserver | keyword: workaround
"> Overall LGTM - I could imagine use cases of this trigger rule. For the long term, maybe we should think about changing the trigger rule mechanism so the definition could be more flexible instead of the hardcoded enum (e.g., this TR will be represented as something like `trigger_rule={'done': 'all', 'success': '>=1'}`). We can look into it after Airflow 3.1 if you have a proposal feel free to suggest it in the mailing list. I think this can be a good idea.",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/eladkal,https://github.com/apache/airflow/pull/53959#issuecomment-3142199235,repo: apache/airflow | issue: Add ALL_DONE_MIN_ONE_SUCCESS trigger rule | keyword: workaround
"Currently assets are only shown as consumed if they are used in schedule, not inlets. I feel we might want to do some more thinking instead of just adding those. It’s probably best to have _three_ fields: producing (things that list the asset as outlet), consuming (list as inlet), and scheduled (list in schedule). Not sure about the names here. cc @bbovenzi for ideas.",,,,,,Anecdotal,comment,,,,,,,,2025-05-21,github/uranusjr,https://github.com/apache/airflow/issues/50873#issuecomment-2899480502,repo: apache/airflow | issue: Assets do not show up as consumed | keyword: gotcha
@ismailsimsek I made some changes to use mocking of the log attribute instead of caplog usage (which was breaking the tests). It's difficult to apply changes to a branch I don't own. I added three commits with the fixes (and some others stuff that's happened in main since). I don't think it applied cleanly. But you can look at the commits and update your branch accordingly if not!,,,,,,Anecdotal,comment,,,,,,,,2025-07-04,github/o-nikolas,https://github.com/apache/airflow/pull/46621#issuecomment-3034009768,repo: apache/airflow | issue: Add support for S3 dag bundle | keyword: gotcha
"FYI. @assignUser and @gopidesupavan -> seems that this is a well known ""feature"" of the `@v4` action that caused a number of users a problems when migrating to `@v4`. The proposed `solution` (which IMHO is just a workaround) is to upload each artifact with a different key and use ""merge-multiple"" feature and glob pattern to download and merge all such uploaded artifacts (!??!!#$@#%!%!$%!#%!).... This is even explained here: https://github.com/actions/download-artifact/blob/main/docs/MIGRATION.m…",,,,,,Anecdotal,comment,,,,,,,,2024-12-30,github/potiuk,https://github.com/apache/airflow/pull/45289#issuecomment-2565964463,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
"OK. @assignUser and @gopidesupavan -> I think I found a solution (and actually this is a better one in general for performance, but slightly more ""distributed"" among the .yml files. Instead of heaving a clear save/restore around installation, I only do: * restore * installl And I make sure to have one separate job that is prerequisite of all other jobs (`build-info` which was already there as a single job that kicks-off the rest - for uv cache and `install-pre-commit` that is added as ""needs"" f…",,,,,,Anecdotal,comment,,,,,,,,2024-12-30,github/potiuk,https://github.com/apache/airflow/pull/45289#issuecomment-2565985028,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
"> FYI. @assignUser and @gopidesupavan -> seems that this is a well known ""feature"" of the `@v4` action that caused a number of users a problems when migrating to `@v4`. The proposed `solution` (which IMHO is just a workaround) is to upload each artifact with a different key and use ""merge-multiple"" feature and glob pattern to download and merge all such uploaded artifacts (!??!!#$@#%!%!$%!#%!).... > > This is even explained here: https://github.com/actions/download-artifact/blob/main/docs/MIGRA…",,,,,,Anecdotal,comment,,,,,,,,2024-12-31,github/gopidesupavan,https://github.com/apache/airflow/pull/45289#issuecomment-2566094221,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
"> OK. @assignUser and @gopidesupavan -> I think I found a solution (and actually this is a better one in general for performance, but slightly more ""distributed"" among the .yml files. > > Instead of heaving a clear save/restore around installation, I only do: > > * restore > * installl > > And I make sure to have one separate job that is prerequisite of all other jobs (`build-info` which was already there as a single job that kicks-off the rest - for uv cache and `install-pre-commit` that is ad…",,,,,,Anecdotal,comment,,,,,,,,2024-12-31,github/gopidesupavan,https://github.com/apache/airflow/pull/45289#issuecomment-2566094564,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
"OK. I think i Implemented all workarounds to not have to wait for any of the `stash` action PRs I created, let's see what will be the savings now",,,,,,Anecdotal,comment,,,,,,,,2024-12-31,github/potiuk,https://github.com/apache/airflow/pull/45289#issuecomment-2566554693,repo: apache/airflow | issue: Improve caching strategy across the board of CI workflow | keyword: gotcha
"@harishkrao Hi. I improved the tests yesterday and I think now it will be easy to write the new test case to handle this sitution.. See: https://github.com/apache/airflow/pull/20095 Now all you have to do is add a test case here. https://github.com/apache/airflow/blob/545ca59ba9a0b346cbbf28cc6958f9575e5e6b0b/tests/providers/snowflake/hooks/test_snowflake.py#L76-L95 To build a URL, we shouldn't use `string.format`. Instead of it, we should use `snowflake.sqlalchemy.URL`. See: https://github.com/…",,,,,,Anecdotal,comment,,,,,,,,2021-12-08,github/mik-laj,https://github.com/apache/airflow/issues/20032#issuecomment-988647949,repo: apache/airflow | issue: Snowflake Provider - Hook's support for not providing a region is broken when using SQLAlchemy | keyword: pro tip
"Hi, here is an implementation of the suggested solution described, would it be possible to have it reviewed? There is a couple things (like that I am unable to run pytests for some reason) that should be addressed, but it is passing the tests that I have written. https://github.com/pandas-dev/pandas/pull/61157",,,,,,Anecdotal,comment,,,,,,,,2025-03-28,github/Jaspvr,https://github.com/pandas-dev/pandas/issues/60895#issuecomment-2762673242,"repo: pandas-dev/pandas | issue: Support for Storing and Retrieving Complex Data Types (e.g., Embeddings) in Pandas DataFrames | keyword: best practice"
"I like the idea. If I understand correctly, the main use of `df.filter(cond)` where cond is a `Series` will be equivalent to now use `df[cond]`. I think implementing the `like` and `regex` behaviors would be trivial with `df.filter(df[""col""].str.contains(""xxx""))` and same for `regex`, right? It does feel we're offering a very reasonable alternative. I see your point for using `.pipe` to filter, and in a way kind of agree. But it feels like `df.filter(lambda x: x[""age""] > 18)` will be together w…",,,,,,Anecdotal,comment,,,,,,,,2025-04-20,github/datapythonista,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2817212049,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
"> I think implementing the `like` and `regex` behaviors would be trivial with `df.filter(df[""col""].str.contains(""xxx""))` and same for `regex`, right? It does feel we're offering a very reasonable alternative. Agreed - I should have said no _new_ alternatives. :laughing: For UDFs, one reason not to have `df.filter(lambda x: x[""age""] > 18)` operate by row is that it is effectively a transpose (`x` being a Series means it can only have one dtype), one of the behaviors I would love to remove from p…",,,,,,Anecdotal,comment,,,,,,,,2025-04-22,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2822457019,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
"That's a reasonable option. I think filter is more clear, and is what everybody else is using. If we were to implement the API from scratch now, I think it would be the obvious choice. For backward compatibility query may be better, and we can surely consider it. But I would rather have a very long deprecation timeline, than keep the API IMHO wrong because of a choice we did that now is not ideal.",,,,,,Anecdotal,comment,,,,,,,,2025-04-23,github/datapythonista,https://github.com/pandas-dev/pandas/issues/61317#issuecomment-2824439041,repo: pandas-dev/pandas | issue: ENH: Make DataFrame.filter accept filters in new formats | keyword: best practice
Hello @TomAugspurger `pandas-slim` sounds like an good workaround. It looks like docs are not a part of the distribution. And right it's tests code only without the data files - in terms of size they are second to `_libs` and almost equal to the rest of the code. ``` 8.0K ./arrays 16K ./errors 24K ./api 40K ./__pycache__ 76K ./compat 88K ./_config 248K ./tseries 308K ./util 440K ./plotting 2.3M ./io 6.7M ./core 17M ./tests 20M ./_libs ``` And what is the reason of having tests as a part of an A…,,,,,,Anecdotal,comment,,,,,,,,2020-01-06,github/vfilimonov,https://github.com/pandas-dev/pandas/issues/30741#issuecomment-571289513,repo: pandas-dev/pandas | issue: Suggestion: remove tests from the distribution | keyword: best practice
"Yes, the values are quite different, but the issue is the same. Partly, the reason for the different values could be because I ran the main example in Jupyter, while this one I ran directly as a file script. I didn't fully understand why I get this big 2GB offset in the main example. Here is the output: ``` Pandas version: 1.5.1 Initial memory usage: 62.513152 MB Memory usage after iteration: 478.826496 MB Memory usage after iteration: 1270.603776 MB Memory usage after iteration: 1547.554816 MB…",,,,,,Anecdotal,comment,,,,,,,,2022-11-08,github/mar-ses,https://github.com/pandas-dev/pandas/issues/49582#issuecomment-1307436102,repo: pandas-dev/pandas | issue: PERF: Memory leak when returning subset of DataFrame and deleting the rest | keyword: best practice
"Very surprised to see that actually; it was my impression that it was actually recommended that you first create the dataframe, with its full size allocated but empty, and then modify its elements, instead of adding/appending rows. Because adding rows results in constant array creation and is less efficient. At least that's what I thought I heard from places like stackoverflow, perhaps I was mistaken.",,,,,,Anecdotal,comment,,,,,,,,2023-02-27,github/mar-ses,https://github.com/pandas-dev/pandas/issues/49582#issuecomment-1447021471,repo: pandas-dev/pandas | issue: PERF: Memory leak when returning subset of DataFrame and deleting the rest | keyword: best practice
"These two sets of docs are relevant to the case, but if I need to do this: ```python df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'], 'B': ['B0', 'B1', 'B2', 'B3'], 'C': ['C0', 'C1', 'C2', 'C3'], 'D': ['D0', 'D1', 'D2', 'D3']}, index=[0, 1, 2, 3]) # add this single row to df1 row = ['A4', 'B4', 'C4', 'D4'] ``` What is the most efficient way to do this? Do I create a DataFrame for the single row and concat it? Do I use append? One of the `loc/iloc` methods?",,,,,,Anecdotal,comment,,,,,,,,2020-07-27,github/achapkowski,https://github.com/pandas-dev/pandas/issues/35378#issuecomment-664239295,repo: pandas-dev/pandas | issue: DOC:  Data Editing Samples/Guide | keyword: best practice
"At this point, both of those defaults are unlikely to be changed. `pd.DataFrame.from_csv` exists for easier round-tripping, though deprecated, see e.g. #10163 Because it is schema-less, csv is never a particularly safe format for round-tripping, consider using something binary like parquet of HDF5 instead",,,,,,Anecdotal,comment,,,,,,,,2018-12-28,github/chris-b1,https://github.com/pandas-dev/pandas/issues/24468#issuecomment-450406181,repo: pandas-dev/pandas | issue: read_csv should default to index_col = 0 | keyword: best practice
"> You're right, it's the inconsistency with everything else being a method that I don't think it's great: So this is just about a syntax preference. I'd probably continue to use the current methodology of ` [[""pickup_longitude"", ""pickup_latitude"", ""trip_miles""]]` instead of `select()` because I'm used to the former. But if you are trying to get people coming from Polars or PySpark to use pandas instead, then your proposal makes sense, although I think the API should accept either a `*args` OR a…",,,,,,Anecdotal,comment,,,,,,,,2025-06-04,github/Dr-Irv,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2941361233,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"We discussed this in today's call, and while not the perfect solution, everybody agreed that allowing both `*args` and a list is better than the alternatives. I fully agree that supporting two different APIs is something that we would want to avoid. But at the same time, forcing a list makes the API more complicated and less Pythonic (`.select(""col1"", ""col2"")` is clearly simpler and more pythonic than `.select([""col1"", ""col2""])`). Also, not allowing `*args` would be counterintuitive for users o…",,,,,,Anecdotal,comment,,,,,,,,2025-06-11,github/datapythonista,https://github.com/pandas-dev/pandas/pull/61527#issuecomment-2964314103,repo: pandas-dev/pandas | issue: ENH: Implement DataFrame.select | keyword: best practice
"> This may be because a new major version of numpy was [released](https://pypi.org/project/numpy/#history) on June 16, and in pandas pyproject.toml (version 2.2.2) there is: > > ``` > ""numpy>=1.22.4; python_version<'3.11'"", > ""numpy>=1.23.2; python_version=='3.11'"", > ""numpy>=1.26.0; python_version>='3.12'"", > ``` > > which was causing problems in my case - I had to pin version of numpy to <2 to fix it. Wondering what the reason was for not setting an upper limit. It doesn't make sense to assum…",,,,,,Anecdotal,comment,,,,,,,,2024-06-17,github/adodo-adoli,https://github.com/pandas-dev/pandas/issues/59023#issuecomment-2173153190,"repo: pandas-dev/pandas | issue: BUG: can't support numpy2.2,how to deal with it | keyword: best practice"
"I had this same issue while working with OpenCV, you can give the solutions a try for Pandas instead. Save your files(if needed) and follow the drill **First try** downgrading NumPy as newer versions sometimes creates the problem if it does not solve your issue then go for the second try **Second try** Uninstall Anaconda (if you are using it) and install an older version of the same *as you have to install OpenCV then NumPy gets installed along with it (may happen with other packages too) and i…",,,,,,Anecdotal,comment,,,,,,,,2024-09-03,github/Handique-lab,https://github.com/pandas-dev/pandas/issues/59023#issuecomment-2327212936,"repo: pandas-dev/pandas | issue: BUG: can't support numpy2.2,how to deal with it | keyword: best practice"
"Thanks @luke396 ! I was hoping that this part would be edited: ```rst Two instances of :class:`~pandas.api.types.CategoricalDtype` compare equal whenever they have the same categories and order. When comparing two unordered categoricals, the order of the ``categories`` is not considered. ``` to be a complete and definitive definition, covering all cases right here. (instead of adding examples and having the user then guess the actual rules)",,,,,,Anecdotal,comment,,,,,,,,2024-02-06,github/VladimirFokow,https://github.com/pandas-dev/pandas/pull/57273#issuecomment-1929049214,repo: pandas-dev/pandas | issue: DOC: Add notice and example for `CategoricalDtype` with different ``categories_dtype`` | keyword: best practice
"@datapythonista I modified this PR's code, title, and original comment, and it is again ready for review. Since I am now testing 2.0-style sqlalchemy connectables instead of checking for warnings, I think this can close #40686. Since I did make a lot of changes, would you prefer that I start a new PR instead?",,,,,,Anecdotal,comment,,,,,,,,2022-09-23,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1256456148,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"@mroeschke I haven't seen a release date for sqlalchemy 2.0. Instead of duplicating all the tests, I could make the `future` argument of `create_engine` depend on either a global constant in test_sql.py or an environment variable. The `future` argument will be supported in sqlalchemy 2.0 and required to be `True`. I could also put this PR on hold until the beta release of sqlalchemy 2.0 comes out.",,,,,,Anecdotal,comment,,,,,,,,2022-09-23,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1256713833,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"I'm planning to make some changes to this PR. Firstly, I noticed that pandas.io.sql.execute is documented, right above this line: https://pandas.pydata.org/docs/user_guide/io.html?highlight=sql%20execute#engine-connection-examples . As it stands, my PR would make this return a context manager instead of a Results Iterable, and I don't think I need to make this change, so I will change it back. I plan to make `SQLDatabase` accept only a SQLAlchemy `Connection` and not an `Engine`. I would change…",,,,,,Anecdotal,comment,,,,,,,,2022-10-28,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1294907916,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"@datapythonista @mroeschke Based on your feedback, I took out all the test duplication, and instead ran the tests with sqlalchemy 2.0.0b2 installed to ensure that this can close #40686. This PR will allow pandas to work with sqlalchemy 1.4.16 (the documented minimum version) and higher, even after sqlalchemy 2.0 is released. I found a note here (written 10/13/2022) on the timing of sqlalchemy 2.0: https://www.sqlalchemy.org/blog/2022/10/13/sqlalchemy-2.0.0b1-released/ >we will likely move from …",,,,,,Anecdotal,comment,,,,,,,,2022-10-31,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1297071602,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"@mroeschke, I found another bug in addition to the one I intend to resolve with #49967. The second bug is with `pandas.io.sql.execute`. If the caller passes a query that returns a resultset, then the connection might close before the caller can fetch the results. I'm having trouble finding a good workaround. Would it be ok to stop allowing either a sqlalchemy `Engine` or `str` to be passed to this method? It is not a well-documented method. The only mention I found of it is above this line: htt…",,,,,,Anecdotal,comment,,,,,,,,2022-12-09,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1344756733,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: lesson learned
"Regarding your alternative solutions, you can rely on [`re.escape(...)`](https://docs.python.org/3/library/re.html#re.escape) to programmatically escape a string in preparation for an exact regex match. A `regex` parameter does not make much sense on `match` and `fullmatch` methods, which clearly refer to the regular expression methods of the same name, and refer to matching a regular expression pattern. Furthermore, I don't exactly see what functionality is missing currently. You are looking f…",,,,,,Anecdotal,comment,,,,,,,,2022-08-19,github/tomaarsen,https://github.com/pandas-dev/pandas/issues/48086#issuecomment-1220680487,repo: pandas-dev/pandas | issue: ENH: add `regex=False` option to pandas.Series.str.match and fullmatch (like in str.contains) | keyword: lesson learned
"One other major breaking change to consider: We should consider making arithmetic between a Series and a DataFrame broadcast across the _columns_ of the dataframe, i.e., aligning `series.index` with `df.index`, rather than the current behavior aligning `series.index` with `df.columns`. I think this would be far more useful than the current behavior, because it's much more common to want to do arithmetic between a series and all columns of a DataFrame. This would make broadcasting in pandas inco…",,,,,,Anecdotal,comment,,,,,,,,2016-07-29,github/shoyer,https://github.com/pandas-dev/pandas/issues/10000#issuecomment-236238297,"repo: pandas-dev/pandas | issue: Towards ""pandas 1.0"" | keyword: lesson learned"
"For > Because this is kind of ""public"" interface > can you make method a required parameter in __finalize__ to avoid this problem in the future. and > also needs a whatsnew note Do we make any claim that `__finalize__` is public? There's just a couple whatsnew mentions in the docs. And the docstring doesn't say anything about it being public. I'm OK with these specific names and making `method` required but not if it's currently part of the public API.",,,,,,Anecdotal,comment,,,,,,,,2020-04-06,github/TomAugspurger,https://github.com/pandas-dev/pandas/pull/33273#issuecomment-609747875,repo: pandas-dev/pandas | issue: Pass method in __finalize__ | keyword: lesson learned
"Fix #25099 set na_rep values before converting to string to prevent data truncation Hi, The code in this pull request fixes #25099, and all tests pass. However, I am not sure if this is the best (or even an elegant) solution. The issue happened because the float `NaN` is converted to `str` (i.e. ""nan""), which gets stored as a `<U3`. When the user-provided `na_rep` is used, it is first truncated down to the data type length (i.e. ""myn""). I tried moving the `na_rep` to before it is converted to s…",,,,,,Anecdotal,issue,,,,,,,,2019-02-03,github/kinow,https://github.com/pandas-dev/pandas/pull/25103,repo: pandas-dev/pandas | keyword: lesson learned | state: closed
"read_html: Handle colspan and rowspan This is essentially a rebased and squashed #17054 (mad props to @jowens for doing all the hard thinking). My tweaks: * test_computer_sales_page (see #17074) no longer tests for ParserError, because the ParserError was a bug caused by missing colspan support. Now, test that MultiIndex works as expected. * I respectfully removed the fill_rowspan argument from #17073. Instead, the virtual cells created by rowspan/colspan are always copies of the real cells' te…",,,,,,Anecdotal,issue,,,,,,,,2018-06-14,github/adamhooper,https://github.com/pandas-dev/pandas/pull/21487,repo: pandas-dev/pandas | keyword: lesson learned | state: closed
"Indeed -- I rebased over adamhooper/master instead of pandas/master. I've learned my lesson: merge, don't rebase. Fixed by ... er ... rebasing. Correctly, this time.",,,,,,Anecdotal,comment,,,,,,,,2018-06-27,github/adamhooper,https://github.com/pandas-dev/pandas/pull/21487#issuecomment-400683621,repo: pandas-dev/pandas | issue: read_html: Handle colspan and rowspan | keyword: lesson learned
"Proposal: New Index type for binned data (IntervalIndex) ### Design The idea is to have a natural representation of the grids that ubiquitously appear in simulations and measurements of physical systems. Instead of referencing a single value, a grid cell references a _range_ of values, based on the chosen discretization. Typically, cells boundaries would be specified by floating point numbers. In one dimension, a grid cell corresponds to an _interval_, the name we use here. The key feature of `…",,,,,,Anecdotal,issue,,,,,,,,2014-07-01,github/shoyer,https://github.com/pandas-dev/pandas/issues/7640,repo: pandas-dev/pandas | keyword: lesson learned | state: closed
"@shoyer Thanks for including me; sorry I didn't notice earlier (mail filter was throwing github alerts out). Indeed, I think a general interval index is probably a great addition; although, I lack the breadth in vision to see a general solution. I did actually implement a hacky version of an interval index in pyuvvis that converts a datetime index to intervals of seconds, minutes etc... The main lesson I learned is that your interval index should be able to map back to the original data. To do …",,,,,,Anecdotal,comment,,,,,,,,2014-07-28,github/hughesadam87,https://github.com/pandas-dev/pandas/issues/7640#issuecomment-50400395,repo: pandas-dev/pandas | issue: Proposal: New Index type for binned data (IntervalIndex) | keyword: lesson learned
"I'm not sure if this belongs here or elsewhere. However, I'm trying to not clutter everything uselessly by just adding to the ever growing list of issues. If this belongs elsewhere, I'm happy to move it. Is there a reason `pd.cut` returns a `CategoricalIndex` instead of an `IntervalIndex`? The current behavior is ```python >>> pd.cut(np.linspace(0,` 100), bins=np.linspace(0, 101, 10)).value_counts().sort_index().index CategoricalIndex([ (0.0, 11.222], (11.222, 22.444], (22.444, 33.667], (33.667…",,,,,,Anecdotal,comment,,,,,,,,2018-05-07,github/blalterman,https://github.com/pandas-dev/pandas/issues/7640#issuecomment-387091110,repo: pandas-dev/pandas | issue: Proposal: New Index type for binned data (IntervalIndex) | keyword: lesson learned
"@adamgreenhall Thanks for the contribution! Can you provide a bit more context? (I never used the ORM part of SQLAlchemy, only the Core part) But as a `Session` has a `begin` method and `execute` method, I suppose `read_sql` would just work regardless you provide it a Session or connection? (as the support for connections was just added in #10105, which should already allow temporary tables when using a Connection) Why does it need sqlalchemy 1.0.0 ? The reason your tests fail, is because it se…",,,,,,Anecdotal,comment,,,,,,,,2015-07-19,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/pull/10617#issuecomment-122649728,repo: pandas-dev/pandas | issue: support sql transactions | keyword: lesson learned
"Should we rather error when a user creates such a Series with an unsupported dtype, instead of allowing to create it but then fail later on in various confusing ways? Or is there enough that works that people would want to use a Series/DataFrame container with such data?",,,,,,Anecdotal,comment,,,,,,,,2024-10-30,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/60108#issuecomment-2446468235,repo: pandas-dev/pandas | issue: BUG: many dataframe operations broken when a column contains numpy structured array | keyword: workaround
@MaximilianR I think you can get the same behavior as in my earlier example by using a boolean mask instead of `where`: ``` In [504]: df['new'] = df.x[ df.x == 2 ].expanding().count() In [505]: df Out[505]: x new 0 1 NaN 1 2 1.0 2 1 NaN 3 2 2.0 ```,,,,,,Anecdotal,comment,,,,,,,,2016-03-17,github/johne13,https://github.com/pandas-dev/pandas/issues/12648#issuecomment-197675532,repo: pandas-dev/pandas | issue: ENH:  cumcount() for DataFrames and Series | keyword: workaround
"The main issue I think is because`dtype` is a string I guess. I'm not 100% sure about how `_pandas_api.pandas_dtype` works, but presumably it's a large `dict` mapping types in string form to the correct type. Due to the infinite nature of possible Arrow datatypes, I guess its not feasible to update the dictionary, so maybe the try except solution is the only reasonable solution? Just my two cents.",,,,,,Anecdotal,comment,,,,,,,,2023-05-12,github/danielhanchen,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1545785525,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"@danielhanchen your approach only works here, and it just ignores the metadata. I'm not a pandas developer but I suppose they generated that metadata for a reason, so it may break some things if we just ignore it. Properly parsing the string is obviously harder, but I still think it is the better solution...",,,,,,Anecdotal,comment,,,,,,,,2023-05-13,github/takacsd,https://github.com/pandas-dev/pandas/issues/53011#issuecomment-1546638599,repo: pandas-dev/pandas | issue: BUG: read_parquet converts pyarrow list type to numpy dtype | keyword: workaround
"While I am building the qemu image, I kept looking at the source code. I believe that the the issue is coming from the mismatch definition of the intp_t type in pandas, numpy AND cython, definitely caused by some compiling parameter in Yocto. I guess that in one (or more) recipe the architecture is not properly detected using 64-bit platform instead of 32-bit. Also, checking out the WHEEL in the dist-info folder for the packages (i.e./usr/lib/python3.10/site-packages/pandas-1.4.2.dist-info/WHEE…",,,,,,Anecdotal,comment,,,,,,,,2023-10-11,github/scinteeb,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1758189838,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"The item #1 can easily be addressed if we create a bbappend file with the content: SETUPTOOLS_BUILD_ARGS += ""--plat-name ${MACHINE}"" This will create the wheel using the machine name instead of x86_64 (i.e. Cython-0.29.28-cp310-cp310-**qemuarm**.whl). Also the WHEEL tag is properly set. Sure that the ${MACHINE} parameter can be replaced with TUNE_ARCH if we want to have the CPU type in there. I think that this should be part of setuptools3.bbclass as it will be a good practice to have the targe…",,,,,,Anecdotal,comment,,,,,,,,2023-10-13,github/scinteeb,https://github.com/pandas-dev/pandas/issues/55305#issuecomment-1761390072,"repo: pandas-dev/pandas | issue: BUG: ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long long' on ARMv7 32 bit | keyword: workaround"
"Add axis argument to DataFrame.corr #### Location of the documentation https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html #### Documentation problem How should I get the correlation among the rows? I can obviously do `df.T.corr()` but this strikes me as a workaround rather than a nice way to do things. I wrote a `df.corr(axis=1)` assuming that would work. Curious what people feel is idiomatic, and would recommend some mention in the docs.",,,,,,Anecdotal,issue,,,,,,,,2020-06-26,github/alexlenail,https://github.com/pandas-dev/pandas/issues/35002,repo: pandas-dev/pandas | keyword: workaround | state: open
"I would currently lean towards maintaining the current behavior of `copy` being a shallow copy because the underlying array being immutable, but open to other thoughts cc @jbrockmendel @jorisvandenbossche Another solution, if you don't care about preserving the original chunking layout like in `take` method, is to just `combine_chunks()` of the underlying array in the `pd.arrays.ArrowExtensionArray` i.e. `values = pa.array(v.array).combine_chunks()`",,,,,,Anecdotal,comment,,,,,,,,2025-07-23,github/mroeschke,https://github.com/pandas-dev/pandas/issues/61930#issuecomment-3109392476,repo: pandas-dev/pandas | issue: PERF: `DataFrame.copy(deep=True)` returns a view on the original pyarrow buffer | keyword: workaround
"Yup that's another workaround. However, I would rather have pandas handling the index, rather than resetting it when saving to parquet and setting it back again when loading the parquet",,,,,,Anecdotal,comment,,,,,,,,2025-06-30,github/elbg,https://github.com/pandas-dev/pandas/issues/61714#issuecomment-3018166444,repo: pandas-dev/pandas | issue: BUG: doing df.to_parquet and then reading it with pd.read_parquet causes KeyError | keyword: workaround
"On initial inspection, this seems to be because dtype compatibility checks were bypassed when dealing with `ExtensionArray`. This coerces False -> 0.0 without warning. A workaround is to do something like ``` pd.Index([1., 2., 3], dtype=""object"").insert(1, False) ``` If we need a fix for this, we can add a dtype compatibility check before inserting into `ExtensionArray`. Should I open a PR?",,,,,,Anecdotal,comment,,,,,,,,2025-06-26,github/kernelism,https://github.com/pandas-dev/pandas/issues/61709#issuecomment-3007161580,"repo: pandas-dev/pandas | issue: BUG: Index[Float64].insert(1, False) casts False to 0 | keyword: workaround"
"This PR is still pretty big. Any reason why you are introducing a new `fast_strftime` keyword instead of just trying to improve performance inplace? I think that would help to reduce the size, though still probably need to break up in smaller subsets. The bigger a PR is, the harder it is to review so ends up in a long review cycle",,,,,,Anecdotal,comment,,,,,,,,2023-02-10,github/WillAyd,https://github.com/pandas-dev/pandas/pull/51298#issuecomment-1426419179,repo: pandas-dev/pandas | issue: [READY] perf improvements for strftime | keyword: workaround
"Thanks for reporting this @mimakaev, I see this on current master as well. Agree that either of your alternatives would be much better - would be good to know if the lack of reusability is a bug or just implicit by design. Worth mentioning there has been discussion about deprecating this (#41297) in favor of alternatives mentioned in that issue (which may serve as a workaround for your use case).",,,,,,Anecdotal,comment,,,,,,,,2021-07-01,github/mzeitlin11,https://github.com/pandas-dev/pandas/issues/41930#issuecomment-872571665,repo: pandas-dev/pandas | issue: BUG: pd.Grouper cannot be reused in some cases  | keyword: workaround
"Just do ``` pandas.set_option(""future.no_silent_downcasting"", True) ``` as suggested on the stack overflow question The series will retain object dtype in pandas 3.0 instead of casting to int64",,,,,,Anecdotal,comment,,,,,,,,2024-03-06,github/phofl,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-1981871396,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"> This is exactly the thing we are trying to solve. replace was previously casting your dtypes and will stop doing so in pandas 3 But it is unclear how to replace and cast. E.g. when I have `[0, 1]` integers they stand for female and male. ``` df.gender = df.gender.astype(str) df.gender = df.gender.replace({'0': 'male', '1': 'female'}) ``` Is that the solution you have in mind? From a users perspective it is a smelling workaround. The other way around is nearly not possible because I can not ca…",,,,,,Anecdotal,comment,,,,,,,,2024-03-13,github/buhtz,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-1994220779,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"> The other way around is nearly not possible because I can not cast a str word to an integer. One alternative (although I realise a non `.replace` supported ""alternative"" may not be what was actually desired) is to use categoricals with `.assign`: ```python import pandas as pd df = pd.DataFrame(['male', 'male', 'female'], columns=['gender']) # from the original example genders = pd.Categorical(df['gender']) df = df.assign(gender=genders.codes) ``` If semantically similar data is spread across …",,,,,,Anecdotal,comment,,,,,,,,2024-03-13,github/jerome-white,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-1994448905,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"I got this because I was trying to filter a dataframe using the output from `Series.str.isnumeric()`. My dataframe contained NA values, so the resulting mask contained NA values. Normally I use `fillna(False)` to get rid of these. What I would normally do: ``` python df = pd.DataFrame({'A': ['1', '2', 'test', pd.NA]}) mask = df['A'].str.isnumeric().fillna(False) ``` What I need to do now: ``` python df = pd.DataFrame({'A': ['1', '2', 'test', pd.NA]}) with pd.option_context('future.no_silent_dow…",,,,,,Anecdotal,comment,,,,,,,,2024-08-08,github/daviewales,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-2274836151,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"In my case, I had to convert strings to either bools or nan depending on the value of the string. Instead of suppressing the warning resulting from using replace(), I think it is generally better to use map() when you need to both change dtypes and deal with null values. Given: ``` print(df['has_attribute']) Name A Yes B No C Unknown Name: has_attribute, Length: 3, dtype: object ``` I do: ``` df = df.map(lambda x: True if x == 'Yes' else (False if x == 'No' else np.nan)) print(df['has_attribute…",,,,,,Anecdotal,comment,,,,,,,,2025-01-07,github/SamLovesHoneyWater,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-2574215022,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"> Just do > > ``` > pandas.set_option(""future.no_silent_downcasting"", True) > ``` > > as suggested on the stack overflow question > > The series will retain object dtype in pandas 3.0 instead of casting to int64 I've done it for now, just wonder when my pandas is upgraded to 3.0 next time, will there be any notification that I can remove this pd.set_option?",,,,,,Anecdotal,comment,,,,,,,,2025-08-13,github/randomseed42,https://github.com/pandas-dev/pandas/issues/57734#issuecomment-3182838142,"repo: pandas-dev/pandas | issue: QST: How to solve pandas (2.2.0) ""FutureWarning: Downcasting behavior in `replace` is deprecated"" on a Series? | keyword: workaround"
"Hi. I just hit this ""bug"" or feature. It's annoying. Here's my fix ``` def bool_fillna_inplace(series: pd.Series) -> pd.Series: """""" Workaround for https://github.com/pandas-dev/pandas/issues/59831 Replaces nan/None with False."""""" series[pd.isna(series)] = False return series ``` or an alternative `return series & ~pd.isna(series)` but that one does a bitwise and which would return False for even number, in case the series is not purely boolean with nans.",,,,,,Anecdotal,comment,,,,,,,,2024-11-26,github/joaoe,https://github.com/pandas-dev/pandas/issues/59831#issuecomment-2500704786,repo: pandas-dev/pandas | issue: ENH: Restore the functionality of `.fillna` | keyword: workaround
"We could probably optionally import regex and append it’s type to the re types we handle. > On Jul 26, 2019, at 10:23, pmav99 <notifications@github.com> wrote: > > @madimov, I think I used vanila re for pandas, and regex for everything else. Not nice ,but there was no feedback and I needed to move on. > > — > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",,,,,,Anecdotal,comment,,,,,,,,2019-07-26,github/TomAugspurger,https://github.com/pandas-dev/pandas/issues/22496#issuecomment-515518510,"repo: pandas-dev/pandas | issue: Add support for ""regex"" library | keyword: gotcha"
"> The reason I believe a generic solution is better than a regex-specific solution is that there are yet other regex libraries that someone might want to use (e.g. RE2). @gwerbin Above is so true. I wish I could use `pythonnet`'s regex engine because I have a few modules in C# and a few in Python and I want to use a single regex engine for both. Basically, we need to be able to switch the internal regex engine used for pandas' string methods.",,,,,,Anecdotal,comment,,,,,,,,2023-03-31,github/alegend4u,https://github.com/pandas-dev/pandas/issues/22496#issuecomment-1491192631,"repo: pandas-dev/pandas | issue: Add support for ""regex"" library | keyword: gotcha"
"I recently had an application where I needed much faster regular expressions in pandas and here are a few of the workarounds I found: # Using The apply Method apply is typically slow, but since the execution time of re2 is so much faster this comes out way ahead. ``` import re2 import pandas def _apply_re2(value, regex): return True if value is not pandas.NA and regex.fullmatch(value) else False regex = re2.compile(r'asdf') series = pandas.Series(['qwer', 'asdf']) regex_match = series.apply(_ap…",,,,,,Anecdotal,comment,,,,,,,,2025-03-04,github/ptth222,https://github.com/pandas-dev/pandas/issues/22496#issuecomment-2698608729,"repo: pandas-dev/pandas | issue: Add support for ""regex"" library | keyword: gotcha"
"DOC: Categorical ""Memory Usage"" uses nbytes instead of `memory_usage(deep=True)` ### Pandas version checks - [X] I have checked that the issue still exists on the latest versions of the docs on `main` [here](https://pandas.pydata.org/docs/dev/) ### Location of the documentation https://pandas.pydata.org/docs/dev/user_guide/categorical.html#memory-usage ### Documentation problem The ""Memory Usage"" section states that the memory usage of an `object` dtype is a constant times the length of the dat…",,,,,,Anecdotal,issue,,,,,,,,2022-09-07,github/tehunter,https://github.com/pandas-dev/pandas/issues/48438,repo: pandas-dev/pandas | keyword: gotcha | state: open
Hmm what are you suggesting? Just make `astype(bytes)` fail? Or is this part of the larger conversation in https://github.com/pandas-dev/pandas/issues/58141 ? Maybe that should default to the pyarrow bytes type instead of numpy?,,,,,,Anecdotal,comment,,,,,,,,2024-04-13,github/WillAyd,https://github.com/pandas-dev/pandas/issues/58205#issuecomment-2053676181,repo: pandas-dev/pandas | issue: BUG: Roundtripping bytes through pandas dataframes leads to data loss | keyword: gotcha
"Both `__iter__` and `__contains__` for DataFrames uses the columns. It therefore seems most natural that they use the index on a Series. This would also agree with a dictionary-analogy of Series. Rather than deprecating, I would suggest changing the `__iter__` to instead iterate over the index. Both Series and DataFrames are data structures with a well-defined order, and as such I think users expect to be able to iterate over them. While there is ambiguity because there are multiple choices for…",,,,,,Anecdotal,comment,,,,,,,,2023-07-10,github/rhshadrach,https://github.com/pandas-dev/pandas/issues/14235#issuecomment-1629652870,repo: pandas-dev/pandas | issue: BUG: Python operator 'in' behaves in two different ways on Series object | keyword: gotcha
"Hey - thanks for the reply @gfyoung , and sorry for my delay in replying. The functions where I use this are part of a library, so temporarily saving to disk isn't ideal (can't be sure what the end-user's local environment will look like). My thought was something like this as a workaround: ``` import gzip from io import BytesIO import pandas as pd df = pd.DataFrame({""A"": [1, 2, 3, 4], ""B"": [5, 6, 7, 8], ""C"": [9, 10, 11, 12]}) b_buf = BytesIO() with gzip.open(b_buf, 'wb') as f: f.write(df.to_st…",,,,,,Anecdotal,comment,,,,,,,,2018-09-06,github/ZaxR,https://github.com/pandas-dev/pandas/issues/22555#issuecomment-418958420,repo: pandas-dev/pandas | issue: In-memory to_csv compression | keyword: gotcha
"@cbrnr Yes, that is intended behaviour. For single ""labels"" of a MultiIndex (so one value for each level), we always use tuples and not a list, because it would otherwise be difficult to distinguish. I think for this case we are quite consistent within pandas. It is the other way around (in a case where we want list-like, do we accept tuple?) that there can be more discussion. Typically we allfow tuples as list-like, but exactly for the reason above (tuples are used to indicate labels of a MI) …",,,,,,Anecdotal,comment,,,,,,,,2018-02-01,github/jorisvandenbossche,https://github.com/pandas-dev/pandas/issues/16943#issuecomment-362248941,repo: pandas-dev/pandas | issue: MultiIndex row indexing with .loc fail with tuple but work with list of indices | keyword: gotcha
"@datapythonista I modified this PR's code, title, and original comment, and it is again ready for review. Since I am now testing 2.0-style sqlalchemy connectables instead of checking for warnings, I think this can close #40686. Since I did make a lot of changes, would you prefer that I start a new PR instead?",,,,,,Anecdotal,comment,,,,,,,,2022-09-23,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1256456148,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"@mroeschke I haven't seen a release date for sqlalchemy 2.0. Instead of duplicating all the tests, I could make the `future` argument of `create_engine` depend on either a global constant in test_sql.py or an environment variable. The `future` argument will be supported in sqlalchemy 2.0 and required to be `True`. I could also put this PR on hold until the beta release of sqlalchemy 2.0 comes out.",,,,,,Anecdotal,comment,,,,,,,,2022-09-23,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1256713833,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"I'm planning to make some changes to this PR. Firstly, I noticed that pandas.io.sql.execute is documented, right above this line: https://pandas.pydata.org/docs/user_guide/io.html?highlight=sql%20execute#engine-connection-examples . As it stands, my PR would make this return a context manager instead of a Results Iterable, and I don't think I need to make this change, so I will change it back. I plan to make `SQLDatabase` accept only a SQLAlchemy `Connection` and not an `Engine`. I would change…",,,,,,Anecdotal,comment,,,,,,,,2022-10-28,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1294907916,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"@datapythonista @mroeschke Based on your feedback, I took out all the test duplication, and instead ran the tests with sqlalchemy 2.0.0b2 installed to ensure that this can close #40686. This PR will allow pandas to work with sqlalchemy 1.4.16 (the documented minimum version) and higher, even after sqlalchemy 2.0 is released. I found a note here (written 10/13/2022) on the timing of sqlalchemy 2.0: https://www.sqlalchemy.org/blog/2022/10/13/sqlalchemy-2.0.0b1-released/ >we will likely move from …",,,,,,Anecdotal,comment,,,,,,,,2022-10-31,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1297071602,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
"@mroeschke, I found another bug in addition to the one I intend to resolve with #49967. The second bug is with `pandas.io.sql.execute`. If the caller passes a query that returns a resultset, then the connection might close before the caller can fetch the results. I'm having trouble finding a good workaround. Would it be ok to stop allowing either a sqlalchemy `Engine` or `str` to be passed to this method? It is not a well-documented method. The only mention I found of it is above this line: htt…",,,,,,Anecdotal,comment,,,,,,,,2022-12-09,github/cdcadman,https://github.com/pandas-dev/pandas/pull/48576#issuecomment-1344756733,repo: pandas-dev/pandas | issue: Make pandas/io/sql.py work with sqlalchemy 2.0 | keyword: gotcha
maybe use breakpoint() instead of ValueError to track down the call args? i still think its really weird to get here with a slice,,,,,,Anecdotal,comment,,,,,,,,2021-02-09,github/jbrockmendel,https://github.com/pandas-dev/pandas/issues/39585#issuecomment-775555273,repo: pandas-dev/pandas | issue: Intermittent error fetching value from multi-indexed dataframe | keyword: gotcha
"I have started over with a failing test, see the results on https://github.com/pandas-dev/pandas/pull/57314/commits/b4c825da2f87131b379ee81119ccdc3ba8237fec. To resolve, I went with a minimally invasive approach that just checks for negative extreme before scaling to microseconds, and then doing the calculation backwards from the minimum valid timestamp. I didn't find any constant defined for this (`NPY_MIN_INT64 + 1`), so instead of polluting the global namespace in `np_datetime.h`, I went wit…",,,,,,Anecdotal,comment,,,,,,,,2024-02-16,github/robert-schmidtke,https://github.com/pandas-dev/pandas/pull/57314#issuecomment-1948430840,repo: pandas-dev/pandas | issue: BUG: Fix near-minimum timestamp handling | keyword: gotcha
>why should we support 32 bit at all? >this is just more work - sure it's probably easy but we are all volunteer Pro tip: by not contributing into open source at all instead of contributing (and by not doing any other work you can avoid doing) you can keep even more work away of you.,,,,,,Anecdotal,comment,,,,,,,,2022-03-01,github/KOLANICH,https://github.com/pandas-dev/pandas/issues/44453#issuecomment-1055636670,repo: pandas-dev/pandas | issue: BLD: remove support for 32 bit | keyword: pro tip
"> Emitting events AND logs LGTM. I still believe the absolute best would be to have something permanent in pod status, because events are ephemeral, but I'll not push for that, let alone block this PR. Things like this have come up before, but a PodCondition doesn't seem quite right. Maybe we need to consider adding a new field/concept to the pod status for this. Something like `Warnings map[string]string`. Obviously out of scope for this PR. > I'm wondering if instead of passing down deep the …",,,,,,Anecdotal,comment,,,,,,,,2025-07-18,github/tallclair,https://github.com/kubernetes/kubernetes/pull/132634#issuecomment-3091239222,repo: kubernetes/kubernetes | issue: [PodLevelResources] Event for pod-level resource manager incompatibility | keyword: best practice
"@tallclair @ffromani > > I'm wondering if instead of passing down deep the recorder, we can't just bubble up a specific error or a new return value and centralize the logging and the event emission. > > Was this option evaluated? > +1 to this approach. If you bubble it up to the admission handler, you could just record the event there. Since I do not want to prevent pod admission, I would have to bubble it up, catch the specific warning and produce the event, but the pod would still have to be …",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/KevinTMtz,https://github.com/kubernetes/kubernetes/pull/132634#issuecomment-3095141396,repo: kubernetes/kubernetes | issue: [PodLevelResources] Event for pod-level resource manager incompatibility | keyword: best practice
"> @tallclair @ffromani > > > > I'm wondering if instead of passing down deep the recorder, we can't just bubble up a specific error or a new return value and centralize the logging and the event emission. > > > Was this option evaluated? > > > +1 to this approach. If you bubble it up to the admission handler, you could just record the event there. > > Since I do not want to prevent pod admission, I would have to bubble it up, catch the specific warning and produce the event, but the pod would s…",,,,,,Anecdotal,comment,,,,,,,,2025-07-21,github/ffromani,https://github.com/kubernetes/kubernetes/pull/132634#issuecomment-3095344963,repo: kubernetes/kubernetes | issue: [PodLevelResources] Event for pod-level resource manager incompatibility | keyword: best practice
"> A ContainerDied event seems more broadly useful, and has less overlap with ContainerStarted. Why the switch to ContainerRestarted? @tallclair I see why the confusing between line 243 addition of restarted event and then 291 stating successfully started container. How about we change the `event reason` from `containerRestarted` to `containerRestarting` i see that fits best to situation where restarting fails once or twice before a successful container starts. Rather than `containerDied` i'd pr…",,,,,,Anecdotal,comment,,,,,,,,2025-02-05,github/Ritikaa96,https://github.com/kubernetes/kubernetes/pull/126474#issuecomment-2635725354,repo: kubernetes/kubernetes | issue: publish an event when the container is restarted | keyword: best practice
"> Btw, this abstraction layer difference is also precisely why I see this annotation in the node-autoscaling prefix instead of the broader node-lifecycle one. My intention is for node-autoscaling.kubernetes.io/safe-to-evict=false to mean ""pod not safe to evict by autoscalers, so autoscalers can't consolidate its node"". In contrast, the way I'd understand node-lifecycle.kubernetes.io/safe-to-evict=false would be ""pod not safe to evict by anyone using the standard draining mechanism, so its node …",,,,,,Anecdotal,comment,,,,,,,,2024-07-09,github/MaciekPytel,https://github.com/kubernetes/kubernetes/pull/124800#issuecomment-2218288529,repo: kubernetes/kubernetes | issue: Define a common Node autoscaling safe-to-evict/do-not-disrupt annotation | keyword: best practice
"Apply best practices to staging repos See: https://github.com/kubernetes/community/pull/8308 for the best practices. To understand more about staging, see the docs at https://github.com/kubernetes/kubernetes/tree/master/staging For every repo in `staging/src/k8s.io` we should: 1. Update the README for each these repos to clarify that contributions including issues and PRs should be made to this repo instead of the ""staged"" copy. 2. Request that issues be disabled in github.com/kubernetes/org fo…",,,,,,Anecdotal,issue,,,,,,,,2025-04-15,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/131315,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"fix: enhance websocket fallback to handle CRI incompatibility Previously, WebSocket fallback to SPDY only occurred for HTTP handshake errors (ErrBadHandshake). When CRI-O's WebSocket server terminates during initialization due to WebSocket v5 protocol incompatibility, kubectl debug would hang with ""websocket server finished before becoming ready"" errors instead of falling back to SPDY. <!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read…",,,,,,Anecdotal,issue,,,,,,,,2025-08-08,github/sohankunkerkar,https://github.com/kubernetes/kubernetes/pull/133448,repo: kubernetes/kubernetes | keyword: best practice | state: closed
"> I got it， you are right. I didn't realize error is so important even though the error occurs during collecting debug info. Thanks, I learned. It is a good lesson for me. @carlory no worries, this happens when you got hit for this multiple times, something only you learn because you suffered it ... now you know 😄",,,,,,Anecdotal,comment,,,,,,,,2024-11-07,github/aojea,https://github.com/kubernetes/kubernetes/issues/128655#issuecomment-2462702572,repo: kubernetes/kubernetes | issue: Failure cluster [245f6915...]: Networking Granular Checks: Services should update endpoints: http | keyword: lesson learned
"I think we should refactor the API to reflect the lessons learned from our first discovery document. Details like separation of GVR from GVK and the handling of subresources should be first class concepts this time. Rather than try to determine that new API in the last two days before freeze, I think we should continue shaping the API with the plan of merging it early in 1.26 instead. Given that the PR was only opened three days ago, I think the timing on this was always very tight. @lavalamp s…",,,,,,Anecdotal,comment,,,,,,,,2022-07-28,github/deads2k,https://github.com/kubernetes/kubernetes/pull/111409#issuecomment-1198536637,repo: kubernetes/kubernetes | issue: Aggregated Discovery Endpoint | keyword: lesson learned
"> I think we should refactor the API to reflect the lessons learned from our first discovery document. Details like separation of GVR from GVK and the handling of subresources should be first class concepts this time. Rather than try to determine that new API in the last two days before freeze, I think we should continue shaping the API with the plan of merging it early in 1.26 instead. Given that the PR was only opened three days ago, I think the timing on this was always very tight. > > Danie…",,,,,,Anecdotal,comment,,,,,,,,2022-07-28,github/seans3,https://github.com/kubernetes/kubernetes/pull/111409#issuecomment-1198609892,repo: kubernetes/kubernetes | issue: Aggregated Discovery Endpoint | keyword: lesson learned
"When kube-proxy is in iptables mode, kube-proxy is not on the data path. By default the AWS ELB HC is configured: - TCP mode on the serviceNodePort. - every 10s with 5s timeout When the LB performs the node HC the TCP handshake reaches the pod doesn't stop at node level, so if you have a single pod in a big cluster, it likely get DDoSed by all this HCs, your pod needs to be able to deal with all this TCP connections. Or you should change the HC to stop at the node level, instead of reaching the…",,,,,,Anecdotal,comment,,,,,,,,2021-02-25,github/ltagliamonte-dd,https://github.com/kubernetes/kubernetes/issues/99373#issuecomment-785483463,repo: kubernetes/kubernetes | issue: AWS ELB healtcheck SYN Flood in pod | keyword: lesson learned
We have the same problem on K8s 1.25 and using HPA autoscaling/v2. The OP is on 1.25 also. Is anyone on 1.26+ having the issue as well? The only solution we could find was to disambiguate our `matchLabels` by including another label so there is no possible substring match on label combination. The saddest part about this workaround is that `matchLabels` are immutable. So you have to delete the Deployment and suffer the downtime until the new Deployment has rolled out. Not only that you lose all…,,,,,,Anecdotal,comment,,,,,,,,2024-04-17,github/ccmcbeck,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-2062615449,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
"> Anyone found a workaround or fix on this, I upgraded my kubernetes to v1.30 and deduct load testing on the environment then the autoscaler start to acting weird and keep scaling down and up on schedule even the metrics don't seem to be over the threshold. first of all I though it is a metrics server issue but I did upgrade it and tried to do scaling policies but without luck to fix the issue. Any update please? I did it using Keda autoscalling instead of default HPA",,,,,,Anecdotal,comment,,,,,,,,2025-01-19,github/gsGabriel,https://github.com/kubernetes/kubernetes/issues/120875#issuecomment-2601035476,repo: kubernetes/kubernetes | issue: HPA stuck at maxReplicas even though metric under target | keyword: workaround
Can we first explain in the issue what is the proposal? There is a long discussion about FIPS complain here https://github.com/kubernetes/kubernetes/issues/129075 and it seems better to tackle all these related issues in a single theme instead of individual PRs,,,,,,Anecdotal,comment,,,,,,,,2025-01-16,github/aojea,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-2595274252,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
"Instead of rewriting unwanted-dependencies, we can use `forbidigo` for preventing new usage: https://github.com/kubernetes/kubernetes/pull/133511 While doing that, I noticed the kubeadm usage is extremely trivial and self-contained, so I migrated it. The endpoints usage still needs working through, as do the others. /sig architecture /area code-organization",,,,,,Anecdotal,comment,,,,,,,,2025-08-14,github/BenTheElder,https://github.com/kubernetes/kubernetes/issues/129652#issuecomment-3186234245,repo: kubernetes/kubernetes | issue: Remove the MD5 hash function for FIPS compliance | keyword: workaround
"> Am I missing something? Is there another solution out there? Looks like someone linked to this issue in kube-image-keeper, which is a proxy to locally cache images from remote registries. It doesn't support mutable tags but it sounds like they're working on it. If they get that working, it would provide *somewhat* of a workaround for this issue in some cases, by making the local caching proxy the point of failure instead of the remote registry. Whether that's a better or a worse point of fail…",,,,,,Anecdotal,comment,,,,,,,,2023-11-07,github/Alibirb,https://github.com/kubernetes/kubernetes/issues/111822#issuecomment-1800277335,repo: kubernetes/kubernetes | issue: imagePullPolicy to pull latest image OR use existing image if repository unavailable | keyword: workaround
"/sig node until we have a wg-resource-management label From @flx42: > By default, kernels from different processes can't run on one GPU simultaneously (concurrency but not parallelism), they are time sliced. The Pascal architecture brings instruction-level preemption instead of block-level preemption, but context switches are not free. > Also, there is no way of partitioning GPU resources (SMs, memory), or even assignin priorities when sharing a card. You also have [MPS](https://docs.nvidia.com…",,,,,,Anecdotal,comment,,,,,,,,2017-09-20,github/RenaudWasTaken,https://github.com/kubernetes/kubernetes/issues/52757#issuecomment-330766518,repo: kubernetes/kubernetes | issue: Is sharing GPU to multiple containers feasible? | keyword: workaround
"I got around this limitation on elasticsearch because of work I had to do to avoid EBS volumes being unable to be assigned because they are in the wrong availability zone, i.e. I had created a statefulset per AZ. If I want to change some storage characteristic, I create a new ""AZ"" using the same storage class, and then migrate all the data to pods in that new AZ, then destroy the old AZ.",,,,,,Anecdotal,comment,,,,,,,,2019-02-22,github/DaveWHarvey,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-466514840,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
"I have a problem with this approach (azure aks). Even after changing the stateful set disk size and applying it again, I still need to restart the pods as stated before but since the stateful set recreates the pods quickly, there's no time for the disk to be detached from the node and resized before the new pod attaches it again. This results on the pvc being stuck with `type: Resizing` (despite still working fine but with the old size instead of the new one). Looks like running `resize2fs` fro…",,,,,,Anecdotal,comment,,,,,,,,2019-05-17,github/guitmz,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-493433066,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
"> I have a problem with this approach (azure aks). Even after changing the stateful set disk size and applying it again, I still need to restart the pods as stated before but since the stateful set recreates the pods quickly, there's no time for the disk to be detached from the node and resized before the new pod attaches it again. This results on the pvc being stuck with `type: Resizing` (despite still working fine but with the old size instead of the new one). > > Looks like running `resize2f…",,,,,,Anecdotal,comment,,,,,,,,2019-09-06,github/infa-ddeore,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-528752771,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
Looks like someone tried to add support for this in https://github.com/kubernetes/enhancements/pull/660 but the PR has been open for ~10 months and awaiting requested changes for ~8 months. Are there any plans to merge the PR or is the workaround listed here the only solution?,,,,,,Anecdotal,comment,,,,,,,,2019-09-17,github/JamesBalazs,https://github.com/kubernetes/kubernetes/issues/68737#issuecomment-532206295,repo: kubernetes/kubernetes | issue: StatefulSet: support resize pvc storage in K8s v1.11 | keyword: workaround
"> This seems to work for embedded resources that must be namespaced. It's awkward for a couple reasons: If I'm following, (1), (2) and (3) all result in sub-par validation compared with the root resource. I'd be very interested in seeing that improved. my notes: (1) - This seems like a bug. We wouldn't see it on the root because we have built-in immutability validation. Is this also a problem with OpenAPI value validations? (e.g. a regex rule or length rule)? (2) - The ability to specify if nam…",,,,,,Anecdotal,comment,,,,,,,,2024-01-24,github/jpbetz,https://github.com/kubernetes/kubernetes/issues/122163#issuecomment-1907223578,repo: kubernetes/kubernetes | issue: Consider allowing CEL validation of metadata.namespace field of embedded resource | keyword: workaround
"can we please reopen this. it's a very valid use case and seems like a bug in k8s I should be able to set that I want 1 pod most time yet bring up the replacement pod in case of `kubectl drain` instead of getting stuck in a loop with ``` error when evicting pods/... (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget. ``` I can't think of a workaround, sadly, to achieve this rather simple mode (I would have expected indeed that either maxsurge or hpa or pdb w…",,,,,,Anecdotal,comment,,,,,,,,2022-09-26,github/ldemailly,https://github.com/kubernetes/kubernetes/issues/93476#issuecomment-1258433810,repo: kubernetes/kubernetes | issue: Allow scaling up to meet PDB constraints | keyword: workaround
"As per sig-node meeting today, thanks for a good discussion. With regard problem to have the ability to maintain CPUs during scaling - Try to find solution making local decision, but not ok to completely delegate to NRI, even lower layer. Reasoning this will create another layer of complexity to the system. - Try to redesign the cpu accumulator with this new requirement in mind. - Check https://github.com/kubernetes/enhancements/pull/4541 for inspiration - Try to find a solution with checkpoint…",,,,,,Anecdotal,comment,,,,,,,,2025-03-11,github/esotsal,https://github.com/kubernetes/kubernetes/pull/129719#issuecomment-2715258098,repo: kubernetes/kubernetes | issue: [WIP][FG:InPlacePodVerticalScaling] Fix Static CPU management policy alongside InPlacePodVerticalScaling | keyword: workaround
"Has there been an issue opened for this problem? The one explained after this specific issue has been closed: > This issue is also cause by changing the secret type. For example, if you have an env var in the deployment as a key/value, then you change the same env var but try to read from a kubernetes secret. Because there have been two workarounds given in the following comments but none of them seems to be the right way this should be working.",,,,,,Anecdotal,comment,,,,,,,,2020-04-22,github/Thematrixme,https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-617838488,"repo: kubernetes/kubernetes | issue: ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty | keyword: workaround"
"I had the same problem. Reason: In my old deployment, I defined some Environment vars literally. But in the new deployment, I was defining them in the secret. Solution: I edited current deployment using `kubectl edit <DEPLOYMENT>` and removed the literally defined vars. Then ran `kubectl apply -f <NEW DEPLOYMENT YML FILE>` Have fun.",,,,,,Anecdotal,comment,,,,,,,,2020-06-12,github/MortezaHosseini,https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-643101281,"repo: kubernetes/kubernetes | issue: ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty | keyword: workaround"
"For me, removing the dash (-) before the ""image"" word solved the problem... Yay! I think it was because I created with kubectl create and tried to update with kubectl apply, as mentioned above. So, I guess those two have a little different data structure...",,,,,,Anecdotal,comment,,,,,,,,2020-10-05,github/immortalize,https://github.com/kubernetes/kubernetes/issues/46861#issuecomment-703858205,"repo: kubernetes/kubernetes | issue: ENV invalid: spec.template.spec.containers[0].env[14].valueFrom: Invalid value: """": may not be specified when `value` is not empty | keyword: workaround"
"I found the issue here in our environment. We had blocked ICMP packets in the SG attached to the ENI of our API ELB (CLB these days). This meant that requests to fragment large packets were not getting back to the ELB. Because of that, the packet was never re-sent by the ELB, which meant it was lost. This breaks the TCP session and the connection resets. In short, make sure that ICMP is allowed between your Load Balancer and your hosts, and your MTU settings are correctly calibrated.",,,,,,Anecdotal,comment,,,,,,,,2018-06-01,github/integrii,https://github.com/kubernetes/kubernetes/issues/60140#issuecomment-393751563,repo: kubernetes/kubernetes | issue: kubectl cp fails on large files  | keyword: workaround
"Hi: I generally find this issue arises from network / service mesh inadequacy. The network is usually the last place people look - we just assume it's super fast. I've tried a myriad of configurations across many providers, local clusters etc., and this issue is common across all setups. The only common denominator in all these setups is the underlying network. Now, when I set up a K8 provider service like DO or Google Cloud I never run into this issue (or it's extremely rare). They obviously h…",,,,,,Anecdotal,comment,,,,,,,,2021-04-30,github/gridworkz,https://github.com/kubernetes/kubernetes/issues/61117#issuecomment-830231248,repo: kubernetes/kubernetes | issue: PLEG is not healthy | keyword: workaround
"> I can run on linux-5.6.10 without problems, but on linux-5.6 and linux-5.6.4 I get errors: @uablrek For v1.29.0, the minimal kernel version is linux-5.6.9 according to previous investigation: https://github.com/kubernetes/kubernetes/pull/122296#issuecomment-1864347600 > Yes, I managed to run it with 5.6 but only after 5.6.9. It's because nftables CLI ""Set NFT_SET_CONCAT flag for sets with concatenated ranges"" since [0.9.5](https://www.netfilter.org/projects/nftables/files/changes-nftables-0.9…",,,,,,Anecdotal,comment,,,,,,,,2024-01-04,github/tnqn,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1877130093,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"picking up `reject connections on invalid ports of service IPs`. Instead of directly linking the services chain to service port chain (using verdict maps), can we create a new chain(service-ports) which links services chain to service-ports chain if daddr matches any of {cluster|loadbalancer|external}IP. We can then add a rule to match on service port verdict map in this new chain and simply reject if nothing matches. ```go // create set of IPs (cluster + loadbalancer + external) add set ip kub…",,,,,,Anecdotal,comment,,,,,,,,2024-01-10,github/aroradaman,https://github.com/kubernetes/kubernetes/issues/122572#issuecomment-1884474235,repo: kubernetes/kubernetes | issue: nftables kube-proxy TODO | keyword: workaround
"@bgrant0607: Cascade deletion should work for both, right? If a new secret is created (instead of updated), then the old will be deleted when is not used. Or am I missing something? In any case, if @IanLewis is working on this, or just to get another opinion, is nice to have :)",,,,,,Anecdotal,comment,,,,,,,,2016-04-13,github/rata,https://github.com/kubernetes/kubernetes/issues/22368#issuecomment-209565217,repo: kubernetes/kubernetes | issue: Facilitate ConfigMap rollouts / management | keyword: gotcha
"@stevekuznetsov It does (ish), see this bullet point: > More difficult to solve the cross-object ordering problem (**you have to wait for object X to hit at least objectVersion Y, instead of waiting to see any object with RV > Z**)",,,,,,Anecdotal,comment,,,,,,,,2022-09-22,github/lavalamp,https://github.com/kubernetes/kubernetes/issues/112684#issuecomment-1255537283,repo: kubernetes/kubernetes | issue: RV vs object-wide logical clock | keyword: gotcha
"I opened this as a separate PR because I have been asked several times for literal signals. Asking people to do something instead of signals, especially something kube-centric is a non-starter for people who have apps that need signals. We could ask them to bundle sidecars as bridges, but because we can not signal across containers, they have to bundle into their own containers. That is gross to max. On Fri, Apr 29, 2016 at 2:32 PM, Prashanth B notifications@github.com wrote: > We can just use …",,,,,,Anecdotal,comment,,,,,,,,2016-04-29,github/thockin,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-215897186,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"You're missing the point. There are other things people want to do that involve sending a signal and ""create a config map, modify your container to include bash and pkill, modify your pod to mount the config map, write a script that runs your process and then watches the configmap and signals your process, and then run that instead of your real app"" is a crappy solution. It's a kludgey workaround for lack of a real feature. On Fri, Apr 29, 2016 at 3:27 PM, Prashanth B notifications@github.com w…",,,,,,Anecdotal,comment,,,,,,,,2016-04-30,github/thockin,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-215940195,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"> write a script that runs your process and then watches the configmap and signals > your process, and then run that instead of your real app"" That's not part of what i want. I explicitly _don't_ want a long running process aware of the config map. > create a config map, modify your container to > include bash and pkill, modify your pod to mount the config map, I think this is trivial. Not bash, sh or ash. Or http. The same things you need for a probe, or a post start hook, or pre stop hook. co…",,,,,,Anecdotal,comment,,,,,,,,2016-04-30,github/bprashanth,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-215940524,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
And I'm trying not to end up with 2 overlapping concepts when you can achieve one with the other. Somehow I dont think people will be against a probe like thing instead of a straight signal.,,,,,,Anecdotal,comment,,,,,,,,2016-04-30,github/bprashanth,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-215940594,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"It feels like a hack to me. I'd be fine with notifiers, and with notifiers attached to configmap changes, and with notifiers that included ""send a signal"". None of that obviates the utility of ""i want to signal my pods"" On Fri, Apr 29, 2016 at 10:55 PM, Prashanth B notifications@github.com wrote: > And I'm trying not to end up with 2 overlapping concepts when you can > achieve one with the other. Somehow I dont think people will be against a > probe like thing instead of a straight signal. > > …",,,,,,Anecdotal,comment,,,,,,,,2016-04-30,github/thockin,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-215940810,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"I also view this as notification. I would prefer to expose an intentional API instead of something in terms of POSIX signals. I would take two actions to start with: 1. `bump` - signal the container somehow that a reload should happen of any config 2. `restart` - what it says on the tin The problems with an intentional API are: 1. The ways of carrying out intent are not going to be universal across platforms; we need to define them for each platform we support; we can say that on Linux, `bump` …",,,,,,Anecdotal,comment,,,,,,,,2016-05-24,github/pmorie,https://github.com/kubernetes/kubernetes/issues/24957#issuecomment-221173305,repo: kubernetes/kubernetes | issue: Feature request: A way to signal pods | keyword: gotcha
"I appreciate that wazero has no external dependencies, but it is still on the order of 50k LOC. Since we may run untrusted code, I would expect us to carefully audit any runtime we use. This is non-trivial, to say the least. Also, Go's lack of WASI support makes it impossible to use most of the k8s library code because tinygo cannot run it (and using an alternative compiler is not a valid approach in many envs). https://github.com/prep/wasmexec is an interesting workaround for that, but such an…",,,,,,Anecdotal,comment,,,,,,,,2022-10-05,github/enj,https://github.com/kubernetes/kubernetes/issues/112851#issuecomment-1267756458,repo: kubernetes/kubernetes | issue: Scheduler: introduce WASM runtime to scheduler and load scheduler plugins via WASM runtime | keyword: gotcha
> Also bumping netlink ([f8c7f3a](https://github.com/kubernetes/kubernetes/commit/f8c7f3a6a67f408b49a79db9b7784aac6ddfce4b)) should be the last commit of the PR instead of first. Done!,,,,,,Anecdotal,comment,,,,,,,,2025-02-07,github/adrianmoisey,https://github.com/kubernetes/kubernetes/pull/129704#issuecomment-2643103678,repo: kubernetes/kubernetes | issue: Add retries to some netlink calls | keyword: gotcha
"The race in https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-2062039161 does not affect the dynamicresource plugin because I am avoiding the root cause (storing a locally modified copy of the object in the assume cache). I believe the volumebinding plugin is affected, but perhaps it doesn't matter there. In the dynamicresource plugin, I am handling the local modification with the ""in-flight claim"" map instead of the assume cache. The other race which does affect the dynamicre…",,,,,,Anecdotal,comment,,,,,,,,2024-04-20,github/pohly,https://github.com/kubernetes/kubernetes/issues/118612#issuecomment-2067619788,repo: kubernetes/kubernetes | issue: DRA: integration with cluster autoscaler | keyword: gotcha
"> I am having the exact same issue. Which repos did you use for install this? kubelet=1.23.17 kubeadm=1.23.17 kubectl=1.23.17 I had this Nameserver limits exceeded with more than three DNS servers when I was using ubuntu 22.04, I used instead Ubuntu 18.04.6 LTS and all worked fine. However after installing containerd, when you need to set systemd cgroup driver, you need to create the directory and file as I found it didn't exist /etc/containerd/config.toml and set it to the content shared on [h…",,,,,,Anecdotal,comment,,,,,,,,2024-03-22,github/Omar-Bensalah,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-2016090391,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
"I also fix my issues, by using crictl commands. One of the reasons I got this messages, was because old, dead pods from other versions were there, running in a zombie state, or the like. To manually remove everything ( pods, pod images) by using crictl, helped a lot to keep the systems clean, before attempt a reinstall.",,,,,,Anecdotal,comment,,,,,,,,2024-04-09,github/bsdero,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-2046154559,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
"Hi everyone, I've had a similar problem with my configuration. I'm working with a React JS app that I built and then shared in the nginx html folder. I created a `yaml` file with a reverse proxy because I need to create a proxy pass to a backend. In this file, I've: ```json location /api { proxy_pass https://domain-backend.com:8443/directory/services; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_b…",,,,,,Anecdotal,comment,,,,,,,,2024-06-26,github/tomasmalio,https://github.com/kubernetes/kubernetes/issues/118461#issuecomment-2191029979,repo: kubernetes/kubernetes | issue: kube-proxy pods continuously CrashLoopBackOff | keyword: pro tip
"Can one of the admins verify that this patch is reasonable to test? If so, please reply ""ok to test"". (Note: ""add to whitelist"" is no longer supported. Please update configurations in [kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull](https://github.com/kubernetes/test-infra/tree/master/jenkins/job-configs/kubernetes-jenkins-pull) instead.) This message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry. Otherwise, if t…",,,,,,Anecdotal,comment,,,,,,,,2016-07-05,github/k8s-bot,https://github.com/kubernetes/kubernetes/pull/28509#issuecomment-230594429,repo: kubernetes/kubernetes | issue: Update HumanResourcePrinter signature w single PrintOptions param | keyword: pro tip
"Can one of the admins verify that this patch is reasonable to test? If so, please reply ""ok to test"". (Note: ""add to whitelist"" is no longer supported. Please update configurations in [kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull](https://github.com/kubernetes/test-infra/tree/master/jenkins/job-configs/kubernetes-jenkins-pull) instead.) This message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry. Otherwise, if t…",,,,,,Anecdotal,comment,,,,,,,,2016-07-05,github/k8s-bot,https://github.com/kubernetes/kubernetes/pull/28509#issuecomment-230594585,repo: kubernetes/kubernetes | issue: Update HumanResourcePrinter signature w single PrintOptions param | keyword: pro tip
"Can one of the admins verify that this patch is reasonable to test? If so, please reply ""ok to test"". (Note: ""add to whitelist"" is no longer supported. Please update configurations in [kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull](https://github.com/kubernetes/test-infra/tree/master/jenkins/job-configs/kubernetes-jenkins-pull) instead.) This message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry. Otherwise, if t…",,,,,,Anecdotal,comment,,,,,,,,2016-07-05,github/k8s-bot,https://github.com/kubernetes/kubernetes/pull/28509#issuecomment-230594663,repo: kubernetes/kubernetes | issue: Update HumanResourcePrinter signature w single PrintOptions param | keyword: pro tip
"Can one of the admins verify that this patch is reasonable to test? If so, please reply ""ok to test"". (Note: ""add to whitelist"" is no longer supported. Please update configurations in [kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull](https://github.com/kubernetes/test-infra/tree/master/jenkins/job-configs/kubernetes-jenkins-pull) instead.) This message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry. Otherwise, if t…",,,,,,Anecdotal,comment,,,,,,,,2016-07-05,github/k8s-bot,https://github.com/kubernetes/kubernetes/pull/28509#issuecomment-230594762,repo: kubernetes/kubernetes | issue: Update HumanResourcePrinter signature w single PrintOptions param | keyword: pro tip
"Can one of the admins verify that this patch is reasonable to test? If so, please reply ""ok to test"". (Note: ""add to whitelist"" is no longer supported. Please update configurations in [kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull](https://github.com/kubernetes/test-infra/tree/master/jenkins/job-configs/kubernetes-jenkins-pull) instead.) This message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry. Otherwise, if t…",,,,,,Anecdotal,comment,,,,,,,,2016-07-05,github/k8s-bot,https://github.com/kubernetes/kubernetes/pull/28509#issuecomment-230594862,repo: kubernetes/kubernetes | issue: Update HumanResourcePrinter signature w single PrintOptions param | keyword: pro tip
"Can one of the admins verify that this patch is reasonable to test? If so, please reply ""ok to test"". (Note: ""add to whitelist"" is no longer supported. Please update configurations in [kubernetes/test-infra/jenkins/job-configs/kubernetes-jenkins-pull](https://github.com/kubernetes/test-infra/tree/master/jenkins/job-configs/kubernetes-jenkins-pull) instead.) This message may repeat a few times in short succession due to https://github.com/jenkinsci/ghprb-plugin/issues/292. Sorry. Otherwise, if t…",,,,,,Anecdotal,comment,,,,,,,,2016-07-07,github/k8s-bot,https://github.com/kubernetes/kubernetes/pull/28509#issuecomment-230994143,repo: kubernetes/kubernetes | issue: Update HumanResourcePrinter signature w single PrintOptions param | keyword: pro tip
"Use data-testid instead of aria-label for test selectors Grafana [currently recommends](https://github.com/grafana/grafana/blob/main/contribute/style-guides/e2e.md) using the aria-label attribute as a way to both describe elements in the UI and create selectors for using in unit and E2E tests. As the Grafana team has scaled, this is no longer a good recommendation because it ends up hampering accessibility by using _too many_ label that might not be that useful, or may be out of date. It also m…",,,,,,Anecdotal,issue,,,,,,,,2021-07-07,github/joshhunt,https://github.com/grafana/grafana/issues/36523,repo: grafana/grafana | keyword: best practice | state: open
"Workaround which I discovered: Discord is compatible with Slack Webhooks, so you can: 1. In Grafana, configure notification point for slack 2. Use the Discord webhook url and append `/slack` to it This will post a slack-style message to the discord webhook which it accepts. The slack-style message is formatted correctly. Best Regards Mydayyy",,,,,,Anecdotal,comment,,,,,,,,2024-10-08,github/Mydayyy,https://github.com/grafana/grafana/issues/86565#issuecomment-2400599992,repo: grafana/grafana | issue: Alerting: Discord alert message is above the title | keyword: workaround
This happens because message content is passed to content field in message. If Grafana would like to include it in embed instead it should be passed in description of the embed. Easiest solution to fix this problem temporarily is to disable it completely. Although I would prefer not using embed at all and instead use message content :smile: Above mentioned @Mydayyy solution wouldn't work when message includes Discord markdown. https://discord.com/developers/docs/resources/webhook#execute-webhook,,,,,,Anecdotal,comment,,,,,,,,2024-10-19,github/cwchristerw,https://github.com/grafana/grafana/issues/86565#issuecomment-2423648786,repo: grafana/grafana | issue: Alerting: Discord alert message is above the title | keyword: workaround
"> Workaround which I discovered: Discord is compatible with Slack Webhooks, so you can: > > 1. In Grafana, configure notification point for slack > > 2. Use the Discord webhook url and append `/slack` to it > > > This will post a slack-style message to the discord webhook which it accepts. The slack-style message is formatted correctly. > > Best Regards Mydayyy Thanks for this!",,,,,,Anecdotal,comment,,,,,,,,2025-01-23,github/erkston,https://github.com/grafana/grafana/issues/86565#issuecomment-2611087544,repo: grafana/grafana | issue: Alerting: Discord alert message is above the title | keyword: workaround
"Hi\! I'd like to work on this issue. From what I understand, the problem is that Discord alert messages appear above the title because the message content is being passed to the 'content' field instead of the embed's 'description' field. I'll investigate the Discord notification implementation and submit a PR to fix this.",,,,,,Anecdotal,comment,,,,,,,,2025-08-16,github/khj68,https://github.com/grafana/grafana/issues/86565#issuecomment-3193703861,repo: grafana/grafana | issue: Alerting: Discord alert message is above the title | keyword: workaround
Any chance this may be implemented? Or at least some sort of workaround. I am really tired of losing hours of works because my browser crashed and I forget to save the panel MANUALLY every 2-3 minutes.,,,,,,Anecdotal,comment,,,,,,,,2017-12-19,github/gusutabopb,https://github.com/grafana/grafana/issues/174#issuecomment-352663222,repo: grafana/grafana | issue: Auto-save option | keyword: workaround
The `Outer join` before the `Reduce` is a working workaround for us as well. For us we're using `Last*` instead of `mean`. Very weird.,,,,,,Anecdotal,comment,,,,,,,,2024-07-26,github/atsai1220,https://github.com/grafana/grafana/issues/44791#issuecomment-2253176909,repo: grafana/grafana | issue: Reduce tranformations shows incorrect values | keyword: workaround
"Transformation: Grouping to matrix header incorrect ### What happened? When using the ""Grouping to matrix"" transformation on table data, the name of the Cell value field appears in the header instead of the value referred by the Column name. ![Screenshot 2024-05-03 at 9 09 04 AM](https://github.com/grafana/grafana/assets/22201598/0c12475d-af6e-4de8-8c2a-7440dfaafeaa) ![Screenshot 2024-05-03 at 9 08 55 AM](https://github.com/grafana/grafana/assets/22201598/21c9fe0c-55ce-4724-b209-9d95a5e6136e) E…",,,,,,Anecdotal,issue,,,,,,,,2024-05-03,github/danialre,https://github.com/grafana/grafana/issues/87332,repo: grafana/grafana | keyword: workaround | state: open
"I found a workaround for the issue in my case, I could use the ""Fields with name"" override on the table, and add the override property for display name. In my case the ""name"" field was correct when looking in the debugger just the config.displayName was wrong, so the override could still select by name and change the display",,,,,,Anecdotal,comment,,,,,,,,2024-09-26,github/jlangy,https://github.com/grafana/grafana/issues/87332#issuecomment-2377906053,repo: grafana/grafana | issue: Transformation: Grouping to matrix header incorrect | keyword: workaround
"Hi, Poking around I may have found a working workaround (influxdb + grafana 11.3.1) The transformation ""transpose"" seems to fix the field name for some reason. So, inverting the column & row then applying a transpose fix the issue. Then you just need to change the type for the Time field, which seem to require two cast (number then time). ![Image](https://github.com/user-attachments/assets/1284b181-a3d2-4704-8739-2a15720df8e3)",,,,,,Anecdotal,comment,,,,,,,,2024-12-04,github/Stormshield-robinc,https://github.com/grafana/grafana/issues/87332#issuecomment-2516922882,repo: grafana/grafana | issue: Transformation: Grouping to matrix header incorrect | keyword: workaround
As far as I can see this is still a problem in Grafana.. Different alerts from different grafana instances using the same opsgenie API key indeeding opening only one Opsgenie ticket for all the alerts due to same alias.. Any solution? even workaround,,,,,,Anecdotal,comment,,,,,,,,2022-08-23,github/ZoharZrihen,https://github.com/grafana/grafana/issues/30080#issuecomment-1224058630,repo: grafana/grafana | issue: Allow a custom prefix for OpsGenie notifications | keyword: workaround
I see that this issue has been opened for quite a while now. Has anyone gotten to a solution/workaround? I just encountered the same issue on SLES Thumbleweed: `Problem: nothing provides 'freetype' needed by the to be installed grafana-10.3.1-1.x86_64`,,,,,,Anecdotal,comment,,,,,,,,2024-02-07,github/KaiserDMC,https://github.com/grafana/grafana/issues/53909#issuecomment-1932891605,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
To follow up my last comment (mostly because I was prodded by this Grafana youtube video - https://www.youtube.com/watch?v=2MWsu0xy5Xc If freetype is changed to libfreetype.so.6()(64bit) (I used rpmrebuild for this) the resulting rpm does install on sles. Also to @kminehart comment from last year it would be good if /sbin/service could be removed as that is legacy system v as well.,,,,,,Anecdotal,comment,,,,,,,,2024-06-05,github/jhansonhpe,https://github.com/grafana/grafana/issues/53909#issuecomment-2150751759,repo: grafana/grafana | issue: Cannot install grafana rpm in SLES | keyword: workaround
"Having the same issue here, ``` Error saving dashboard get client for kind: the server could not find the requested resource ```` I'm having to manually copy the JSON modal -> paste it into the git tree -> commit and only then it works. Not sure how to fix this. I have **GitSync** setup with `grafana/dashboards` folder, but I'm assuming it's trying to get the file info from `{root}/<folder_name>/<dash_name>` instead of `{grafana_root}/<folder_name>/<dash_name>`. I'm on `kube-prometheus-stack` c…",,,,,,Anecdotal,comment,,,,,,,,2025-06-27,github/CaffeineDuck,https://github.com/grafana/grafana/issues/105092#issuecomment-3011640497,"repo: grafana/grafana | issue: GitSync provisioning in v12: ""get client for kind: the server could not find the requested resource"" | keyword: workaround"
"> I have **GitSync** setup with `grafana/dashboards` folder, but I'm assuming it's trying to get the file info from `{root}/<folder_name>/<dash_name>` instead of `{grafana_root}/<folder_name>/<dash_name>`. I have it directly in the root and also have this issue",,,,,,Anecdotal,comment,,,,,,,,2025-06-27,github/ojsef39,https://github.com/grafana/grafana/issues/105092#issuecomment-3011950021,"repo: grafana/grafana | issue: GitSync provisioning in v12: ""get client for kind: the server could not find the requested resource"" | keyword: workaround"
"> > I have **GitSync** setup with `grafana/dashboards` folder, but I'm assuming it's trying to get the file info from `{root}/<folder_name>/<dash_name>` instead of `{grafana_root}/<folder_name>/<dash_name>`. > > I have it directly in the root and also have this issue Did you find any solution/ workaround? How are you navigating around it?",,,,,,Anecdotal,comment,,,,,,,,2025-06-27,github/CaffeineDuck,https://github.com/grafana/grafana/issues/105092#issuecomment-3012016112,"repo: grafana/grafana | issue: GitSync provisioning in v12: ""get client for kind: the server could not find the requested resource"" | keyword: workaround"
"Hey @chennin - this is a regression we slipped in from https://github.com/grafana/grafana/pull/102254. Sorry about that! This PR incorrectly put a new config section in the middle of the `[date_formats]` section, which 'moved' the rest of the date format values. We look for GF_ env vars based on the structure for the config files. Ther should be two workarounds until we land a fix: - use `GF_TIME_PICKER_INTERVAL_DAY` environment variable instead until we fix this - add a `config.ini` that conta…",,,,,,Anecdotal,comment,,,,,,,,2025-08-06,github/joshhunt,https://github.com/grafana/grafana/issues/108808#issuecomment-3160570136,repo: grafana/grafana | issue: Config: date formats no longer configurable via environment variables in 12.1.0 | keyword: workaround
"Prometheus: Fallback series endpoint when labels endpoint is not supported or implemented For some edge cases we have datasources which don't have support for `labels` endpoint support. Instead of that they have `series` endpoint support. See `labels` API - https://prometheus.io/docs/prometheus/latest/querying/api/#getting-label-names - https://prometheus.io/docs/prometheus/latest/querying/api/#querying-label-values So while requesting the resources like labels and label values, if the labels a…",,,,,,Anecdotal,issue,,,,,,,,2025-05-16,github/itsmylife,https://github.com/grafana/grafana/issues/105562,repo: grafana/grafana | keyword: workaround | state: closed
"**Issue Summary** We have identified compatibility challenges with accounts that utilize a specialized fork of Thanos, which presents limitations when interfacing with standard Prometheus implementations. The current workaround requires implementing custom modifications specifically tailored to accommodate this particular Thanos variant. This approach introduces additional complexity to our Prometheus data source integration. Wa need to prioritize maintaining universal solutions that can serve …",,,,,,Anecdotal,comment,,,,,,,,2025-08-04,github/k-munoz,https://github.com/grafana/grafana/issues/105562#issuecomment-3150752081,repo: grafana/grafana | issue: Prometheus: Fallback series endpoint when labels endpoint is not supported or implemented | keyword: workaround
"refactor(alerting): save state on a ticker async What is this PR about? This PR adds the possibility to save the state of alert instances periodically using a ticker instead of saving them on each evaluation. This will help us to scale beyond what is possible today, as Grafana servers with a lot of alert instances run into write amplification problems over time. What does this PR change? - A new feature flag `alertSaveStateAsync` is introduced which controls if the state is saved on evaluation …",,,,,,Anecdotal,issue,,,,,,,,2023-09-18,github/JohnnyQQQQ,https://github.com/grafana/grafana/pull/74998,repo: grafana/grafana | keyword: gotcha | state: closed
"> @hugohaggmark end-to-end tests can only be run if both backend and frontend have been built. So either we have to always build backend and frontend if there are backend or frontend changes or only run end-to-end test if there are backend and frontend changes. > > Third alternative would be to always build backend if backend or frontend changes and always run end-to-end if frontend changes. > > Thought? Gotcha, as far as I can remember when e2e has failed it's because of changes in FrontEnd (t…",,,,,,Anecdotal,comment,,,,,,,,2020-06-15,github/hugohaggmark,https://github.com/grafana/grafana/pull/25540#issuecomment-643895597,repo: grafana/grafana | issue: POC: Only run certain build steps for PR's if certain files have changed | keyword: gotcha
"There's going to be a much more comprehensive way to handle row/cell styling. It will clean up the code in a lot of good ways. Instead of this logic (targeting the parent on hover), we should use the work implemented here in react-data-grid: https://github.com/adazzle/react-data-grid/pull/3775",,,,,,Anecdotal,comment,,,,,,,,2025-05-16,github/alexjonspencer1,https://github.com/grafana/grafana/pull/104178#issuecomment-2886964772,repo: grafana/grafana | issue: TableNG: Handle all cell overflow using css | keyword: gotcha
"unified-storage: setup ring to shard requests This PR introduces an experimental feature to shard the requests to the index server based on the tenant. It makes use of the `ring` (and friends) utilities from `dskit` 1. Updates the `instrumentation_server` service to use `mux` instead of the builtin router, and have it store the router in the module server: this is so we can register the `/ring` endpoint to check the status of the ring 2. Create a new `Ring` service that depends on the instrumen…",,,,,,Anecdotal,issue,,,,,,,,2025-04-10,github/gassiss,https://github.com/grafana/grafana/pull/103783,repo: grafana/grafana | keyword: gotcha | state: closed
"Gauge/BarGauge: Added support for value mapping of ""no data""-state to text/value **What this PR does / why we need it**: This enables our users to map a ""no data""-state and display an value/text instead of the ""no data"" text. I tried to prevent breaking and old behaviour by checking for mappings of null. The only case where this might happen is if null already is mapped to another value then ""no data"". **Which issue(s) this PR fixes**: Fixes #20706 **Special notes for your reviewer**: I wanted …",,,,,,Anecdotal,issue,,,,,,,,2019-12-03,github/mckn,https://github.com/grafana/grafana/pull/20842,repo: grafana/grafana | keyword: pro tip | state: closed
Seems like we're lacking a specific documentation page for teams why adding a link to protip may be hard or we need to create such a page? The only reference right now is http://docs.grafana.org/guides/whats-new-in-v5/#teams Suggestion: ProTip: Assign folder and dashboard permissions to teams instead of users to ease administration.,,,,,,Anecdotal,comment,,,,,,,,2018-08-13,github/marefr,https://github.com/grafana/grafana/pull/12854#issuecomment-412518587,repo: grafana/grafana | issue: [wip]added empty list cta to team list | keyword: pro tip
"I'm an enterprise user and doing a for_each over the vault provider would reduce a lot of code if you could just do: ``` variable ""namespaces"" { type = set(string) default = [] description = ""Names to be created"" } resource ""vault_namespace"" ""namespace"" { for_each = var.namespaces path = each.key } provider ""vault"" { for_each = var.namespaces alias = each.key namespace = each.key } ``` instead of copy pasting the same piece for each namespace.",,,,,,Anecdotal,comment,,,,,,,,2020-09-16,github/rgevaert,https://github.com/hashicorp/terraform/issues/19932#issuecomment-693464489,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"Hey all 👋 - I found a ~~solution~~ workaround that works well enough to create multiple sets of S3 buckets (or any resource) across multiple regions using multiple [providers](https://www.terraform.io/docs/language/meta-arguments/module-providers.html) and [modules](https://www.terraform.io/docs/language/modules/index.html). It's not as clean as provider interpolation would be, but it **does** work. An example using two S3 buckets in _different_ regions: ``` provider ""aws"" { alias = ""use1"" regi…",,,,,,Anecdotal,comment,,,,,,,,2021-04-21,github/egeexyz,https://github.com/hashicorp/terraform/issues/19932#issuecomment-823748793,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: best practice
"Consider Meta-Argument `condition` or `include_if` so people stop using `count= 0 or 1 ` or `for_each empty or single item` And `content` which allows passing a data structure instead of DSL When choosing a Meta-Argument consider classes with existing resources. ``` locals { something = { prevent_destroy = true } } dynamic ""lifecycle"" { condition = can(var.ENVIRONMENT) # include_if = can(var.ENVIRONMENT) content = something # as long as the map is what the resource expects in this block. i.e. i…",,,,,,Anecdotal,comment,,,,,,,,2021-08-07,github/damon-atkins,https://github.com/hashicorp/terraform/issues/21953#issuecomment-894608054,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"@apparentlymart I think `enabled = <condition>` could work just like `count = <condition> ? 1 : 0` and people would have to use `one(aws_instance.foo[*])`. Basically, `enbaled` would be syntactic sugar and nothing more and thus fully backward-compatible. I've also seen people writing `for_each = <condition> ? [1] : []`, which could also be replaced with `enabled = <condition>`. Lastly, HCL could be extended with the comprehension-like `resource ... {} if <condition>` instead of making this an a…",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/nikolay,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1216071967,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"@damon-atkins you can already do what you describe with `one()`, no? ``` output ""web_server_ip_addresses"" { value = one(aws_instance.webserver[*].private_ip) } ``` will be null if the expression does not evaluate and will be the specified attribute of `item[0]` if there is a zeroth item. I've never sat down and tried to see if `one()` might be a suitable workaround for the problem that this current github issue attempts to address. It is certainly not a solution, but it might be a workaround.",,,,,,Anecdotal,comment,,,,,,,,2022-08-19,github/schollii,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1221169985,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: best practice
"prevent_destroy should let you succeed Call me crazy, but I'm willing to call the current implementation of `prevent_destroy` a bug. Here is why: The current implementation of this flag prevents you from using it for 1/2 the use case. The net result is more frustration when trying to get Terraform to succeed instead of destroying your resources.. `prevent_destroy` adds to the frustration more than alleviating it. `prevent_destroy` is for these two primary use cases, right? 1) you don't want thi…",,,,,,Anecdotal,issue,,,,,,,,2015-11-12,github/ketzacoatl,https://github.com/hashicorp/terraform/issues/3874,repo: hashicorp/terraform | keyword: best practice | state: open
"Would it be possible to get an additional flag when calling: terraform plan -destroy [ -keep-prevent-destroy ] I have the same problem, I have a few EIP associated with some instances. I want to be able to destroy every but keep the EIP for obvious reasons like whitelisting but I get the same kind of problem. I understand what destroy is all about, but in some cases it would be nice getting a warning saying this and that didn't get destroyed because of lifecycle.prevent_destroy = true. @ketzaco…",,,,,,Anecdotal,comment,,,,,,,,2015-11-16,github/mrfoobar1,https://github.com/hashicorp/terraform/issues/3874#issuecomment-157086868,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"Here's the use case we'd like this for: we have a module that we can use either for production (where some resources like Elastic IPs should not be accidentally deleted) or for running integration tests (where all resources should be destroyed afterwards). Because of #10730/#3116, we can't set these resources to be conditionally prevent_destroy, which would be the ideal solution. As a workaround, we'd be happy to have our integration test scripts run `terraform destroy --ignore-prevent-destroy`…",,,,,,Anecdotal,comment,,,,,,,,2017-01-19,github/glasser,https://github.com/hashicorp/terraform/issues/3874#issuecomment-273662886,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"Is there any work being done on this? It feels like this feature ""prevent_destroy"" is designed as ""annoy you because you put this flag in if you want to destroy resources"" rather than... destroy what I want to destroy except for the things I don't want to destroy, notated by the ""prevent_destroy"" flag. Use case 1 in the original post seems like a silly use case because it's only designed to alert you and error out. In reality, adding prevent_destroy on a resource actually seems to mean prevent …",,,,,,Anecdotal,comment,,,,,,,,2017-06-06,github/HighwayofLife,https://github.com/hashicorp/terraform/issues/3874#issuecomment-306638305,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: best practice
"While not strictly the same as #9448 I think they might be solved together. First, like @vivanov-dp said, thanks for adding the `for_each` support for modules. I had been expecting it for a long time. However I had not realized that provider configuration in modules was deprecated. Here is my use case: * I use terraform to manage the list of AWS accounts I have in my organization * When I create a new account (from a variable list), I then want to provision it with a few common standard resourc…",,,,,,Anecdotal,comment,,,,,,,,2020-09-04,github/gbataille,https://github.com/hashicorp/terraform/issues/24476#issuecomment-686968681,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"@nikolay What I understand is that you propose to have this: ``` locals { modules_vars = { instance_1 = { var1 = ... var2 = ... region = ... profile = ... role_arn = ... } instance_2 = { var1 = ... var2 = ... region = ... profile = ... role_arn = ... } } } module ""some_module"" { source = ""./some-module"" for_each = local.modules_vars provider ""aws"" { region = each.region profile = each.profile assume_role { role_arn = each.role_arn } } var1 = each.var1 var2 = each.var2 } ``` instead of: ``` prov…",,,,,,Anecdotal,comment,,,,,,,,2020-09-25,github/vivanov-dp,https://github.com/hashicorp/terraform/issues/24476#issuecomment-698825661,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: best practice
"I managed to get it working by using AWS profiles instead of the access keys directly. What I did though was not optimal; but in my build steps, I ran a bash script that called AWS configure that ultimately set the default access key and secret.",,,,,,Anecdotal,comment,,,,,,,,2017-04-11,github/darrensimio,https://github.com/hashicorp/terraform/issues/13022#issuecomment-293172268,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
I used workspaces to create my dev and prod environments. Now I need to store state for them in different aws accounts. What kind of workaround do you recommend? I just need to pass one variable to my backend config... somehow...,,,,,,Anecdotal,comment,,,,,,,,2018-11-28,github/laur1s,https://github.com/hashicorp/terraform/issues/13022#issuecomment-442489869,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"The best workaround for this I've ended up with takes advantage of the fact that whatever you pass into `init` gets rememebered by terraform. So instead of `terraform init`, I use a small wrapper script which grabs these variables from somewhere (like a .tf.json file used elsewhere, or an environment variable perhaps) and then does the call to `init` along with the correct `-backend-config` flags.",,,,,,Anecdotal,comment,,,,,,,,2018-11-28,github/glenjamin,https://github.com/hashicorp/terraform/issues/13022#issuecomment-442499201,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"@jantman I understand your comment and this is what we do on our side, we wrap execution of terraform with another layer of scripts. *But* it would be nicer if this could work OOTB so that we provide only environment vars and then execute`init`, `plan` etc. without thinking about the partial backend setup parameters from CLI. Thus, I still think that there is place for a nicer framework solution, although a workaround exists.",,,,,,Anecdotal,comment,,,,,,,,2018-12-11,github/milanaleksic,https://github.com/hashicorp/terraform/issues/13022#issuecomment-446316890,repo: hashicorp/terraform | issue: Using variables in terraform backend config block | keyword: best practice
"@apparentlymart What about creating the folder and instead of copying the files, on those OS+FS that allow it we just produce symlinks? Would that work ?? TBH, I'm not sure what kind of size we are talking about .. but AFAIK, if u follow best practices for both Terraform Modules & Git repositories..u shouldn't be on scenarios where the storage can become a problem?",,,,,,Anecdotal,comment,,,,,,,,2021-09-05,github/fblgit,https://github.com/hashicorp/terraform/issues/29503#issuecomment-913185996,repo: hashicorp/terraform | issue: Don't make redundant copies of the same module package | keyword: best practice
"Support provisioning using `docker exec` Instead of requiring an ssh connection to run a provisioner on a docker container, it would be nice to just do a `docker exec` so that we don't need to set up an ssh daemon on the container. (I know I know, I'm not supposed to run a provisioner on a docker image, configuration should be done at build time. But I'm trying to mirror my prod installation which isn't docker)",,,,,,Anecdotal,issue,,,,,,,,2016-01-15,github/clofresh,https://github.com/hashicorp/terraform/issues/4686,repo: hashicorp/terraform | keyword: best practice | state: open
"It also can help in automation when you want to run terraform init from operational directory, instead of going to each directory of the project.",,,,,,Anecdotal,comment,,,,,,,,2019-01-16,github/vasilij-icabbi,https://github.com/hashicorp/terraform/issues/18632#issuecomment-454983247,repo: hashicorp/terraform | issue: terraform init seems to not support basic terraform best practice | keyword: best practice
"Is possible to achieve something like this, similar code but able to run it independently for each region/env and dynamic backend setup using the variable `TF_DATA_DIR` to use `.terraform-REGION-ENV` instead of `.terraform` and additional backend setup running terraform init like ```sh TF_DATA_DIR=.terraform-$(MY_REGION) terraform init \ -backend-config=""region=$(MY_REGION)""` \ -backend-config=""bucket=$(BUCKET_NAME)"" \ . . . ```",,,,,,Anecdotal,comment,,,,,,,,2024-04-04,github/julian-alarcon,https://github.com/hashicorp/terraform/issues/18632#issuecomment-2036465697,repo: hashicorp/terraform | issue: terraform init seems to not support basic terraform best practice | keyword: best practice
"I'm using a destroy provisioner to uninstall an application on a remote device, and the only way to provide connection details currently is to hard-code them. The `triggers` workaround is not a proper workaround as it produces significantly different behavior. One solution would be to introduce a new meta-argument that has no effect for storing arbitrary data.",,,,,,Anecdotal,comment,,,,,,,,2020-01-28,github/xanderflood,https://github.com/hashicorp/terraform/issues/23679#issuecomment-579030392,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"Hi, Using triggers is not applicable for me for security reasons. On ressources creation, I need of a null ressource + local exec to build an object on remote server. On ressources destroy, I need to remove this object on the remote server. To connect to the remote server, I need of a token. Using triggers would store the token in the tfstate which is not a good practise here. So the workaround I imagine when I need to destroy the resources - terraform apply with a condition using a variable (d…",,,,,,Anecdotal,comment,,,,,,,,2020-03-25,github/mldmld68,https://github.com/hashicorp/terraform/issues/23679#issuecomment-603713100,repo: hashicorp/terraform | issue: Allow destroy-time provisioners to access variables | keyword: best practice
"Hi @kderck, What I was asking, I suppose, is whether it's truly necessary to publish that wrapper module as a separate module, rather than just putting the `module ""vpc""` and `module ""eks""` blocks directly in the root module. The module composition guide recommends keeping things ""flat"" specifically _because_ it avoids all of this extra boilerplate of declaring the union of all variables of the child modules you're wrapping. I'd typically expect a useful module to _encapsulate_ additional infor…",,,,,,Anecdotal,comment,,,,,,,,2022-08-19,github/apparentlymart,https://github.com/hashicorp/terraform/issues/31485#issuecomment-1221143323,repo: hashicorp/terraform | issue: [Feature] Module Variable and Output Inheritance (or simular) | keyword: best practice
"Moving secrets to vault, and using consul-template or integration with other custom solutions you have for CM certainly helps for a lot of cases, but completely avoiding secrets in TF or ending up in TF state is not always reasonable.",,,,,,Anecdotal,comment,,,,,,,,2015-10-30,github/ketzacoatl,https://github.com/hashicorp/terraform/issues/516#issuecomment-152605787,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
"@gtmtech a hash isn't cryptographically safe either because it can be reversed. The right solution here is something that can store the value securely, doing anything else IMO would be a waste of energy.",,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/johnrengelman,https://github.com/hashicorp/terraform/issues/516#issuecomment-173926752,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: best practice
Hi @barryib! Thanks for working on this. Not having _dynamic_ templates was an intentional designed decision for `templatefile` after seeing how much confusion and frustration the design of [the `template_file` data source](https://www.terraform.io/docs/providers/template/d/file.html) had caused: folks would frequently get the double-escaping wrong and then be unsure how to interpret the error messages that would result due to them being returned at the Terraform layer instead of at the templat…,,,,,,Anecdotal,comment,,,,,,,,2020-05-18,github/apparentlymart,https://github.com/hashicorp/terraform/pull/24978#issuecomment-630337484,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: best practice
Hi @apparentlymart Thanks for your answer. > Not having dynamic templates was an intentional designed decision for templatefile after seeing how much confusion and frustration the design of the template_file data source had caused: folks would frequently get the double-escaping wrong and then be unsure how to interpret the error messages that would result due to them being returned at the Terraform layer instead of at the template rendering layer. I'm know getting some explanation about why we …,,,,,,Anecdotal,comment,,,,,,,,2020-05-19,github/barryib,https://github.com/hashicorp/terraform/pull/24978#issuecomment-631062552,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: best practice
"Hi @jbigtani! Thanks for this feature request. In an earlier issue hashicorp/terraform#15419 I discussed the idea of making it so that changes detected during refresh would be explicit in the UI, for similar reasons as you've mentioned here. I included in a later comment a UI mockup of what such a thing might look like. Some details of it are now outdated because I made that mockup during the 0.12 development phase some time ago, but I think it still gives a general impression of our team's cur…",,,,,,Anecdotal,comment,,,,,,,,2020-09-03,github/apparentlymart,https://github.com/hashicorp/terraform/issues/26093#issuecomment-686763063,repo: hashicorp/terraform | issue: Ability to preview changes for Refresh Command | keyword: best practice
"Hey there, I don't actually work on Terraform anymore, but as I continue to clear out a random pile of old things I had previously participated in but were not closed out I _think_ today's Terraform has a feature sufficient to close this issue. Instead of running `terraform refresh`, you can run `terraform apply -refresh-only`. That activates one of Terraform's other [planning modes](https://developer.hashicorp.com/terraform/cli/commands/plan#planning-modes) -- ""Refresh-only"" mode -- which effe…",,,,,,Anecdotal,comment,,,,,,,,2025-01-28,github/apparentlymart,https://github.com/hashicorp/terraform/issues/26093#issuecomment-2617489040,repo: hashicorp/terraform | issue: Ability to preview changes for Refresh Command | keyword: best practice
"👍 +1 Dependency resolution is a giant pain, and what works today should work 6-12 months from now, short of a provider deprecating APIs. The module that successfully deployed the infrastructure should be the same version, same code, as when you have to update or apply it 6 months from now. We already commit our vendor dependencies with our Golang projects because of the inability to lock dependencies at a version (you can but it is a pain and if the upstream dev deletes the code, you're hosed u…",,,,,,Anecdotal,comment,,,,,,,,2018-05-22,github/ooglek,https://github.com/hashicorp/terraform/issues/4234#issuecomment-390854571,repo: hashicorp/terraform | issue: Fast-changing terraform modules - tracking module git commit? | keyword: best practice
"The forthcoming version 0.9 contains some reworking of Terraform's handling of states that will, amongst other things, make this easier to implement in a future release. I can't say exactly when that will be (I don't have visibility into the official roadmap) but the technical blockers on this will be much diminished once 0.9 is released. --- I suppose it's worth noting that the usage examples in my original proposal here are no longer valid with the changes in 0.9. Instead of configuring encry…",,,,,,Anecdotal,comment,,,,,,,,2017-02-06,github/apparentlymart,https://github.com/hashicorp/terraform/issues/9556#issuecomment-277847071,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"@FernandoMiguel consul or every other solution does not change the context. Tfstate is always in cleartext somewhere, and someone can access the file and so secrets inside (at least if you do not take all on a server in a private room detached from networks and always watched). Sops like logic instead allow you to save JSON file (and so potentially tfstate json too) only with values (all or some) encrypted using e.g. AWS KMS CMK. Such an approach increse security (and probably sufficient risk m…",,,,,,Anecdotal,comment,,,,,,,,2022-01-13,github/Roxyrob,https://github.com/hashicorp/terraform/issues/9556#issuecomment-1012271024,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: best practice
"Hi @esirK, For situations like that a typical strategy would be to add some sort of indirection. That means that instead of passing the step function ARN directly to the Lambda function, you'd instead pass some information that the Lambda function can use to find the step function dynamically at runtime. Of course, you will need to be able to tolerate there being a brief period at the start of the Lambda function's life when the step function doesn't exist yet. Another possibility would be to s…",,,,,,Anecdotal,comment,,,,,,,,2024-06-10,github/apparentlymart,https://github.com/hashicorp/terraform/issues/27188#issuecomment-2158640986,repo: hashicorp/terraform | issue: Feature: Allow circular dependencies in resources | keyword: lesson learned
A simplifed load balancer resource for AzureRM This commit adds a resource for the ARM load balancer. The ARM LB is quite complicated and has many different configuration options. Instead of trying to implement them all in a potential confusing user experience this resources exposes the most common uses. With it a load balancer can be configured on a single public IP or subnet which can route to a single backend pool according to one load balancing rule and 1 probe. This could be expanded to su…,,,,,,Anecdotal,issue,,,,,,,,2016-04-30,github/buzztroll,https://github.com/hashicorp/terraform/pull/6429,repo: hashicorp/terraform | keyword: lesson learned | state: closed
"Hi @lkysow! Unfortunately that is a backend-specific implementation coincidence rather than an intended feature: I expect it happens because these backends implement both ""create"" and ""update"" as a ""put"" action, and so if you bypass Terraform's existence check (which happens in `terraform workspace select`) then the new workspace is ""created"" just as a side-effect of writing its new state, not as an explicit action. At the moment the intent is that workspace creation is an explicit action since…",,,,,,Anecdotal,comment,,,,,,,,2018-03-22,github/apparentlymart,https://github.com/hashicorp/terraform/issues/15874#issuecomment-375447707,repo: hashicorp/terraform | issue: Document $TF_WORKSPACE | keyword: lesson learned
"Support use cases with conditional logic It's been important from the beginning that Terraform's configuration language is declarative, which has meant that the core team has intentionally avoided adding flow-control statements like conditionals and loops to the language. But in the real world, there are still plenty of perfectly reasonable scenarios that are difficult to express in the current version of Terraform without copious amounts of duplication because of the lack of conditionals. We'd…",,,,,,Anecdotal,issue,,,,,,,,2015-04-20,github/phinze,https://github.com/hashicorp/terraform/issues/1604,repo: hashicorp/terraform | keyword: lesson learned | state: closed
"This would be really useful, so I can destroy everything except the resources marked with `prevent_destroy`. At the moment, because of `prevent_destroy`, I comment out everything except that code and run apply instead of destroy. Very unintuitive.",,,,,,Anecdotal,comment,,,,,,,,2017-10-09,github/ColOfAbRiX,https://github.com/hashicorp/terraform/issues/2253#issuecomment-335145477,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
"+1 :) My current workaround is to taket the outout of ""terraform plan list"" , grep out all resource I wanna keep, and then create a list of -target parameters from the rest with a shell script. Another thing that would make it supereasy to destroy everything unless the things you want to keep is to destroy all resources instead of those protected by the ""prevent_destroy"" flag. Actually, in my opinion the behaviout for that flag is not ideal - if I call destroy, I want to destroy the configured …",,,,,,Anecdotal,comment,,,,,,,,2017-10-11,github/henning,https://github.com/hashicorp/terraform/issues/2253#issuecomment-335886155,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
"I found this thread because I was learning Terraform and thought I needed this capability. But I found a workaround that no longer required me to need `exclude`. Hopefully, it will help others that might be in the same boat as me and help them find their way around Terraform. If I restructured my configurations, I am able to use `data` instead of `resource`, which then allowed me to destroy all of my resources without triggering Terraform to destroy any resource that is managed elsewhere. So fo…",,,,,,Anecdotal,comment,,,,,,,,2018-11-26,github/zhao-li,https://github.com/hashicorp/terraform/issues/2253#issuecomment-441843773,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: workaround
"Hi @BarakHc! Thanks for reporting this. I guess this is the _provider_ registry equivalent of #28659, where we were discussing the same behavior for _module_ installation. Again, unfortunately this behavior is intended rather than a bug because from Terraform's perspective the registry is just a pointer to the location for the files, and the files themselves are treated as an entirely separate system. As in that other issue, I do see the benefit of having a way to treat the package files as if …",,,,,,Anecdotal,comment,,,,,,,,2021-08-16,github/apparentlymart,https://github.com/hashicorp/terraform/issues/29349#issuecomment-899881338,repo: hashicorp/terraform | issue: Allow private provider registries to self-host provider packages/checksums/signatures under the same credentials as the registry itself | keyword: workaround
"Thanks for reporting this, @dnozay. For most commands `-state=...` is [a legacy option for the local backend only](https://www.terraform.io/docs/language/settings/backends/local.html#command-line-arguments), but it seems like it intentionally has a different meaning for `terraform workspace new`, because that command is handling the option inline itself rather than passing it over to the backend as other commands do: https://github.com/hashicorp/terraform/blob/de105595e2788b5614081a295268cdb759…",,,,,,Anecdotal,comment,,,,,,,,2021-10-29,github/apparentlymart,https://github.com/hashicorp/terraform/issues/29819#issuecomment-955087649,repo: hashicorp/terraform | issue: Creating a new workspace with `terraform workspace new -state=tf.state.default` does not work for s3 remote state. | keyword: workaround
I came here because I couldn't see exactly what is going to happen and only turn to closed https://github.com/hashicorp/terraform/issues/28906. I don't see a reason for hiding things and not allowing people see exact plan.,,,,,,Anecdotal,comment,,,,,,,,2021-11-04,github/vojkny,https://github.com/hashicorp/terraform/issues/27547#issuecomment-960474318,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"Hi All, Do we have any alternative or workaround to get the full Verbose Plan? `terraform state show` for remote state or any other options? It is quite hard to justify as an issue bing open long time. 😒",,,,,,Anecdotal,comment,,,,,,,,2022-02-13,github/dduleep,https://github.com/hashicorp/terraform/issues/27547#issuecomment-1038019629,repo: hashicorp/terraform | issue: Feature Request:  Verbose option for plan | keyword: workaround
"Use xdg basedir spec on linux ### Terraform Version 0.9.8 https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html TLDR: Instead of ~/.terraform.d, configuration should be in the `$XDG_CONFIG_HOME/terraform/` and cache (safely-deletable files) should be in `$XDG_CACHE_HOME/terraform`. If not defined, `$XDG_CONFIG_HOME` should be defaulted to `$HOME/.config` and `$XDG_CACHE_HOME` should be defaulted to `$HOME/.cache`. I believe everything currently in .terraform.d is consider…",,,,,,Anecdotal,issue,,,,,,,,2017-06-23,github/jleclanche,https://github.com/hashicorp/terraform/issues/15389,repo: hashicorp/terraform | keyword: workaround | state: open
"I honestly find the basedirs more useful because of the reduced cognitive load when performing backup and/or synchronization. Instead of cherry-picking from `~/Library`, we just have the XDG directories.",,,,,,Anecdotal,comment,,,,,,,,2019-06-26,github/0az,https://github.com/hashicorp/terraform/issues/15389#issuecomment-506050093,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
"So instead of applications on MacOS conforming to the OS's official standards they should move to match XDG?... The only reason for cognitive load is because of people not following the OS's standards. If all Linux programs followed XDG, Windows programs go to `AppData` etc, Mac go to `~/Library` etc, the only cognitive load is knowing where the standard locations are. There is more cognitive load due to the mishmash caused by ignoring the OS's standards and having them go in various different …",,,,,,Anecdotal,comment,,,,,,,,2019-06-26,github/06kellyjac,https://github.com/hashicorp/terraform/issues/15389#issuecomment-506066764,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
"As per the spec, the alternate directories should be `~/.config/*` instead of `~/config/*`. In addition, I believe the spec mandates `$XDG_CONFIG_HOME/terraform/terraformrc` instead of `$XDG_CONFIG_HOME/terraform/.terraformrc` (note the lack of full stop in the previous). Even if it isn't mandated, all other spec implementations I'm aware of drop the ""dot"". I am strongly against `$XDG_*` taking lower priority, especially as other projects will gracefully fall back to the legacy locations.",,,,,,Anecdotal,comment,,,,,,,,2019-06-30,github/0az,https://github.com/hashicorp/terraform/issues/15389#issuecomment-507049496,repo: hashicorp/terraform | issue: Use xdg basedir spec on linux | keyword: workaround
"I'm an enterprise user and doing a for_each over the vault provider would reduce a lot of code if you could just do: ``` variable ""namespaces"" { type = set(string) default = [] description = ""Names to be created"" } resource ""vault_namespace"" ""namespace"" { for_each = var.namespaces path = each.key } provider ""vault"" { for_each = var.namespaces alias = each.key namespace = each.key } ``` instead of copy pasting the same piece for each namespace.",,,,,,Anecdotal,comment,,,,,,,,2020-09-16,github/rgevaert,https://github.com/hashicorp/terraform/issues/19932#issuecomment-693464489,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"Hey all 👋 - I found a ~~solution~~ workaround that works well enough to create multiple sets of S3 buckets (or any resource) across multiple regions using multiple [providers](https://www.terraform.io/docs/language/meta-arguments/module-providers.html) and [modules](https://www.terraform.io/docs/language/modules/index.html). It's not as clean as provider interpolation would be, but it **does** work. An example using two S3 buckets in _different_ regions: ``` provider ""aws"" { alias = ""use1"" regi…",,,,,,Anecdotal,comment,,,,,,,,2021-04-21,github/egeexyz,https://github.com/hashicorp/terraform/issues/19932#issuecomment-823748793,repo: hashicorp/terraform | issue: Instantiating Multiple Providers with a loop | keyword: workaround
"Consider Meta-Argument `condition` or `include_if` so people stop using `count= 0 or 1 ` or `for_each empty or single item` And `content` which allows passing a data structure instead of DSL When choosing a Meta-Argument consider classes with existing resources. ``` locals { something = { prevent_destroy = true } } dynamic ""lifecycle"" { condition = can(var.ENVIRONMENT) # include_if = can(var.ENVIRONMENT) content = something # as long as the map is what the resource expects in this block. i.e. i…",,,,,,Anecdotal,comment,,,,,,,,2021-08-07,github/damon-atkins,https://github.com/hashicorp/terraform/issues/21953#issuecomment-894608054,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"@apparentlymart I think `enabled = <condition>` could work just like `count = <condition> ? 1 : 0` and people would have to use `one(aws_instance.foo[*])`. Basically, `enbaled` would be syntactic sugar and nothing more and thus fully backward-compatible. I've also seen people writing `for_each = <condition> ? [1] : []`, which could also be replaced with `enabled = <condition>`. Lastly, HCL could be extended with the comprehension-like `resource ... {} if <condition>` instead of making this an a…",,,,,,Anecdotal,comment,,,,,,,,2022-08-16,github/nikolay,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1216071967,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
"@damon-atkins you can already do what you describe with `one()`, no? ``` output ""web_server_ip_addresses"" { value = one(aws_instance.webserver[*].private_ip) } ``` will be null if the expression does not evaluate and will be the specified attribute of `item[0]` if there is a zeroth item. I've never sat down and tried to see if `one()` might be a suitable workaround for the problem that this current github issue attempts to address. It is certainly not a solution, but it might be a workaround.",,,,,,Anecdotal,comment,,,,,,,,2022-08-19,github/schollii,https://github.com/hashicorp/terraform/issues/21953#issuecomment-1221169985,repo: hashicorp/terraform | issue: `enabled` parameter to avoid logical statements in `count` | keyword: workaround
> on our project fixed by switching from terraform plugin cache dir to terragrunt cache as described here: [terragrunt.gruntwork.io/docs/features/provider-cache-server](https://terragrunt.gruntwork.io/docs/features/provider-cache-server/) I'm a long time terragrunt user anyway and tried out their provider caching as described in that link. I ended up turning it off again because it was slow and wonky. It also changes the terraform provider lock file to be exact versions rather than reflect what…,,,,,,Anecdotal,comment,,,,,,,,2025-01-19,github/grimm26,https://github.com/hashicorp/terraform/issues/31964#issuecomment-2600884749,repo: hashicorp/terraform | issue: Allow multiple Terraform instances to write to `plugin_cache_dir` concurrently | keyword: workaround
"I'm upgrading some ingresses from `kubernetes_ingress` to `kubernetes_ingress_v1`, which should be backward compatible. Because the resource types differ, I'm not able to use `state mv`. However because querying for v1 ingresses produces the old ingress, I always have a conflict where one gets added before the other is deleted, and the deletion seems to fail.",,,,,,Anecdotal,comment,,,,,,,,2022-07-14,github/arunv,https://github.com/hashicorp/terraform/issues/24704#issuecomment-1184981731,repo: hashicorp/terraform | issue: Terraform does not wait for the resource to be destroyed before creating it | keyword: workaround
"prevent_destroy should let you succeed Call me crazy, but I'm willing to call the current implementation of `prevent_destroy` a bug. Here is why: The current implementation of this flag prevents you from using it for 1/2 the use case. The net result is more frustration when trying to get Terraform to succeed instead of destroying your resources.. `prevent_destroy` adds to the frustration more than alleviating it. `prevent_destroy` is for these two primary use cases, right? 1) you don't want thi…",,,,,,Anecdotal,issue,,,,,,,,2015-11-12,github/ketzacoatl,https://github.com/hashicorp/terraform/issues/3874,repo: hashicorp/terraform | keyword: workaround | state: open
"Would it be possible to get an additional flag when calling: terraform plan -destroy [ -keep-prevent-destroy ] I have the same problem, I have a few EIP associated with some instances. I want to be able to destroy every but keep the EIP for obvious reasons like whitelisting but I get the same kind of problem. I understand what destroy is all about, but in some cases it would be nice getting a warning saying this and that didn't get destroyed because of lifecycle.prevent_destroy = true. @ketzaco…",,,,,,Anecdotal,comment,,,,,,,,2015-11-16,github/mrfoobar1,https://github.com/hashicorp/terraform/issues/3874#issuecomment-157086868,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"Here's the use case we'd like this for: we have a module that we can use either for production (where some resources like Elastic IPs should not be accidentally deleted) or for running integration tests (where all resources should be destroyed afterwards). Because of #10730/#3116, we can't set these resources to be conditionally prevent_destroy, which would be the ideal solution. As a workaround, we'd be happy to have our integration test scripts run `terraform destroy --ignore-prevent-destroy`…",,,,,,Anecdotal,comment,,,,,,,,2017-01-19,github/glasser,https://github.com/hashicorp/terraform/issues/3874#issuecomment-273662886,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"Is there any work being done on this? It feels like this feature ""prevent_destroy"" is designed as ""annoy you because you put this flag in if you want to destroy resources"" rather than... destroy what I want to destroy except for the things I don't want to destroy, notated by the ""prevent_destroy"" flag. Use case 1 in the original post seems like a silly use case because it's only designed to alert you and error out. In reality, adding prevent_destroy on a resource actually seems to mean prevent …",,,,,,Anecdotal,comment,,,,,,,,2017-06-06,github/HighwayofLife,https://github.com/hashicorp/terraform/issues/3874#issuecomment-306638305,repo: hashicorp/terraform | issue: prevent_destroy should let you succeed | keyword: workaround
"> Using reverse(list)[N-1] as a substitute for list[-N] is an anti-pattern as far as I'm concerned I agree, and would love if some author or contributor could have a look on it. I only wanted to provide a workaround for whoever stumbles into this thread and is looking for a possible solution",,,,,,Anecdotal,comment,,,,,,,,2021-07-07,github/txomon,https://github.com/hashicorp/terraform/issues/21793#issuecomment-875534495,repo: hashicorp/terraform | issue: Allow negative negative indices in slice() | keyword: workaround
"> I only wanted to provide a workaround for whoever stumbles into this thread and is looking for a possible solution The solution until then should be as suggested by the OP, since we've all seen how ""temporary workarounds"" almost always end up permanent in your code. The general pattern of OP, ie not just for replacing last item but other parts of string segmented by CHAR, is: ``` locals { split_var = split(CHAR, variable) sliced = slice(local.split_var, START, END) new_var = join(CHAR, expres…",,,,,,Anecdotal,comment,,,,,,,,2021-07-07,github/schollii,https://github.com/hashicorp/terraform/issues/21793#issuecomment-875632796,repo: hashicorp/terraform | issue: Allow negative negative indices in slice() | keyword: workaround
"FWIW, I don't expect that implementing this would require any change to my ""go-versions"" module. Terraform could already call [`MeetingConstraintsExact`](https://pkg.go.dev/github.com/apparentlymart/go-versions@v1.0.3/versions#MeetingConstraintsExact) instead of [`MeetingConstraints`](https://pkg.go.dev/github.com/apparentlymart/go-versions@v1.0.3/versions#MeetingConstraints) in any situation where it wants to accept selecting any prerelease that otherwise meets the constraints.",,,,,,Anecdotal,comment,,,,,,,,2025-05-05,github/apparentlymart,https://github.com/hashicorp/terraform/issues/36985#issuecomment-2852587893,repo: hashicorp/terraform | issue: allow optional support for matching prerelease versions in module constraints | keyword: workaround
"Hi @dr-yd, Thanks for filing the issue. I'm not sure what exactly has changed from that commit, mostly because I surprised that the use case you've shown previously worked. The [registry protocol](https://www.terraform.io/internals/module-registry-protocol) was defined such that a path returned in `X-Terraform-Get` from an http URL is considered a relative path to the first request. > The value of X-Terraform-Get may instead be a relative URL, indicated by beginning with `/`, `./` or `../`, in …",,,,,,Anecdotal,comment,,,,,,,,2022-07-08,github/jbardin,https://github.com/hashicorp/terraform/issues/31404#issuecomment-1178990550,repo: hashicorp/terraform | issue: Regression due to #31237: Local paths not valid return values any more | keyword: workaround
"@AlmogCohen the reason I wanted to look into DNS issues is that golang uses its own DNS resolver rather than the native one, and this has caused issues in Terraform (and many other projects) that can currently only be worked around with a cgo-enabled build. However, if you're not seeing it on a homebrew build then that's probably not the issue. Most of the Terraform Core engineering team is out until next week, so I'm just doing basic triage of urgent issues. As other folks run into this - plea…",,,,,,Anecdotal,comment,,,,,,,,2020-12-30,github/danieldreier,https://github.com/hashicorp/terraform/issues/27379#issuecomment-752720887,repo: hashicorp/terraform | issue: Terraform init is extremely slow | keyword: workaround
"> When using resource targeting, and building a graph of all dependencies, find all data dependencies. Instead of loading all data sources, only load those determined to be downstream dependencies of targeted resources. Since the proposal spelled out here is exactly what Terraform already does, I'm going to close the issue. If there is a case where data sources are being read which are not dependencies of the target, then that would need to be isolated and addressed outside of a general enhance…",,,,,,Anecdotal,comment,,,,,,,,2025-08-01,github/jbardin,https://github.com/hashicorp/terraform/issues/37388#issuecomment-3144627886,"repo: hashicorp/terraform | issue: When using resource targeting, don't fetch data sources if not needed for targeted resources | keyword: workaround"
"Make default value typed instead of `null`, in object-variable optional attributes ### Terraform Version ```shell Terraform v1.9.8 on linux_amd64 ``` ### Use Cases I want to easily manipulate an optional attribute with no default value, in my object-variable: ```hcl # main.tf # Define an object-variable, with an optional attribute of type `list(string)` that has no default value variable ""this"" { type = object({ list_attribute = optional(list(string)) # --> will get set to null }) default = {} …",,,,,,Anecdotal,issue,,,,,,,,2025-06-09,github/elouanKeryell-Even,https://github.com/hashicorp/terraform/issues/37223,repo: hashicorp/terraform | keyword: workaround | state: closed
"Moving secrets to vault, and using consul-template or integration with other custom solutions you have for CM certainly helps for a lot of cases, but completely avoiding secrets in TF or ending up in TF state is not always reasonable.",,,,,,Anecdotal,comment,,,,,,,,2015-10-30,github/ketzacoatl,https://github.com/hashicorp/terraform/issues/516#issuecomment-152605787,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
"@gtmtech a hash isn't cryptographically safe either because it can be reversed. The right solution here is something that can store the value securely, doing anything else IMO would be a waste of energy.",,,,,,Anecdotal,comment,,,,,,,,2016-01-22,github/johnrengelman,https://github.com/hashicorp/terraform/issues/516#issuecomment-173926752,repo: hashicorp/terraform | issue: Storing sensitive values in state files | keyword: workaround
Hi @barryib! Thanks for working on this. Not having _dynamic_ templates was an intentional designed decision for `templatefile` after seeing how much confusion and frustration the design of [the `template_file` data source](https://www.terraform.io/docs/providers/template/d/file.html) had caused: folks would frequently get the double-escaping wrong and then be unsure how to interpret the error messages that would result due to them being returned at the Terraform layer instead of at the templat…,,,,,,Anecdotal,comment,,,,,,,,2020-05-18,github/apparentlymart,https://github.com/hashicorp/terraform/pull/24978#issuecomment-630337484,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: workaround
Hi @apparentlymart Thanks for your answer. > Not having dynamic templates was an intentional designed decision for templatefile after seeing how much confusion and frustration the design of the template_file data source had caused: folks would frequently get the double-escaping wrong and then be unsure how to interpret the error messages that would result due to them being returned at the Terraform layer instead of at the template rendering layer. I'm know getting some explanation about why we …,,,,,,Anecdotal,comment,,,,,,,,2020-05-19,github/barryib,https://github.com/hashicorp/terraform/pull/24978#issuecomment-631062552,repo: hashicorp/terraform | issue: lang/funcs: add template function | keyword: workaround
"Optional keys in object variables are automatically null during evaluation instead of being errors when left undefined ### Terraform Version ```shell Terraform v1.10.3 on darwin_arm64 ``` ### Terraform Configuration Files ```terraform variable ""enable_auto_scaling"" { default = null type = bool } variable ""node_pool_node_count"" { default = null type = number } variable ""worker_nodepool_names"" { default = [ ""greenz1"", ""greenz2"", ""bluez1"", ""bluez2"" ] nullable = false type = set(string) } variable …",,,,,,Anecdotal,issue,,,,,,,,2025-05-16,github/tspearconquest,https://github.com/hashicorp/terraform/issues/37069,repo: hashicorp/terraform | keyword: workaround | state: closed
"This would be really useful, so I can destroy everything except the resources marked with `prevent_destroy`. At the moment, because of `prevent_destroy`, I comment out everything except that code and run apply instead of destroy. Very unintuitive.",,,,,,Anecdotal,comment,,,,,,,,2017-10-09,github/ColOfAbRiX,https://github.com/hashicorp/terraform/issues/2253#issuecomment-335145477,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
"+1 :) My current workaround is to taket the outout of ""terraform plan list"" , grep out all resource I wanna keep, and then create a list of -target parameters from the rest with a shell script. Another thing that would make it supereasy to destroy everything unless the things you want to keep is to destroy all resources instead of those protected by the ""prevent_destroy"" flag. Actually, in my opinion the behaviout for that flag is not ideal - if I call destroy, I want to destroy the configured …",,,,,,Anecdotal,comment,,,,,,,,2017-10-11,github/henning,https://github.com/hashicorp/terraform/issues/2253#issuecomment-335886155,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
"I found this thread because I was learning Terraform and thought I needed this capability. But I found a workaround that no longer required me to need `exclude`. Hopefully, it will help others that might be in the same boat as me and help them find their way around Terraform. If I restructured my configurations, I am able to use `data` instead of `resource`, which then allowed me to destroy all of my resources without triggering Terraform to destroy any resource that is managed elsewhere. So fo…",,,,,,Anecdotal,comment,,,,,,,,2018-11-26,github/zhao-li,https://github.com/hashicorp/terraform/issues/2253#issuecomment-441843773,repo: hashicorp/terraform | issue: feature request: inverse targeting / exclude | keyword: gotcha
"While not strictly the same as #9448 I think they might be solved together. First, like @vivanov-dp said, thanks for adding the `for_each` support for modules. I had been expecting it for a long time. However I had not realized that provider configuration in modules was deprecated. Here is my use case: * I use terraform to manage the list of AWS accounts I have in my organization * When I create a new account (from a variable list), I then want to provision it with a few common standard resourc…",,,,,,Anecdotal,comment,,,,,,,,2020-09-04,github/gbataille,https://github.com/hashicorp/terraform/issues/24476#issuecomment-686968681,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"@nikolay What I understand is that you propose to have this: ``` locals { modules_vars = { instance_1 = { var1 = ... var2 = ... region = ... profile = ... role_arn = ... } instance_2 = { var1 = ... var2 = ... region = ... profile = ... role_arn = ... } } } module ""some_module"" { source = ""./some-module"" for_each = local.modules_vars provider ""aws"" { region = each.region profile = each.profile assume_role { role_arn = each.role_arn } } var1 = each.var1 var2 = each.var2 } ``` instead of: ``` prov…",,,,,,Anecdotal,comment,,,,,,,,2020-09-25,github/vivanov-dp,https://github.com/hashicorp/terraform/issues/24476#issuecomment-698825661,repo: hashicorp/terraform | issue: Ability to pass providers to modules in for_each | keyword: gotcha
"wasted a some good hours today going through my whole configuration again and again, believing I did something fundamentally wrong when my config file was being written into the directory name rather than on the full path. I gave up occasionally and decided to search the web and found this issue... I am quite sad and depressed right now and my neck hurts because of the stress, specially because [in the docs](https://www.terraform.io/docs/provisioners/file.html#directory-uploads) it is actually …",,,,,,Anecdotal,comment,,,,,,,,2019-07-02,github/gchamon,https://github.com/hashicorp/terraform/issues/16330#issuecomment-507858885,repo: hashicorp/terraform | issue: file provisioner behaves surprisingly when target folder not present | keyword: gotcha
"so instead of safely creating the ~/.ssh dir, and putting a single ssh key's pub file into authorized_keys, the easiest workaround, is to copy my entire ~/.ssh dir? ``` resource ""null_resource"" ""controllerpi"" { connection { type = ""ssh"" user = var.initial_user password = var.initial_password host = ""10.10.10.129"" } provisioner ""file"" { #TODO: this is an aweful workaround to https://github.com/hashicorp/terraform/issues/16330 # source = ""~/.ssh/id_rsa.pub"" # destination = ""/home/pi/.ssh/authoriz…",,,,,,Anecdotal,comment,,,,,,,,2020-04-04,github/SvenDowideit,https://github.com/hashicorp/terraform/issues/16330#issuecomment-608970920,repo: hashicorp/terraform | issue: file provisioner behaves surprisingly when target folder not present | keyword: gotcha
"If it's currently printing the warning to stdout (rather than stderr) then I guess that situation is _effectively_ useless for the machine-readable use case this was intended for (easy interpolation into a shell script). Therefore I think it could be defensible to change the behavior in spite of the compatibility promises here, because the current behavior isn't useful and therefore is unlikely to be a real compatibility constraint. Our intent when implementing this was for it to be essentially…",,,,,,Anecdotal,comment,,,,,,,,2022-12-13,github/apparentlymart,https://github.com/hashicorp/terraform/issues/32384#issuecomment-1348932019,repo: hashicorp/terraform | issue: `terraform output` flag `-raw` should behave like `-json` when state file is empty | keyword: gotcha
"This is also a problem when you try to follow the advice on resource targeting: > Instead of using -target as a means to operate on isolated portions of very large configurations, prefer instead to break large configurations into several smaller configurations that can each be independently applied. [Data sources](https://developer.hashicorp.com/terraform/language/data-sources) can be used to access information about resources created in other configurations, allowing a complex system architect…",,,,,,Anecdotal,comment,,,,,,,,2023-01-13,github/hallvors,https://github.com/hashicorp/terraform/issues/32384#issuecomment-1382443381,repo: hashicorp/terraform | issue: `terraform output` flag `-raw` should behave like `-json` when state file is empty | keyword: gotcha
"@radeksimko gotcha, I don't think there is much that can be one from the particular distro side. So I hope the team maybe move on from grabing codename by running `$(lsb_release -cs)` and instead go for something that will work on more systems without breaking anything else My suggestion is, instead of using the existing: ```echo ""deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main""``` do:…",,,,,,Anecdotal,comment,,,,,,,,2025-02-13,github/OTonGitHub,https://github.com/hashicorp/terraform/issues/36466#issuecomment-2655566636,"repo: hashicorp/terraform | issue: Issue with linux installation on Linux Mint 22.1 ""Xia"" | keyword: gotcha"
"Request: Enable simple/native way to read .env files ### Current Terraform Version ``` Terraform v0.12.19 ``` ### Use-cases It's standard for code bases to have .env files, but I am not aware of any way to read their contents into terraform scripts. ### Attempted Solutions There is the option for `terraform.tfvars`, but this requires my code base to maintain two secret-variable files (and their templates). ### Proposal At minimum, instead of requiring that the `-var-file` flag take only files n…",,,,,,Anecdotal,issue,,,,,,,,2020-01-21,github/MagnusBrzenk,https://github.com/hashicorp/terraform/issues/23906,repo: hashicorp/terraform | keyword: gotcha | state: closed
"Sure. For example, when I make front-end applications involving static assets (html, css, js), I always have a `.env` file in order to, at minimum, build and deploy those assets to github pages. Rather than hardcoding my username and repo in the code, I pull them out into a .env file. Here is an example [repo](https://github.com/MagnusBrzenk/ng7-material-boilerplate) and its [`.env` file](https://github.com/MagnusBrzenk/ng7-material-boilerplate/blob/master/.env-template). And if my web app need…",,,,,,Anecdotal,comment,,,,,,,,2020-01-24,github/MagnusBrzenk,https://github.com/hashicorp/terraform/issues/23906#issuecomment-578307773,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"@cedricbastin are you sure that regex pattern you provided is correct? When I test it none of my env variables are picked up. And also when I test it using regex101.com nothing gets selected either. Your solution works when I use it with the simpler `(.*?)=(.*)` pattern as described in that SO question, but not with your more complex one. But I'm guessing your complex one covers more use cases so I am hoping to get it working.",,,,,,Anecdotal,comment,,,,,,,,2024-01-18,github/BenJackGill,https://github.com/hashicorp/terraform/issues/23906#issuecomment-1897871720,repo: hashicorp/terraform | issue: Request: Enable simple/native way to read .env files | keyword: gotcha
"I ended up writing a Makefile, you could use concourse as a better alternative. I think that if you need something done before or after the apply it is reasonable to use some other tooling. Would it make more sense to use vault for your secrets ? I am sure the vault provider will have proper dependancies. I do end up wondering what the actual use case for the local-exec is if we cant control when it runs.",,,,,,Anecdotal,comment,,,,,,,,2019-09-03,github/mutt13y,https://github.com/hashicorp/terraform/issues/22036#issuecomment-527365696,repo: hashicorp/terraform | issue: depends_on should should defer interpolation | keyword: gotcha
"Hi @zachwhaley! Thanks for sharing that use-case and partial workaround. I think the reason your workaround causes the configuration to be non-converging is because `base64sha256` means ""calculate a SHA256 hash of this string and then base64 encode it"", rather than ""decode this base64-encoded string and then generate a SHA256 hash of the result"". The result is therefore syntactically valid (it's a base64-encoded SHA256 hash) but it's not _semantically_ valid: it's a hash of the wrong source con…",,,,,,Anecdotal,comment,,,,,,,,2022-08-31,github/apparentlymart,https://github.com/hashicorp/terraform/issues/22036#issuecomment-1233324723,repo: hashicorp/terraform | issue: depends_on should should defer interpolation | keyword: gotcha
"The forthcoming version 0.9 contains some reworking of Terraform's handling of states that will, amongst other things, make this easier to implement in a future release. I can't say exactly when that will be (I don't have visibility into the official roadmap) but the technical blockers on this will be much diminished once 0.9 is released. --- I suppose it's worth noting that the usage examples in my original proposal here are no longer valid with the changes in 0.9. Instead of configuring encry…",,,,,,Anecdotal,comment,,,,,,,,2017-02-06,github/apparentlymart,https://github.com/hashicorp/terraform/issues/9556#issuecomment-277847071,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"@FernandoMiguel consul or every other solution does not change the context. Tfstate is always in cleartext somewhere, and someone can access the file and so secrets inside (at least if you do not take all on a server in a private room detached from networks and always watched). Sops like logic instead allow you to save JSON file (and so potentially tfstate json too) only with values (all or some) encrypted using e.g. AWS KMS CMK. Such an approach increse security (and probably sufficient risk m…",,,,,,Anecdotal,comment,,,,,,,,2022-01-13,github/Roxyrob,https://github.com/hashicorp/terraform/issues/9556#issuecomment-1012271024,repo: hashicorp/terraform | issue: Proposal: State Encryption | keyword: gotcha
"More info - if I replace all the locals with hardcoded strings (e.g. `""my-repo""` instead of `local.test_workload_1_repository_name`, it doesn't crash. Are locals not supported, or am I referencing them incorrectly?",,,,,,Anecdotal,comment,,,,,,,,2024-01-28,github/novekm,https://github.com/hashicorp/terraform/issues/34584#issuecomment-1913430431,repo: hashicorp/terraform | issue: `terraform test` Consistently Crashing | keyword: gotcha
"How about an option (either in configuration or as a command line argument) to use the local ssh binary instead of the native Go implementation? This is how Docker Machine solves the same problem (actually, Docker Machine will use the local ssh binary unless explicitly instructed to use crypto/ssh via the --native-ssh option). https://docs.docker.com/machine/reference/ssh/",,,,,,Anecdotal,comment,,,,,,,,2017-12-06,github/dangregorysony,https://github.com/hashicorp/terraform/issues/4523#issuecomment-349686963,repo: hashicorp/terraform | issue: Support SSH over HTTP Proxy | keyword: pro tip
